	sentence1	sentence2
0	How much of an effort would it be to use AWS instead of GCP for assignments?	No, you cannot.
1	How much of an effort would it be to use AWS instead of GCP for assignments?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
2	How much of an effort would it be to use AWS instead of GCP for assignments?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
3	How much of an effort would it be to use AWS instead of GCP for assignments?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
4	How much of an effort would it be to use AWS instead of GCP for assignments?	"Alexey
Probably more than you want to put in. I mean, if you have time, why not? But we will not be able to support you. I struggle to come up with an estimate. Does anyone here on this call have an estimate?
Victoria
I think the hardest part is – you shouldn't be a data engineer if you’re taking this course. You shouldn't have the knowledge that we're teaching. And if you don't have the knowledge, you're trying to learn it, and then everything is showing you something else. On top of that, you want to learn something new on your own that you won't have support for. That's going to be really hard. If you really want AWS because they use AWS at your work, they're going to help you with that one. It's probably not going to be worth it. It's going to be very, very stressful. I would add at least two hours on top of the normal hours."
5	Can you talk about linear regression and regularization?	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
6	Can you talk about linear regression and regularization?	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
7	Can you talk about linear regression and regularization?	"To be honest, I don't know what the typical requirements for computer vision jobs are. They can vary from one company to another. I will give you a general piece of advice that I would give in any other case. You need to do some research about what the company is doing and understand what their problems are. What kinds of problems are they solving? Then try to solve a similar problem. For example, if it's a company like where I work – online classifieds – you can go to Kaggle and look at what kind of computer vision problems companies in the online classifieds domain are putting out there. 
Or just go through the tech blog and see what kind of articles that this company is publishing. Then try to build the project around that. For example, let's take OLX, the company where I work. You can find an article here, Fighting fraud with Triplet Loss. In this article, we show how we use computer vision to find duplicates – how to find image duplicates – and then this article describes that. So if you do something similar in your project, you can just add it to your CV and then talk about this during your interview. That's much better than just a random project. It will certainly help to pique attention. I guess that's the most important factor, try to do some research, find what kind of problems the company is interested in solving and then solve these problems. Then use it to build your portfolio."
8	Can you talk about linear regression and regularization?	Yes, I can. There is actually an entire module about that. You probably mean something specific that you didn't understand. Maybe ask about that in Slack.
9	Can you talk about linear regression and regularization?	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
10	Can you please explain the Python Black setup in Visual Studio Code? Also, can you explain good Python coding standards as you write docstrings and type strings?	"Jeff
I can try. I like Black a lot. If you just Google “Python Black” and go to the GitHub readme, it’s become super popular in the last couple of years for automatically formatting your code. It is really nice. Mine is set up in Visual Studio Code, so when I hit “save,” it automatically formats. As it says, it’s an uncompromising code formatter. I think it says here about asking Henry Ford for color back in the day. Is that right? It's colored cars or color telephones from Alexander Graham Bell or somebody, and it's like, “They can have any color they want as long as it's black.” He wasn't going to compromise. You don't have to think about it now. It's easy. Just go right ahead and get your gear code formatted automatically. So it’s things like two lines after function or before – that kind of stuff just happens automatically with it. Getting it set up in VS Code can be a little tricky sometimes. But there are guides to that. Googling is what I do for that, usually.
Alexey
I just wanted to do a shameless plug, because we have another course called MLOps Zoomcamp. By the way, there is also a Prefect model there. One of the things there is best practices. In best practices, we have this video called Code quality: linting and formatting. It does not show how to integrate Black with VS Code, I think. I don't remember if we do this. But if you want to learn more about testing, and Black, and other things like pre-commit hooks, make files, and so on – you can check this out.
Jeff
Yeah, it's not too tricky to set up. It's like pip install a package in your environment and then you have to give it a path maybe or set one setting in VS code. If all goes well, cross your fingers, it should just work after you reload it. Not always the case. If you have trouble, a lot of people use it, so there are a lot of good resources online. Black is great. This is an awesome set of some resources here on best practices. Type hints are just getting more and more popular. They're very helpful, so that people know what kind – especially with autocomplete and little pop-up type – things like that in Visual Studio Code and other code editors. You can see what kind of argument type you should put in and then Prefect uses that information as well, in our flows, for example, to make sure that if it's a block in the UI or a parameter in the UI – it'll be smart. It'll be like, “Oh, is this a number? Okay.” It will give you options to put in numbers. “Is this a different kind of form field?” It'll have different options. It also can then do some validation to make sure that people actually put in something that conforms to that type-in. Python is slowly getting more and more smart about how it handles typing and newer versions keep adding more functionality. Type hints are nice to use. It takes a little bit of writing, but it makes your docstring shorter. The last thing that was asked about here was docstrings. It's great to have in every function to tell people what it's about. It's something that maybe you don't always do if you're in a hurry, but you should do it, especially if other people are going to read the code. Code is read like 20 times more often than it's written, or however you translate that – some stat. So do it. It's so helpful for you in the future and it's helpful for other people in the future, who are going to read your code to see, “What were you thinking? What is the purpose of this function?” Keep your function small, explain it in your doc string – it's good stuff. Then it shows up in your code editor, if you're lucky (if you have a good code editor). That's all to say about that.
Alexey
Do you know any resources where people can learn about setting up? Or learning more about these things, like good Python coding standards? What I showed is obviously a good resource, but it does not cover all these things that this question asks about.
Jeff
It's a good question. I do have a link to Google Style, or there are a couple different styles of docstrings. It seems like they're a little bit much these days, maybe. But there are links for different ways to do type hinting. I do have a few things if I look around for them. I don't have them at my fingertips right now. But Michael looks like maybe he's got one he shared there.
Michael
Yes, this one is a little bit older, but it is great. It goes into using virtual environments, Poetry… there's a lot to unpack, but I think that's still pretty much the standard best practice at the moment."
11	Can you please explain the Python Black setup in Visual Studio Code? Also, can you explain good Python coding standards as you write docstrings and type strings?	"Alexey
You can create a Python path variable in your .bashrc and it will be there permanently. Just in the same way as you do with PATH. I don't know why we do it like that. Maybe I just didn't mention that. But, of course, you can just put everything you want in .bashrc and this alliance (this code) will be executed every time you launch your virtual machine. This way you can execute these things automatically."
12	Can you please explain the Python Black setup in Visual Studio Code? Also, can you explain good Python coding standards as you write docstrings and type strings?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
13	Can you please explain the Python Black setup in Visual Studio Code? Also, can you explain good Python coding standards as you write docstrings and type strings?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
14	Can you please explain the Python Black setup in Visual Studio Code? Also, can you explain good Python coding standards as you write docstrings and type strings?	No, you cannot.
15	How many portfolio projects apart from the course are needed for getting a job?	"Tim
Yeah, we have a lot of users who use MLflow and BentoML together. Once you have a model that’s sort of your “finalized” model – you save it to your MLflow registry and then (it depends on what your CI/CD pipeline looks like) but BentoML has a command to import from an MLflow registry. You can tell which MLflow model is the one that you want to deploy. It's very similar to save_model, except instead of pulling from your local environment, you're pulling from an MLflow registry. So you import the model, it automatically gets pulled into your Bento and then you can deploy it. On the BentoML documentation site, there's a big framework document on how to integrate with MLflow. 
Alexey
When searching, one of the suggestions was BentoML vs MLflow. I guess there are also some similarities because with MLflow, you can serve models. 
Tim
The thing about serving models is that it's a part of the pipeline that you just have to do. Every single framework out there has some way to serve a model – you train the model and then it just puts it up there to be able to serve. Typically, frameworks don't specialize in serving those. BentoML specializes in making the serving part really, really fast and really, really easy. What you find a lot of the time is, when you have serving logic in a really large end-to-end pipeline, it just puts the model there and then the only thing that you could submit to the model is the data. But we know in practice that there's usually business logic around this – pre-transformation logic, post-transformation logic, there's the version of the model that was saved. There are a lot of these other components that you want around the model, not just the model for inference. And of course there's the performance layer underneath. The BentoML architecture kind of brings that all together into one deployable, rather than just giving you one point to call inference for your model. 
Alexey
That is a comment in the live chat that says “There is PyCaret, too.” This is how you use PyCaret with Bento. 
Tim
Right. BentoML has lots and lots of integrations and we're only adding to them every day. The idea is to get the model from wherever you built it, bring it into your Bento, and then you can deploy it anywhere. Once it's in the Bento – in this standard packaging – it's nice because then we've got lots of different tools to deploy to Lambda, SageMaker, and lots of different places. And the places to deploy, we're adding to every week and month as well.
Alexey  
Since you can have a Docker container at the end – you do “bento_containerize” right? That's the command? And then you have a Docker container (Docker image) that you can deploy everywhere where you can deploy a Docker container, which is pretty much every place on the internet, right? 
Tim
Right. Some places, though, require specialized… 
Alexey
Like Lambda, right? 
Tim
Yeah, like Lambda won't accept just a normal Docker container. You have to actually create special endpoints. When we deploy to Lambda, we create those endpoints for you and make sure that it's behind an API gateway and things like that. For certain services, there are a lot of nuances that we kind of take care of for you – as long as you're in the Bento standard format.
Alexey 
For those who have no idea what MLflow is, and why we are talking about this – you don't need to worry about this right now. But at some point, this tool and other similar tools will be quite handy. You can check out our MLOps Zoomcamp course, after you finish this one. Don't try to do multiple courses at the same time. That could be too much. But of course, if you're curious, go check it out."
16	How many portfolio projects apart from the course are needed for getting a job?	"Again, you’ll probably hate me soon for saying this, but the answer is “it depends”. Maybe it's zero, maybe it's one, maybe it's two. You never know. You just need to start interviewing and in parallel to that, get projects done. Maybe you’ll get lucky and get hired from the first interview. Probably not, but you will already start learning what companies need. Then, at the same time, you try to implement this and you see, “Okay. This is what companies care about. Let me use some of the technologies they want to see that I’ve used.” You can look at the job descriptions to figure out what is important. You can talk to them when you have interviews and you can ask them, “Hey, what kind of technologies should I use for my portfolio projects to be a good fit for this position?” for example. 
It never hurts to ask. And keep doing this. Maybe you will get a job on the third project, maybe to be on project zero, when you haven't even started doing this. I think two or three should be enough, but yeah. When I got my first job in data science, I had been working back then as a freelancer already in data science. I had some projects from past clients that I showed. But in that interview for the job I got, most of the time was spent talking about my Master’s thesis, which was about processing Wikipedia data. I was working with mathematics there and the interviewer for that job was really interested in this. So we spent most of the time talking about that project. So maybe a good answer to this question would be to have one project that is relevant for the company you interview, and then you will just spend most of the time of the interview discussing this project."
17	How many portfolio projects apart from the course are needed for getting a job?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
18	How many portfolio projects apart from the course are needed for getting a job?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
19	How many portfolio projects apart from the course are needed for getting a job?	I guess not. That's the process of learning? Maybe more senior people don't… Yeah, I see where you're going with this. Senior people are not supposed to go bankrupt and default. They are probably supposed to pay back the debt. I guess these are the particularities of this dataset, I assume. Indeed, it's probably a data issue ,or not an issue, but a “characteristic” of this dataset.
20	Can you talk more about the final project? What should we be thinking about now to prepare us?	"Alexey
The first thing about the dataset – what kind of dataset do you want to use? Or what kind of problem do you want to solve? Once you figure this out, then you're basically ready to start working on a project. Then in the project, you need to decide if you want to do streaming or batch. For batch, it's using things like Prefect, Spark, or DBT. For streaming, it’s using the materials from the last lecture (week 2). Once you decide that, you will just implement this and you will find all the information you need here in the week 7 project repo. Just go through this and if you have any questions left, let us know. Keep in mind that these are the criteria that other people (your peers) will use when evaluating your project. Perhaps you can already think about that and how you want to implement your project in such a way that you maximize the score you get from these criteria."
21	Can you talk more about the final project? What should we be thinking about now to prepare us?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
22	Can you talk more about the final project? What should we be thinking about now to prepare us?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
23	Can you talk more about the final project? What should we be thinking about now to prepare us?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
24	Can you talk more about the final project? What should we be thinking about now to prepare us?	Well, what you can always do is create a new account. Don't forget to unlink your credit card if you want to use the same credit card. Unlink it from the billing from the old account, and then link it with a new one. Or if you have a different credit card, then you can just use that. For example, I use Revolut as a bank and in Revolut, you can create one-time virtual cards. That could be an option, too.
25	Is RMSE (vs MAPE or MAE, etc.) a common metric to use for regression models? Would it be a good metric for optimization in retail store planning forecasts?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
26	Is RMSE (vs MAPE or MAE, etc.) a common metric to use for regression models? Would it be a good metric for optimization in retail store planning forecasts?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
27	Is RMSE (vs MAPE or MAE, etc.) a common metric to use for regression models? Would it be a good metric for optimization in retail store planning forecasts?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
28	Is RMSE (vs MAPE or MAE, etc.) a common metric to use for regression models? Would it be a good metric for optimization in retail store planning forecasts?	"Alexey
It is quite important. I wouldn't say it's a must for all the projects. For example, if you use something like a tree-based model, then you shouldn't worry about scaling. I think I shared links to our Office Hours from the previous year. If you go to cohorts, and the previous year, and then you go to midterm project – in week ten, we show you how to use SciKit Learn pipelines. This is an amazing concept. So far, what we've been doing is Dictionary Vectorizer (dict_vectorizer) and then it was followed by a model. In the case of Bento, we needed to save this Dictionary Vectorizer separately, as an artifact (as a binary) and then we needed to load this and then we needed to apply the model. But with a pipeline, you can actually train a pipeline and just use BentoML to save the pipeline. It can be saved, right Tim? You can just take a pipeline and do BentoML.sklean.save_model and then the entire pipeline will be saved. Right? [Tim agrees] This is a cool thing. I don't have an example, but you can include this feature scaler here in your pipeline. There should be an example somewhere. I think we did that last year. There is an example without the pipeline. It should be here – classification, and then notebooks scaling this one. In this notebook, we show how to scale your features with standard scaling, or min/max scaler. If you check Office Hours #3 from last year I spoke more about that. It is an important topic. We intentionally did not include this because we cannot include everything, but this is an important topic. I recommend that you check it out. And using pipelines is cool because you will not need to save all this Scaler, Dictionary Vectorizer – all these things separately. Let's say with Bento, you can just take one pipeline, save it with Bento, and then you will have one binary on model. It makes things easier.
Tim 
And if you do have any kind of custom Tokenizer, or Scaler, or Vectorizer – you can always add it to that “custom objects” field.
Alexey
We talked about MLflow. It's very annoying, to be honest. With MLflow, I need to choose to save this in a file. Then I have a Pickle file and I tell MLflow, “Okay, there is an artifact that is already in this file that I want to upload together with the model.” So I upload it together with the model and then when I want to use it, I need to download this from the registry, then I need again to use Pickle to load this into the memory. There is too much overhead. With Bento, I really like how easy it is to save extra stuff.
Tim
Right. In our first version of BentoML, we were just saving models but then we had so many users saying “Well, how do I save my Tokenizer along with the model?” A lot of the time, the Tokenizer, the Vectorizer, is sort of one-to-one with the model. You may have a version of that as well. So it's nice to be able to version it with the model."
29	Is RMSE (vs MAPE or MAE, etc.) a common metric to use for regression models? Would it be a good metric for optimization in retail store planning forecasts?	RMSE and all these metrics are good. I am not an expert in that. I would suggest going to our YouTube channel, where we recently had a talk just a few weeks ago called Probabilistic Demand - Forecasting at Scale by Hagop Dippel. Check it out. He also talks about metrics there and you will see what exactly to use – what kind of metrics to use to evaluate your models for this specific case.
30	7 points for not posting on social media is harsh. I'm recovering from mental health issues and try to avoid social media if I can. Please reconsider.	"I'm very sorry to hear about your mental health problems and I want to remind you that posting in social media is not required. You don't have to do this. In fact, many of the students from the previous iteration did not post anything on social media and still were able graduate from the course with a certificate. Since they did the projects, they ended up quite high on the leaderboard, which allowed them to be on the page with top 100 names. If this is what you're after – if you want to end up on that page – just keep on working, don't post on social media, and you'll be fine. 
Don't worry about the points because, again, nobody knows which of the hashes is you (only you know) and these points are virtual. So don't… it's not required to post on social media. If you don't feel like doing this, then don't. But I think it will be valuable for you. Maybe, after some time, when it becomes easier for you, I do recommend taking a look at social media and posting there. Not right now, but later it will be very valuable for your career."
31	7 points for not posting on social media is harsh. I'm recovering from mental health issues and try to avoid social media if I can. Please reconsider.	"Yes or no. Accuracy can be used for multi-class classification – precision, recall and ROC curve, they're all for binary classification. But it's still possible to extend them for multi-class. First of all, there is a thing called one versus all classification. In this case, let's say we have three classes. [Image for reference] We have the green, blue and red classes. 
First, we actually see this problem as three binary classification problems and we train a separate model for each. For example, we can train a model for distinguishing green from the rest – maybe that will be our first logistic regression model. Then we can train a model for distinguishing the blue squares from the rest. And then we can have a third model for distinguishing the red crosses from the rest. This is called the “one vs all” approach. When we do this, we will end up with three binary models, and then we can compute precision, recall, F1 score, ROC, for each of these models separately and then we can merge them. We can take an average and see what the precision and recall are for this average. So that's one of the approaches. 
There are other approaches you can take. Let's say, ROC curve SciKit Learn, there is a way to extend it. [Image for reference] You see, for this roc_auc_score, they can actually do this for multi-class cases. There are different ways of how you can take average between these three classifiers. These micro/macro weighted samples – you can just read this description, this recommendation and see what the different ways of doing this are. I think this is a common approach. This is the one I just described – you compute them independently and then you kind of take the average."
32	7 points for not posting on social media is harsh. I'm recovering from mental health issues and try to avoid social media if I can. Please reconsider.	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
33	7 points for not posting on social media is harsh. I'm recovering from mental health issues and try to avoid social media if I can. Please reconsider.	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
34	7 points for not posting on social media is harsh. I'm recovering from mental health issues and try to avoid social media if I can. Please reconsider.	"Actually, I don't remember exactly how I do this. The right approach would be to use only train data and not use validation data or test dataset. This is done on purpose for exactly this reason that you mentioned. We want to model a situation in which there is some unseen data – it's truly unseen, we do not see that all – and anything can happen to this unseen data, including the situation that you described, such as there is a value that is only present in the test data. 
Imagine that there is a new iPhone – when you train your model, this new iPhone didn't exist (you didn't have this category) but you apply this model, and then, all of a sudden, there is a new iPhone. How will your model react to this? You need to somehow model this situation. This is totally normal. This can and will happen in practice. We just want to understand what would be the effect of that. That's why we need to see this in training data. This is good and this is how it should be. I hope that is a good answer."
35	With evaluating our model in validation, what to do if our validation model is high accuracy and validation on test is very low?	Okay. Yeah
36	With evaluating our model in validation, what to do if our validation model is high accuracy and validation on test is very low?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
37	With evaluating our model in validation, what to do if our validation model is high accuracy and validation on test is very low?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
38	With evaluating our model in validation, what to do if our validation model is high accuracy and validation on test is very low?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
39	With evaluating our model in validation, what to do if our validation model is high accuracy and validation on test is very low?	When you have a very good score on validation, but a very low score on the test, it means your model became lucky. Remember, we had this explanation at the very first module – we had a lesson about model selection. Sometimes, the model can be lucky and it doesn't necessarily translate well to the test. It happens. What can also happen is that sometimes, validation and test datasets are different. For example, if you speed split your dataset by time – so you have a dataset for one year, and then for training, you use everything from January till September, then validation is October, November, and test is December. So you evaluate your model in validation and October, November looks great – but then we know that in December, it's Christmas time and many models that you trained during the normal months are different. For them, this December could be a surprise. So maybe you have something like that in the test data. It's normal. For these cases, you just need to think about how exactly you can build a validation dataset and training dataset in such a way that they are similar. For example, if you have a lot of data, then maybe you can use the previous year for validation or for testing. Something like this. It's all problem-specific. Usually it's an example of overfitting.
40	I read a book that said the data engineering lifecycle is generation, storage, ingestion, transforming, and serving data. Do we do ingestion because we use an open dataset?	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
41	I read a book that said the data engineering lifecycle is generation, storage, ingestion, transforming, and serving data. Do we do ingestion because we use an open dataset?	"Alexey
Please check the FAQ. If you don't find the answer there, ask in Slack. If you get an answer in Slack, please put it into the FAQ."
42	I read a book that said the data engineering lifecycle is generation, storage, ingestion, transforming, and serving data. Do we do ingestion because we use an open dataset?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
43	I read a book that said the data engineering lifecycle is generation, storage, ingestion, transforming, and serving data. Do we do ingestion because we use an open dataset?	Yes, you will have to come up with your own individual projects. It's not easy, I know. But we'll help you. We will help you to come up with a project idea. That's actually on purpose because we want every project to be unique. Then it becomes a part of your portfolio when you look for a job.
44	I read a book that said the data engineering lifecycle is generation, storage, ingestion, transforming, and serving data. Do we do ingestion because we use an open dataset?	I don't understand the question, to be honest. We don't have the generation part. The generation part is what was done already for us by the New York Taxi Limousine Company. Storage – yeah, they also store it. They host the data. And yeah, we cover ingestion, transform, and serving with the dashboard.
45	If we consider professional certification for getting into the industry, which one would you recommend?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
46	If we consider professional certification for getting into the industry, which one would you recommend?	We’re still in progress (as of January 26, 2023). I thought we would do this earlier. But it's still not finished.
47	If we consider professional certification for getting into the industry, which one would you recommend?	I would not recommend any certificates. Just focus on projects.
48	If we consider professional certification for getting into the industry, which one would you recommend?	"Alexey
I don't know when this question was asked, but we did extend it."
49	If we consider professional certification for getting into the industry, which one would you recommend?	If it's a question for me, specifically – in Windows, I use a tool called Windows Terminal. This is the tool I use. In the settings, they have Git Bash, the usual command prompt, then they have multiple subsystems for Linux terminals. There also used to be PowerShell. I don't use PowerShell, so I don't have it here. This is what it looks like. I really like this. It’s just called Windows terminal. Check it out.
50	How can I know what the cutoff point is for a numerical variable? For example, when I use 0-2 months to compare churn with tenure? Was this trial error?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
51	How can I know what the cutoff point is for a numerical variable? For example, when I use 0-2 months to compare churn with tenure? Was this trial error?	I don't think I understand your question, to be honest. Maybe I'll try to rephrase it… Or ask in Slack, maybe?
52	How can I know what the cutoff point is for a numerical variable? For example, when I use 0-2 months to compare churn with tenure? Was this trial error?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
53	How can I know what the cutoff point is for a numerical variable? For example, when I use 0-2 months to compare churn with tenure? Was this trial error?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
54	How can I know what the cutoff point is for a numerical variable? For example, when I use 0-2 months to compare churn with tenure? Was this trial error?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
55	Will this course show me more than tools? I mean learning to do ETL, system design, etc.?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
56	Will this course show me more than tools? I mean learning to do ETL, system design, etc.?	No, you cannot.
57	Will this course show me more than tools? I mean learning to do ETL, system design, etc.?	This is a very practical course and because of the nature of the course, it is very tool-oriented. But, again, we do not just show the tools, but also explain the main concepts. These concepts are transferable between tools. For example, we cover Prefect, but when you start working, or when you’re at work, you encounter a different orchestrator like Dagter or Airflow or something else, you will already know the concepts. Therefore, it will not be too difficult for you. But yeah, the focus is more on tools rather than other things. We don't really cover system design here.
58	Will this course show me more than tools? I mean learning to do ETL, system design, etc.?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
59	Will this course show me more than tools? I mean learning to do ETL, system design, etc.?	Our thoughts are that this is indeed an exciting area. This is an in-demand skill and that's why we have a section about analytics engineering in this course.
60	What are the types of live calls we will have apart from Office Hours? And what are the durations?	"There are lots of different varieties of approaches here. One thing that you might do is use some kind of data ingestion tool, as I mentioned in the live FAQ discussion – something like Airbyte or Fivetran – if you have lots of data that you're ingesting, that can be a common way to stay on top of that. Or you could do it through a script, kind of like we do here. 
But what you might do is run your agent on a virtual environment in Google Cloud VM, for example, or in AWS, or somewhere else. You could run it in Kubernetes. There are a number of places where maybe you would have that running and have your infrastructure there, so that you were just pulling down the code not to your local machine, but it would just be in your VM. 
If you're actually going and using Pandas, you're putting the data into memory, so you do need to have some scripts that will do that. And you can go directly between Google Cloud Storage and BigQuery with the Prefect GCP library. So that Prefect GCP library that you installed, that you've been using, does have some options in there. I think it's in the BigQuery module to actually get data directly into BigQuery from GCS. So there are a lot of options there. Check out Prefect GCP to see how to do it."
61	What are the types of live calls we will have apart from Office Hours? And what are the durations?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
62	What are the types of live calls we will have apart from Office Hours? And what are the durations?	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
63	What are the types of live calls we will have apart from Office Hours? And what are the durations?	Just Office Hours. Like this particular one, not all of them will actually happen live. Sometimes I cannot be there, for example, so we do it asynchronously. But hopefully, you still get to ask your questions and I answer these questions. This is what we’re doing now.
64	What are the types of live calls we will have apart from Office Hours? And what are the durations?	Our thoughts are that this is indeed an exciting area. This is an in-demand skill and that's why we have a section about analytics engineering in this course.
65	What tools do you use to draw on your PC screen?	I guess, use validation.
66	What tools do you use to draw on your PC screen?	This question is so commonly asked that I think I should put it in the FAQ. I use a tool called Drawboard. I actually don't have it on this computer. This is a generic laptop. I don't have a touchscreen. But for Drawboard, you need to have a touchscreen. You actually need to have a pen. I have another tablet, which is Microsoft Surface tablet. This is also Windows and this is actually what I used for recording like 99% of the videos for this course. On this tablet, I have a pen. I just open Drawboard and I start drawing there. So it's a combination of tablet plus Drawboard.
67	What tools do you use to draw on your PC screen?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
68	What tools do you use to draw on your PC screen?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
69	What tools do you use to draw on your PC screen?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
70	How should I choose the number of convolutional layers or dense layers?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
71	How should I choose the number of convolutional layers or dense layers?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
72	How should I choose the number of convolutional layers or dense layers?	"That's like a pretty generic question, so the answer will also be generic. They will look at the skills you have, they will look at the match – how the skills you have match what is needed. I guess that's it. This is what recruiters usually look for in candidates. If you're interested, another thing you can check out is our podcasts. 
We have an interview, Recruiting Data Professionals with Alicja Notowska. Alicja is a recruiter. She was working as a recruiter at Zalando, which is a very large company in Germany and she shared some tips from your point of view of a recruiter and what she looks for when hiring people. So check it out. It’s quite insightful. You will probably find useful things there."
73	How should I choose the number of convolutional layers or dense layers?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
74	How should I choose the number of convolutional layers or dense layers?	Yeah. [chuckles] It depends. Use validation to find out what the best number of convolutional layers or dense layers is. There is no “correct” answer to these questions. It’s all case-dependent. For this one, it probably makes sense to use existing architectures – to take things like Inception or ResNet, or Efficient Net – all these models that somebody else trained and just use that. If you really want to experiment with this, use validation to find out what works best.
75	Can you share the machine learning curriculum with us as a guide?	I guess not. That's the process of learning? Maybe more senior people don't… Yeah, I see where you're going with this. Senior people are not supposed to go bankrupt and default. They are probably supposed to pay back the debt. I guess these are the particularities of this dataset, I assume. Indeed, it's probably a data issue ,or not an issue, but a “characteristic” of this dataset.
76	Can you share the machine learning curriculum with us as a guide?	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
77	Can you share the machine learning curriculum with us as a guide?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
78	Can you share the machine learning curriculum with us as a guide?	Yeah, of course. This is why we are all here, right? You just go to ML Zoomcamp and you follow that – that will be your curriculum. And I'm sharing it now with you. You're welcome.
79	Can you share the machine learning curriculum with us as a guide?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
80	I missed the last two homework assignments. Can I still catch up? Can I still get the certificate?	Yes. I will remind you that homework assignments are not required to get the certificate. You need to pass two projects out of three. If you do that, you will get a certificate. The important part for the project is to know how to deploy a model. So that’s what you will have to catch up on if you want to pass your project. If you don't catch up right now, you can spend the time we allocated for the midterm project towards doing this. Then you can work on the capstone 1 project and capstone 2 project and then submit these two projects and then pass the course and then get the certificate. You don't have to do everything in this course. For example, even if you don't learn TensorFlow and if you don't learn Kubernetes, you can still do two projects and then pass the course. But what you need to learn (what you have to learn) is the deployment sections – without them, you will not be able to finish the course. So chapter 5, definitely. 7 is also helpful – and 7 builds on 5. In my opinion, 7 is actually easier. Once you know the foundation from 5, taking 7 and then applying this is actually easier. You will need to do less work, if you use Bento compared to Flask. You can use Bento for your project, and I do suggest using Bento for the project. Then we will also have a chapter about serverless deployment and then Kubernetes. It's up to you whether you want to take them or not. I do recommend taking them, but use your own judgment to understand if you have the energy to actually do this. I know some people sign up for this course just to do the Kubernetes module, and there's nothing wrong with that. But if module five was difficult for you, maybe Kubernetes would be even more difficult. So maybe you can focus on something a bit simpler. Or… there is no harm in trying, right? I would still suggest trying Kubernetes. You will have quite some time for the Kubernetes homework. According to our schedule, it will actually be scheduled over the holidays, so you will have quite a lot of time to finish it. Let me check. I think for Kubernetes it was last year that it took a bit more time. We actually left the submission form open for quite some time, so the people who needed to catch up did this over the holidays. Don't worry, you will be able to catch up. But the important thing is to focus on the deployment modules. It will help you with the project.
81	I missed the last two homework assignments. Can I still catch up? Can I still get the certificate?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
82	I missed the last two homework assignments. Can I still catch up? Can I still get the certificate?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
83	I missed the last two homework assignments. Can I still catch up? Can I still get the certificate?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
84	I missed the last two homework assignments. Can I still catch up? Can I still get the certificate?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
85	How do you see the future roadmap of Prefect? Is it publicly available somewhere?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
86	How do you see the future roadmap of Prefect? Is it publicly available somewhere?	Everything you do here is individual. You don't form teams.
87	How do you see the future roadmap of Prefect? Is it publicly available somewhere?	"Jeff
That's a good question. We don't have a publicly available feature roadmap, probably just because we iterate really fast on things. Every week, you'll see there's about a new release. In this course, and I'll point this out here again for folks, we pinned everything. We pinned our versions in all the work to try to make sure that when people come here in a year, hopefully everything still works well. There’s a much higher probability of that. It’s a good idea to pin your versions a lot of times for your packages, but there's a lot of iteration taking place. We have new views in the last couple of weeks. We have other new features. So if you want to see what's happening and what's the latest, just don't pin it – just do an upgrade – just do a pip install of Su_prefect. Same thing with other packages. Then you’ll see what's happening. We do have release announcements and release notes every week when we release at least once a week."
88	How do you see the future roadmap of Prefect? Is it publicly available somewhere?	"Alexey
The best way to contribute is to take notes and include them to the notes section. And tell your friends about the course. As a part of your Learning in Public, spread the word. It will help us. And don't forget to give us a star if you have."
89	How do you see the future roadmap of Prefect? Is it publicly available somewhere?	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
90	Any recommended folder structure for large and complete ML projects (EDA, model selection, services, deploy, etc.)?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
91	Any recommended folder structure for large and complete ML projects (EDA, model selection, services, deploy, etc.)?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
92	Any recommended folder structure for large and complete ML projects (EDA, model selection, services, deploy, etc.)?	"Very, very often. This is probably the model that I use the most at my work. This is something I use pretty much for every project – for some projects. Not only is it a good first baseline, but also sometimes we just deploy this logistic regression, it works and then no further work is needed and we just leave it there. Sometimes these models are then improved with something like XGBoost or something, but logistic regression is like the workhorse of machine learning. It's used in many, many, many stations. 
Actually, in some situations, it's not really possible to use anything else except logistic regression. For example, at the previous company where I worked, which was an advertisement company, and there, it was very, very, very, very important to be able to make predictions very fast. Logistic regression is the best model for that. You cannot beat the speed of logistic regression with any other model. Maybe with a decision tree, but it will not be as good as logistic regression. Logistic regression works really well when you have a lot of features and this was the case in the company where I worked. It's really an important and useful model."
93	Any recommended folder structure for large and complete ML projects (EDA, model selection, services, deploy, etc.)?	I like a thing called Cookiecutter Data Science. There’s a good directory structure here. You can use it. In the projects I do, the structure is pretty similar. So do check this out. Also Kedro project structure they have a good structure. You can check it out too. Kedro is a library for creating machine learning pipelines. But I don't think there is a common standard. There are good standards like this Cookiecutter one, but I don't think there is a single one that everyone uses.
94	Any recommended folder structure for large and complete ML projects (EDA, model selection, services, deploy, etc.)?	I guess not. That's the process of learning? Maybe more senior people don't… Yeah, I see where you're going with this. Senior people are not supposed to go bankrupt and default. They are probably supposed to pay back the debt. I guess these are the particularities of this dataset, I assume. Indeed, it's probably a data issue ,or not an issue, but a “characteristic” of this dataset.
95	How can we thank you for teaching us or how can we help your project to develop further?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
96	How can we thank you for teaching us or how can we help your project to develop further?	Yeah. If it works for you, use it. I don't mind.
97	How can we thank you for teaching us or how can we help your project to develop further?	I guess not. That's the process of learning? Maybe more senior people don't… Yeah, I see where you're going with this. Senior people are not supposed to go bankrupt and default. They are probably supposed to pay back the debt. I guess these are the particularities of this dataset, I assume. Indeed, it's probably a data issue ,or not an issue, but a “characteristic” of this dataset.
98	How can we thank you for teaching us or how can we help your project to develop further?	"Well, you can just write thank you – that's already good. Also, I want to thank you for helping each other – that's also great. Please keep doing this. This is really awesome. Then there are also many other things you can help in the community. For example, I need some help with preparing for podcasts. Or… There are many things. Maybe if you're interested in volunteering for some things, please write in Slack. 
If you just want to sponsor in GitHub, there is also a way to support me with like $5 per month. If you go to my profile, there should be a link, Support me on GitHub. You can decide if you want to support me monthly, or just with one-time donations. So that's an option as well. 
Another thing – I don't know where you work and what your company is doing, but if you work at a company that might be interested in hiring interns, maybe you can help us get in touch. Last year, we had a great experience with a company called Delphi. We collaborated (partnered) with them and they took two graduates from the Machine Learning Zoomcamp as interns. They're really happy with the result. The students are also happy, of course. So if you can talk to your employer and suggest considering this, that would also be awesome. Or if you have some other ideas, please share them."
99	How can we thank you for teaching us or how can we help your project to develop further?	That's a very similar question to when we should use the lambda and when we should use Kubernetes. If you want to use Kubernetes, then go with TensorFlow Serving. Otherwise, TensorFlow Lite. There is one thing where you have to use TensorFlow Lite – where you cannot use TensorFlow Serving – and this is when you want to deploy your model on a mobile device. I have a phone here. If you want to create an app with a model, then you will have to use TensorFlow Lite because you will not be able to use TensorFlow Serving. TensorFlow Serving also adds a bit of complexity. You will need to have this gateway service, you will need to have GRPC service, so that becomes more complex. If you have a lot of load, then you need to do this otherwise, TensorFlow Lite.
100	What is your definition of a data scientist and what would you advise between that and ML engineer or data engineer (salary wise)?	"It all depends on what you mean by “full stack ML engineer”. As you’ve noticed, data engineer Zoomcamp is not about machine learning – it's about data engineering. If in your opinion a “full stack ML engineer” needs to know data engineering, then you should do data engineering. If not, then no. From my point of view, there is a “full stack of data scientist”. What I put in the definition of a full stack data scientist is – a person who can do all the steps in the CRISP-DM process. They can help product managers with business understanding, they can work with analysts in data understanding, they can help data engineers in the data preparation step, they can do the modeling, and they can deploy the models. If you want to be that kind of person, then, of course, you also need to learn a bit of product management and analytics, which we don't have courses for. But data engineering – preparing data before it goes to a model – will certainly be helpful. 
Also, machine learning ops Zoomcamp (MLOps Zoomcamp) will be helpful as well. I would suggest, if you really want to focus on machine learning engineering, then probably doing MLOps Zoomcamp will make more sense for you. But data engineering Zoomcamp will be quite useful in the future, because data scientists and ML engineers tend to work on data pipelines as well. 
Maybe not all the content will be useful for you. For example, the content about data warehousing – as a data scientist, I don't find it super useful in my work to me personally. Also, the module about analytics engineering – it's nice, but it's not something I do day to day. But the modules about batch, the modules about Spark, the modules about streaming – this is something that can be useful, because this is something we could use in building machine learning pipelines as well. 
To put data in a machine learning model, we need to prepare it – we need to create data pipelines – and this is pretty relevant. But you can just check it out. All the content is already there. You can just go and check it out. If you like it, do it – if you don't, don’t. I would suggest maybe going with this one (ML Zoomcamp)."
101	What is your definition of a data scientist and what would you advise between that and ML engineer or data engineer (salary wise)?	Salary wise, I don't think it matters. There are all types of salaries. Regarding the difference – there is an article in DataTalks.Club, which is called Roles in a Data Team that describes all the possible roles and what exactly people in the team do. There is the data scientist, data engineer, and machine learning engineer. You can go through this article and see the main differences. But maybe I can just quickly tell you. The data scientist is somebody who works on modeling. This is the first four modules, and then trees, and then deep learning. If you focus on that and you know a bit of deployment, then you're good to go for data science. A machine learning engineer focuses more on engineering. An ML engineer should know some machine learning, not to the same extent as a data scientist, but they should still know some machine learning. And they focus more on deployment. Usually, the way it happens is that both data scientists and machine learning engineers work together on training the model, and then they both work on deployment. But in the first case, the data scientist is kind of the expert on modeling, while the machine learning engineer is the expert on deployment. But typically, both of them should work together. Then, when it comes to a data engineer, this is a person who prepares the data for them – or maybe not for them, but together with them. Let's say, without a data scientist in this process, it will take a lot more time. It's best when both data engineers and data scientists work on this together and then the data engineer explains what is needed for the machine learning project. So please go through this article and hopefully, it will answer all your questions.
102	What is your definition of a data scientist and what would you advise between that and ML engineer or data engineer (salary wise)?	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
103	What is your definition of a data scientist and what would you advise between that and ML engineer or data engineer (salary wise)?	"Actually, I don't remember exactly how I do this. The right approach would be to use only train data and not use validation data or test dataset. This is done on purpose for exactly this reason that you mentioned. We want to model a situation in which there is some unseen data – it's truly unseen, we do not see that all – and anything can happen to this unseen data, including the situation that you described, such as there is a value that is only present in the test data. 
Imagine that there is a new iPhone – when you train your model, this new iPhone didn't exist (you didn't have this category) but you apply this model, and then, all of a sudden, there is a new iPhone. How will your model react to this? You need to somehow model this situation. This is totally normal. This can and will happen in practice. We just want to understand what would be the effect of that. That's why we need to see this in training data. This is good and this is how it should be. I hope that is a good answer."
104	What is your definition of a data scientist and what would you advise between that and ML engineer or data engineer (salary wise)?	I do sometimes. Not very often these days, but we have quite a few models that require deep learning. Usually, these models are related to images. We have some images on the platform, so when we need to understand what's happening on these images, we use deep learning.
105	Are the project criteria for the capstone projects 1 and 2 the same as the midterm project?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
106	Are the project criteria for the capstone projects 1 and 2 the same as the midterm project?	Yeah. If it works for you, use it. I don't mind.
107	Are the project criteria for the capstone projects 1 and 2 the same as the midterm project?	Correct, they are the same.
108	Are the project criteria for the capstone projects 1 and 2 the same as the midterm project?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
109	Are the project criteria for the capstone projects 1 and 2 the same as the midterm project?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
110	How and where can we start freelancing in machine learning after the courses. What are your recommendations after courses?	Well, I'm glad you asked, because we have a data engineering course. If you take it, you will understand a bit more – we do not go into details about how exactly it connects with this course. If you remember, when we talked about CRISP DM, one of the steps there is data preparation and this is what data engineering covers. Once the data is prepared, once the data is in a data lake or data warehouse, then we, data scientists, machine learning engineers, get the data, train the model, deploy this model, and take it to the other steps of the process.
111	How and where can we start freelancing in machine learning after the courses. What are your recommendations after courses?	"When I was freelancing, it was through a website that’s similar to Upwork. You can try that. Or Fiver or something like this. But, depending on where you live, you can just use LinkedIn for that and if somebody wants to invite you for an interview, you can ask them, “Hey, do you consider freelancers?” Some of the recruiters who reach out to you will say, “Yes, we do consider freelancers.” And then you can just start doing this. I don't think I can give you a better recommendation, because I'm not a freelancer myself. 
Maybe what you can do is go to DataTalks.Club site’s podcasts, where we have two podcast episodes about freelancing. The first one is Freelancing and Consulting with Data Engineering and then the other one is Freelancing in Machine Learning. In both cases, the guests talk about finding your first client, how they started freelancing, what they do. They know more about this than me."
112	How and where can we start freelancing in machine learning after the courses. What are your recommendations after courses?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
113	How and where can we start freelancing in machine learning after the courses. What are your recommendations after courses?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
114	How and where can we start freelancing in machine learning after the courses. What are your recommendations after courses?	Yeah. If it works for you, use it. I don't mind.
115	Is Julia the future of data science?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
116	Is Julia the future of data science?	Hmm. Probably not. I don't think so. Maybe it is, I don't know. Let's see in 10 years.
117	Is Julia the future of data science?	That's a very similar question to when we should use the lambda and when we should use Kubernetes. If you want to use Kubernetes, then go with TensorFlow Serving. Otherwise, TensorFlow Lite. There is one thing where you have to use TensorFlow Lite – where you cannot use TensorFlow Serving – and this is when you want to deploy your model on a mobile device. I have a phone here. If you want to create an app with a model, then you will have to use TensorFlow Lite because you will not be able to use TensorFlow Serving. TensorFlow Serving also adds a bit of complexity. You will need to have this gateway service, you will need to have GRPC service, so that becomes more complex. If you have a lot of load, then you need to do this otherwise, TensorFlow Lite.
118	Is Julia the future of data science?	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
119	Is Julia the future of data science?	Yeah. If it works for you, use it. I don't mind.
120	I see that there is a question about Bento – whether it can be integrated with AutoML tools like PyCaret and the like.	I wish Tim was here and would help me answer that because I don't know. I assume if you wanted to use Bento, you would probably use TensorFlow Lite – you don't want to use the usual TensorFlow for the same reasons that we talked about in the serverless module, because it's simply too large. Probably for Bento, you would use something like TensorFlow Lite and it will still work. Maybe another thing you can check – in our channel, we had a talk from one of the Bento folks, Building an ML Service Platform from the Ground Up. Here, Sean talks about the differences between TensorFlow Serving and Bento. With TensorFlow Serving, it's good – it's quite optimized for deep learning. There are some problems. First of all, it's written in C++ and expects your model in a certain format – this “saved model format”. Adding anything extra on top of that (like pre-processing, post-processing) becomes difficult, because you will need to have an extra step there. Bento makes it easier. But, again, I don't know. It's very difficult to give you a concrete recommendation without knowing what exactly you want to do. So it's case dependent. If you already use TensorFlow Serving in your company, then maybe it makes sense to stick to that. If we have Tim on Office Hours next week, maybe you can ask him that. He will be a better person to answer this question.
121	I see that there is a question about Bento – whether it can be integrated with AutoML tools like PyCaret and the like.	I don't have a “favorite” project, but please check out the demos from the students of the previous cohort. And please check the midterm project link that I shared. You can also find it in the midterm page of the course. And maybe just come up with your own favorite. You can also share it in Slack, actually. So maybe you can see, “Okay, I went through all these projects. I really like this one,” and you can just share it with others.
122	I see that there is a question about Bento – whether it can be integrated with AutoML tools like PyCaret and the like.	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
123	I see that there is a question about Bento – whether it can be integrated with AutoML tools like PyCaret and the like.	"Alexey
Actually, we didn't cover AutoML here in this course, so maybe a little bit of background. Tim, you probably can tell us a few words about what AutoML is for those who don't know.
Tim
AutoML is automatically training and generating a whole bunch of machine learning models and then seeing which one is the best. It's kind of brute force machine learning. 
Alexey
Here, for example, in module six, when we were talking about XGBoost – remember the approach we took? It's called manual tuning. We started with tuning the learning rate ETA, then we tuned the depth parameter, then we tuned to some other parameter (I don't remember which one). Then at the end, we arrived at the solution that is good. But with AutoML, you don't need to worry about this. So you give AutoML a data frame, or numpy.array – you give it some criteria, then you throw it at the AutoML, you go drink some tea, and then you come back five minutes or five hours later and then you have the perfect model. Right?
Tim
Yep. Yeah, BentoML doesn't really... We rely on the ML training frameworks to do the training part. Once you have the model itself, that's when you come in and you say “BentoML.save”. You can integrate it pretty easily with a lot of different training frameworks, as long as you can save the model at the end. MLflow is a tool that generates a lot of models and experiments and once you find the best one, essentially, you save that one with BentoML and that's the one that you deploy to your service.
Alexey
And speaking of that (short shameless plug). We have a course called MLOps Zoomcamp. In this course, AutoML wasn't really a topic, but we used it for illustrating some of the concepts from experiment tracking. In this module, we talked about experiment tracking. Here, we used Hyperopt, which is a library for finding the best XGBoost model. Then we got the best model and we saved it with MLflow."
124	I see that there is a question about Bento – whether it can be integrated with AutoML tools like PyCaret and the like.	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
125	Will we see any fundamental topics, like database design, schemas, etc.?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
126	Will we see any fundamental topics, like database design, schemas, etc.?	"Jeff
There are lots of different things there. This is something I used to teach folks doing data science-type work and it applies for data engineering, too. I don't have a good checklist at the tip of my fingers, but you do want to look at things like “Are your data in the right type? Do you have missing values, libraries, rate expectations (which we have integration for)?” That can help with trying to make sure your data looks how you think it'll look, in terms of the statistical properties of it. Hypothesis can also generate some ideas there, or some other Python library. Pandera for everything being how you expect, values matching what you expect. So there are a lot of different tools in Python that you can use there. It's a good question. It can be a lot of different things."
127	Will we see any fundamental topics, like database design, schemas, etc.?	"Alexey
I think Victoria covers a little bit of this, right? 
Victoria
Briefly, yes. Ankush, I believe there was an intro in week three. You start a little bit with data warehouse, and then what we do with DBT is use that schema and we talk a little bit about data modeling design. We talk specifically about the star schema and Kimball methodology.
Alexey
We wanted to make this course more practical. This is a more theoretical topic and I think there are good resources where you can pick this up. We did not aim to replace these resources. We wanted to give you something that is very hands on.
Victoria
I think it's still in the document with the questions. I wonder if we could do that again. We had prepared a list last year with some books and stuff like that. We could definitely share that as well to make sure you have it. 
Alexey
I think the list was Awesome Data Engineering, I will look it up. 
Victoria
Ah, yes! We have Awesome Data Engineering. It's bookmarked in the channel. There’s an amazing list of resources. We have a lot. 
Alexey
The idea there was to put it to GitHub and do something like Awesome Data Engineering GitHub repo. There are maybe 10 already. Maybe it should be a more unique name. I actually don't see Kimball and other stuff. 
Victoria
No, but I can add that part. 
Alexey
Please do. We can even have a separate section which we can call Database Design or something and put it there."
128	Will we see any fundamental topics, like database design, schemas, etc.?	No, you cannot.
129	Will we see any fundamental topics, like database design, schemas, etc.?	I would say to focus on projects, not on tools. In this course, you will get a chance to do this. If you open a job description for data engineers, you will see a lot, a lot, a lot of different technologies. Maybe think of a project that you can do for this particular company and do this project using, for example, the tools you learn here or the tools you want to learn, and build a portfolio. I guess this is more of a tip for getting a data engineering job, rather than for somebody who's starting their data engineering career. But if you already have a job, then maybe think of the business impact of what you need. I think I will not spend too much time talking about that because this requires a one hour long discussion. Perhaps we should have a podcast interview about that. If you can think of any potential guests for this podcast interview, you can let me know and we can try to invite these people and talk about this in more detail. One thing I want to share with you is that in our YouTube channel, we had a few interesting talks. We had this Getting a Data Engineering Job with Jeff talk. It’s a very nice presentation. Then we also had an interview with Jeff. So these two podcasts are quite interesting. You can also check out other ones. I think you'll find a lot of interesting content here.
130	Can you please explain the Prefect Radar in UI (the concentric circles with green lines) and how to interpret them?	No, you cannot.
131	Can you please explain the Prefect Radar in UI (the concentric circles with green lines) and how to interpret them?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
132	Can you please explain the Prefect Radar in UI (the concentric circles with green lines) and how to interpret them?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
133	Can you please explain the Prefect Radar in UI (the concentric circles with green lines) and how to interpret them?	"Alexey
I don't know when this question was asked, but we did extend it."
134	Can you please explain the Prefect Radar in UI (the concentric circles with green lines) and how to interpret them?	"Alexey
I'll try to Google it so we have an example. If I just put it into Google, then we should have illustrations. I personally find it not the easiest thing to interpret. [image 1]
Kalise 
We'll just kind of go over the high level. The intention of the Radar chart – as you can see, there's different rings of circles and it is because Prefect does not require a DAG anymore (Prefect 2). What that means is you can think of each of the dots as the nodes, or your tasks, or your sub-flows. Think of them as nodes and edges and how they're connected. As it expands out, it's going down, with the data dependencies, for example. A task could come back through the middle and call an inside ring that has already been called again. That is really why it’s circular. Now, like Jeff said, we also now have a timeline chart in Prefect Cloud as well, if you were to update your Prefect version. That was released as well. It's a timeline view.
Jeff
It's pretty new. It's pretty cool. We have a picture in the release and I put a picture in Slack today. It’s even more updated with the nodes and edges. Now it has edges too. It's linking from one to the other in time, so that's even spiffier than that now. But it kind of gives you more of what you might think of as a fairly DAG-like view for cases where it makes sense to do that. Yeah, it's a little tricky. We've had some good feedback today on this. It's not intuitive, necessarily, to understand the Radar View chart. It's not something that we're used to seeing as a DAG, because things can be more than just DAGs. We wanted more than one way to show that everywhere. We're working on having kind of multiple different views for folks – some are basically easily understandable, and some that are for more advanced use cases."
135	Will there be any guidance for interview preparation after the capstone project?	If you need it, we can organize a session for that. Why not? I think we can do something like that. We have some materials. We can probably see how to best organize that for the interview. But Google is your friend – I'm sure you can find a lot of information on Google.
136	Will there be any guidance for interview preparation after the capstone project?	"Actually, I don't remember exactly how I do this. The right approach would be to use only train data and not use validation data or test dataset. This is done on purpose for exactly this reason that you mentioned. We want to model a situation in which there is some unseen data – it's truly unseen, we do not see that all – and anything can happen to this unseen data, including the situation that you described, such as there is a value that is only present in the test data. 
Imagine that there is a new iPhone – when you train your model, this new iPhone didn't exist (you didn't have this category) but you apply this model, and then, all of a sudden, there is a new iPhone. How will your model react to this? You need to somehow model this situation. This is totally normal. This can and will happen in practice. We just want to understand what would be the effect of that. That's why we need to see this in training data. This is good and this is how it should be. I hope that is a good answer."
137	Will there be any guidance for interview preparation after the capstone project?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
138	Will there be any guidance for interview preparation after the capstone project?	"Yes or no. Accuracy can be used for multi-class classification – precision, recall and ROC curve, they're all for binary classification. But it's still possible to extend them for multi-class. First of all, there is a thing called one versus all classification. In this case, let's say we have three classes. [Image for reference] We have the green, blue and red classes. 
First, we actually see this problem as three binary classification problems and we train a separate model for each. For example, we can train a model for distinguishing green from the rest – maybe that will be our first logistic regression model. Then we can train a model for distinguishing the blue squares from the rest. And then we can have a third model for distinguishing the red crosses from the rest. This is called the “one vs all” approach. When we do this, we will end up with three binary models, and then we can compute precision, recall, F1 score, ROC, for each of these models separately and then we can merge them. We can take an average and see what the precision and recall are for this average. So that's one of the approaches. 
There are other approaches you can take. Let's say, ROC curve SciKit Learn, there is a way to extend it. [Image for reference] You see, for this roc_auc_score, they can actually do this for multi-class cases. There are different ways of how you can take average between these three classifiers. These micro/macro weighted samples – you can just read this description, this recommendation and see what the different ways of doing this are. I think this is a common approach. This is the one I just described – you compute them independently and then you kind of take the average."
139	Will there be any guidance for interview preparation after the capstone project?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
140	Where to inform if there is an error in the evaluation of my homework (classification chapter)? My answer is correct but the system gives 0 points.	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
141	Where to inform if there is an error in the evaluation of my homework (classification chapter)? My answer is correct but the system gives 0 points.	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
142	Where to inform if there is an error in the evaluation of my homework (classification chapter)? My answer is correct but the system gives 0 points.	That's a very similar question to when we should use the lambda and when we should use Kubernetes. If you want to use Kubernetes, then go with TensorFlow Serving. Otherwise, TensorFlow Lite. There is one thing where you have to use TensorFlow Lite – where you cannot use TensorFlow Serving – and this is when you want to deploy your model on a mobile device. I have a phone here. If you want to create an app with a model, then you will have to use TensorFlow Lite because you will not be able to use TensorFlow Serving. TensorFlow Serving also adds a bit of complexity. You will need to have this gateway service, you will need to have GRPC service, so that becomes more complex. If you have a lot of load, then you need to do this otherwise, TensorFlow Lite.
143	Where to inform if there is an error in the evaluation of my homework (classification chapter)? My answer is correct but the system gives 0 points.	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
144	Where to inform if there is an error in the evaluation of my homework (classification chapter)? My answer is correct but the system gives 0 points.	You can ask that in Slack. You can send a direct message to me and we'll figure this out.
145	Do you have any courses except Zoomcamp?	I don't know what that means. We have three courses, which are all Zoomcamps. You can just find the links in the course project. So yes and no, I guess?
146	Do you have any courses except Zoomcamp?	That's a very similar question to when we should use the lambda and when we should use Kubernetes. If you want to use Kubernetes, then go with TensorFlow Serving. Otherwise, TensorFlow Lite. There is one thing where you have to use TensorFlow Lite – where you cannot use TensorFlow Serving – and this is when you want to deploy your model on a mobile device. I have a phone here. If you want to create an app with a model, then you will have to use TensorFlow Lite because you will not be able to use TensorFlow Serving. TensorFlow Serving also adds a bit of complexity. You will need to have this gateway service, you will need to have GRPC service, so that becomes more complex. If you have a lot of load, then you need to do this otherwise, TensorFlow Lite.
147	Do you have any courses except Zoomcamp?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
148	Do you have any courses except Zoomcamp?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
149	Do you have any courses except Zoomcamp?	"Tim
Yeah, we have a lot of users who use MLflow and BentoML together. Once you have a model that’s sort of your “finalized” model – you save it to your MLflow registry and then (it depends on what your CI/CD pipeline looks like) but BentoML has a command to import from an MLflow registry. You can tell which MLflow model is the one that you want to deploy. It's very similar to save_model, except instead of pulling from your local environment, you're pulling from an MLflow registry. So you import the model, it automatically gets pulled into your Bento and then you can deploy it. On the BentoML documentation site, there's a big framework document on how to integrate with MLflow. 
Alexey
When searching, one of the suggestions was BentoML vs MLflow. I guess there are also some similarities because with MLflow, you can serve models. 
Tim
The thing about serving models is that it's a part of the pipeline that you just have to do. Every single framework out there has some way to serve a model – you train the model and then it just puts it up there to be able to serve. Typically, frameworks don't specialize in serving those. BentoML specializes in making the serving part really, really fast and really, really easy. What you find a lot of the time is, when you have serving logic in a really large end-to-end pipeline, it just puts the model there and then the only thing that you could submit to the model is the data. But we know in practice that there's usually business logic around this – pre-transformation logic, post-transformation logic, there's the version of the model that was saved. There are a lot of these other components that you want around the model, not just the model for inference. And of course there's the performance layer underneath. The BentoML architecture kind of brings that all together into one deployable, rather than just giving you one point to call inference for your model. 
Alexey
That is a comment in the live chat that says “There is PyCaret, too.” This is how you use PyCaret with Bento. 
Tim
Right. BentoML has lots and lots of integrations and we're only adding to them every day. The idea is to get the model from wherever you built it, bring it into your Bento, and then you can deploy it anywhere. Once it's in the Bento – in this standard packaging – it's nice because then we've got lots of different tools to deploy to Lambda, SageMaker, and lots of different places. And the places to deploy, we're adding to every week and month as well.
Alexey  
Since you can have a Docker container at the end – you do “bento_containerize” right? That's the command? And then you have a Docker container (Docker image) that you can deploy everywhere where you can deploy a Docker container, which is pretty much every place on the internet, right? 
Tim
Right. Some places, though, require specialized… 
Alexey
Like Lambda, right? 
Tim
Yeah, like Lambda won't accept just a normal Docker container. You have to actually create special endpoints. When we deploy to Lambda, we create those endpoints for you and make sure that it's behind an API gateway and things like that. For certain services, there are a lot of nuances that we kind of take care of for you – as long as you're in the Bento standard format.
Alexey 
For those who have no idea what MLflow is, and why we are talking about this – you don't need to worry about this right now. But at some point, this tool and other similar tools will be quite handy. You can check out our MLOps Zoomcamp course, after you finish this one. Don't try to do multiple courses at the same time. That could be too much. But of course, if you're curious, go check it out."
150	Is it possible to do the course locally?	Yes, you can do many things locally. You can use Postgres and run all your queries there. It will not be the same as using BigQuery. But still, you will practice SQL, you will practice analytical queries, and that's what you need.
151	Is it possible to do the course locally?	"Alexey
Yeah, there are some issues with the new Terraform videos. I don't know. We will probably finish them before the end of this course. I'm not going to promise anything right now. Let's see. Now the first week is over, so it's kind of late anyways. We'll try."
152	Is it possible to do the course locally?	Yeah, I think one project can host several things. I don't know exactly, to be honest. I think there is probably an official recommendation from GCP. Check that out.
153	Is it possible to do the course locally?	No, you cannot.
154	Is it possible to do the course locally?	"Alexey
Well, I can suggest a project for this course. At the end, after you complete this course, after you complete all the modules, as a part of this course, you will have to work on the project. This is how you will get a certificate at the end – if you pass the project. This is a good project to include in your portfolio. 
Michael
I don't know if I have much more to add other than being active in online communities. This course is a great place to start. There are a lot of large cities that have meetup groups, which are always great. One of the good side effects of the whole COVID pandemic is that a lot of those are remote now. If you Google “San Francisco Big Data meetup,” meet people, talk with them, get their insights. I think that'd be a good way to approach that.
Jeff
Plus one just for getting involved in communities, whether it's conferences, online or in-person is great. Volunteer, if you can, at those. That's often a great way to help out and work with people. Go to meetups. I run a meetup in DC on data science. If anyone's in the DC area, there are data engineering meetups in town too. If there isn’t one somewhere, and if there's a few people around, I encourage you to start one. That's something you can do, regardless of your level of experience.
30:06  
Alexey
Then I'm going to do another shameless plug. We have a podcast and the topic of career changing comes up pretty often there. One interesting one is from Juan Pablo. Here he's actually not talking about… well, it's kind of related to analytics engineering. He was a math teacher, which doesn't really qualify as an IT person. He was also working as an Uber driver. In the podcast, he tells his story of how he actually did this. He talks exactly about that. He talks about going to meetups. Instead of not talking about this, just go and check it out. That's not the only relevant podcast episode. You can just go through everything we have and see if anything catches your interest. I'm sure you'll find a lot of interesting stuff."
155	Do you have experience with ClickHouse?	I actually do not.
156	Do you have experience with ClickHouse?	"It all depends on what you mean by “full stack ML engineer”. As you’ve noticed, data engineer Zoomcamp is not about machine learning – it's about data engineering. If in your opinion a “full stack ML engineer” needs to know data engineering, then you should do data engineering. If not, then no. From my point of view, there is a “full stack of data scientist”. What I put in the definition of a full stack data scientist is – a person who can do all the steps in the CRISP-DM process. They can help product managers with business understanding, they can work with analysts in data understanding, they can help data engineers in the data preparation step, they can do the modeling, and they can deploy the models. If you want to be that kind of person, then, of course, you also need to learn a bit of product management and analytics, which we don't have courses for. But data engineering – preparing data before it goes to a model – will certainly be helpful. 
Also, machine learning ops Zoomcamp (MLOps Zoomcamp) will be helpful as well. I would suggest, if you really want to focus on machine learning engineering, then probably doing MLOps Zoomcamp will make more sense for you. But data engineering Zoomcamp will be quite useful in the future, because data scientists and ML engineers tend to work on data pipelines as well. 
Maybe not all the content will be useful for you. For example, the content about data warehousing – as a data scientist, I don't find it super useful in my work to me personally. Also, the module about analytics engineering – it's nice, but it's not something I do day to day. But the modules about batch, the modules about Spark, the modules about streaming – this is something that can be useful, because this is something we could use in building machine learning pipelines as well. 
To put data in a machine learning model, we need to prepare it – we need to create data pipelines – and this is pretty relevant. But you can just check it out. All the content is already there. You can just go and check it out. If you like it, do it – if you don't, don’t. I would suggest maybe going with this one (ML Zoomcamp)."
157	Do you have experience with ClickHouse?	I think for binary features, just one and zero makes more sense than OneHotEncoding. OneHotEncoding will take care of that but you will have two columns that are the exact opposite of each other. So that is a bit redundant. I think it's better just to convert it to a Boolean column with ones and zeros.
158	Do you have experience with ClickHouse?	Yes. We will actually cover cross validation this week. Sometimes, if your dataset is small, (this was one of the questions today) then using cross validation is a very good idea.
159	Do you have experience with ClickHouse?	I wish Tim was here and would help me answer that because I don't know. I assume if you wanted to use Bento, you would probably use TensorFlow Lite – you don't want to use the usual TensorFlow for the same reasons that we talked about in the serverless module, because it's simply too large. Probably for Bento, you would use something like TensorFlow Lite and it will still work. Maybe another thing you can check – in our channel, we had a talk from one of the Bento folks, Building an ML Service Platform from the Ground Up. Here, Sean talks about the differences between TensorFlow Serving and Bento. With TensorFlow Serving, it's good – it's quite optimized for deep learning. There are some problems. First of all, it's written in C++ and expects your model in a certain format – this “saved model format”. Adding anything extra on top of that (like pre-processing, post-processing) becomes difficult, because you will need to have an extra step there. Bento makes it easier. But, again, I don't know. It's very difficult to give you a concrete recommendation without knowing what exactly you want to do. So it's case dependent. If you already use TensorFlow Serving in your company, then maybe it makes sense to stick to that. If we have Tim on Office Hours next week, maybe you can ask him that. He will be a better person to answer this question.
160	When will the Terraform videos be released?	"Alexey
The best way to contribute is to take notes and include them to the notes section. And tell your friends about the course. As a part of your Learning in Public, spread the word. It will help us. And don't forget to give us a star if you have."
161	When will the Terraform videos be released?	Yes, you will have to come up with your own individual projects. It's not easy, I know. But we'll help you. We will help you to come up with a project idea. That's actually on purpose because we want every project to be unique. Then it becomes a part of your portfolio when you look for a job.
162	When will the Terraform videos be released?	"Michael
It's not necessary. We do have recipes for using Terraform with Prefect with different things. We talked about the Prefect recipes repo, but I'll put a link to it here. 
Jeff
This is in a docs and we have a GitHub repo where we have lots of different recipes. Some of them here explain things like, say you want to run an agent – how can you do that? There are so many different cloud environments. You can put things on Kubernetes, you can put things on something like a more serverless platform like Fargate. For example, on the right there, if you were wanting to use AWS with Prefect and ECS (elastic container service) then with Terraform – we have some recipes for that. So there's just so many different permutations of possible ways to do things. A lot of these are helpful ways to go and look at options, but it depends on what you're using in the real world (in a use case). I don't think we have anything specific for this course that requires us doing this, so we don't have anything further for it.
Alexey
I guess, the usual setup that I saw (not necessarily Prefect-related) you typically use Terraform for setting up infrastructure or things like a Kubernetes cluster and configure it there. But then there is a separate repo with configuration for Kubernetes that is not maintained/managed through Terraform but through something else. For that, you don't really need to Terraform – and then Prefect could live in this Kubernetes cluster.
Jeff 
Yeah, we do have some guys working with Kubernetes in here, for sure. I see some of them here. We do have a Helm chart that we can use. If you're into the Kubernetes world, there are lots of things there. It's a whole rabbit hole.
Alexey
Here are the Prefect Helm charts.
Kalise
Also, there are a lot of YouTube videos we have that can help if you're trying to use some of these resources – how to go about deploying the Helm chart with Kubernetes and stuff, like step-by-step tutorials as well. Generally, if you just go to the Prefect YouTube channel, you'll see a lot of those tutorials. There should be some playlists there that have various things.
Alexey
Is this a duck in the cover or what?
Jeff
That is. It's the top of a duck. Blue ducks!"
163	When will the Terraform videos be released?	"Alexey
Yeah, there are some issues with the new Terraform videos. I don't know. We will probably finish them before the end of this course. I'm not going to promise anything right now. Let's see. Now the first week is over, so it's kind of late anyways. We'll try."
164	When will the Terraform videos be released?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
165	I'm just curious, the target audience for BentoML – is it data scientists or is it machine learning engineers?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
166	I'm just curious, the target audience for BentoML – is it data scientists or is it machine learning engineers?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
167	I'm just curious, the target audience for BentoML – is it data scientists or is it machine learning engineers?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
168	I'm just curious, the target audience for BentoML – is it data scientists or is it machine learning engineers?	"Tim
It's kind of both. We try to make the deployment of your ML model as easy as possible, whether that's for the data scientists or the engineers. I think we have a lot more detailed features for the engineering and Ops side that maybe a data scientist may not care about immediately. I think it's both. I would say it's half and half.
Alexey 
There’s an interesting thing. This course is based on a book and when I was writing this book, I had a software engineer who wanted to switch to machine learning engineering in mind. Then we ran the course, and in the forum there was this field poll, where I asked everyone who signed up for the course to specify what they're currently doing. Funny thing was, most of the people (most of the students) were data scientists. It was a surprise for me. I was expecting software engineers to join this because this was the target audience, but the reality was different. I guess maybe for you, something similar happened, right?
Tim 
Yeah, I think so. I'm guessing we see probably more engineers than your ML Zoomcamp does. An engineer like me is not going to know much about training. And I think that's one of the questions down below. I think it's good to know the concepts of training and that's why I participate in ML Zoomcamps like this. But as you scale the team, I think you learn to specialize a little bit more. It's just like a small company versus a big company – if you're a small company you wear a lot of hats and then the bigger you get, the more people you get to hire, and you wear fewer and fewer hats. That's just the nature of being either a small or a big company, it just depends on what you like.
Alexey 
Maybe I'll also add to this question. Even if you have a team where you have ML engineers, backend engineers, other engineers, data engineers – there could still be cases when they're busy with something else because they have other priorities. In our case, it was a big monolith and they were very busy decoupling it and creating microservices because it was very important. But I was annoying them with deploying the model and then they simply did not have time for that. Sometimes it may happen that even though you have people on the team, they cannot necessarily jump on this problem right away. Being able to deploy a model is helpful. It doesn't have to be super resilient, super available, super reliable, but if it's deployed (if it can be used) then it's already a good thing. And tools like Bento help a lot with that. They help data scientists to just do a good enough version that doesn't break and when engineers become available, then they can take care of tuning it.
Tim 
Right. Even when the team gets big enough, engineers will hand the deployment over to DevOps as well. As you get bigger, you get more and more separation and specialization."
169	I'm just curious, the target audience for BentoML – is it data scientists or is it machine learning engineers?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
170	Depending on the passing certification after the project, can we expect some opportunities (jobs)?	We currently do not have any course partners yet. If you work at a company and you want to partner with us, please reach out and we will see how we can do it. In one of our courses, which was our first iteration of the Machine Learning Zoomcamp, we partnered with a company called Delphi. They got two interns from our course and they were quite satisfied with the outcome. You can actually read more about the work of these interns in our articles – Interview with Valerii Chetvertakov and then another one, Interview with Ken Wu. You can learn more about the interns and if you think that, at your company, you need good interns or juniors or you want to partner in any other way, please reach out and we can see how to make it work.
171	Depending on the passing certification after the project, can we expect some opportunities (jobs)?	"Jeff
I can try. I like Black a lot. If you just Google “Python Black” and go to the GitHub readme, it’s become super popular in the last couple of years for automatically formatting your code. It is really nice. Mine is set up in Visual Studio Code, so when I hit “save,” it automatically formats. As it says, it’s an uncompromising code formatter. I think it says here about asking Henry Ford for color back in the day. Is that right? It's colored cars or color telephones from Alexander Graham Bell or somebody, and it's like, “They can have any color they want as long as it's black.” He wasn't going to compromise. You don't have to think about it now. It's easy. Just go right ahead and get your gear code formatted automatically. So it’s things like two lines after function or before – that kind of stuff just happens automatically with it. Getting it set up in VS Code can be a little tricky sometimes. But there are guides to that. Googling is what I do for that, usually.
Alexey
I just wanted to do a shameless plug, because we have another course called MLOps Zoomcamp. By the way, there is also a Prefect model there. One of the things there is best practices. In best practices, we have this video called Code quality: linting and formatting. It does not show how to integrate Black with VS Code, I think. I don't remember if we do this. But if you want to learn more about testing, and Black, and other things like pre-commit hooks, make files, and so on – you can check this out.
Jeff
Yeah, it's not too tricky to set up. It's like pip install a package in your environment and then you have to give it a path maybe or set one setting in VS code. If all goes well, cross your fingers, it should just work after you reload it. Not always the case. If you have trouble, a lot of people use it, so there are a lot of good resources online. Black is great. This is an awesome set of some resources here on best practices. Type hints are just getting more and more popular. They're very helpful, so that people know what kind – especially with autocomplete and little pop-up type – things like that in Visual Studio Code and other code editors. You can see what kind of argument type you should put in and then Prefect uses that information as well, in our flows, for example, to make sure that if it's a block in the UI or a parameter in the UI – it'll be smart. It'll be like, “Oh, is this a number? Okay.” It will give you options to put in numbers. “Is this a different kind of form field?” It'll have different options. It also can then do some validation to make sure that people actually put in something that conforms to that type-in. Python is slowly getting more and more smart about how it handles typing and newer versions keep adding more functionality. Type hints are nice to use. It takes a little bit of writing, but it makes your docstring shorter. The last thing that was asked about here was docstrings. It's great to have in every function to tell people what it's about. It's something that maybe you don't always do if you're in a hurry, but you should do it, especially if other people are going to read the code. Code is read like 20 times more often than it's written, or however you translate that – some stat. So do it. It's so helpful for you in the future and it's helpful for other people in the future, who are going to read your code to see, “What were you thinking? What is the purpose of this function?” Keep your function small, explain it in your doc string – it's good stuff. Then it shows up in your code editor, if you're lucky (if you have a good code editor). That's all to say about that.
Alexey
Do you know any resources where people can learn about setting up? Or learning more about these things, like good Python coding standards? What I showed is obviously a good resource, but it does not cover all these things that this question asks about.
Jeff
It's a good question. I do have a link to Google Style, or there are a couple different styles of docstrings. It seems like they're a little bit much these days, maybe. But there are links for different ways to do type hinting. I do have a few things if I look around for them. I don't have them at my fingertips right now. But Michael looks like maybe he's got one he shared there.
Michael
Yes, this one is a little bit older, but it is great. It goes into using virtual environments, Poetry… there's a lot to unpack, but I think that's still pretty much the standard best practice at the moment."
172	Depending on the passing certification after the project, can we expect some opportunities (jobs)?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
173	Depending on the passing certification after the project, can we expect some opportunities (jobs)?	No, you cannot.
174	Depending on the passing certification after the project, can we expect some opportunities (jobs)?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
175	Any guide to document a project or repo professionally? Can you recommend a standard or reference example?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
176	Any guide to document a project or repo professionally? Can you recommend a standard or reference example?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
177	Any guide to document a project or repo professionally? Can you recommend a standard or reference example?	"I don't have a standard or reference example. Check out the projects from the past year. I think that will be a good answer and just pick the format you like and follow it. Another source of projects could be our project of the week. If you don't know, for example, this is something we'll have this week. So there is a plan, broken by what you need to do on each of the days and then you need to follow this plan. At the end, you will have a project. There is actually a section there with all the projects. If you take part in these projects, you can add links here. Then you can just go back to some of the projects and look at them. 
This is my project. I don't think this is a good example, but maybe another source of inspiration – a far better source of inspiration – would be projects from other students, from the previous cohort. There were real gems. This year, too – I saw really good projects. So go through them, find the ones you like, and maybe if you have time, you can create a template for that. You can say, “Okay, this is the reference example. I saw this in this project. I saw this in this project. I combined it.” And reference the examples. People will like it and people will appreciate it. You can also use it for your projects and then share it with the community. That's a good idea."
178	Any guide to document a project or repo professionally? Can you recommend a standard or reference example?	"Very, very often. This is probably the model that I use the most at my work. This is something I use pretty much for every project – for some projects. Not only is it a good first baseline, but also sometimes we just deploy this logistic regression, it works and then no further work is needed and we just leave it there. Sometimes these models are then improved with something like XGBoost or something, but logistic regression is like the workhorse of machine learning. It's used in many, many, many stations. 
Actually, in some situations, it's not really possible to use anything else except logistic regression. For example, at the previous company where I worked, which was an advertisement company, and there, it was very, very, very, very important to be able to make predictions very fast. Logistic regression is the best model for that. You cannot beat the speed of logistic regression with any other model. Maybe with a decision tree, but it will not be as good as logistic regression. Logistic regression works really well when you have a lot of features and this was the case in the company where I worked. It's really an important and useful model."
179	Any guide to document a project or repo professionally? Can you recommend a standard or reference example?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
180	Do I need to publish a link to a public GitHub repo or should we zip and share the file?	Yeah, just the link. You cannot upload a zip file to the form.
181	Do I need to publish a link to a public GitHub repo or should we zip and share the file?	"Jeff
As Alexey said many times in the class, you can use whatever tool you want for things. So if you want to accomplish things with Workflows, that's fine. There are different tools out there. I'd say the big difference is that Prefect is agnostic to any cloud. You can easily drop in different storage blocks if you want to, or have just different options. Anyone who's using Python first wants to be able to use lots of different tools. That's a good option there. With Workflows, you're kind of tied to Google's infrastructure at that point, in a pretty significant way. But use a tool that works for you. That’s my advice there."
182	Do I need to publish a link to a public GitHub repo or should we zip and share the file?	"Jeff
I can try. I like Black a lot. If you just Google “Python Black” and go to the GitHub readme, it’s become super popular in the last couple of years for automatically formatting your code. It is really nice. Mine is set up in Visual Studio Code, so when I hit “save,” it automatically formats. As it says, it’s an uncompromising code formatter. I think it says here about asking Henry Ford for color back in the day. Is that right? It's colored cars or color telephones from Alexander Graham Bell or somebody, and it's like, “They can have any color they want as long as it's black.” He wasn't going to compromise. You don't have to think about it now. It's easy. Just go right ahead and get your gear code formatted automatically. So it’s things like two lines after function or before – that kind of stuff just happens automatically with it. Getting it set up in VS Code can be a little tricky sometimes. But there are guides to that. Googling is what I do for that, usually.
Alexey
I just wanted to do a shameless plug, because we have another course called MLOps Zoomcamp. By the way, there is also a Prefect model there. One of the things there is best practices. In best practices, we have this video called Code quality: linting and formatting. It does not show how to integrate Black with VS Code, I think. I don't remember if we do this. But if you want to learn more about testing, and Black, and other things like pre-commit hooks, make files, and so on – you can check this out.
Jeff
Yeah, it's not too tricky to set up. It's like pip install a package in your environment and then you have to give it a path maybe or set one setting in VS code. If all goes well, cross your fingers, it should just work after you reload it. Not always the case. If you have trouble, a lot of people use it, so there are a lot of good resources online. Black is great. This is an awesome set of some resources here on best practices. Type hints are just getting more and more popular. They're very helpful, so that people know what kind – especially with autocomplete and little pop-up type – things like that in Visual Studio Code and other code editors. You can see what kind of argument type you should put in and then Prefect uses that information as well, in our flows, for example, to make sure that if it's a block in the UI or a parameter in the UI – it'll be smart. It'll be like, “Oh, is this a number? Okay.” It will give you options to put in numbers. “Is this a different kind of form field?” It'll have different options. It also can then do some validation to make sure that people actually put in something that conforms to that type-in. Python is slowly getting more and more smart about how it handles typing and newer versions keep adding more functionality. Type hints are nice to use. It takes a little bit of writing, but it makes your docstring shorter. The last thing that was asked about here was docstrings. It's great to have in every function to tell people what it's about. It's something that maybe you don't always do if you're in a hurry, but you should do it, especially if other people are going to read the code. Code is read like 20 times more often than it's written, or however you translate that – some stat. So do it. It's so helpful for you in the future and it's helpful for other people in the future, who are going to read your code to see, “What were you thinking? What is the purpose of this function?” Keep your function small, explain it in your doc string – it's good stuff. Then it shows up in your code editor, if you're lucky (if you have a good code editor). That's all to say about that.
Alexey
Do you know any resources where people can learn about setting up? Or learning more about these things, like good Python coding standards? What I showed is obviously a good resource, but it does not cover all these things that this question asks about.
Jeff
It's a good question. I do have a link to Google Style, or there are a couple different styles of docstrings. It seems like they're a little bit much these days, maybe. But there are links for different ways to do type hinting. I do have a few things if I look around for them. I don't have them at my fingertips right now. But Michael looks like maybe he's got one he shared there.
Michael
Yes, this one is a little bit older, but it is great. It goes into using virtual environments, Poetry… there's a lot to unpack, but I think that's still pretty much the standard best practice at the moment."
183	Do I need to publish a link to a public GitHub repo or should we zip and share the file?	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
184	Do I need to publish a link to a public GitHub repo or should we zip and share the file?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
185	Can I make my public learning posts using other languages apart from English?	Since we had more signups this year than last year, I hope it will be more than 100. But let's see.
186	Can I make my public learning posts using other languages apart from English?	I would say to focus on projects, not on tools. In this course, you will get a chance to do this. If you open a job description for data engineers, you will see a lot, a lot, a lot of different technologies. Maybe think of a project that you can do for this particular company and do this project using, for example, the tools you learn here or the tools you want to learn, and build a portfolio. I guess this is more of a tip for getting a data engineering job, rather than for somebody who's starting their data engineering career. But if you already have a job, then maybe think of the business impact of what you need. I think I will not spend too much time talking about that because this requires a one hour long discussion. Perhaps we should have a podcast interview about that. If you can think of any potential guests for this podcast interview, you can let me know and we can try to invite these people and talk about this in more detail. One thing I want to share with you is that in our YouTube channel, we had a few interesting talks. We had this Getting a Data Engineering Job with Jeff talk. It’s a very nice presentation. Then we also had an interview with Jeff. So these two podcasts are quite interesting. You can also check out other ones. I think you'll find a lot of interesting content here.
187	Can I make my public learning posts using other languages apart from English?	No, you cannot.
188	Can I make my public learning posts using other languages apart from English?	Yes, you can.
189	Can I make my public learning posts using other languages apart from English?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
190	This is for Prefect HQ. It would be better if you place a link to the documentation on the Blocks page. Any plans?	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
191	This is for Prefect HQ. It would be better if you place a link to the documentation on the Blocks page. Any plans?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
192	This is for Prefect HQ. It would be better if you place a link to the documentation on the Blocks page. Any plans?	"Jeff
There is a link here, as I shared in Slack. If you click on the Blocks page, anywhere in the UI here, there’s Prefect docs right here on the bottom left, if you're logged into Prefect Cloud."
193	This is for Prefect HQ. It would be better if you place a link to the documentation on the Blocks page. Any plans?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
194	This is for Prefect HQ. It would be better if you place a link to the documentation on the Blocks page. Any plans?	No, you cannot.
195	Which area of Data Science/Engineering/Machine Learning/Deep Learning is best for finding a remote job?	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
196	Which area of Data Science/Engineering/Machine Learning/Deep Learning is best for finding a remote job?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
197	Which area of Data Science/Engineering/Machine Learning/Deep Learning is best for finding a remote job?	Pretty much any of these areas. I don't know which one is best. What does “best” mean here? Remote jobs exist for any of these areas. Just pick up what you like, build a portfolio in that area, and then start looking for a job?
198	Which area of Data Science/Engineering/Machine Learning/Deep Learning is best for finding a remote job?	"Tim
Yeah, we have a lot of users who use MLflow and BentoML together. Once you have a model that’s sort of your “finalized” model – you save it to your MLflow registry and then (it depends on what your CI/CD pipeline looks like) but BentoML has a command to import from an MLflow registry. You can tell which MLflow model is the one that you want to deploy. It's very similar to save_model, except instead of pulling from your local environment, you're pulling from an MLflow registry. So you import the model, it automatically gets pulled into your Bento and then you can deploy it. On the BentoML documentation site, there's a big framework document on how to integrate with MLflow. 
Alexey
When searching, one of the suggestions was BentoML vs MLflow. I guess there are also some similarities because with MLflow, you can serve models. 
Tim
The thing about serving models is that it's a part of the pipeline that you just have to do. Every single framework out there has some way to serve a model – you train the model and then it just puts it up there to be able to serve. Typically, frameworks don't specialize in serving those. BentoML specializes in making the serving part really, really fast and really, really easy. What you find a lot of the time is, when you have serving logic in a really large end-to-end pipeline, it just puts the model there and then the only thing that you could submit to the model is the data. But we know in practice that there's usually business logic around this – pre-transformation logic, post-transformation logic, there's the version of the model that was saved. There are a lot of these other components that you want around the model, not just the model for inference. And of course there's the performance layer underneath. The BentoML architecture kind of brings that all together into one deployable, rather than just giving you one point to call inference for your model. 
Alexey
That is a comment in the live chat that says “There is PyCaret, too.” This is how you use PyCaret with Bento. 
Tim
Right. BentoML has lots and lots of integrations and we're only adding to them every day. The idea is to get the model from wherever you built it, bring it into your Bento, and then you can deploy it anywhere. Once it's in the Bento – in this standard packaging – it's nice because then we've got lots of different tools to deploy to Lambda, SageMaker, and lots of different places. And the places to deploy, we're adding to every week and month as well.
Alexey  
Since you can have a Docker container at the end – you do “bento_containerize” right? That's the command? And then you have a Docker container (Docker image) that you can deploy everywhere where you can deploy a Docker container, which is pretty much every place on the internet, right? 
Tim
Right. Some places, though, require specialized… 
Alexey
Like Lambda, right? 
Tim
Yeah, like Lambda won't accept just a normal Docker container. You have to actually create special endpoints. When we deploy to Lambda, we create those endpoints for you and make sure that it's behind an API gateway and things like that. For certain services, there are a lot of nuances that we kind of take care of for you – as long as you're in the Bento standard format.
Alexey 
For those who have no idea what MLflow is, and why we are talking about this – you don't need to worry about this right now. But at some point, this tool and other similar tools will be quite handy. You can check out our MLOps Zoomcamp course, after you finish this one. Don't try to do multiple courses at the same time. That could be too much. But of course, if you're curious, go check it out."
199	Which area of Data Science/Engineering/Machine Learning/Deep Learning is best for finding a remote job?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
200	Rule-based modeling versus data modeling: what points should be considered to pick one over the other?	"Again, it really depends on your problem. But you probably want to start with rule-based modeling. The very first approach you can take is to develop a simple rule. Then you can actually learn this rule. Maybe you can train a decision tree on this data and then you can see what kind of rules the tree learned. Then you can just use that as your first model. But then, over time, it's growing. Actually, I think I'm just describing data modeling here. [chuckles] I just realized that training a decision tree and then using it kind of becomes data modeling. 
But honestly, for some systems, it makes sense to have rule-based modeling, because sometimes you want to have flexibility of overriding some of the model’s decisions. You can even have both. You have a machine learning model, and then you have rules and they kind of coexist. This is actually what we have at OLX for moderation. Moderation is the process of deciding if a listing (this is an online marketplace) and let's say you want to sell your phone. You go to OLX and you want to sell it. You fill in the title, the description, you upload some pictures, and then you hit “publish”. The listing is not published immediately. It goes through the moderation process. We look at this listing, and we conclude that it's safe to publish. There are a few checks. The first check might be “Is it a duplicate of another listing?” If it's yes, it's not published. If it's not a duplicate, then we do another check. Actually, these checks happen in parallel. Another check could be like, “Is there a gun or some other prohibited item?” Or “Is it not safe for work content?” 
There are a bunch of checks. Most of these checks are machine learning models. But also, a lot of these checks are rule-based models. I will not tell you these rules, but they could be “If something then not published,” for example, or “If something, then show it to moderators.” These rules alone, they're very flexible and moderators can just go to our moderation platform and add these rules manually. This is very helpful, because retraining a model takes some time. You don't always have this time, especially in cases like moderation systems. 
You want to have the flexibility of adding rules and reacting to things that are happening almost immediately – as they happen. Rules allow for this flexibility. These two things coexist. In summary, you can start with a rule-based model then you do data modeling – and, actually, one does not exclude the other. You can use these rules as features to your models and then you can use both in your production systems."
201	Rule-based modeling versus data modeling: what points should be considered to pick one over the other?	"Tim 
Not really. We have users who use both Streamlit and Gradio. I think Streamlit and Gradio are a bit more about the presentation side. We do have a couple of users who have asked us about deeper Streamlit or Gradio support and I think that's something that's probably coming up in the next few months.
Alexey
For those who don't know what Streamlit or Gradio is – this is a way to create an interface for your services. Well, Gradio is focused on machine learning, meaning you create interfaces specifically for models. And then Streamlit is just a Python package for creating interfaces. We actually had a project at DataTalks.Club about Streamlit. I actually even have a tweet about this. This is what Streamlit looks like. In the video, you can see that you have this interface, and then based on the toggles that you move the core of your model changes. It gives you an easy way to build an interface like that. Meanwhile Bento focuses on quite a different use case, right? It’s not building interfaces, but serving models.
Tim 
Right, yeah. I think one of our users was asking for a really easy way, so that once you have your model and your Streamlit UI to be able to create a Bento deployment with one line in that way, anybody that you gave your Streamlit app to would be calling to that service and it could work anywhere. Which is a nice feature, I think.
Alexey 
Yeah, I think it is. I imagine if you have a Pydentic class there, then you can easily generate a Streamlit app for that.
Tim 
Right. Oh, that's a good idea. I hadn't thought of that. Yeah.
Alexey
And then in addition to that, the Swagger UI that you have, could be like having the actual interface. 
Tim
Oh, yeah, that would be cool. [chuckles]
Alexey
That could be a good Hackathon project, right?
Tim 
Yeah, for sure.
Alexey
From what I see – let's say you’ve built a model. Then you deployed this model and you have this nice API. You can query it with CURL. But what if you want to demo this project to your manager, or a manager of your manager, or somebody who does not necessarily have a technical background? If you give them the URL and say, “Okay, you need to execute the CURL command with a post request,” they would be like, “CURL what? What do you want from me? Just show me how it looks.” This is when tools like Streamlit come in handy. Instead of giving them a command line interface to query it, you just give them “Okay, this is the link. Play with this.” If Bento could do something like that – automatically generate things like that – many data scientists would thank you.
Tim
Yeah, I was just looking because I figured there's a library that turns Pydantic models into Streamlit apps. It looks like there is one that does that. I wonder if we could just plug into that app and then have that same thing. 
Alexey
That's cool. 
Tim
Yeah, that's awesome. That's kind of the cool part of open source – there are so many people out there with so many different projects and we get to collaborate with them and kind of build the best collaborations and combinations of these tools."
202	Rule-based modeling versus data modeling: what points should be considered to pick one over the other?	"I don't know how to answer that, because I work as a principal data scientist. I work on pretty much everything that the company needs me to work with. This means that all the models are related, to some extent, to online classifieds – to online marketplaces. I need to deal more with domain expertise, maybe, rather than ML knowledge. I guess that's the answer. 
We have a lot of different use cases – we have recommender systems, we have search, we have moderation. We actually have an article about this. For example, Learning-To-Rank: Sorting Search Results. My colleague wrote this one. I think we have an article about data science, Data Science at OLX which happens to be written by me. You can see what kind of use cases we solve. Maybe it could be interesting for you."
203	Rule-based modeling versus data modeling: what points should be considered to pick one over the other?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
204	Rule-based modeling versus data modeling: what points should be considered to pick one over the other?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
205	For data science job applications, should one send a cover letter or only sending the CV is enough?	"That, again, depends. If a company wants to have a cover letter, send it. I don't read cover letters, to be honest, even though I’m a hiring manager – I don't read cover letters. I usually look at the CV. Quite often, actually, I try to not even look at the CV. The CV screening is done by a recruiter. She looks at the CV and she probably also looks at the cover letter (I don't know if she actually does). When I do the technical interview, I try not to bias myself and I don't look at the CV. I already trust the recruiter that she did the screening, so the candidate already satisfies the requirements, at least in terms of the keyword match. But I don't want to bias myself too much, that's why I look at the CV maybe after the interview. 
Sometimes, for example, I have a story where I wanted to apply for a computer vision position, which was in the real estate domain. They were working with some real estate images. I didn't have experience in computer vision. In my cover letter, I said “I don't have experience in computer vision, but worked on a project that is related to real estate and I can talk more about that in the interview.” They said, “Okay, come in.” I then went to the interview and I asked them if they invited me because of the cover letter and they said yes. So sometimes it makes sense. But for many cases – I personally don't care much about them."
206	For data science job applications, should one send a cover letter or only sending the CV is enough?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
207	For data science job applications, should one send a cover letter or only sending the CV is enough?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
208	For data science job applications, should one send a cover letter or only sending the CV is enough?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
209	For data science job applications, should one send a cover letter or only sending the CV is enough?	"Tim
I think a couple people asked this in ML Zoomcamp Slack. I was thinking about this a little bit. I think it's sort of a philosophical thing [chuckles] where we think about models and Bento is sort of separate a little bit from code. So your project directory is where all the stuff that you'll commit to Git. I don't think we think that it's the right model to commit your models to Git, for example. We think of ourselves a little bit more like Docker. You don't have Docker images in your project directory, right? You just have your Docker file and that's what you commit to Git. In the same way, we store your models and your Bento in that dedicated directory, and then sort of leave it up to you to push it to different places where you might build or where you might need it
Alexey
Do you know why you decided to save it locally on your local file system? For example, what MLflow does – you can save it to S3 directly, but in the case of Bento, you always first save it to local storage and then you decide “This model I want to export, but for the rest, I don't care.”
Tim
Right. First, I think it’s because that's the easiest way to do it, but also because I think with BentoML, the workflow is that a lot of times, somebody's building a model on their laptop, or somebody's building a model in a slightly less distributed system and they're assembling their Bento there. So I think that the fastest way to access that model is locally. We do give you the ability to push that model and pull that model from other places. But ultimately, when you assemble your Bento and then you assemble your container, all that stuff has to be there. I don't think we wanted separate abstractions for when a person is training and saving, versus when a person is building a Bento and pulling in the model and assembling the container. It's just the same abstraction, which is just a local repository which contains models.
Alexey 
Maybe you don't want to save every model every single time you change one single parameter to the cloud. [Tim agrees] Maybe at the end, after all the experiments, you want to save the final one. Meanwhile, for MLflow, the use case is different – you actually want to save every single experiment because you want to see the results of these experiments.
Tim
Right."
210	Can you suggest a book about Kubernetes?	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
211	Can you suggest a book about Kubernetes?	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
212	Can you suggest a book about Kubernetes?	"Well, there are multiple ways you can deal with this. First – get more data, if possible. If not, try to reduce the number of features you have. Sometimes, when the data set is small, if you have a lot of features, it might be confusing for the model. Or try to have a model with a lot of regularization. This should help. Here, the main thing is that you need to come up with a reliable way of evaluating the performance of your model. 
You really need to think about how you can set up your validation strategy in such a way that it's reliable. Once you have that, you can start experimenting with all these regularization things, with dropping some columns, with techniques like upsampling and downsampling and things like this. The important thing here is setting up your validation strategy. Once you have that, everything else will come. You will just need to experiment and the experiments will show what the best way is."
213	Can you suggest a book about Kubernetes?	Yeah. If it works for you, use it. I don't mind.
214	Can you suggest a book about Kubernetes?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
215	How should we provide you with the homework assignment results?	We’re still in progress (as of January 26, 2023). I thought we would do this earlier. But it's still not finished.
216	How should we provide you with the homework assignment results?	I would say to focus on projects, not on tools. In this course, you will get a chance to do this. If you open a job description for data engineers, you will see a lot, a lot, a lot of different technologies. Maybe think of a project that you can do for this particular company and do this project using, for example, the tools you learn here or the tools you want to learn, and build a portfolio. I guess this is more of a tip for getting a data engineering job, rather than for somebody who's starting their data engineering career. But if you already have a job, then maybe think of the business impact of what you need. I think I will not spend too much time talking about that because this requires a one hour long discussion. Perhaps we should have a podcast interview about that. If you can think of any potential guests for this podcast interview, you can let me know and we can try to invite these people and talk about this in more detail. One thing I want to share with you is that in our YouTube channel, we had a few interesting talks. We had this Getting a Data Engineering Job with Jeff talk. It’s a very nice presentation. Then we also had an interview with Jeff. So these two podcasts are quite interesting. You can also check out other ones. I think you'll find a lot of interesting content here.
217	How should we provide you with the homework assignment results?	I will show you. In the Data Engineer Zoomcamp, we have the cohorts folder, and then we have the 2023 folder, where you can see the homework. Actually, there are two homeworks. I think we need to change that. I'll just set a reminder for myself. This is the first homework, and then there is another one here. There are two homework assignments for week one. One is SQL. You go through these questions, and then you submit the answers using this form. And then you do the same for that TerraForm homework. There is a form, you submit the answer here. It's just one question. This is how you do this. After the deadline, we grade the homeworks and you will see the leaderboard.
218	How should we provide you with the homework assignment results?	Yep, projects are the best way. Just start working. That's already good enough. You can also think about giving talks and so on.
219	How should we provide you with the homework assignment results?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
220	If I want to be called a full stack data scientist with machine learning data science and data engineering knowledge, what should I learn after ML Zoomcamp?	Well, I’m glad you asked. We have a data engineering Zoomcamp, which starts in January and we have the MLOps Zoomcamp, which starts in May. The only thing that is missing here to become a full stack data scientist is the business understanding part. Remember, when we talked about the CRISP DM process, the first part was business understanding. This is business domain knowledge and things like that. So the only thing that is missing to become a full stack data scientist is picking up this part. For that, I don't know what the best way to learn it is, apart from just joining a company and then talking to stakeholders (to the users of your model) and trying to understand more and more from them. I do recommend doing this, but let's say if you’re not working yet and you want to be a generalist, taking all these three courses is fine. You don't have to do everything there. For example, in the data engineering course, maybe data warehousing is not as important for data scientists as for data engineers, so maybe you can skip that part. But chances are that, as a data scientist, you will need to work with data warehouses too. So you might as well just watch the whole thing.
221	If I want to be called a full stack data scientist with machine learning data science and data engineering knowledge, what should I learn after ML Zoomcamp?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
222	If I want to be called a full stack data scientist with machine learning data science and data engineering knowledge, what should I learn after ML Zoomcamp?	That's a very similar question to when we should use the lambda and when we should use Kubernetes. If you want to use Kubernetes, then go with TensorFlow Serving. Otherwise, TensorFlow Lite. There is one thing where you have to use TensorFlow Lite – where you cannot use TensorFlow Serving – and this is when you want to deploy your model on a mobile device. I have a phone here. If you want to create an app with a model, then you will have to use TensorFlow Lite because you will not be able to use TensorFlow Serving. TensorFlow Serving also adds a bit of complexity. You will need to have this gateway service, you will need to have GRPC service, so that becomes more complex. If you have a lot of load, then you need to do this otherwise, TensorFlow Lite.
223	If I want to be called a full stack data scientist with machine learning data science and data engineering knowledge, what should I learn after ML Zoomcamp?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
224	If I want to be called a full stack data scientist with machine learning data science and data engineering knowledge, what should I learn after ML Zoomcamp?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
225	Do you plan on creating a Zoomcamp targeting data analysts?	We do not. But if you want, or know somebody who would be interested, let's talk.
226	Do you plan on creating a Zoomcamp targeting data analysts?	"Alexey
You can create a Python path variable in your .bashrc and it will be there permanently. Just in the same way as you do with PATH. I don't know why we do it like that. Maybe I just didn't mention that. But, of course, you can just put everything you want in .bashrc and this alliance (this code) will be executed every time you launch your virtual machine. This way you can execute these things automatically."
227	Do you plan on creating a Zoomcamp targeting data analysts?	"There are lots of different varieties of approaches here. One thing that you might do is use some kind of data ingestion tool, as I mentioned in the live FAQ discussion – something like Airbyte or Fivetran – if you have lots of data that you're ingesting, that can be a common way to stay on top of that. Or you could do it through a script, kind of like we do here. 
But what you might do is run your agent on a virtual environment in Google Cloud VM, for example, or in AWS, or somewhere else. You could run it in Kubernetes. There are a number of places where maybe you would have that running and have your infrastructure there, so that you were just pulling down the code not to your local machine, but it would just be in your VM. 
If you're actually going and using Pandas, you're putting the data into memory, so you do need to have some scripts that will do that. And you can go directly between Google Cloud Storage and BigQuery with the Prefect GCP library. So that Prefect GCP library that you installed, that you've been using, does have some options in there. I think it's in the BigQuery module to actually get data directly into BigQuery from GCS. So there are a lot of options there. Check out Prefect GCP to see how to do it."
228	Do you plan on creating a Zoomcamp targeting data analysts?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
229	Do you plan on creating a Zoomcamp targeting data analysts?	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
230	Is Prefect sponsoring the course in favor of showing off their tech?	Since we had more signups this year than last year, I hope it will be more than 100. But let's see.
231	Is Prefect sponsoring the course in favor of showing off their tech?	If it's a question for me, specifically – in Windows, I use a tool called Windows Terminal. This is the tool I use. In the settings, they have Git Bash, the usual command prompt, then they have multiple subsystems for Linux terminals. There also used to be PowerShell. I don't use PowerShell, so I don't have it here. This is what it looks like. I really like this. It’s just called Windows terminal. Check it out.
232	Is Prefect sponsoring the course in favor of showing off their tech?	"Alexey
I don't think I understand the question. But I'll tell you a story. Last year, when we ran our Data Engineering Zoomcamp, we used Airflow. Airflow was quite problematic. In Airflow, it was very difficult to set up locally. We thought it would be easy. It was in Docker – it had a Docker compose file. We thought, “All you need to run is just Docker Compose Hub, and everything will work fine.” It was very naive of us to assume that. It gave many people a lot of problems. Meanwhile, in MLOps Zoomcamp, we used Prefect from the start. We didn't consider using Airflow. And it was a lot smarter. That's why we thought that for teaching the course, Prefect will be easier to get started, because it's a more lightweight thing. 
Prefect is a nice tool. It's a really cool tool. I'm a big fan of Prefect. But also Airflow is probably more popular. That's why we don't remove Airflow from the videos altogether. They are optional and we still encourage you to try them, too. But if you first walked through the Prefect video, I think it will be a lot easier for you to start with Airflow. It's more lightweight, the code is less verbose there, and it's easier to get started. Once you get up to speed with Prefect, then everything will be easier.
Somebody shared a report in the data engineering course channel. The report was from the AI infrastructure Alliance, I think. They also explain why people like Prefect more than Airflow, so maybe find it."
233	Is Prefect sponsoring the course in favor of showing off their tech?	No, you cannot.
234	Is Prefect sponsoring the course in favor of showing off their tech?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
235	What is the best tip you have for somebody who's just starting their data engineering career?	I would say to focus on projects, not on tools. In this course, you will get a chance to do this. If you open a job description for data engineers, you will see a lot, a lot, a lot of different technologies. Maybe think of a project that you can do for this particular company and do this project using, for example, the tools you learn here or the tools you want to learn, and build a portfolio. I guess this is more of a tip for getting a data engineering job, rather than for somebody who's starting their data engineering career. But if you already have a job, then maybe think of the business impact of what you need. I think I will not spend too much time talking about that because this requires a one hour long discussion. Perhaps we should have a podcast interview about that. If you can think of any potential guests for this podcast interview, you can let me know and we can try to invite these people and talk about this in more detail. One thing I want to share with you is that in our YouTube channel, we had a few interesting talks. We had this Getting a Data Engineering Job with Jeff talk. It’s a very nice presentation. Then we also had an interview with Jeff. So these two podcasts are quite interesting. You can also check out other ones. I think you'll find a lot of interesting content here.
236	What is the best tip you have for somebody who's just starting their data engineering career?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
237	What is the best tip you have for somebody who's just starting their data engineering career?	"Alexey
Yeah, we talked about that in Slack, so please check it. I think this happens because some of the columns have missing values and when you use Pandas for converting CSV to Parquet, it reads columns with missing values as double. That's why this happens."
238	What is the best tip you have for somebody who's just starting their data engineering career?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
239	What is the best tip you have for somebody who's just starting their data engineering career?	Our thoughts are that this is indeed an exciting area. This is an in-demand skill and that's why we have a section about analytics engineering in this course.
240	Are there alternatives for cross-validation and hyperparameter tuning in deep learning?	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
241	Are there alternatives for cross-validation and hyperparameter tuning in deep learning?	In deep learning, if you saw the lectures already, you might have noticed that we don't use cross-validation there, simply because it's too time consuming. We set aside a validation dataset and we use this to guide us – to select the best parameter. That's the usual approach I follow, because with proper cross-validation, with creating three folds, it’s simply too time-consuming. That's why I follow a simpler approach. I guess that is the alternative.
242	Are there alternatives for cross-validation and hyperparameter tuning in deep learning?	I guess not. That's the process of learning? Maybe more senior people don't… Yeah, I see where you're going with this. Senior people are not supposed to go bankrupt and default. They are probably supposed to pay back the debt. I guess these are the particularities of this dataset, I assume. Indeed, it's probably a data issue ,or not an issue, but a “characteristic” of this dataset.
243	Are there alternatives for cross-validation and hyperparameter tuning in deep learning?	Yeah. If it works for you, use it. I don't mind.
244	Are there alternatives for cross-validation and hyperparameter tuning in deep learning?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
245	Is it possible to tune the environment fully in GCP?	"Alexey
No. I mean, we kind of told you. Yes, you maybe didn't see the intro video. But, sorry. No, it's late. We will actually recalculate the points for the second homework (HW 1B) because in the forum, it said that you can get up to one point, but we calculated that the cap there was seven. We are going to recalculate that. Thus, people who got a lot of points will get just one. Sorry about that. But don't focus on points. If you go to the leaderboard, we know that the person with this hash (722366d8b29ece9be3a7605363562c7c60d6918e) did seven links. Do you know who that is? Probably you don't. 
These points are virtual. It's nice to have a good score, but remember that these points mean whatever you want them to mean – not more than not less. Maybe it's okay if you get a few points less than others for the first homework, but the important thing here is consistency. If you just do your homework, even without doing any Learning in Public stuff, which I think you should do (you should post about the course). If you don't do this and you just do all the homework, by the end, you will be somewhere in the top 20. If you look at the leaderboard from the previous iteration, you will see that not everyone stuck around till the end. 
If you just do your homework, don't worry about these points, and you'll be fine. Sorry about that. I know that it can be discouraging that you didn't get as many points as others, but don't worry about that. Focus on learning. Maybe Michael has some other suggestions or some other things that I didn't mention, because he took part last year. But I think he was one of the people who did a few posts every week, right?
Michael
Yes. Actually, when I went through it, I missed the whole… you can get up to seven points, but I was just doing one every week and I just stuck with that. I was still near the top. I think the real value in that isn't necessarily the points but just being comfortable sharing – writing articles and making videos. That's the real value there. But everyone likes the clout value, too. As Alexey said, it will fluctuate, so if you keep up with it going forward, you'll probably be just fine."
246	Is it possible to tune the environment fully in GCP?	"Alexey
Well, I can suggest a project for this course. At the end, after you complete this course, after you complete all the modules, as a part of this course, you will have to work on the project. This is how you will get a certificate at the end – if you pass the project. This is a good project to include in your portfolio. 
Michael
I don't know if I have much more to add other than being active in online communities. This course is a great place to start. There are a lot of large cities that have meetup groups, which are always great. One of the good side effects of the whole COVID pandemic is that a lot of those are remote now. If you Google “San Francisco Big Data meetup,” meet people, talk with them, get their insights. I think that'd be a good way to approach that.
Jeff
Plus one just for getting involved in communities, whether it's conferences, online or in-person is great. Volunteer, if you can, at those. That's often a great way to help out and work with people. Go to meetups. I run a meetup in DC on data science. If anyone's in the DC area, there are data engineering meetups in town too. If there isn’t one somewhere, and if there's a few people around, I encourage you to start one. That's something you can do, regardless of your level of experience.
30:06  
Alexey
Then I'm going to do another shameless plug. We have a podcast and the topic of career changing comes up pretty often there. One interesting one is from Juan Pablo. Here he's actually not talking about… well, it's kind of related to analytics engineering. He was a math teacher, which doesn't really qualify as an IT person. He was also working as an Uber driver. In the podcast, he tells his story of how he actually did this. He talks exactly about that. He talks about going to meetups. Instead of not talking about this, just go and check it out. That's not the only relevant podcast episode. You can just go through everything we have and see if anything catches your interest. I'm sure you'll find a lot of interesting stuff."
247	Is it possible to tune the environment fully in GCP?	Yes, that's why we have the environment preparation video
248	Is it possible to tune the environment fully in GCP?	I would say to focus on projects, not on tools. In this course, you will get a chance to do this. If you open a job description for data engineers, you will see a lot, a lot, a lot of different technologies. Maybe think of a project that you can do for this particular company and do this project using, for example, the tools you learn here or the tools you want to learn, and build a portfolio. I guess this is more of a tip for getting a data engineering job, rather than for somebody who's starting their data engineering career. But if you already have a job, then maybe think of the business impact of what you need. I think I will not spend too much time talking about that because this requires a one hour long discussion. Perhaps we should have a podcast interview about that. If you can think of any potential guests for this podcast interview, you can let me know and we can try to invite these people and talk about this in more detail. One thing I want to share with you is that in our YouTube channel, we had a few interesting talks. We had this Getting a Data Engineering Job with Jeff talk. It’s a very nice presentation. Then we also had an interview with Jeff. So these two podcasts are quite interesting. You can also check out other ones. I think you'll find a lot of interesting content here.
249	Is it possible to tune the environment fully in GCP?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
250	To the Prefect team, how does the deployment really happen? What is the difference between Prefect Orion and Prefect Cloud during deployment in terms of the local code?	"Jeff
With deployment, it’s server again – back to the server. We have a server, when you want to do a pip install Prefect and Prefect Orion start – you have the server running locally. You communicate that with that server through the command line, and then information goes to the server. You can put the deployment information on the server. It's the same thing with Prefect Cloud, except it's hosted. It's our database that's getting that information. In both cases, there's a database backing things. Locally, it's a SQLite database by default. If you want to, you can use Postgres for bigger, higher use operations with multiple people. We have a Postgres database on Google Cloud (GCP) that is backing our stuff on the cloud, too. That's what we're using behind the scenes too. Those are just different servers in different places, as Kalise mentioned before, ours can do things like authorizations so only certain people have access to data, and you can share/collaborate on Prefect Cloud. Basically, the information lives on the server there and when it's time to go and run that, you can schedule it as you've seen from the homework and elsewhere in the course. The agent, somewhere, will be pulling and saying, “Hey, alright. Yeah, I got something ready to run here. There's something that's waiting. I'm going to run it.” And the agent will kick off a process either on your local computer or in some infrastructure you specify, like Docker, and it'll make sure that has the flow code available there. Question 4 on the homework – lots of questions there – we asked you to put the code on GitHub, because GitHub is super popular. You're gonna be able to collaborate with people on GitHub. The code is being found on a GitHub repo, and then the processes are being started in that infrastructure and then it all runs beautifully. You never have any errors and everything works."
251	To the Prefect team, how does the deployment really happen? What is the difference between Prefect Orion and Prefect Cloud during deployment in terms of the local code?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
252	To the Prefect team, how does the deployment really happen? What is the difference between Prefect Orion and Prefect Cloud during deployment in terms of the local code?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
253	To the Prefect team, how does the deployment really happen? What is the difference between Prefect Orion and Prefect Cloud during deployment in terms of the local code?	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
254	To the Prefect team, how does the deployment really happen? What is the difference between Prefect Orion and Prefect Cloud during deployment in terms of the local code?	"Alexey
The best way to contribute is to take notes and include them to the notes section. And tell your friends about the course. As a part of your Learning in Public, spread the word. It will help us. And don't forget to give us a star if you have."
255	How does regularization affect the inference output in linear regression, such as the original t-statistics, f-statistics, standard errors?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
256	How does regularization affect the inference output in linear regression, such as the original t-statistics, f-statistics, standard errors?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
257	How does regularization affect the inference output in linear regression, such as the original t-statistics, f-statistics, standard errors?	The best way is just to test it. Take a linear regression, look at all these t-statistics, f-statistic, standard errors, and then play with the regularization parameter, and then see how it changes. Then you will see how it affects it and this effect will be different for different datasets. The answer is – it depends. Just go and test it yourself. To be honest, I don't remember exactly what the statistics show to give you an answer. But this is how I would approach this if I wanted to know it.
258	How does regularization affect the inference output in linear regression, such as the original t-statistics, f-statistics, standard errors?	"We had a talk about this on our YouTube channel. As far as I remember, it was called Machine Learning Design Patterns. These patterns were mostly about engineering – engineering patterns – but the first pattern, as far as I remember, was about rebalancing. So check it out. Sara explains when to use these techniques. Here, the important part is that you apply these techniques only to your training dataset. You keep your validation dataset intact – you don't change it – because you want to have a reliable way of evaluating the performance of your model. You apply different techniques to your training dataset and then you see how exactly it changes the performance on your validation dataset. 
So you apply these techniques to train, experiment, try different ones, and then go with the one that has the best uplift for your score on validation. That's usually how you do this. This will tell you if you actually need any of these techniques, or whether just throwing all the data that you have into the model is good enough, so you don't need to make the process more complicated than it should be."
259	How does regularization affect the inference output in linear regression, such as the original t-statistics, f-statistics, standard errors?	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
260	Data Analysts and Data Scientists are not the same thing. If some of the role tasks overlap, it’s usually not for ML. Maybe you can learn SQL machine learning.	"Tim
I think a couple people asked this in ML Zoomcamp Slack. I was thinking about this a little bit. I think it's sort of a philosophical thing [chuckles] where we think about models and Bento is sort of separate a little bit from code. So your project directory is where all the stuff that you'll commit to Git. I don't think we think that it's the right model to commit your models to Git, for example. We think of ourselves a little bit more like Docker. You don't have Docker images in your project directory, right? You just have your Docker file and that's what you commit to Git. In the same way, we store your models and your Bento in that dedicated directory, and then sort of leave it up to you to push it to different places where you might build or where you might need it
Alexey
Do you know why you decided to save it locally on your local file system? For example, what MLflow does – you can save it to S3 directly, but in the case of Bento, you always first save it to local storage and then you decide “This model I want to export, but for the rest, I don't care.”
Tim
Right. First, I think it’s because that's the easiest way to do it, but also because I think with BentoML, the workflow is that a lot of times, somebody's building a model on their laptop, or somebody's building a model in a slightly less distributed system and they're assembling their Bento there. So I think that the fastest way to access that model is locally. We do give you the ability to push that model and pull that model from other places. But ultimately, when you assemble your Bento and then you assemble your container, all that stuff has to be there. I don't think we wanted separate abstractions for when a person is training and saving, versus when a person is building a Bento and pulling in the model and assembling the container. It's just the same abstraction, which is just a local repository which contains models.
Alexey 
Maybe you don't want to save every model every single time you change one single parameter to the cloud. [Tim agrees] Maybe at the end, after all the experiments, you want to save the final one. Meanwhile, for MLflow, the use case is different – you actually want to save every single experiment because you want to see the results of these experiments.
Tim
Right."
261	Data Analysts and Data Scientists are not the same thing. If some of the role tasks overlap, it’s usually not for ML. Maybe you can learn SQL machine learning.	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
262	Data Analysts and Data Scientists are not the same thing. If some of the role tasks overlap, it’s usually not for ML. Maybe you can learn SQL machine learning.	Okay. Yeah
263	Data Analysts and Data Scientists are not the same thing. If some of the role tasks overlap, it’s usually not for ML. Maybe you can learn SQL machine learning.	"Tim
Yeah, we have a lot of users who use MLflow and BentoML together. Once you have a model that’s sort of your “finalized” model – you save it to your MLflow registry and then (it depends on what your CI/CD pipeline looks like) but BentoML has a command to import from an MLflow registry. You can tell which MLflow model is the one that you want to deploy. It's very similar to save_model, except instead of pulling from your local environment, you're pulling from an MLflow registry. So you import the model, it automatically gets pulled into your Bento and then you can deploy it. On the BentoML documentation site, there's a big framework document on how to integrate with MLflow. 
Alexey
When searching, one of the suggestions was BentoML vs MLflow. I guess there are also some similarities because with MLflow, you can serve models. 
Tim
The thing about serving models is that it's a part of the pipeline that you just have to do. Every single framework out there has some way to serve a model – you train the model and then it just puts it up there to be able to serve. Typically, frameworks don't specialize in serving those. BentoML specializes in making the serving part really, really fast and really, really easy. What you find a lot of the time is, when you have serving logic in a really large end-to-end pipeline, it just puts the model there and then the only thing that you could submit to the model is the data. But we know in practice that there's usually business logic around this – pre-transformation logic, post-transformation logic, there's the version of the model that was saved. There are a lot of these other components that you want around the model, not just the model for inference. And of course there's the performance layer underneath. The BentoML architecture kind of brings that all together into one deployable, rather than just giving you one point to call inference for your model. 
Alexey
That is a comment in the live chat that says “There is PyCaret, too.” This is how you use PyCaret with Bento. 
Tim
Right. BentoML has lots and lots of integrations and we're only adding to them every day. The idea is to get the model from wherever you built it, bring it into your Bento, and then you can deploy it anywhere. Once it's in the Bento – in this standard packaging – it's nice because then we've got lots of different tools to deploy to Lambda, SageMaker, and lots of different places. And the places to deploy, we're adding to every week and month as well.
Alexey  
Since you can have a Docker container at the end – you do “bento_containerize” right? That's the command? And then you have a Docker container (Docker image) that you can deploy everywhere where you can deploy a Docker container, which is pretty much every place on the internet, right? 
Tim
Right. Some places, though, require specialized… 
Alexey
Like Lambda, right? 
Tim
Yeah, like Lambda won't accept just a normal Docker container. You have to actually create special endpoints. When we deploy to Lambda, we create those endpoints for you and make sure that it's behind an API gateway and things like that. For certain services, there are a lot of nuances that we kind of take care of for you – as long as you're in the Bento standard format.
Alexey 
For those who have no idea what MLflow is, and why we are talking about this – you don't need to worry about this right now. But at some point, this tool and other similar tools will be quite handy. You can check out our MLOps Zoomcamp course, after you finish this one. Don't try to do multiple courses at the same time. That could be too much. But of course, if you're curious, go check it out."
264	Data Analysts and Data Scientists are not the same thing. If some of the role tasks overlap, it’s usually not for ML. Maybe you can learn SQL machine learning.	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
265	I need to reorganize my homework directory in GitHub. This may relocate the location of homework 1 (already submitted). Is that fine?	It's totally fine. It's your GitHub repo – you can do whatever you want. You can delete it if you want.
266	I need to reorganize my homework directory in GitHub. This may relocate the location of homework 1 (already submitted). Is that fine?	I have not used it, so I don't know.
267	I need to reorganize my homework directory in GitHub. This may relocate the location of homework 1 (already submitted). Is that fine?	Okay. Yeah
268	I need to reorganize my homework directory in GitHub. This may relocate the location of homework 1 (already submitted). Is that fine?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
269	I need to reorganize my homework directory in GitHub. This may relocate the location of homework 1 (already submitted). Is that fine?	"I guess? I think yes, because data scientists do not live in isolation from the rest of the company – the rest of the team. The article I referred to actually describes how people work in a team. MLOps is not just about tools, it's also about processes – how exactly your work should be organized in such a way that you can easily maintain it, scale it, and so on. 
This is why data scientists should know some things about this. I guess that answers your question? There are also tools for experiment tracking, so data scientists definitely need to use them. I think I'm just trying to say “Yes, it is important.”"
270	Maybe there are videos or material on the channel about graph algorithms or case studies with network analysis?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
271	Maybe there are videos or material on the channel about graph algorithms or case studies with network analysis?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
272	Maybe there are videos or material on the channel about graph algorithms or case studies with network analysis?	I'm not sure I understand the question. The question is “Why test_train_split splits in two, but here, we split into three parts? For that there is a video in week one that explains why we need three datasets, not two. So you can go check it out. There is a problem called “multiple comparison problem” and that's why we use test/split to account for that. You will see later, in module three – there we can also use SciKit Learn train_test_split to split our original dataset into three parts and this is exactly what we will do.
273	Maybe there are videos or material on the channel about graph algorithms or case studies with network analysis?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
274	Maybe there are videos or material on the channel about graph algorithms or case studies with network analysis?	Yes, I think so. There was a talk about graph analytics. We had two, actually. Getting Started with Network Analytics in Python from Eric and then Modeling the Human Brain by Jessie. So these two. It’s funny that YouTube found it even though it actually doesn't mention the word “graph” in the title, nor in the description. But Jessie talks a lot about graphs. It's actually her research area. I think she graduated already. And she has a PhD now. She was talking about her PhD project. We also had a Book of the Week about graphs. Practitioners Guide to Graph Data. Via this link you can see all the questions and answers that we asked. You can check this book out. This book is more about graph databases, rather than graph learning.
275	Do you consider that by completing the homework of this course we have a portfolio to apply for an MLE or AS role?	"Tim
I think a couple people asked this in ML Zoomcamp Slack. I was thinking about this a little bit. I think it's sort of a philosophical thing [chuckles] where we think about models and Bento is sort of separate a little bit from code. So your project directory is where all the stuff that you'll commit to Git. I don't think we think that it's the right model to commit your models to Git, for example. We think of ourselves a little bit more like Docker. You don't have Docker images in your project directory, right? You just have your Docker file and that's what you commit to Git. In the same way, we store your models and your Bento in that dedicated directory, and then sort of leave it up to you to push it to different places where you might build or where you might need it
Alexey
Do you know why you decided to save it locally on your local file system? For example, what MLflow does – you can save it to S3 directly, but in the case of Bento, you always first save it to local storage and then you decide “This model I want to export, but for the rest, I don't care.”
Tim
Right. First, I think it’s because that's the easiest way to do it, but also because I think with BentoML, the workflow is that a lot of times, somebody's building a model on their laptop, or somebody's building a model in a slightly less distributed system and they're assembling their Bento there. So I think that the fastest way to access that model is locally. We do give you the ability to push that model and pull that model from other places. But ultimately, when you assemble your Bento and then you assemble your container, all that stuff has to be there. I don't think we wanted separate abstractions for when a person is training and saving, versus when a person is building a Bento and pulling in the model and assembling the container. It's just the same abstraction, which is just a local repository which contains models.
Alexey 
Maybe you don't want to save every model every single time you change one single parameter to the cloud. [Tim agrees] Maybe at the end, after all the experiments, you want to save the final one. Meanwhile, for MLflow, the use case is different – you actually want to save every single experiment because you want to see the results of these experiments.
Tim
Right."
276	Do you consider that by completing the homework of this course we have a portfolio to apply for an MLE or AS role?	"That's also a very generic question. It depends on what kind of data you have. Can you take a sample from this data or do you need to use the entire dataset? Also, as I said, what kind of data is it? Is it images? Is it text? Is it tables? I would just read a part of this. I actually don't know if it's possible with the HDF format. Maybe you can just read the first couple of thousand rows? I don't know. The best practices could be, let's say – the first one is to take a sample from this. 
The second one is maybe seeing how you can parallelize it with something like Spark. Then again, I don't know exactly what HDF format is and if you can read with Spark. Maybe not. Maybe you will need to first turn it into something else and then read it with something like Spark. Then the third one would be to get a big machine with 500 megs of RAM, for example, and then just read it from there. The last option seems to be the easiest one if you actually need to read all the data. In some cases you might not. Is it related to neural networks? I think some of the data was saved in this format. 
I might be wrong, but I think you might iterate over this data and then read it in chunks. You don't need to read the entire thing. You just read one gigabyte at a time, for example. That's another strategy - chunking. This is actually something that Spark relies on. When dealing with large datasets, Spark chunks the dataset into small partitions, and then it processes each partition separately. Then it combines the result at the end."
277	Do you consider that by completing the homework of this course we have a portfolio to apply for an MLE or AS role?	I don't know what AS is – applied scientist? Yes and no. I would, again, put emphasis on projects. Course homework is okay too. When looking at someone’s portfolio and I see that this is a homework project, I would ask myself, “How much of this was guided? How much of this did the candidate actually do themselves?” With projects it’s different. Projects are usually more independent. Thus, with projects I will have a lot less questions for you. Homework assignments are fine too, but they are more meant to make sure that you understand the material. They are good for you, but… maybe it makes sense to include them in your portfolio as well. But do projects – that's much better.
278	Do you consider that by completing the homework of this course we have a portfolio to apply for an MLE or AS role?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
279	Do you consider that by completing the homework of this course we have a portfolio to apply for an MLE or AS role?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
280	Do you use deep learning often at your work?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
281	Do you use deep learning often at your work?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
282	Do you use deep learning often at your work?	Yeah. If it works for you, use it. I don't mind.
283	Do you use deep learning often at your work?	I do sometimes. Not very often these days, but we have quite a few models that require deep learning. Usually, these models are related to images. We have some images on the platform, so when we need to understand what's happening on these images, we use deep learning.
284	Do you use deep learning often at your work?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
285	With local interfaces of Prefect, the flows seem to use up a lot of space in ~prefect/storage. Is there a good way to manage this space usage?	"Jeff
Cleaning up resources on your computer is something that we don't have a direct thing that we do with Prefect, I don't believe. But you could have a script that is scheduled by a Prefect to run and clean out anything that's sitting someplace after a certain amount of time, for example. That's one option if you're saving a lot of results to your local file. If you want to see more about results, and just how they work, I encourage you to check out the data here. 
You can see information about persisting results and caching – this is one thing that came up on the homework. I know that because earlier in the class, we used a little bit of caching with Kalise. Sometimes, they need people to try to run something in Docker and wouldn't be able to access the cache because it was on your local machine. There are ways to clear that cache. You can refresh that now with a new version of Prefect itself. And you can read about that in the tasks. There are a number of strategies to do that and there's some information in the FAQ and in Slack about that. Refreshing the cache starts with 2.7.8, so make sure you have Prefect 2.7.8 or newer installed."
286	With local interfaces of Prefect, the flows seem to use up a lot of space in ~prefect/storage. Is there a good way to manage this space usage?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
287	With local interfaces of Prefect, the flows seem to use up a lot of space in ~prefect/storage. Is there a good way to manage this space usage?	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
288	With local interfaces of Prefect, the flows seem to use up a lot of space in ~prefect/storage. Is there a good way to manage this space usage?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
289	With local interfaces of Prefect, the flows seem to use up a lot of space in ~prefect/storage. Is there a good way to manage this space usage?	No, you cannot.
290	Are the videos on the GitHub page from last year relevant for us?	"Alexey
No. I mean, we kind of told you. Yes, you maybe didn't see the intro video. But, sorry. No, it's late. We will actually recalculate the points for the second homework (HW 1B) because in the forum, it said that you can get up to one point, but we calculated that the cap there was seven. We are going to recalculate that. Thus, people who got a lot of points will get just one. Sorry about that. But don't focus on points. If you go to the leaderboard, we know that the person with this hash (722366d8b29ece9be3a7605363562c7c60d6918e) did seven links. Do you know who that is? Probably you don't. 
These points are virtual. It's nice to have a good score, but remember that these points mean whatever you want them to mean – not more than not less. Maybe it's okay if you get a few points less than others for the first homework, but the important thing here is consistency. If you just do your homework, even without doing any Learning in Public stuff, which I think you should do (you should post about the course). If you don't do this and you just do all the homework, by the end, you will be somewhere in the top 20. If you look at the leaderboard from the previous iteration, you will see that not everyone stuck around till the end. 
If you just do your homework, don't worry about these points, and you'll be fine. Sorry about that. I know that it can be discouraging that you didn't get as many points as others, but don't worry about that. Focus on learning. Maybe Michael has some other suggestions or some other things that I didn't mention, because he took part last year. But I think he was one of the people who did a few posts every week, right?
Michael
Yes. Actually, when I went through it, I missed the whole… you can get up to seven points, but I was just doing one every week and I just stuck with that. I was still near the top. I think the real value in that isn't necessarily the points but just being comfortable sharing – writing articles and making videos. That's the real value there. But everyone likes the clout value, too. As Alexey said, it will fluctuate, so if you keep up with it going forward, you'll probably be just fine."
291	Are the videos on the GitHub page from last year relevant for us?	Yes, they are. Most of the videos are still the same. There is no point in re-recording them
292	Are the videos on the GitHub page from last year relevant for us?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
293	Are the videos on the GitHub page from last year relevant for us?	No, you cannot.
294	Are the videos on the GitHub page from last year relevant for us?	Building projects. Maybe build another one, where instead of batch, you use streaming, maybe. If you're interested in a particular employer, try to build a project that is similar to what they’re working on. For example, if you want to apply for Spotify, try to find some music data and build a project for that.
295	I joined the course late. I'm still struggling with Docker installation on Windows 10.	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
296	I joined the course late. I'm still struggling with Docker installation on Windows 10.	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
297	I joined the course late. I'm still struggling with Docker installation on Windows 10.	Oh, you did not join the course late (Week 1). Late would be like week 6. You're not the first one who says that they’re struggling with Docker. Windows and Mac OS for Docker are often problematic. Linux is the easiest way. If you’re still struggling with it, go with a GCP VM.
298	I joined the course late. I'm still struggling with Docker installation on Windows 10.	You will not be able to do this because they require a credit card.
299	I joined the course late. I'm still struggling with Docker installation on Windows 10.	"Alexey
No. I mean, we kind of told you. Yes, you maybe didn't see the intro video. But, sorry. No, it's late. We will actually recalculate the points for the second homework (HW 1B) because in the forum, it said that you can get up to one point, but we calculated that the cap there was seven. We are going to recalculate that. Thus, people who got a lot of points will get just one. Sorry about that. But don't focus on points. If you go to the leaderboard, we know that the person with this hash (722366d8b29ece9be3a7605363562c7c60d6918e) did seven links. Do you know who that is? Probably you don't. 
These points are virtual. It's nice to have a good score, but remember that these points mean whatever you want them to mean – not more than not less. Maybe it's okay if you get a few points less than others for the first homework, but the important thing here is consistency. If you just do your homework, even without doing any Learning in Public stuff, which I think you should do (you should post about the course). If you don't do this and you just do all the homework, by the end, you will be somewhere in the top 20. If you look at the leaderboard from the previous iteration, you will see that not everyone stuck around till the end. 
If you just do your homework, don't worry about these points, and you'll be fine. Sorry about that. I know that it can be discouraging that you didn't get as many points as others, but don't worry about that. Focus on learning. Maybe Michael has some other suggestions or some other things that I didn't mention, because he took part last year. But I think he was one of the people who did a few posts every week, right?
Michael
Yes. Actually, when I went through it, I missed the whole… you can get up to seven points, but I was just doing one every week and I just stuck with that. I was still near the top. I think the real value in that isn't necessarily the points but just being comfortable sharing – writing articles and making videos. That's the real value there. But everyone likes the clout value, too. As Alexey said, it will fluctuate, so if you keep up with it going forward, you'll probably be just fine."
300	What are the advantages and disadvantages of BigQuery and AWS Athena?	BigQuery is a data warehouse and it's optimized. It's usually faster. AWS Athena is a data lake. You will see the difference in week 2 – there is a video that explains what a data lake is. Then in week 3, you will see what the data warehouse is. But AWS Athena is more like a data lake. You can still run all these queries, but maybe they will be slower and I also think they will be cheaper. For analytical queries that – for queries where you want to get results quickly, I usually use BigQuery. Again, like these are two different clouds, two different technologies. The counterpart of BigQuery would be Redshift. But I think Redshift is slower than BigQuery as well.
301	What are the advantages and disadvantages of BigQuery and AWS Athena?	Everything you do here is individual. You don't form teams.
302	What are the advantages and disadvantages of BigQuery and AWS Athena?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
303	What are the advantages and disadvantages of BigQuery and AWS Athena?	No. From what I understood, from what Ankush said, you don't need to know Java.
304	What are the advantages and disadvantages of BigQuery and AWS Athena?	"Alexey
No. I mean, we kind of told you. Yes, you maybe didn't see the intro video. But, sorry. No, it's late. We will actually recalculate the points for the second homework (HW 1B) because in the forum, it said that you can get up to one point, but we calculated that the cap there was seven. We are going to recalculate that. Thus, people who got a lot of points will get just one. Sorry about that. But don't focus on points. If you go to the leaderboard, we know that the person with this hash (722366d8b29ece9be3a7605363562c7c60d6918e) did seven links. Do you know who that is? Probably you don't. 
These points are virtual. It's nice to have a good score, but remember that these points mean whatever you want them to mean – not more than not less. Maybe it's okay if you get a few points less than others for the first homework, but the important thing here is consistency. If you just do your homework, even without doing any Learning in Public stuff, which I think you should do (you should post about the course). If you don't do this and you just do all the homework, by the end, you will be somewhere in the top 20. If you look at the leaderboard from the previous iteration, you will see that not everyone stuck around till the end. 
If you just do your homework, don't worry about these points, and you'll be fine. Sorry about that. I know that it can be discouraging that you didn't get as many points as others, but don't worry about that. Focus on learning. Maybe Michael has some other suggestions or some other things that I didn't mention, because he took part last year. But I think he was one of the people who did a few posts every week, right?
Michael
Yes. Actually, when I went through it, I missed the whole… you can get up to seven points, but I was just doing one every week and I just stuck with that. I was still near the top. I think the real value in that isn't necessarily the points but just being comfortable sharing – writing articles and making videos. That's the real value there. But everyone likes the clout value, too. As Alexey said, it will fluctuate, so if you keep up with it going forward, you'll probably be just fine."
305	Will there be any mention of CI/CD?	Mention, yes, but we will not actually cover it. If you're interested in CI/CD, we have another course which is the MLOps course where we'll cover it. The course will start again in May. This is the “best practices” one. In best practices, we have things like best coding practices, as well as infrastructures, code, and CI/CD. You can check it out. Here, the focus is on one of the things we cover in the course. So just taking these modules by themselves alone, I'm not sure how useful that is. But if you take the course, it should be useful. However, we don't cover CI/CD in this course (DE Zoomcamp).
306	Will there be any mention of CI/CD?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
307	Will there be any mention of CI/CD?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
308	Will there be any mention of CI/CD?	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
309	Will there be any mention of CI/CD?	No, but you can suggest some stuff in Slack. If we see that this is useful, we can also include this in the project repo
310	In the exercises, you make us copy data locally and later upload to GCS/BigQuery. In an actual production environment, what would be the real approach?	"Alexey
In an actual production environment, usually you have some processes that generate data, you capture this data and you put them in the data lake. Then the transformations you do take the data that you already have from the data lake and put it again in a data lake or in a data warehouse. In this case, the data lake is GCS and the data warehouse is BigQuery. This is what it typically looks like."
311	In the exercises, you make us copy data locally and later upload to GCS/BigQuery. In an actual production environment, what would be the real approach?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
312	In the exercises, you make us copy data locally and later upload to GCS/BigQuery. In an actual production environment, what would be the real approach?	"Jeff
I can try. I like Black a lot. If you just Google “Python Black” and go to the GitHub readme, it’s become super popular in the last couple of years for automatically formatting your code. It is really nice. Mine is set up in Visual Studio Code, so when I hit “save,” it automatically formats. As it says, it’s an uncompromising code formatter. I think it says here about asking Henry Ford for color back in the day. Is that right? It's colored cars or color telephones from Alexander Graham Bell or somebody, and it's like, “They can have any color they want as long as it's black.” He wasn't going to compromise. You don't have to think about it now. It's easy. Just go right ahead and get your gear code formatted automatically. So it’s things like two lines after function or before – that kind of stuff just happens automatically with it. Getting it set up in VS Code can be a little tricky sometimes. But there are guides to that. Googling is what I do for that, usually.
Alexey
I just wanted to do a shameless plug, because we have another course called MLOps Zoomcamp. By the way, there is also a Prefect model there. One of the things there is best practices. In best practices, we have this video called Code quality: linting and formatting. It does not show how to integrate Black with VS Code, I think. I don't remember if we do this. But if you want to learn more about testing, and Black, and other things like pre-commit hooks, make files, and so on – you can check this out.
Jeff
Yeah, it's not too tricky to set up. It's like pip install a package in your environment and then you have to give it a path maybe or set one setting in VS code. If all goes well, cross your fingers, it should just work after you reload it. Not always the case. If you have trouble, a lot of people use it, so there are a lot of good resources online. Black is great. This is an awesome set of some resources here on best practices. Type hints are just getting more and more popular. They're very helpful, so that people know what kind – especially with autocomplete and little pop-up type – things like that in Visual Studio Code and other code editors. You can see what kind of argument type you should put in and then Prefect uses that information as well, in our flows, for example, to make sure that if it's a block in the UI or a parameter in the UI – it'll be smart. It'll be like, “Oh, is this a number? Okay.” It will give you options to put in numbers. “Is this a different kind of form field?” It'll have different options. It also can then do some validation to make sure that people actually put in something that conforms to that type-in. Python is slowly getting more and more smart about how it handles typing and newer versions keep adding more functionality. Type hints are nice to use. It takes a little bit of writing, but it makes your docstring shorter. The last thing that was asked about here was docstrings. It's great to have in every function to tell people what it's about. It's something that maybe you don't always do if you're in a hurry, but you should do it, especially if other people are going to read the code. Code is read like 20 times more often than it's written, or however you translate that – some stat. So do it. It's so helpful for you in the future and it's helpful for other people in the future, who are going to read your code to see, “What were you thinking? What is the purpose of this function?” Keep your function small, explain it in your doc string – it's good stuff. Then it shows up in your code editor, if you're lucky (if you have a good code editor). That's all to say about that.
Alexey
Do you know any resources where people can learn about setting up? Or learning more about these things, like good Python coding standards? What I showed is obviously a good resource, but it does not cover all these things that this question asks about.
Jeff
It's a good question. I do have a link to Google Style, or there are a couple different styles of docstrings. It seems like they're a little bit much these days, maybe. But there are links for different ways to do type hinting. I do have a few things if I look around for them. I don't have them at my fingertips right now. But Michael looks like maybe he's got one he shared there.
Michael
Yes, this one is a little bit older, but it is great. It goes into using virtual environments, Poetry… there's a lot to unpack, but I think that's still pretty much the standard best practice at the moment."
313	In the exercises, you make us copy data locally and later upload to GCS/BigQuery. In an actual production environment, what would be the real approach?	Yeah. Last year, we needed at least 16 GB of RAM to run Airflow. Here, we don't use Airflow, so probably 8 GB of RAM should be enough. But I will still go with a VM – I would still take 16 GB of RAM. It will just be better. Let's say you have a laptop with 8 GB of RAM and you already have Chrome running there and VSL and other things, there will really be no space for running stuff.
314	In the exercises, you make us copy data locally and later upload to GCS/BigQuery. In an actual production environment, what would be the real approach?	No, you cannot.
315	Many did not know about one point per post for Learning in Public and many lost 12 points since two homework assignments were scored together. Can you please have ways to gain those lost points?	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
316	Many did not know about one point per post for Learning in Public and many lost 12 points since two homework assignments were scored together. Can you please have ways to gain those lost points?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
317	Many did not know about one point per post for Learning in Public and many lost 12 points since two homework assignments were scored together. Can you please have ways to gain those lost points?	Everything you do here is individual. You don't form teams.
318	Many did not know about one point per post for Learning in Public and many lost 12 points since two homework assignments were scored together. Can you please have ways to gain those lost points?	"Alexey
No. I mean, we kind of told you. Yes, you maybe didn't see the intro video. But, sorry. No, it's late. We will actually recalculate the points for the second homework (HW 1B) because in the forum, it said that you can get up to one point, but we calculated that the cap there was seven. We are going to recalculate that. Thus, people who got a lot of points will get just one. Sorry about that. But don't focus on points. If you go to the leaderboard, we know that the person with this hash (722366d8b29ece9be3a7605363562c7c60d6918e) did seven links. Do you know who that is? Probably you don't. 
These points are virtual. It's nice to have a good score, but remember that these points mean whatever you want them to mean – not more than not less. Maybe it's okay if you get a few points less than others for the first homework, but the important thing here is consistency. If you just do your homework, even without doing any Learning in Public stuff, which I think you should do (you should post about the course). If you don't do this and you just do all the homework, by the end, you will be somewhere in the top 20. If you look at the leaderboard from the previous iteration, you will see that not everyone stuck around till the end. 
If you just do your homework, don't worry about these points, and you'll be fine. Sorry about that. I know that it can be discouraging that you didn't get as many points as others, but don't worry about that. Focus on learning. Maybe Michael has some other suggestions or some other things that I didn't mention, because he took part last year. But I think he was one of the people who did a few posts every week, right?
Michael
Yes. Actually, when I went through it, I missed the whole… you can get up to seven points, but I was just doing one every week and I just stuck with that. I was still near the top. I think the real value in that isn't necessarily the points but just being comfortable sharing – writing articles and making videos. That's the real value there. But everyone likes the clout value, too. As Alexey said, it will fluctuate, so if you keep up with it going forward, you'll probably be just fine."
319	Many did not know about one point per post for Learning in Public and many lost 12 points since two homework assignments were scored together. Can you please have ways to gain those lost points?	No, you cannot.
320	Will there be a step-by-step solution to question 4 in week 2?	"There are lots of different varieties of approaches here. One thing that you might do is use some kind of data ingestion tool, as I mentioned in the live FAQ discussion – something like Airbyte or Fivetran – if you have lots of data that you're ingesting, that can be a common way to stay on top of that. Or you could do it through a script, kind of like we do here. 
But what you might do is run your agent on a virtual environment in Google Cloud VM, for example, or in AWS, or somewhere else. You could run it in Kubernetes. There are a number of places where maybe you would have that running and have your infrastructure there, so that you were just pulling down the code not to your local machine, but it would just be in your VM. 
If you're actually going and using Pandas, you're putting the data into memory, so you do need to have some scripts that will do that. And you can go directly between Google Cloud Storage and BigQuery with the Prefect GCP library. So that Prefect GCP library that you installed, that you've been using, does have some options in there. I think it's in the BigQuery module to actually get data directly into BigQuery from GCS. So there are a lot of options there. Check out Prefect GCP to see how to do it."
321	Will there be a step-by-step solution to question 4 in week 2?	"Alexey
YouTube channel, right?
Jeff
I'm teaching a two day course on Prefect next week in New York. If anyone's around New York – it's an in person course – you can come and join that. We have a couple in a few other places, too. Coming to London, Washington DC, and San Francisco are on the books. We don't yet have a remote one, but maybe we'll do something in the future on that. Let us know if that would be of interest to you. You can just reach out on Slack. That's the short answer. We don't have a book, although we've talked about working on it. The thing is, we are rapidly improving things, making changes. Just today in Slack chat, I was sharing with people that we have a new Timeline view. Since I recorded these videos, like 3-4 weeks ago, and now we have some new visualizations – we have all kinds of new features. We're still having lots and lots of improvements and updates and exciting things happening, so we need a little bit more standardization there to do something like a book. But that's the short answer. If you want to come join me, we can spend a couple of days together working on it.
Alexey
So I know that you also have people in Berlin, so any chance you will have workshops in Berlin too?
Jeff
I think there's definitely a chance there. Anna, who you see in Slack is in Berlin and we've had a couple people asking about that. So if that's something that's of interest to people, do definitely reach out."
322	Will there be a step-by-step solution to question 4 in week 2?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
323	Will there be a step-by-step solution to question 4 in week 2?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
324	Will there be a step-by-step solution to question 4 in week 2?	"Jeff
Yeah, I got the video recorded already. You just can't see it. We will release it after the deadline."
325	If somebody wants to contribute to BentoML and help with SnapML support or whatever it is they want to help with, they just go there and say, “Okay, I want to take this feature.”? What does the process look like? Maybe they should first join the Slack community you have and then say, “Okay, I want to contribute. This looks like a good issue. Should I take this?”	"I don't know how to answer that, because I work as a principal data scientist. I work on pretty much everything that the company needs me to work with. This means that all the models are related, to some extent, to online classifieds – to online marketplaces. I need to deal more with domain expertise, maybe, rather than ML knowledge. I guess that's the answer. 
We have a lot of different use cases – we have recommender systems, we have search, we have moderation. We actually have an article about this. For example, Learning-To-Rank: Sorting Search Results. My colleague wrote this one. I think we have an article about data science, Data Science at OLX which happens to be written by me. You can see what kind of use cases we solve. Maybe it could be interesting for you."
326	If somebody wants to contribute to BentoML and help with SnapML support or whatever it is they want to help with, they just go there and say, “Okay, I want to take this feature.”? What does the process look like? Maybe they should first join the Slack community you have and then say, “Okay, I want to contribute. This looks like a good issue. Should I take this?”	"Tim
I think the way that it works is that you can always join the Slack community and ask. That's usually the best way to get in touch with one of us. Joining the community is always helpful. Anybody there can point you in the right direction. Then, depending on what you want to contribute, we'll kind of walk you through how to structure it. A lot of the time, people will need certain features that we don't have and we'll go ahead and fork the repository and open PRs themselves. That's typically the fastest way to contribute. If you need something, then you build it yourself and then open the PR, and then we'll review it and probably have a few comments and then commit it 
Alexey
A word of caution – sometimes just opening a PR without talking to any of the maintainers first may lead to frustration and a lack of understanding of why a PR was not accepted, like “Hey! I spent two weeks of my time contributing and they don't want to accept my project. I'm not going to contribute to open source anymore in my life.” This has happened to me. That was frustrating. What I learned from that experience is that, first, it's best to talk to the maintainers and ask how exactly they want this feature to be implemented. The way you might want to implement may not be the same way the maintainers see this feature being implemented. So it's worth discussing first. 
Tim
That's definitely true, Alexey. I think most of the features out there, we've probably heard of, and we already have some thoughts on how we want to do it. Also, it's likely that we've seen a lot more edge cases as well. That's why it's probably a good idea to run your approach past us before you start coding it. That’s if you want to contribute. If you don't want to contribute and you just want to solve the problem for yourself, you can do it however you want."
327	If somebody wants to contribute to BentoML and help with SnapML support or whatever it is they want to help with, they just go there and say, “Okay, I want to take this feature.”? What does the process look like? Maybe they should first join the Slack community you have and then say, “Okay, I want to contribute. This looks like a good issue. Should I take this?”	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
328	If somebody wants to contribute to BentoML and help with SnapML support or whatever it is they want to help with, they just go there and say, “Okay, I want to take this feature.”? What does the process look like? Maybe they should first join the Slack community you have and then say, “Okay, I want to contribute. This looks like a good issue. Should I take this?”	"Alexey
It is quite important. I wouldn't say it's a must for all the projects. For example, if you use something like a tree-based model, then you shouldn't worry about scaling. I think I shared links to our Office Hours from the previous year. If you go to cohorts, and the previous year, and then you go to midterm project – in week ten, we show you how to use SciKit Learn pipelines. This is an amazing concept. So far, what we've been doing is Dictionary Vectorizer (dict_vectorizer) and then it was followed by a model. In the case of Bento, we needed to save this Dictionary Vectorizer separately, as an artifact (as a binary) and then we needed to load this and then we needed to apply the model. But with a pipeline, you can actually train a pipeline and just use BentoML to save the pipeline. It can be saved, right Tim? You can just take a pipeline and do BentoML.sklean.save_model and then the entire pipeline will be saved. Right? [Tim agrees] This is a cool thing. I don't have an example, but you can include this feature scaler here in your pipeline. There should be an example somewhere. I think we did that last year. There is an example without the pipeline. It should be here – classification, and then notebooks scaling this one. In this notebook, we show how to scale your features with standard scaling, or min/max scaler. If you check Office Hours #3 from last year I spoke more about that. It is an important topic. We intentionally did not include this because we cannot include everything, but this is an important topic. I recommend that you check it out. And using pipelines is cool because you will not need to save all this Scaler, Dictionary Vectorizer – all these things separately. Let's say with Bento, you can just take one pipeline, save it with Bento, and then you will have one binary on model. It makes things easier.
Tim 
And if you do have any kind of custom Tokenizer, or Scaler, or Vectorizer – you can always add it to that “custom objects” field.
Alexey
We talked about MLflow. It's very annoying, to be honest. With MLflow, I need to choose to save this in a file. Then I have a Pickle file and I tell MLflow, “Okay, there is an artifact that is already in this file that I want to upload together with the model.” So I upload it together with the model and then when I want to use it, I need to download this from the registry, then I need again to use Pickle to load this into the memory. There is too much overhead. With Bento, I really like how easy it is to save extra stuff.
Tim
Right. In our first version of BentoML, we were just saving models but then we had so many users saying “Well, how do I save my Tokenizer along with the model?” A lot of the time, the Tokenizer, the Vectorizer, is sort of one-to-one with the model. You may have a version of that as well. So it's nice to be able to version it with the model."
329	If somebody wants to contribute to BentoML and help with SnapML support or whatever it is they want to help with, they just go there and say, “Okay, I want to take this feature.”? What does the process look like? Maybe they should first join the Slack community you have and then say, “Okay, I want to contribute. This looks like a good issue. Should I take this?”	I guess not. That's the process of learning? Maybe more senior people don't… Yeah, I see where you're going with this. Senior people are not supposed to go bankrupt and default. They are probably supposed to pay back the debt. I guess these are the particularities of this dataset, I assume. Indeed, it's probably a data issue ,or not an issue, but a “characteristic” of this dataset.
330	Can you suggest some theoretical machine learning course that is inclined with this course?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
331	Can you suggest some theoretical machine learning course that is inclined with this course?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
332	Can you suggest some theoretical machine learning course that is inclined with this course?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
333	Can you suggest some theoretical machine learning course that is inclined with this course?	I don't know what inclined here means. But I think I already gave a recommendation – I think these two machine learning by Andrew Ng and this other one are quite orthogonal. They focus on different things.
334	Can you suggest some theoretical machine learning course that is inclined with this course?	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
335	Looking at a DE portfolio, would every project be an own GCP project (i.e. a new VM for every project) or could one GCP project hold several IRL project setups?	No, you cannot.
336	Looking at a DE portfolio, would every project be an own GCP project (i.e. a new VM for every project) or could one GCP project hold several IRL project setups?	Yeah, I think one project can host several things. I don't know exactly, to be honest. I think there is probably an official recommendation from GCP. Check that out.
337	Looking at a DE portfolio, would every project be an own GCP project (i.e. a new VM for every project) or could one GCP project hold several IRL project setups?	"Jeff
I'm not sure exactly what's being referenced here. But I'm guessing maybe the GitHub question from the homework – the GitHub block question turned out to be a little trickier than anticipated for folks. This is a great question. I got a couple of things on it here. One is that there is some stuff in the documentation, but maybe not a complete handhold of things. The storage doc does talk about code storage and flow storage in general here, and a little bit about how to do it. 
If you would like more detailed information, it’s always a great idea to look and think about me opening an issue. If you go to the GitHub repository for Prefect, you can click on “new issue”. If you want, you can propose a feature enhancement, if you want to call it that, or report a bug if you think you should have more information, and then put in what you would like to see. 
If you want to go one step further at something I really recommend, even like the next extension of Learning in Public is contributing to projects. This is an open source project here. You're welcome to contribute to it. You could go ahead and fork the repository, clone it down, and make a change to the documentation once you've figured things out there. Lots of lots of folks I know did get there with some support, through the FAQ and through Slack. It's partly what the videos are also for, in the course, to help you out as you're going. That's what I would suggest there. 
But I will also suggest to our docs folks, that we perhaps enhance the GitHub section in particular."
338	Looking at a DE portfolio, would every project be an own GCP project (i.e. a new VM for every project) or could one GCP project hold several IRL project setups?	"Ankush
I think if it's about finding a job, spend time learning AWS, because… come on – everybody's using AWS and you have a higher probability of finding a company which will have AWS. After that choose GCS or Azure or whatever you feel. But I think the main point is to learn the concepts. For example, let's talk about data warehouses for partitioning and clustering. In Prefect, it’s how to create your DAGs in particular structure or paralyzing your flow. I think if you learn this concept, it doesn't really matter which cloud platform you use at the end of the day, because all of their services will be pretty similar. I think once you grab the basic concepts and your base layer is good, it's very easy to adjust to both of the cloud environments. I have personally worked on AWS and Google Cloud. I've also worked a bit with Snowflake. Once you can get the ideas, or the concepts, it's really easy to make that switch.
Alexey
I'll add that my experience with Cloud is mostly AWS. Not mostly, I think, it’s only AWS. For this course, I used GCP for the first time and it wasn't that difficult. The UI is different. I actually think GCP has a better UI, in my opinion. It’s more intuitive. The only thing I needed to figure out was, “In AWS this thing is called this way. How was it called in GCP?” Then I would just Google it, find it, and then just use it. Most of the time, it was like that.
Ankush
I have a follow up question. What if ChatGPT is only integrated in Azure? Then what do we do? 
Alexey
Well… I'll have to use Azure. [chuckles]
Ankush
So we’ll need to migrate? [laughs]"
339	Looking at a DE portfolio, would every project be an own GCP project (i.e. a new VM for every project) or could one GCP project hold several IRL project setups?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
340	If anyone wants to contribute to any open source project like Prefect, where do they actually start? I don't understand how Prefect’s source code works at the moment.	"Alexey
You pick a project where you want to contribute and usually, they have some contributing guide. If you don't know which project to pick, we have a thing called Open Source Spotlight in our YouTube channel. Here, we invite different open source authors to talk about their tools. The tools are very different. They're not all data engineering-related or machine learning engineering-related. They're all different. You can perhaps go through the tools and if you like any of these tools, usually one of the questions I ask the author is “How can people contribute?” In each video, you will find out how you can contribute to this specific tool."
341	If anyone wants to contribute to any open source project like Prefect, where do they actually start? I don't understand how Prefect’s source code works at the moment.	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
342	If anyone wants to contribute to any open source project like Prefect, where do they actually start? I don't understand how Prefect’s source code works at the moment.	I would say to focus on projects, not on tools. In this course, you will get a chance to do this. If you open a job description for data engineers, you will see a lot, a lot, a lot of different technologies. Maybe think of a project that you can do for this particular company and do this project using, for example, the tools you learn here or the tools you want to learn, and build a portfolio. I guess this is more of a tip for getting a data engineering job, rather than for somebody who's starting their data engineering career. But if you already have a job, then maybe think of the business impact of what you need. I think I will not spend too much time talking about that because this requires a one hour long discussion. Perhaps we should have a podcast interview about that. If you can think of any potential guests for this podcast interview, you can let me know and we can try to invite these people and talk about this in more detail. One thing I want to share with you is that in our YouTube channel, we had a few interesting talks. We had this Getting a Data Engineering Job with Jeff talk. It’s a very nice presentation. Then we also had an interview with Jeff. So these two podcasts are quite interesting. You can also check out other ones. I think you'll find a lot of interesting content here.
343	If anyone wants to contribute to any open source project like Prefect, where do they actually start? I don't understand how Prefect’s source code works at the moment.	"Alexey
The first thing about the dataset – what kind of dataset do you want to use? Or what kind of problem do you want to solve? Once you figure this out, then you're basically ready to start working on a project. Then in the project, you need to decide if you want to do streaming or batch. For batch, it's using things like Prefect, Spark, or DBT. For streaming, it’s using the materials from the last lecture (week 2). Once you decide that, you will just implement this and you will find all the information you need here in the week 7 project repo. Just go through this and if you have any questions left, let us know. Keep in mind that these are the criteria that other people (your peers) will use when evaluating your project. Perhaps you can already think about that and how you want to implement your project in such a way that you maximize the score you get from these criteria."
344	If anyone wants to contribute to any open source project like Prefect, where do they actually start? I don't understand how Prefect’s source code works at the moment.	No, you cannot.
345	In one of your podcasts, I heard the phrase “Converting a business problem into a machine learning problem.” Can you give us two brief examples to understand what it means?	That's a very similar question to when we should use the lambda and when we should use Kubernetes. If you want to use Kubernetes, then go with TensorFlow Serving. Otherwise, TensorFlow Lite. There is one thing where you have to use TensorFlow Lite – where you cannot use TensorFlow Serving – and this is when you want to deploy your model on a mobile device. I have a phone here. If you want to create an app with a model, then you will have to use TensorFlow Lite because you will not be able to use TensorFlow Serving. TensorFlow Serving also adds a bit of complexity. You will need to have this gateway service, you will need to have GRPC service, so that becomes more complex. If you have a lot of load, then you need to do this otherwise, TensorFlow Lite.
346	In one of your podcasts, I heard the phrase “Converting a business problem into a machine learning problem.” Can you give us two brief examples to understand what it means?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
347	In one of your podcasts, I heard the phrase “Converting a business problem into a machine learning problem.” Can you give us two brief examples to understand what it means?	"Imagine that you're running an ecommerce company – simple ecommerce. Our CEO comes and says, “Our users have big problems discovering items. How can we help them?” Then you need to understand “Okay, what does it mean ‘they have problems discovering items’?” You need to understand what problems they have and how we can help solve these problems. Once you understand the problems, then you can generate possible solutions. One example is coming up with a good search. Another solution could be coming up with good recommender systems. It also depends on a business goal. What do we want to optimize? Do we want to make sure that our users buy more or so that it's actually easier for them to find what they're looking for when they are just exploring? This is all input that comes from the business people – the stakeholders in the company – and the users. Taking all this input into account and thinking, “Okay, now I know what I need to work on to solve this problem. What can it look like?” And the solution could be designing or coming up with a recommender system that is on the item page that shows similar items. Or, for example, showing what people buy in addition to this item. So that's one example. 
Another example. Let's say we have a video hosting coming – like YouTube. All of a sudden, people start uploading some content that we don't want to have on the platform. We want to get rid of this, right? Business comes to us, the data scientists or software engineers, and says, “Can we somehow make sure this doesn't happen?” And then we can think, “Okay, what are the ways that we can make the bad content go away?” It could be introducing a content moderation platform, for example, where we apply some different machine learning algorithms to a video in order to understand that this video should not be on the platform. Instead of a video, it can be anything. 
So this process of listening to the stakeholders, understanding what the problem is, and then up to the point where you have an idea of how this could be implemented – this is what I call “converting a business problem into a machine learning problem.” I hope it makes sense now."
348	In one of your podcasts, I heard the phrase “Converting a business problem into a machine learning problem.” Can you give us two brief examples to understand what it means?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
349	In one of your podcasts, I heard the phrase “Converting a business problem into a machine learning problem.” Can you give us two brief examples to understand what it means?	I guess not. That's the process of learning? Maybe more senior people don't… Yeah, I see where you're going with this. Senior people are not supposed to go bankrupt and default. They are probably supposed to pay back the debt. I guess these are the particularities of this dataset, I assume. Indeed, it's probably a data issue ,or not an issue, but a “characteristic” of this dataset.
350	How much hands-on expertise does a data engineer need in DevOps (DataOps)?	We’re still in progress (as of January 26, 2023). I thought we would do this earlier. But it's still not finished.
351	How much hands-on expertise does a data engineer need in DevOps (DataOps)?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
352	How much hands-on expertise does a data engineer need in DevOps (DataOps)?	"Ankush
Are you in a company which has a lot of DevOps (DataOps)? Then none – then you don't need much. But if you are working in a startup, I would say some DevOps knowledge (Terraform, Docker, how to fix Docker issues, how to go inside and see what's going on, logs) would be helpful, in general. Even if you are a backend developer or a software developer, this knowledge would be valuable in any case. If you're working in a very big company where there are thousands of developers with this kind of experience, then maybe you can rely on them more.

Alexey  
I think of companies like Zalando, or for example, OLX where I work, there is already a data platform. If I need to schedule a SQL query, I don't need to do much. I just write the SQL query, commit it to Git and then it just works. I don't need to do any Docker, Terraform, or anything. I'm not a data engineer, but this is how it works."
353	How much hands-on expertise does a data engineer need in DevOps (DataOps)?	No, you cannot.
354	How much hands-on expertise does a data engineer need in DevOps (DataOps)?	"Alexey
I don't know when this question was asked, but we did extend it."
355	Would you recommend doing Andrew Ng’s updated machine learning specialization after this course?	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
356	Would you recommend doing Andrew Ng’s updated machine learning specialization after this course?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
357	Would you recommend doing Andrew Ng’s updated machine learning specialization after this course?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
358	Would you recommend doing Andrew Ng’s updated machine learning specialization after this course?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
359	Would you recommend doing Andrew Ng’s updated machine learning specialization after this course?	Maybe. I haven't seen the updated machine learning specialization. I don't know what that is and what kind of content there is. I took the course back in 2012 (I think) when it was still using Okta. I do recommend taking this course. You still need to do interviews. Let's say you're looking for a job and your goal is to get a job – you start going to the interviews and you see what questions they ask. If these questions are about theory, then I think the best way to learn this theory – the theory that wasn't covered in this course, such as various bias trade-offs, gradient descent, we didn't cover many things – if you want to learn these things, then I don't have better recommendations than Andrew Ng’s course, at least the old one. But I think the updated one should be even better, right? That's why they updated it.
360	Could I replace BigQuery with Snowflake? What infrastructure is the most frequent to use in the real world? Or how does one decide in this course?	Contributions are welcome. If you know how to do this, then please create a guide and then share it with us. You can also create a pull request and include the link in the course repo.
361	Could I replace BigQuery with Snowflake? What infrastructure is the most frequent to use in the real world? Or how does one decide in this course?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
362	Could I replace BigQuery with Snowflake? What infrastructure is the most frequent to use in the real world? Or how does one decide in this course?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
363	Could I replace BigQuery with Snowflake? What infrastructure is the most frequent to use in the real world? Or how does one decide in this course?	"In this course, we just made it simpler for you – we use BigQuery because it's a part of Google Cloud Platform. When you register, you get free credits, so you don't need to think about this. In practice, I guess what’s really important here is that the concepts stay the same across different warehouses, whether it’s Redshift, BigQuery, Snowflake, Firebolt, or other stuff. The concepts are similar. 
BigQuery is quite popular, Snowflake is quite popular, so if you come into a company that uses a particular warehouse, just stick to that. If you're starting a new team and you need to decide, I don't have a good solution for you. You will need to really think about what you need, what kind of features there are available and then evaluate. The answer is – it depends. 
How to decide what to use in this course? In this course, just go with BigQuery. Play with BigQuery and that should be okay. That should be enough."
364	Could I replace BigQuery with Snowflake? What infrastructure is the most frequent to use in the real world? Or how does one decide in this course?	No, but you can suggest some stuff in Slack. If we see that this is useful, we can also include this in the project repo
365	Can we make the whole course on a Google Cloud VM?	No, you cannot.
366	Can we make the whole course on a Google Cloud VM?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
367	Can we make the whole course on a Google Cloud VM?	Yes, you can.
368	Can we make the whole course on a Google Cloud VM?	I would say to focus on projects, not on tools. In this course, you will get a chance to do this. If you open a job description for data engineers, you will see a lot, a lot, a lot of different technologies. Maybe think of a project that you can do for this particular company and do this project using, for example, the tools you learn here or the tools you want to learn, and build a portfolio. I guess this is more of a tip for getting a data engineering job, rather than for somebody who's starting their data engineering career. But if you already have a job, then maybe think of the business impact of what you need. I think I will not spend too much time talking about that because this requires a one hour long discussion. Perhaps we should have a podcast interview about that. If you can think of any potential guests for this podcast interview, you can let me know and we can try to invite these people and talk about this in more detail. One thing I want to share with you is that in our YouTube channel, we had a few interesting talks. We had this Getting a Data Engineering Job with Jeff talk. It’s a very nice presentation. Then we also had an interview with Jeff. So these two podcasts are quite interesting. You can also check out other ones. I think you'll find a lot of interesting content here.
369	Can we make the whole course on a Google Cloud VM?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
370	Apart from the docs, where can I learn more about Prefect? Any books or course suggestions?	"Alexey
YouTube channel, right?
Jeff
I'm teaching a two day course on Prefect next week in New York. If anyone's around New York – it's an in person course – you can come and join that. We have a couple in a few other places, too. Coming to London, Washington DC, and San Francisco are on the books. We don't yet have a remote one, but maybe we'll do something in the future on that. Let us know if that would be of interest to you. You can just reach out on Slack. That's the short answer. We don't have a book, although we've talked about working on it. The thing is, we are rapidly improving things, making changes. Just today in Slack chat, I was sharing with people that we have a new Timeline view. Since I recorded these videos, like 3-4 weeks ago, and now we have some new visualizations – we have all kinds of new features. We're still having lots and lots of improvements and updates and exciting things happening, so we need a little bit more standardization there to do something like a book. But that's the short answer. If you want to come join me, we can spend a couple of days together working on it.
Alexey
So I know that you also have people in Berlin, so any chance you will have workshops in Berlin too?
Jeff
I think there's definitely a chance there. Anna, who you see in Slack is in Berlin and we've had a couple people asking about that. So if that's something that's of interest to people, do definitely reach out."
371	Apart from the docs, where can I learn more about Prefect? Any books or course suggestions?	I would say to focus on projects, not on tools. In this course, you will get a chance to do this. If you open a job description for data engineers, you will see a lot, a lot, a lot of different technologies. Maybe think of a project that you can do for this particular company and do this project using, for example, the tools you learn here or the tools you want to learn, and build a portfolio. I guess this is more of a tip for getting a data engineering job, rather than for somebody who's starting their data engineering career. But if you already have a job, then maybe think of the business impact of what you need. I think I will not spend too much time talking about that because this requires a one hour long discussion. Perhaps we should have a podcast interview about that. If you can think of any potential guests for this podcast interview, you can let me know and we can try to invite these people and talk about this in more detail. One thing I want to share with you is that in our YouTube channel, we had a few interesting talks. We had this Getting a Data Engineering Job with Jeff talk. It’s a very nice presentation. Then we also had an interview with Jeff. So these two podcasts are quite interesting. You can also check out other ones. I think you'll find a lot of interesting content here.
372	Apart from the docs, where can I learn more about Prefect? Any books or course suggestions?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
373	Apart from the docs, where can I learn more about Prefect? Any books or course suggestions?	"Alexey
Yeah, we talked about that in Slack, so please check it. I think this happens because some of the columns have missing values and when you use Pandas for converting CSV to Parquet, it reads columns with missing values as double. That's why this happens."
374	Apart from the docs, where can I learn more about Prefect? Any books or course suggestions?	No, you cannot.
375	Is the course Linux friendly?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
376	Is the course Linux friendly?	"Alexey
No. I mean, we kind of told you. Yes, you maybe didn't see the intro video. But, sorry. No, it's late. We will actually recalculate the points for the second homework (HW 1B) because in the forum, it said that you can get up to one point, but we calculated that the cap there was seven. We are going to recalculate that. Thus, people who got a lot of points will get just one. Sorry about that. But don't focus on points. If you go to the leaderboard, we know that the person with this hash (722366d8b29ece9be3a7605363562c7c60d6918e) did seven links. Do you know who that is? Probably you don't. 
These points are virtual. It's nice to have a good score, but remember that these points mean whatever you want them to mean – not more than not less. Maybe it's okay if you get a few points less than others for the first homework, but the important thing here is consistency. If you just do your homework, even without doing any Learning in Public stuff, which I think you should do (you should post about the course). If you don't do this and you just do all the homework, by the end, you will be somewhere in the top 20. If you look at the leaderboard from the previous iteration, you will see that not everyone stuck around till the end. 
If you just do your homework, don't worry about these points, and you'll be fine. Sorry about that. I know that it can be discouraging that you didn't get as many points as others, but don't worry about that. Focus on learning. Maybe Michael has some other suggestions or some other things that I didn't mention, because he took part last year. But I think he was one of the people who did a few posts every week, right?
Michael
Yes. Actually, when I went through it, I missed the whole… you can get up to seven points, but I was just doing one every week and I just stuck with that. I was still near the top. I think the real value in that isn't necessarily the points but just being comfortable sharing – writing articles and making videos. That's the real value there. But everyone likes the clout value, too. As Alexey said, it will fluctuate, so if you keep up with it going forward, you'll probably be just fine."
377	Is the course Linux friendly?	It is. Linux is the recommended operational system. But MacOs and Windows will also work.
378	Is the course Linux friendly?	If it's a question for me, specifically – in Windows, I use a tool called Windows Terminal. This is the tool I use. In the settings, they have Git Bash, the usual command prompt, then they have multiple subsystems for Linux terminals. There also used to be PowerShell. I don't use PowerShell, so I don't have it here. This is what it looks like. I really like this. It’s just called Windows terminal. Check it out.
379	Is the course Linux friendly?	"Alexey
It's possible to apply to jobs right now. You don't have to take the course. Just start applying. Why do you need to take the course to start applying, right? It was possible to do it last year. Nobody is stopping you from doing this. But this course will give you some knowledge and add projects to your portfolio. That will be helpful in your job search. 
But I would encourage you to start applying to jobs right now to see what they actually want from you – what kind of topics come up, what kind of test assignments you get. You don't need to wait until the course is finished to start doing this. You can already learn a lot about the job market in your area or in the domain where you want to work.
Luis
I just want to add that when I did the 2022 cohort, I was in another company and the process of data engineering in that company was really similar to the Zoomcamp. What I started to notice was that the companies mostly use this architecture in engineering. So it's good for you to show yourself that you're starting to understand all this data engineering stuff. I think it's important.
Ankush
I just wanted to add. I don't know about applying for jobs, but once you have a job, if you want to get an increment, contact Michael. [chuckles]

Victoria
Michael moneymaker. [chuckles]"
380	Is it worth doing machine learning in R in 2022 or is it better to learn Python?	Yeah. If it works for you, use it. I don't mind.
381	Is it worth doing machine learning in R in 2022 or is it better to learn Python?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
382	Is it worth doing machine learning in R in 2022 or is it better to learn Python?	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
383	Is it worth doing machine learning in R in 2022 or is it better to learn Python?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
384	Is it worth doing machine learning in R in 2022 or is it better to learn Python?	Well, I think it's better to learn Python simply because it's more popular. There are still some problems where R is more suited to solve, like time series, some statistical stuff, but in general, Python is pretty versatile. When it comes to deployment, it's much easier. I would focus on Python and then if your work requires you to learn R, then do that. If nobody specifically asks you to learn R, then go with Python. It's not like if you don't know R, people will not hire you. They will still hire you – you will just need to learn R at work. It's not a bad language, it’s a good language. If you need to learn R because the company uses it, just learn it.
385	In real life, each environment has its own warehouse: dev, staging, prod? How to keep in sync? Any articles on a typical workday from feature branch to merge?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
386	In real life, each environment has its own warehouse: dev, staging, prod? How to keep in sync? Any articles on a typical workday from feature branch to merge?	No, you cannot.
387	In real life, each environment has its own warehouse: dev, staging, prod? How to keep in sync? Any articles on a typical workday from feature branch to merge?	"Victoria
I don't have a good article where it would explain this that I know. In my experience, this depends a lot on the company and the person setting up. I've worked in companies where we had completely separate accounts for each stage. We would have something like one for development, one completely separate account for something like QA, and one completely different for prod, and then we would have something like a cloning production into our QA environment and our dev environment, for example. 
But normally, the architectures I always see work with DBT are – you would have one warehouse where you'd have multiple data sets (as it’s called in BigQuery). In others, like Postgres, Redshift, and Snowflake, it's called schemas. Or you'd have one warehouse with separate databases. Then you could use something that all of the data warehouses tend to have, which is some way of doing tasks, where you can copy. For example, doing something like copy clients, where you can move the data from production to dev. 
Another topic that is super related to this question, in my opinion, is CI/CD. We didn't go through CI. I would recommend this blog as well. Here, what you could also do is all merge. You would have normally open a pull request and you would have in there something like a CI check that would make sure that the code that you're trying to merge works. Then you could run something like a DBT Cloud or merge, like a shove in here or on a pull request. That is also another way where all of these stages would communicate."
388	In real life, each environment has its own warehouse: dev, staging, prod? How to keep in sync? Any articles on a typical workday from feature branch to merge?	Well, what you can always do is create a new account. Don't forget to unlink your credit card if you want to use the same credit card. Unlink it from the billing from the old account, and then link it with a new one. Or if you have a different credit card, then you can just use that. For example, I use Revolut as a bank and in Revolut, you can create one-time virtual cards. That could be an option, too.
389	In real life, each environment has its own warehouse: dev, staging, prod? How to keep in sync? Any articles on a typical workday from feature branch to merge?	"Alexey
Yeah, we talked about that in Slack, so please check it. I think this happens because some of the columns have missing values and when you use Pandas for converting CSV to Parquet, it reads columns with missing values as double. That's why this happens."
390	How long is the extension for Homework 8?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
391	How long is the extension for Homework 8?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
392	How long is the extension for Homework 8?	That's a very similar question to when we should use the lambda and when we should use Kubernetes. If you want to use Kubernetes, then go with TensorFlow Serving. Otherwise, TensorFlow Lite. There is one thing where you have to use TensorFlow Lite – where you cannot use TensorFlow Serving – and this is when you want to deploy your model on a mobile device. I have a phone here. If you want to create an app with a model, then you will have to use TensorFlow Lite because you will not be able to use TensorFlow Serving. TensorFlow Serving also adds a bit of complexity. You will need to have this gateway service, you will need to have GRPC service, so that becomes more complex. If you have a lot of load, then you need to do this otherwise, TensorFlow Lite.
393	How long is the extension for Homework 8?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
394	How long is the extension for Homework 8?	Let's say two days. So Wednesday, November 23, 2022 at 11 PM Berlin time. That should be enough, hopefully. If it's not, let me know.
395	Do you recommend ML developing solutions directly with cloud? For example, using AWS Sagemaker inbuilt algorithms without using Docker.	"I personally would not, but that’s just me. I usually prefer… I want to understand what's going on there. If it's just a prebuilt thing and I put some data in and then something comes out and I don't really know what's going on there – I don't trust this thing. But that's just me. Again, maybe the important bit here is –you set up your validation framework and whatever blackbox AutoML solution you use gives you a good score on this validation framework, then you're good, right? If you don't worry about how exactly it's done there. Just use it. 
It’s probably a good thing not to worry about these things, especially if you're a startup and you want to move fast – you just want to get something that is already available for you. You just want to start using it and then see the benefits of using it. Maybe just go for this. But I usually like to have a bit of control of what's exactly inside. That's why, for me, I usually prefer to have a Docker image that I deploy to the cloud (to AWS Sagemaker, for example). It just gives me peace of mind, I guess."
396	Do you recommend ML developing solutions directly with cloud? For example, using AWS Sagemaker inbuilt algorithms without using Docker.	I guess not. That's the process of learning? Maybe more senior people don't… Yeah, I see where you're going with this. Senior people are not supposed to go bankrupt and default. They are probably supposed to pay back the debt. I guess these are the particularities of this dataset, I assume. Indeed, it's probably a data issue ,or not an issue, but a “characteristic” of this dataset.
397	Do you recommend ML developing solutions directly with cloud? For example, using AWS Sagemaker inbuilt algorithms without using Docker.	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
398	Do you recommend ML developing solutions directly with cloud? For example, using AWS Sagemaker inbuilt algorithms without using Docker.	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
399	Do you recommend ML developing solutions directly with cloud? For example, using AWS Sagemaker inbuilt algorithms without using Docker.	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
400	What is the reason behind BentoML generating a dedicated directory of models? (This is what we just talked about.)	"Tim
I think a couple people asked this in ML Zoomcamp Slack. I was thinking about this a little bit. I think it's sort of a philosophical thing [chuckles] where we think about models and Bento is sort of separate a little bit from code. So your project directory is where all the stuff that you'll commit to Git. I don't think we think that it's the right model to commit your models to Git, for example. We think of ourselves a little bit more like Docker. You don't have Docker images in your project directory, right? You just have your Docker file and that's what you commit to Git. In the same way, we store your models and your Bento in that dedicated directory, and then sort of leave it up to you to push it to different places where you might build or where you might need it
Alexey
Do you know why you decided to save it locally on your local file system? For example, what MLflow does – you can save it to S3 directly, but in the case of Bento, you always first save it to local storage and then you decide “This model I want to export, but for the rest, I don't care.”
Tim
Right. First, I think it’s because that's the easiest way to do it, but also because I think with BentoML, the workflow is that a lot of times, somebody's building a model on their laptop, or somebody's building a model in a slightly less distributed system and they're assembling their Bento there. So I think that the fastest way to access that model is locally. We do give you the ability to push that model and pull that model from other places. But ultimately, when you assemble your Bento and then you assemble your container, all that stuff has to be there. I don't think we wanted separate abstractions for when a person is training and saving, versus when a person is building a Bento and pulling in the model and assembling the container. It's just the same abstraction, which is just a local repository which contains models.
Alexey 
Maybe you don't want to save every model every single time you change one single parameter to the cloud. [Tim agrees] Maybe at the end, after all the experiments, you want to save the final one. Meanwhile, for MLflow, the use case is different – you actually want to save every single experiment because you want to see the results of these experiments.
Tim
Right."
401	What is the reason behind BentoML generating a dedicated directory of models? (This is what we just talked about.)	I do sometimes. Not very often these days, but we have quite a few models that require deep learning. Usually, these models are related to images. We have some images on the platform, so when we need to understand what's happening on these images, we use deep learning.
402	What is the reason behind BentoML generating a dedicated directory of models? (This is what we just talked about.)	I think the easiest way to understand what it means is to just try different ETAs, and then see exactly how your learning curves – the plots where you show the performance – how exactly they look like. If it's too big, you will see that your model overfits very quickly – on the training set, it reaches 100% performance very quickly, but on the validation dataset, it becomes worse and worse after just a few trees (after a few iterations). So it happens very fast. If you set a learning rate that’s too small, then you will need a huge load of trees to actually have anything meaningful. It will simply take too much time to learn anything, which would be the case of underfitting. So selecting the right learning rate also takes some trial and error – you need to try different values to see what works best. What I usually do is try 0.3, I try 0.1, and then I try 0.01 0.05 – then I just look at these plots and based on that, I make my decision. Also, keep in mind that when you have a lot of trees, your model becomes slower. You don't want to have a slow model. Sometimes it's better to set the learning rate a bit higher so that you have fewer trees. So it's better that way.
403	What is the reason behind BentoML generating a dedicated directory of models? (This is what we just talked about.)	Rescale is already preprocessing. Actually, in exception, this is what they do. It's actually the scaling there. There are two operations, I think. I don't remember exactly, but it's rescaling plus something else. So rescaling is preprocessing.
404	What is the reason behind BentoML generating a dedicated directory of models? (This is what we just talked about.)	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
405	How should I choose the number of pixels to train the image model?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
406	How should I choose the number of pixels to train the image model?	Yes. We will actually cover cross validation this week. Sometimes, if your dataset is small, (this was one of the questions today) then using cross validation is a very good idea.
407	How should I choose the number of pixels to train the image model?	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
408	How should I choose the number of pixels to train the image model?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
409	How should I choose the number of pixels to train the image model?	You mean the input size, I assume. If it's a pre-trained model, it's easy, because you just take whatever is available there. You also take into account speed and accuracy requirements – and you will probably have them – you will have some idea of how good your model is or how good your model should be. If that's okay for the model, for example, if you want to make it faster, is it okay to sacrifice some of the accuracy? So you need to take all the business requirements into account to answer your question. The answer, again, is “It depends.”
410	Any plans for next books or paid courses in DataTalks.Club?	I would say to focus on projects, not on tools. In this course, you will get a chance to do this. If you open a job description for data engineers, you will see a lot, a lot, a lot of different technologies. Maybe think of a project that you can do for this particular company and do this project using, for example, the tools you learn here or the tools you want to learn, and build a portfolio. I guess this is more of a tip for getting a data engineering job, rather than for somebody who's starting their data engineering career. But if you already have a job, then maybe think of the business impact of what you need. I think I will not spend too much time talking about that because this requires a one hour long discussion. Perhaps we should have a podcast interview about that. If you can think of any potential guests for this podcast interview, you can let me know and we can try to invite these people and talk about this in more detail. One thing I want to share with you is that in our YouTube channel, we had a few interesting talks. We had this Getting a Data Engineering Job with Jeff talk. It’s a very nice presentation. Then we also had an interview with Jeff. So these two podcasts are quite interesting. You can also check out other ones. I think you'll find a lot of interesting content here.
411	Any plans for next books or paid courses in DataTalks.Club?	No, you cannot.
412	Any plans for next books or paid courses in DataTalks.Club?	"Alexey
Yes. Well, maybe. Nothing concrete. If you have some ideas, again, please share them with us. I know that many people asked for a course in deep learning. I unfortunately don't know deep learning that well to make the course about that. But maybe there are some other areas that you have ideas about – share them with us. Again, for deep learning – I think maybe we will eventually do this. I just need to find people who know it a lot better than me and I will just help with the process. So nothing concrete, but if you have ideas – share them."
413	Any plans for next books or paid courses in DataTalks.Club?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
414	Any plans for next books or paid courses in DataTalks.Club?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
415	I am really a beginner with IaC (Infrastructure as Code), but I saw a Reddit post in the DevOps sub about Pulumi being better than Terraform. What are your opinions on Pulumi?	"Alexey
I have never used Pulumi. Again, we use Terraform because it's just one of the tools. The principles apply, Infrastructures as Code applies. It doesn't matter if you use CloudFormation, Terraform, Pulumi, or something else – the principle is still the same. The syntax is better. Pulumi might be better than Terraform when it comes to syntax. I think everything is better than Terraform when it comes to syntax. I don't know why it's so popular because the syntax, in my opinion, is terrible. But yeah, I never used Pulumi. If you like it, you can use it for your project."
416	I am really a beginner with IaC (Infrastructure as Code), but I saw a Reddit post in the DevOps sub about Pulumi being better than Terraform. What are your opinions on Pulumi?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
417	I am really a beginner with IaC (Infrastructure as Code), but I saw a Reddit post in the DevOps sub about Pulumi being better than Terraform. What are your opinions on Pulumi?	No, you cannot.
418	I am really a beginner with IaC (Infrastructure as Code), but I saw a Reddit post in the DevOps sub about Pulumi being better than Terraform. What are your opinions on Pulumi?	Okay, I can. All the course videos you will need are here, Data Engineering Zoomcamp. And all the live videos, all the homework, everything that is specifically related to this cohort, will be in the Data Engineering Zoomcamp 2023. But you don't actually need to use a playlist because all the videos are linked here. But if it's more convenient for you to use a playlist, then this is the playlist to use. And this one is just materials for live streams, homeworks, and so on.
419	I am really a beginner with IaC (Infrastructure as Code), but I saw a Reddit post in the DevOps sub about Pulumi being better than Terraform. What are your opinions on Pulumi?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
420	Is there a way to read all the answered questions from this Slido?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
421	Is there a way to read all the answered questions from this Slido?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
422	Is there a way to read all the answered questions from this Slido?	I'm not sure what you mean. Do you want to take a look at all the questions? Or you're interested in the answers too? I think there is a way to export the questions because it's the same Slido link for all the sessions. I'm also working on actually turning all my answers into text. There will be a table from AirTable with all the answers. Maybe I'll share it later. It's not finished yet. But yeah, there will be all the answers I gave in text form. As you can imagine, of course, it's very time consuming. It takes a bit of time, but I will share this thing with you soon.
423	Is there a way to read all the answered questions from this Slido?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
424	Is there a way to read all the answered questions from this Slido?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
425	Do you have plans to expand on information related to cost optimizing data warehouses/cloud services?	"Ankush
[corrupted sound] talk about pricing when it comes to BigQuery. I don't think we're gonna compare it with other data warehouse solutions outside Snowflake because Snowflake costing is a bit more complicated. I don't think so. I think we’re going to stick to BigQuery for now. And just remember, the less you pay, the better it is. But the human cost is always more than machines. There is always a bit of a compromise. [chuckle] To answer it simply – no, we're not going to do it."
426	Do you have plans to expand on information related to cost optimizing data warehouses/cloud services?	Yes
427	Do you have plans to expand on information related to cost optimizing data warehouses/cloud services?	"Alexey
Prefect is an orchestrator. If you need to produce a lot of data, Prefect might not be the best tool. Prefect really shines when it needs to execute tasks in a specific order, one after another. Usually the approach many people use at work, from what I see, is they use an orchestrator for orchestrating, and then you delegate all the compute to something external, like Kubernetes, AWS Batch, or Spark. Spark is an external thing. You submit a Spark job from your Prefect graph – from your Prefect DAG. Of course, you can do many things with Prefect, but when it comes to large datasets, then Prefect executors might simply not be able to handle that. This is when you need to use Spark. One does not exclude the other. You use Prefect for scaling Spark jobs, because maybe after this Spark job, you have another job that is not Spark – and Prefect knows how to execute them and in which order."
428	Do you have plans to expand on information related to cost optimizing data warehouses/cloud services?	Yes. If Michael was here, he would probably confirm this because this is the background he has. He's an analyst. We also had other analysts who did pretty well in the course. So yeah, certainly you can do well. As a beginner in data engineering, I would say that we don't expect any data engineering knowledge for taking this course. If you're a beginner already – if you know some data engineering concepts – then it's more likely that you will do well in this course.
429	Do you have plans to expand on information related to cost optimizing data warehouses/cloud services?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
430	Are there passing points for the project (how many points you need to get in order to pass the project)?	"Yes, indeed there is. You need to get something like 50% in order to pass the project. I don’t remember the exact score we used last year. This is how we did it last year – we looked at the distribution of the scores. We didn’t want to mark many projects as failed, so we looked at the distribution and we made sure that most of you passed the project. I think the passing score was pretty low. Most of the students who did the project – who did put some effort there in doing the project and who met at least half of the criteria or most of the criteria but partly – they passed the project. It wasn't a big problem, so don't worry about that too much. 
As long as you put in some effort, then you'll pass it. The people who didn't pass last year submitted empty projects, or projects that were copied from somewhere, or projects where they just added some readme file and maybe copied some Kaggle kernel and that was it. So nothing was done in addition to that. There were people who passed, but they just didn't put any effort into attempting to do the project. If you put in some genuine effort, don't worry, you will pass the project."
431	Are there passing points for the project (how many points you need to get in order to pass the project)?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
432	Are there passing points for the project (how many points you need to get in order to pass the project)?	"Alexey
CI stands for continuous integration. This is a way to – let's say you wrote some code, and then you push it to GitHub. Then on GitHub, there's GitHub actions, which is a way to automatically run some checks on your code – run tests, deploy things somewhere. Actually, again, I will do a shameless plug – in our MLOps Zoomcamp, we talked about GitHub actions in the best practices module. Right now, don't worry about this. Focus on your projects. But after you do your projects, it's really worth spending some time learning about best practices. These best practices include things like writing tests, using make files and also tools like CI/CD, infrastructure as code – we cover all that in the module. But if you are interested in these things, maybe it's actually worth taking that course too. We'll have another iteration in May. Not super soon – I’m just telling you that to get you excited."
433	Are there passing points for the project (how many points you need to get in order to pass the project)?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
434	Are there passing points for the project (how many points you need to get in order to pass the project)?	"You will probably also need to learn how to pass interviews. This course does not teach that, but you will need to acquire the skill somehow. The best way to acquire these skills is by interviewing. But apart from that, I think this course covers let's say 80% of the skills that you will need for a job and the remaining 20% depends on the company. 
So you will need to do some research on the company where you want to work, see what kind of tools they use, what kind of problems they solve, how much emphasis they make in the interviews on the theoretical part. You will need to somehow figure this out and, based on that, see what you need to learn. I think with just this content, you should be able to get a machine learning-related job."
435	Would k-fold cross validation be a good idea to check our validation framework?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
436	Would k-fold cross validation be a good idea to check our validation framework?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
437	Would k-fold cross validation be a good idea to check our validation framework?	Yes. We will actually cover cross validation this week. Sometimes, if your dataset is small, (this was one of the questions today) then using cross validation is a very good idea.
438	Would k-fold cross validation be a good idea to check our validation framework?	Yeah. If it works for you, use it. I don't mind.
439	Would k-fold cross validation be a good idea to check our validation framework?	So for the first part “What should be my approach if I'm juggling between both?” Well, I don't know how much time you have. If you don't have a lot of time, I would suggest focusing on one and then come back to the other one. Actually, I need to make an announcement. As the course team from the Data Engineering Zoomcamp, we actually discussed it, and I'm happy to announce that there will be another iteration of Data Engineering Zoomcamp. Maybe what you can do now is just focus on ML Zoomcamp, and then come back to Data Engineering Zoomcamp when we launch it in January. There will be another announcement, but now you can know it. So, yes, we'll have another iteration in January.
440	Any open source ELT tools?	Yes, you will have to come up with your own individual projects. It's not easy, I know. But we'll help you. We will help you to come up with a project idea. That's actually on purpose because we want every project to be unique. Then it becomes a part of your portfolio when you look for a job.
441	Any open source ELT tools?	"Alexey
Let me just copy this and paste it to Google and you'll find a way to answer your question. Is actually asking for the ELT tools. Not helpful, I guess. But yeah, Airbyte is definitely one of them. There are others too."
442	Any open source ELT tools?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
443	Any open source ELT tools?	I would say to focus on projects, not on tools. In this course, you will get a chance to do this. If you open a job description for data engineers, you will see a lot, a lot, a lot of different technologies. Maybe think of a project that you can do for this particular company and do this project using, for example, the tools you learn here or the tools you want to learn, and build a portfolio. I guess this is more of a tip for getting a data engineering job, rather than for somebody who's starting their data engineering career. But if you already have a job, then maybe think of the business impact of what you need. I think I will not spend too much time talking about that because this requires a one hour long discussion. Perhaps we should have a podcast interview about that. If you can think of any potential guests for this podcast interview, you can let me know and we can try to invite these people and talk about this in more detail. One thing I want to share with you is that in our YouTube channel, we had a few interesting talks. We had this Getting a Data Engineering Job with Jeff talk. It’s a very nice presentation. Then we also had an interview with Jeff. So these two podcasts are quite interesting. You can also check out other ones. I think you'll find a lot of interesting content here.
444	Any open source ELT tools?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
445	Is there any guide to set up on a local machine?	"Ankush
I think if it's about finding a job, spend time learning AWS, because… come on – everybody's using AWS and you have a higher probability of finding a company which will have AWS. After that choose GCS or Azure or whatever you feel. But I think the main point is to learn the concepts. For example, let's talk about data warehouses for partitioning and clustering. In Prefect, it’s how to create your DAGs in particular structure or paralyzing your flow. I think if you learn this concept, it doesn't really matter which cloud platform you use at the end of the day, because all of their services will be pretty similar. I think once you grab the basic concepts and your base layer is good, it's very easy to adjust to both of the cloud environments. I have personally worked on AWS and Google Cloud. I've also worked a bit with Snowflake. Once you can get the ideas, or the concepts, it's really easy to make that switch.
Alexey
I'll add that my experience with Cloud is mostly AWS. Not mostly, I think, it’s only AWS. For this course, I used GCP for the first time and it wasn't that difficult. The UI is different. I actually think GCP has a better UI, in my opinion. It’s more intuitive. The only thing I needed to figure out was, “In AWS this thing is called this way. How was it called in GCP?” Then I would just Google it, find it, and then just use it. Most of the time, it was like that.
Ankush
I have a follow up question. What if ChatGPT is only integrated in Azure? Then what do we do? 
Alexey
Well… I'll have to use Azure. [chuckles]
Ankush
So we’ll need to migrate? [laughs]"
446	Is there any guide to set up on a local machine?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
447	Is there any guide to set up on a local machine?	You can follow the same setup here. Apart from creating a virtual machine and doing port forwarding, everything else that you do in this video also applies to a local Linux environment. Maybe you will also need to install Google Cloud SDK, because on a virtual machine, you already have it – you don't need to install it. Locally, you will need to install it. Apart from that, you can just follow the same stuff for a Linux computer. For Windows or Mac, I don't think we have a guideline.
448	Is there any guide to set up on a local machine?	I would say to focus on projects, not on tools. In this course, you will get a chance to do this. If you open a job description for data engineers, you will see a lot, a lot, a lot of different technologies. Maybe think of a project that you can do for this particular company and do this project using, for example, the tools you learn here or the tools you want to learn, and build a portfolio. I guess this is more of a tip for getting a data engineering job, rather than for somebody who's starting their data engineering career. But if you already have a job, then maybe think of the business impact of what you need. I think I will not spend too much time talking about that because this requires a one hour long discussion. Perhaps we should have a podcast interview about that. If you can think of any potential guests for this podcast interview, you can let me know and we can try to invite these people and talk about this in more detail. One thing I want to share with you is that in our YouTube channel, we had a few interesting talks. We had this Getting a Data Engineering Job with Jeff talk. It’s a very nice presentation. Then we also had an interview with Jeff. So these two podcasts are quite interesting. You can also check out other ones. I think you'll find a lot of interesting content here.
449	Is there any guide to set up on a local machine?	Yes
450	Why train models anymore? Isn't there a huge range of models available on Hugging Face. Do you know other model hubs online apart from Hugging Face?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
451	Why train models anymore? Isn't there a huge range of models available on Hugging Face. Do you know other model hubs online apart from Hugging Face?	I mean, if there is already a model in Hugging Face for your problem, that's good for you. But I guess that's only like 1% of problems that you solve in the industry. Data scientists wouldn't be working at companies if that was the case. [chuckles] We still need to get data and we still need to train these models. And then if you think about it, somebody actually put these models on Hugging Face, right? I suspect that it was done by data scientists. Right? [chuckles] I don't know any other model hub apart from that. Maybe Kaggle. I would say that’s also a good source for models – more for data, but in data, you have a dataset and you can also train a model there. There are many notebooks that show how to do this.
452	Why train models anymore? Isn't there a huge range of models available on Hugging Face. Do you know other model hubs online apart from Hugging Face?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
453	Why train models anymore? Isn't there a huge range of models available on Hugging Face. Do you know other model hubs online apart from Hugging Face?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
454	Why train models anymore? Isn't there a huge range of models available on Hugging Face. Do you know other model hubs online apart from Hugging Face?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
455	Would you need machine learning for recommender systems? If yes, in which case?	For pretty much every case, I guess. But yeah, you definitely need machine learning for recommender systems. Maybe for a start – if you're just starting with a recommendation system for your platform – there are simple things that you could do, like seeing what the most popular thing in a certain age group is, or what the most popular thing in a certain category is, and then the command in that. But, quite quickly, you will realize that you need something more substantial, let's say, and that there is no way to do it without machine learning. So you will have to use machine learning. The answer is for any – for every case.
456	Would you need machine learning for recommender systems? If yes, in which case?	"It all depends on what you mean by “full stack ML engineer”. As you’ve noticed, data engineer Zoomcamp is not about machine learning – it's about data engineering. If in your opinion a “full stack ML engineer” needs to know data engineering, then you should do data engineering. If not, then no. From my point of view, there is a “full stack of data scientist”. What I put in the definition of a full stack data scientist is – a person who can do all the steps in the CRISP-DM process. They can help product managers with business understanding, they can work with analysts in data understanding, they can help data engineers in the data preparation step, they can do the modeling, and they can deploy the models. If you want to be that kind of person, then, of course, you also need to learn a bit of product management and analytics, which we don't have courses for. But data engineering – preparing data before it goes to a model – will certainly be helpful. 
Also, machine learning ops Zoomcamp (MLOps Zoomcamp) will be helpful as well. I would suggest, if you really want to focus on machine learning engineering, then probably doing MLOps Zoomcamp will make more sense for you. But data engineering Zoomcamp will be quite useful in the future, because data scientists and ML engineers tend to work on data pipelines as well. 
Maybe not all the content will be useful for you. For example, the content about data warehousing – as a data scientist, I don't find it super useful in my work to me personally. Also, the module about analytics engineering – it's nice, but it's not something I do day to day. But the modules about batch, the modules about Spark, the modules about streaming – this is something that can be useful, because this is something we could use in building machine learning pipelines as well. 
To put data in a machine learning model, we need to prepare it – we need to create data pipelines – and this is pretty relevant. But you can just check it out. All the content is already there. You can just go and check it out. If you like it, do it – if you don't, don’t. I would suggest maybe going with this one (ML Zoomcamp)."
457	Would you need machine learning for recommender systems? If yes, in which case?	In some domains, maybe the color doesn't matter much – on the shapes matter. If it's that domain, then yes. For example, I think for clothing, it might be the case. Maybe colors don't matter much, because you can have all different colors. But on the other hand, maybe it's actually still important because you want to distinguish the background from the foreground (from the actual item). This is where the color information could be important. If you think it will give you some performance increase in terms of making predictions faster – just use validation to find out if there is any predictive performance drop (drop in accuracy) when you switch to grayscale. If you see that there is no drop, then just stick to that.
458	Would you need machine learning for recommender systems? If yes, in which case?	I wish Tim was here and would help me answer that because I don't know. I assume if you wanted to use Bento, you would probably use TensorFlow Lite – you don't want to use the usual TensorFlow for the same reasons that we talked about in the serverless module, because it's simply too large. Probably for Bento, you would use something like TensorFlow Lite and it will still work. Maybe another thing you can check – in our channel, we had a talk from one of the Bento folks, Building an ML Service Platform from the Ground Up. Here, Sean talks about the differences between TensorFlow Serving and Bento. With TensorFlow Serving, it's good – it's quite optimized for deep learning. There are some problems. First of all, it's written in C++ and expects your model in a certain format – this “saved model format”. Adding anything extra on top of that (like pre-processing, post-processing) becomes difficult, because you will need to have an extra step there. Bento makes it easier. But, again, I don't know. It's very difficult to give you a concrete recommendation without knowing what exactly you want to do. So it's case dependent. If you already use TensorFlow Serving in your company, then maybe it makes sense to stick to that. If we have Tim on Office Hours next week, maybe you can ask him that. He will be a better person to answer this question.
459	Would you need machine learning for recommender systems? If yes, in which case?	[chuckles] I'm not prepared to answer that question. I don't have a top three for guidelines. Maybe this is something we can discuss next time. I don't know how to answer that. Maybe what you can do is go to our YouTube channel (I know I keep doing this) where we just had this Responsible and Explainable AI Interview about that. So maybe check it out. I will mark this as “answered” and then maybe this is something we can go through next time, like we did last time – some of the questions I didn't answer and we moved it to this week. So probably we’re going to keep doing it like this.
460	How do we decide what to put in a Docker container and what doesn't need one?	"Alexey
Yeah, that's not going to happen. I think ChatGPT will help us do our job, but it will not replace people, at least in my opinion. But I use it in my work and it's helpful. You should try using it too – if you want, of course."
461	How do we decide what to put in a Docker container and what doesn't need one?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
462	How do we decide what to put in a Docker container and what doesn't need one?	I don't know – scripts? It would be nice to have an example. Usually, your longer jobs would require a container. It also depends on where you will run these jobs later. A typical scenario, for example, is using something like Kubernetes or AWS batch for executing these jobs. These environments require a Docker container. There, basically the rule is – if you want to deploy anything there, you have to put it in a Docker container. Another alternative could be, for example, if you use Prefect, there are two options. One thing you can do is just do all the work in Prefect, so Prefect will execute this thing. Or another option is to use Prefect as a simple orchestrator and delegate the execution to something like, again, Kubernetes, AWS Batch, Spark, and so on. In this case, if you use Prefect and the agents have all the required libraries, you can just use Prefect for executing things. You don't necessarily need to put these things in a Docker container. But if you're going to use Prefect only for orchestrating and the actual jobs will run in some external environment where a container is expected, then yeah, you Dockerize everything. If you have some examples in mind that you want to talk about, let's do this in Slack.
463	How do we decide what to put in a Docker container and what doesn't need one?	No, you cannot.
464	How do we decide what to put in a Docker container and what doesn't need one?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
465	Can you talk more about using GCP for completing the course? I was able to use Vertex AI to create a pipeline, and it was really cool.	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
466	Can you talk more about using GCP for completing the course? I was able to use Vertex AI to create a pipeline, and it was really cool.	"You probably mean for logistic regression, right? For logistic regression, it's doing something very similar to what we saw in module 2. It is trying to minimize the… (I'm just trying to figure out how I can explain it without going into too many details and not to get lost myself and not to confuse you). It's probably a good idea to refer to some other course. I think I partly talked about this. Let me check. Here I talk a little bit about this, in ML Zoomcamp Office Hours week #4, where I explain a little bit how exactly it works. But it's a very superficial, let's say, overview of exactly what's happening there. 
If you want to learn in more detail how exactly the process works there, maybe you can just go to Google and search for “gradient descent, logistic regression,” or something like this. The first result will probably have a good explanation of what's happening under the hood. SciKit Learn does not use this exact method – it doesn't use gradient descent – it uses something more complex (more advanced) than that. But this is roughly what's happening under the hood. I hope you're satisfied by this answer. I don't think I can give you a better one with the time we have. It will be like a separate lecture. If you don't want to go into details, you can think of this as something that’s similar to logistic regression happening there but we add sigmoid on top of that."
467	Can you talk more about using GCP for completing the course? I was able to use Vertex AI to create a pipeline, and it was really cool.	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
468	Can you talk more about using GCP for completing the course? I was able to use Vertex AI to create a pipeline, and it was really cool.	"When I was freelancing, it was through a website that’s similar to Upwork. You can try that. Or Fiver or something like this. But, depending on where you live, you can just use LinkedIn for that and if somebody wants to invite you for an interview, you can ask them, “Hey, do you consider freelancers?” Some of the recruiters who reach out to you will say, “Yes, we do consider freelancers.” And then you can just start doing this. I don't think I can give you a better recommendation, because I'm not a freelancer myself. 
Maybe what you can do is go to DataTalks.Club site’s podcasts, where we have two podcast episodes about freelancing. The first one is Freelancing and Consulting with Data Engineering and then the other one is Freelancing in Machine Learning. In both cases, the guests talk about finding your first client, how they started freelancing, what they do. They know more about this than me."
469	Can you talk more about using GCP for completing the course? I was able to use Vertex AI to create a pipeline, and it was really cool.	It was this above average thing. That was the y_train variable.
470	Where can I get the given that we are supposed to work on?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
471	Where can I get the given that we are supposed to work on?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
472	Where can I get the given that we are supposed to work on?	You'll have to come up with a project idea yourself. We will help you, of course. We will share some datasets with you, but will need to go through these datasets and you will need to find a project yourself. Then if you're not sure if this is a good project or not, you can just, again, ask that in Slack. You can say “Okay, I found this project. Will this dataset be a good dataset for the project?” I can already tell you that datasets like Iris, Titanic, Buying Quality, MNIST – these datasets that you see in every tutorial – are not good for the project. Try to find something a little bit more unique.
473	Where can I get the given that we are supposed to work on?	"Yes or no. Accuracy can be used for multi-class classification – precision, recall and ROC curve, they're all for binary classification. But it's still possible to extend them for multi-class. First of all, there is a thing called one versus all classification. In this case, let's say we have three classes. [Image for reference] We have the green, blue and red classes. 
First, we actually see this problem as three binary classification problems and we train a separate model for each. For example, we can train a model for distinguishing green from the rest – maybe that will be our first logistic regression model. Then we can train a model for distinguishing the blue squares from the rest. And then we can have a third model for distinguishing the red crosses from the rest. This is called the “one vs all” approach. When we do this, we will end up with three binary models, and then we can compute precision, recall, F1 score, ROC, for each of these models separately and then we can merge them. We can take an average and see what the precision and recall are for this average. So that's one of the approaches. 
There are other approaches you can take. Let's say, ROC curve SciKit Learn, there is a way to extend it. [Image for reference] You see, for this roc_auc_score, they can actually do this for multi-class cases. There are different ways of how you can take average between these three classifiers. These micro/macro weighted samples – you can just read this description, this recommendation and see what the different ways of doing this are. I think this is a common approach. This is the one I just described – you compute them independently and then you kind of take the average."
474	Where can I get the given that we are supposed to work on?	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
475	Does this course cover end-to-end projects for better understanding (put data in a database, apply ML to it, convert them to PyScript from Jupyter)?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
476	Does this course cover end-to-end projects for better understanding (put data in a database, apply ML to it, convert them to PyScript from Jupyter)?	"There was no registration deadline at all for this course. So you can still register. You can still sign up and you can follow the course. You will not be able to submit the homework assignments where the due date is already over because there are already solutions posted, so it doesn't make much sense to submit homework when the solutions are already there. You can follow the course but you will not be able to submit the homework. You can check our frequently asked questions and I do recommend doing this. Please remember the rule – if you have a question, first go check the frequently asked questions, and if you don't find the question there, go and ask in Slack. 
For this particular question, it’s already there. “Don't worry, you can take the course.” Maybe right now it's becoming a little bit more challenging to catch up with everything, but note that you can skip the midterm project. Right now, if you want to catch up with everything, if you skip the midterm project, it's fine. You will be able to do two capstone projects and still get the certificate. Also, you can just take the course in self-paced mode. There is nothing wrong with that either."
477	Does this course cover end-to-end projects for better understanding (put data in a database, apply ML to it, convert them to PyScript from Jupyter)?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
478	Does this course cover end-to-end projects for better understanding (put data in a database, apply ML to it, convert them to PyScript from Jupyter)?	"We don't touch databases here. It's “semi-end-to-end,” if you will. [chuckles] If we think about the process for data projects – first, we have “business understanding” and “data understanding”. This course focuses more on this part:

Partly data preparation, and partly evaluation – maybe more deployment. There’s a focus on modeling and deployment. I would say that the focus is more on Data Preparation, Modeling and Deployment.

For example, Data Understanding and Data Preparation would be data engineering.

And then Evaluation and Deployment would be MLOps.

And then business understanding is more related to product management. There is also a thing called AI product management or ML product management, where they talk more about how exactly the process should look like and so on."
479	Does this course cover end-to-end projects for better understanding (put data in a database, apply ML to it, convert them to PyScript from Jupyter)?	Not always. For logistic regression, it does. For decision trees and random forest and XGBoost – not really. Then there is a thing called “calibration”. SciKit Learn calibration. You will need to check this out. I think there are also examples of how to do this with random forest. If you can't find examples here, there are examples on Kaggle. But practically speaking, I don't remember the last time I needed to use something like this. It's helpful when you want to train multiple models. Let's say you've been using logistic regression in production for some time and then you make some decisions based on this threshold. But then you want to deploy a new model (XGBoost) and you want to make sure that the ranges you have in the previous model are the same in the new model so the distribution looks kind of similar. This is where calibration helps. It's a useful thing, but not every application needs it.
480	How would you go about adjusting the price data for inflation?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
481	How would you go about adjusting the price data for inflation?	I think the easiest way to understand what it means is to just try different ETAs, and then see exactly how your learning curves – the plots where you show the performance – how exactly they look like. If it's too big, you will see that your model overfits very quickly – on the training set, it reaches 100% performance very quickly, but on the validation dataset, it becomes worse and worse after just a few trees (after a few iterations). So it happens very fast. If you set a learning rate that’s too small, then you will need a huge load of trees to actually have anything meaningful. It will simply take too much time to learn anything, which would be the case of underfitting. So selecting the right learning rate also takes some trial and error – you need to try different values to see what works best. What I usually do is try 0.3, I try 0.1, and then I try 0.01 0.05 – then I just look at these plots and based on that, I make my decision. Also, keep in mind that when you have a lot of trees, your model becomes slower. You don't want to have a slow model. Sometimes it's better to set the learning rate a bit higher so that you have fewer trees. So it's better that way.
482	How would you go about adjusting the price data for inflation?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
483	How would you go about adjusting the price data for inflation?	I do sometimes. Not very often these days, but we have quite a few models that require deep learning. Usually, these models are related to images. We have some images on the platform, so when we need to understand what's happening on these images, we use deep learning.
484	How would you go about adjusting the price data for inflation?	"Here, what we can do is – we have a process for collecting data. Let's say we run a website where we sell cars and we can see that inflation is there because the prices that people set for selling the cars are growing. What we can do is, every month we can just use this data that we have for retraining our model. Perhaps we can – I don't know if it makes sense or if people do this – perhaps for the prices in the past, we can retroactively adjust this based on inflation. 
Let's say that if we know that this year in Europe, the inflation is 9% or something like this, or we know that for cars specifically the inflation is this number, then perhaps we can adjust it. But I don't know if people actually do this."
485	Is it possible to schedule two tasks at different time intervals in the same DAG (e.g. one at every one hour, and another one every four hours in the same DAG)?	No, but you can suggest some stuff in Slack. If we see that this is useful, we can also include this in the project repo
486	Is it possible to schedule two tasks at different time intervals in the same DAG (e.g. one at every one hour, and another one every four hours in the same DAG)?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
487	Is it possible to schedule two tasks at different time intervals in the same DAG (e.g. one at every one hour, and another one every four hours in the same DAG)?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
488	Is it possible to schedule two tasks at different time intervals in the same DAG (e.g. one at every one hour, and another one every four hours in the same DAG)?	Yes, you will have to come up with your own individual projects. It's not easy, I know. But we'll help you. We will help you to come up with a project idea. That's actually on purpose because we want every project to be unique. Then it becomes a part of your portfolio when you look for a job.
489	Is it possible to schedule two tasks at different time intervals in the same DAG (e.g. one at every one hour, and another one every four hours in the same DAG)?	"Jeff
Again, Prefect is DAGless, so you don't have to have a DAG. It gives you flexibility that if you want your things to run in a directed acyclic graph, you can structure things like that. But Python is much more flexible and workflows can be more flexible with Prefect. Maybe it's an Airflow kind of question – I'm not sure. You can schedule things to do what you want. With the recurring rules schedule, our schedule, you’ll have a good bit of flexibility there. It seems extremely, extremely flexible. I encourage you to check out current rules and see if there's something that works there for what you need. Worst case, you could have two different deployments and have them running on different schedules. That’s pretty lightweight, still."
490	When creating a histogram, is there any rule of thumb to decide the bin number?	"Yeah, you definitely need to use cross-validation here. Tree models overfit. That's why you need to use cross-validation – to make sure that this doesn't happen. Usually, when you have such a small dataset, it helps to use very simple models. Trees – when you let them grow indefinitely deep, they do tend to overfit. 
So maybe you stick to the depth of three, or use something like linear regression or logistic regression with very few features. Because you have a small dataset, you need to be very careful. I guess, that’s the general rule – use simpler models and then use cross-validation."
491	When creating a histogram, is there any rule of thumb to decide the bin number?	I wish Tim was here and would help me answer that because I don't know. I assume if you wanted to use Bento, you would probably use TensorFlow Lite – you don't want to use the usual TensorFlow for the same reasons that we talked about in the serverless module, because it's simply too large. Probably for Bento, you would use something like TensorFlow Lite and it will still work. Maybe another thing you can check – in our channel, we had a talk from one of the Bento folks, Building an ML Service Platform from the Ground Up. Here, Sean talks about the differences between TensorFlow Serving and Bento. With TensorFlow Serving, it's good – it's quite optimized for deep learning. There are some problems. First of all, it's written in C++ and expects your model in a certain format – this “saved model format”. Adding anything extra on top of that (like pre-processing, post-processing) becomes difficult, because you will need to have an extra step there. Bento makes it easier. But, again, I don't know. It's very difficult to give you a concrete recommendation without knowing what exactly you want to do. So it's case dependent. If you already use TensorFlow Serving in your company, then maybe it makes sense to stick to that. If we have Tim on Office Hours next week, maybe you can ask him that. He will be a better person to answer this question.
492	When creating a histogram, is there any rule of thumb to decide the bin number?	"Yeah, I honestly think that there is not much of a difference between this. In one case, if we don't put limits on the bias term, we say that we don't want to penalize it – it can be as large as possible or not. But in the case of Andrew Ng (in his machine learning lectures) they say “Okay, bias term can be anything. We only want to control weights for things.” Practically, I don't think there is a significant difference, but it becomes a little bit more difficult to implement it. 
When you add these to diagonal lines, for our case implementation is super easy. You just take this identity matrix and multiply it by something. But then in case we don't want to penalize for the bias term, it becomes a little bit more involved. Since I didn't notice any practical difference in these two approaches, I decided not to spend time explaining that it's also possible to do it this way. I hope that answers the question."
493	When creating a histogram, is there any rule of thumb to decide the bin number?	You're probably in week two already, which is when we look at histograms. There, just follow your intuition. The answer is – enough to get an understanding of the shape of the distribution. This answer is pretty vague, I know. But when you start experimenting with this, you will kind of know when it's too many or too few. When it's too few, you basically have a few giant buckets and it's very hard to tell for you what the distribution is. But then, when you get too granular, then maybe it's just too much. Usually, for me, around 30-50 is a good rule of thumb to go with. Again, it depends. Just plot and then let your aesthetic feelings guide you. There is no right or wrong answer, I think. Someone wrote a comment that there are actually some rules for binning called Freedman–Diaconis. That's the first time I’ve heard of this, but you could probably check it out.
494	When creating a histogram, is there any rule of thumb to decide the bin number?	Image data, text data, time series (to some extent) although it is tabular. So images and text are mostly the ones that come to mind when I talk about non-tabular data. In module eight, we will see what to do with images and with NLP, you can check out a lot of resources on the internet on how to use neural networks, or how to use other things. Because if we use traditional methods, like the count vectorizer that I showed you in the Office Hours, it will still take non-tabular data and turn it into tabular data. But with neural networks, it does a bit more than just that. It's more complicated – more advanced. You can find a lot of examples on the internet.
495	Would gradient descent give completely different weights than the normal equation?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
496	Would gradient descent give completely different weights than the normal equation?	"Tim 
Not really. We have users who use both Streamlit and Gradio. I think Streamlit and Gradio are a bit more about the presentation side. We do have a couple of users who have asked us about deeper Streamlit or Gradio support and I think that's something that's probably coming up in the next few months.
Alexey
For those who don't know what Streamlit or Gradio is – this is a way to create an interface for your services. Well, Gradio is focused on machine learning, meaning you create interfaces specifically for models. And then Streamlit is just a Python package for creating interfaces. We actually had a project at DataTalks.Club about Streamlit. I actually even have a tweet about this. This is what Streamlit looks like. In the video, you can see that you have this interface, and then based on the toggles that you move the core of your model changes. It gives you an easy way to build an interface like that. Meanwhile Bento focuses on quite a different use case, right? It’s not building interfaces, but serving models.
Tim 
Right, yeah. I think one of our users was asking for a really easy way, so that once you have your model and your Streamlit UI to be able to create a Bento deployment with one line in that way, anybody that you gave your Streamlit app to would be calling to that service and it could work anywhere. Which is a nice feature, I think.
Alexey 
Yeah, I think it is. I imagine if you have a Pydentic class there, then you can easily generate a Streamlit app for that.
Tim 
Right. Oh, that's a good idea. I hadn't thought of that. Yeah.
Alexey
And then in addition to that, the Swagger UI that you have, could be like having the actual interface. 
Tim
Oh, yeah, that would be cool. [chuckles]
Alexey
That could be a good Hackathon project, right?
Tim 
Yeah, for sure.
Alexey
From what I see – let's say you’ve built a model. Then you deployed this model and you have this nice API. You can query it with CURL. But what if you want to demo this project to your manager, or a manager of your manager, or somebody who does not necessarily have a technical background? If you give them the URL and say, “Okay, you need to execute the CURL command with a post request,” they would be like, “CURL what? What do you want from me? Just show me how it looks.” This is when tools like Streamlit come in handy. Instead of giving them a command line interface to query it, you just give them “Okay, this is the link. Play with this.” If Bento could do something like that – automatically generate things like that – many data scientists would thank you.
Tim
Yeah, I was just looking because I figured there's a library that turns Pydantic models into Streamlit apps. It looks like there is one that does that. I wonder if we could just plug into that app and then have that same thing. 
Alexey
That's cool. 
Tim
Yeah, that's awesome. That's kind of the cool part of open source – there are so many people out there with so many different projects and we get to collaborate with them and kind of build the best collaborations and combinations of these tools."
497	Would gradient descent give completely different weights than the normal equation?	No. They shouldn't. They should give roughly the same weights. At the end, the minimum of your loss function is the same, but gradient descent and normal equation arrive at the solution differently. And then at the end, the solution should be the same.
498	Would gradient descent give completely different weights than the normal equation?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
499	Would gradient descent give completely different weights than the normal equation?	I guess not. That's the process of learning? Maybe more senior people don't… Yeah, I see where you're going with this. Senior people are not supposed to go bankrupt and default. They are probably supposed to pay back the debt. I guess these are the particularities of this dataset, I assume. Indeed, it's probably a data issue ,or not an issue, but a “characteristic” of this dataset.
500	For backend engineers, please suggest some practical hands-on machine learning courses online.	I mean, I don't know why you asked me that, because what do you expect me to answer? [chuckles] Of course, take this course that you're already taking.
501	For backend engineers, please suggest some practical hands-on machine learning courses online.	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
502	For backend engineers, please suggest some practical hands-on machine learning courses online.	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
503	For backend engineers, please suggest some practical hands-on machine learning courses online.	That's a very similar question to when we should use the lambda and when we should use Kubernetes. If you want to use Kubernetes, then go with TensorFlow Serving. Otherwise, TensorFlow Lite. There is one thing where you have to use TensorFlow Lite – where you cannot use TensorFlow Serving – and this is when you want to deploy your model on a mobile device. I have a phone here. If you want to create an app with a model, then you will have to use TensorFlow Lite because you will not be able to use TensorFlow Serving. TensorFlow Serving also adds a bit of complexity. You will need to have this gateway service, you will need to have GRPC service, so that becomes more complex. If you have a lot of load, then you need to do this otherwise, TensorFlow Lite.
504	For backend engineers, please suggest some practical hands-on machine learning courses online.	"You will probably also need to learn how to pass interviews. This course does not teach that, but you will need to acquire the skill somehow. The best way to acquire these skills is by interviewing. But apart from that, I think this course covers let's say 80% of the skills that you will need for a job and the remaining 20% depends on the company. 
So you will need to do some research on the company where you want to work, see what kind of tools they use, what kind of problems they solve, how much emphasis they make in the interviews on the theoretical part. You will need to somehow figure this out and, based on that, see what you need to learn. I think with just this content, you should be able to get a machine learning-related job."
505	Pulling raw data from GCS into BigQuery gives data type errors. How do I handle this? Transform data types before loading data to GCS?	"Alexey
Yeah, we talked about that in Slack, so please check it. I think this happens because some of the columns have missing values and when you use Pandas for converting CSV to Parquet, it reads columns with missing values as double. That's why this happens."
506	Pulling raw data from GCS into BigQuery gives data type errors. How do I handle this? Transform data types before loading data to GCS?	Yes, you will have to come up with your own individual projects. It's not easy, I know. But we'll help you. We will help you to come up with a project idea. That's actually on purpose because we want every project to be unique. Then it becomes a part of your portfolio when you look for a job.
507	Pulling raw data from GCS into BigQuery gives data type errors. How do I handle this? Transform data types before loading data to GCS?	"Alexey
I think this is related to the question I answered a few questions ago about this wrong column type. In week 5, we actually used Spark to set the schema. In this case, when we process CSV files like that with schema in Spark and then we use it in BigQuery, everything should be fine. Here, the main idea is not using Spark but specifying the schema. If you specify the schema (if you force the schema) then all the files you create will have the same schema and all the columns will have the same format. Then reading this data in BigQuery will be fine because it will not be confused that in one file, one column has one type, in another file, the same column has a different type. Because of that, there are data type errors. But if we force the schema, all the columns will have the same type and it should be fine."
508	Pulling raw data from GCS into BigQuery gives data type errors. How do I handle this? Transform data types before loading data to GCS?	Both. You can do it both ways and there are pros and cons. It really depends on the use case. From what I understood, it’s usually cheaper to first transform the data, put it into a data lake, and then load it to Google BigQuery. For example, in cases of DBT, this is what you do with DBT. You transform the data that is already in the data warehouse. Thus, both options work.
509	Pulling raw data from GCS into BigQuery gives data type errors. How do I handle this? Transform data types before loading data to GCS?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
510	Is there an option to host an Office Hours episode outside of the office hours?	You can follow the same setup here. Apart from creating a virtual machine and doing port forwarding, everything else that you do in this video also applies to a local Linux environment. Maybe you will also need to install Google Cloud SDK, because on a virtual machine, you already have it – you don't need to install it. Locally, you will need to install it. Apart from that, you can just follow the same stuff for a Linux computer. For Windows or Mac, I don't think we have a guideline.
511	Is there an option to host an Office Hours episode outside of the office hours?	No, you cannot.
512	Is there an option to host an Office Hours episode outside of the office hours?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
513	Is there an option to host an Office Hours episode outside of the office hours?	"Alexey
No. I mean, we kind of told you. Yes, you maybe didn't see the intro video. But, sorry. No, it's late. We will actually recalculate the points for the second homework (HW 1B) because in the forum, it said that you can get up to one point, but we calculated that the cap there was seven. We are going to recalculate that. Thus, people who got a lot of points will get just one. Sorry about that. But don't focus on points. If you go to the leaderboard, we know that the person with this hash (722366d8b29ece9be3a7605363562c7c60d6918e) did seven links. Do you know who that is? Probably you don't. 
These points are virtual. It's nice to have a good score, but remember that these points mean whatever you want them to mean – not more than not less. Maybe it's okay if you get a few points less than others for the first homework, but the important thing here is consistency. If you just do your homework, even without doing any Learning in Public stuff, which I think you should do (you should post about the course). If you don't do this and you just do all the homework, by the end, you will be somewhere in the top 20. If you look at the leaderboard from the previous iteration, you will see that not everyone stuck around till the end. 
If you just do your homework, don't worry about these points, and you'll be fine. Sorry about that. I know that it can be discouraging that you didn't get as many points as others, but don't worry about that. Focus on learning. Maybe Michael has some other suggestions or some other things that I didn't mention, because he took part last year. But I think he was one of the people who did a few posts every week, right?
Michael
Yes. Actually, when I went through it, I missed the whole… you can get up to seven points, but I was just doing one every week and I just stuck with that. I was still near the top. I think the real value in that isn't necessarily the points but just being comfortable sharing – writing articles and making videos. That's the real value there. But everyone likes the clout value, too. As Alexey said, it will fluctuate, so if you keep up with it going forward, you'll probably be just fine."
514	Is there an option to host an Office Hours episode outside of the office hours?	Maybe tell us more in Slack what exactly you mean by that and what you want to see there? What kind of content?
515	What team are you supporting in the Football World Cup?	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
516	What team are you supporting in the Football World Cup?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
517	What team are you supporting in the Football World Cup?	"In summary, “Yes, you can join late. Not a problem. If you care about the certificate, it will not impact your chances of receiving one. All we care about for that are projects. We will have three projects. If you complete two out of the three projects, then you will receive a certificate.” 
For this part “Would submitting the homework late be a problem?” It will be a problem in the sense that you will not be able to do this. Once we close the form, the form does not accept any submissions, so you will simply not be able to do this. There is little point in this, because once we close the form, we also publish the answers. Theoretically, after this, you can just look the answers up in the solutions and then put it in the form. Then it kind of defeats the purpose. 
Submitting homework assignments after the due date will not be possible. It doesn't mean you will not be able to get the certificate. Let's say if you're joining this course in November, then you can binge watch everything until the end (I know some people did this last year) and then you just focus on projects for the remaining time and then you get the certificate at the end anyway."
518	What team are you supporting in the Football World Cup?	For me, as a Russian citizen, it was actually surprising to learn that the World Cup started, because I simply didn't know that until last week, when people all of a sudden started watching football. I see that Ukraine isn't playing there – they have other problems right now, I suspect. I would root for them if they played. I guess Poland. I know that they won a match recently against Saudi Arabia – so, good job. I didn't watch it though. But if you asked me which team I would root for, then maybe Poland.
519	What team are you supporting in the Football World Cup?	Yeah. If it works for you, use it. I don't mind.
520	Is it possible to extend the week 3 homework deadline?	For Spark, we use Python. So it's PySpark. So what Java aspects will be covered? Ankush already uploaded the videos for week 6, just go and check that out.
521	Is it possible to extend the week 3 homework deadline?	I will not answer this right now. We did not plan anything like that. I think Anna prepared some materials about how to deploy Prefect, not with Terraform, but with some Google Cloud Platform stuff. I might be mistaken, but in practice, I think we use something like Kubernetes. Kubernetes is not managed through Terraform. I will leave this question answered and in the next Office Hours when we'll talk more about Prefect. Somebody from the Prefect team will answer this question. Probably Jeff will be in our next Office Hours.
522	Is it possible to extend the week 3 homework deadline?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
523	Is it possible to extend the week 3 homework deadline?	"Alexey
I don't know when this question was asked, but we did extend it."
524	Is it possible to extend the week 3 homework deadline?	Yes, you will have to come up with your own individual projects. It's not easy, I know. But we'll help you. We will help you to come up with a project idea. That's actually on purpose because we want every project to be unique. Then it becomes a part of your portfolio when you look for a job.
525	What are some other EDA/DA could we do on the housing dataset?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
526	What are some other EDA/DA could we do on the housing dataset?	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
527	What are some other EDA/DA could we do on the housing dataset?	"Yes, indeed there is. You need to get something like 50% in order to pass the project. I don’t remember the exact score we used last year. This is how we did it last year – we looked at the distribution of the scores. We didn’t want to mark many projects as failed, so we looked at the distribution and we made sure that most of you passed the project. I think the passing score was pretty low. Most of the students who did the project – who did put some effort there in doing the project and who met at least half of the criteria or most of the criteria but partly – they passed the project. It wasn't a big problem, so don't worry about that too much. 
As long as you put in some effort, then you'll pass it. The people who didn't pass last year submitted empty projects, or projects that were copied from somewhere, or projects where they just added some readme file and maybe copied some Kaggle kernel and that was it. So nothing was done in addition to that. There were people who passed, but they just didn't put any effort into attempting to do the project. If you put in some genuine effort, don't worry, you will pass the project."
528	What are some other EDA/DA could we do on the housing dataset?	This is such a broad question, I don't even know how to answer that. You need to come up with a question and then try to answer this question with data. So only your imagination is the limit here. Right now, maybe my imagination about this is not very good [chuckles]. I cannot just go ahead and generate a lot of ideas of what you can do. But maybe just open this dataset, take a look at this, stare at it for a couple of minutes, and then you will get a question. Then try to answer this question with data.
529	What are some other EDA/DA could we do on the housing dataset?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
530	I am used to train_test_split() from SciKit Learn but in our example, it was split/train/test/val. Could you explain the difference between these two approaches?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
531	I am used to train_test_split() from SciKit Learn but in our example, it was split/train/test/val. Could you explain the difference between these two approaches?	I'm not sure I understand the question. The question is “Why test_train_split splits in two, but here, we split into three parts? For that there is a video in week one that explains why we need three datasets, not two. So you can go check it out. There is a problem called “multiple comparison problem” and that's why we use test/split to account for that. You will see later, in module three – there we can also use SciKit Learn train_test_split to split our original dataset into three parts and this is exactly what we will do.
532	I am used to train_test_split() from SciKit Learn but in our example, it was split/train/test/val. Could you explain the difference between these two approaches?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
533	I am used to train_test_split() from SciKit Learn but in our example, it was split/train/test/val. Could you explain the difference between these two approaches?	"Tim 
I think, yes. We have a few examples on our website. I think with Colab, it becomes a little bit more difficult once you're hosting the Bento to access it, if it's running on a Google Colab server. But yeah, you can save it. What I would say is that you can save your model and then export your model to somewhere where you can then build your Bento there.
Alexey 
By default, Bento saves in a local file system (in your /home) so you need to have a way to save this model somewhere, let's say on the cloud, S3, whatever – somewhere externally. The workflow here is that you train your model in Colab, you save it, and then you deploy it somewhere. You don't deploy it on a Colab instance.
Tim
Right. When you save your model, I think it'll save it to the Colab instance. Then you have to use BentoML Export to push it to an S3 bucket or something where you can pick it up and deploy your Bento.
Alexey 
And there are options for deploying. We already discussed this in the course, when we looked at Elastic Beanstalk. I assume it can work with Bento. I haven't checked. [Tim agrees] But then what we saw this week, we saw how to deploy it with ECS, and then we can also deploy to Kubernetes EKS, we can also deploy to lambda – there are a ton of other options. Everywhere where you can deploy a Docker container (Docker image) you can deploy it there. Right? [Tim agrees]"
534	I am used to train_test_split() from SciKit Learn but in our example, it was split/train/test/val. Could you explain the difference between these two approaches?	"Tim
I think a couple people asked this in ML Zoomcamp Slack. I was thinking about this a little bit. I think it's sort of a philosophical thing [chuckles] where we think about models and Bento is sort of separate a little bit from code. So your project directory is where all the stuff that you'll commit to Git. I don't think we think that it's the right model to commit your models to Git, for example. We think of ourselves a little bit more like Docker. You don't have Docker images in your project directory, right? You just have your Docker file and that's what you commit to Git. In the same way, we store your models and your Bento in that dedicated directory, and then sort of leave it up to you to push it to different places where you might build or where you might need it
Alexey
Do you know why you decided to save it locally on your local file system? For example, what MLflow does – you can save it to S3 directly, but in the case of Bento, you always first save it to local storage and then you decide “This model I want to export, but for the rest, I don't care.”
Tim
Right. First, I think it’s because that's the easiest way to do it, but also because I think with BentoML, the workflow is that a lot of times, somebody's building a model on their laptop, or somebody's building a model in a slightly less distributed system and they're assembling their Bento there. So I think that the fastest way to access that model is locally. We do give you the ability to push that model and pull that model from other places. But ultimately, when you assemble your Bento and then you assemble your container, all that stuff has to be there. I don't think we wanted separate abstractions for when a person is training and saving, versus when a person is building a Bento and pulling in the model and assembling the container. It's just the same abstraction, which is just a local repository which contains models.
Alexey 
Maybe you don't want to save every model every single time you change one single parameter to the cloud. [Tim agrees] Maybe at the end, after all the experiments, you want to save the final one. Meanwhile, for MLflow, the use case is different – you actually want to save every single experiment because you want to see the results of these experiments.
Tim
Right."
535	How do you handle categorical variables with several options in the dictionary vectorizer, since it consumes a lot of memory when training it?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
536	How do you handle categorical variables with several options in the dictionary vectorizer, since it consumes a lot of memory when training it?	"One thing I would suggest is to check out Office Hours from last year (2021) if you go to the midterm project, there are these Office Hours. I think it's week nine, where I show how to use the count vectorizer for doing one hot encoding. With count vectorizer, you can apply some filtering. For example, you can say that “I'm only interested in categories that appear at least in 100 observations.” Then, instead of looking at all possible values, you look at only the frequent ones. So go through this document. 
I think there are a lot of different ways you can add filtering here. It could be minimal frequency, it could be something else. It's actually kind of misusing count vectorizer a bit, because count vectorizer is supposed to be used with text features, and not with usual categorical features. But you kind of hack it, in a way, if you say, “Okay, turn these categories into text, and then you train the count vectorizer on them.” So yeah, go through this thing. You can also find the video from that, where I do it live. That's very useful. Again, if you have a huge load of different categories – different values – then the count vectorizer will still require some memory to train. 
There is an alternative. It's called a hashing vectorizer. This one. It does not require training. So you can say, “Okay, I only want to have only like 10,000 features and not more than that.” And it will actually take a word, and it will compute a hash of this word, or value by category, and it will randomly put in one of the columns of this vectorizer. Not randomly – it will compute hash and then put in one of them. So it will be deterministic, of course. Sorry, I didn't choose the right words. 
This is a good way to save memory if you have a lot of categories. So, hashing vectorizer – it works in the same way as count vectorizer, except you don't need to do fit. Actually, you can actually just go through this and read it yourself. It explains everything that you need to know when you compare this one versus count vectorizer."
537	How do you handle categorical variables with several options in the dictionary vectorizer, since it consumes a lot of memory when training it?	"Tim 
Not really. We have users who use both Streamlit and Gradio. I think Streamlit and Gradio are a bit more about the presentation side. We do have a couple of users who have asked us about deeper Streamlit or Gradio support and I think that's something that's probably coming up in the next few months.
Alexey
For those who don't know what Streamlit or Gradio is – this is a way to create an interface for your services. Well, Gradio is focused on machine learning, meaning you create interfaces specifically for models. And then Streamlit is just a Python package for creating interfaces. We actually had a project at DataTalks.Club about Streamlit. I actually even have a tweet about this. This is what Streamlit looks like. In the video, you can see that you have this interface, and then based on the toggles that you move the core of your model changes. It gives you an easy way to build an interface like that. Meanwhile Bento focuses on quite a different use case, right? It’s not building interfaces, but serving models.
Tim 
Right, yeah. I think one of our users was asking for a really easy way, so that once you have your model and your Streamlit UI to be able to create a Bento deployment with one line in that way, anybody that you gave your Streamlit app to would be calling to that service and it could work anywhere. Which is a nice feature, I think.
Alexey 
Yeah, I think it is. I imagine if you have a Pydentic class there, then you can easily generate a Streamlit app for that.
Tim 
Right. Oh, that's a good idea. I hadn't thought of that. Yeah.
Alexey
And then in addition to that, the Swagger UI that you have, could be like having the actual interface. 
Tim
Oh, yeah, that would be cool. [chuckles]
Alexey
That could be a good Hackathon project, right?
Tim 
Yeah, for sure.
Alexey
From what I see – let's say you’ve built a model. Then you deployed this model and you have this nice API. You can query it with CURL. But what if you want to demo this project to your manager, or a manager of your manager, or somebody who does not necessarily have a technical background? If you give them the URL and say, “Okay, you need to execute the CURL command with a post request,” they would be like, “CURL what? What do you want from me? Just show me how it looks.” This is when tools like Streamlit come in handy. Instead of giving them a command line interface to query it, you just give them “Okay, this is the link. Play with this.” If Bento could do something like that – automatically generate things like that – many data scientists would thank you.
Tim
Yeah, I was just looking because I figured there's a library that turns Pydantic models into Streamlit apps. It looks like there is one that does that. I wonder if we could just plug into that app and then have that same thing. 
Alexey
That's cool. 
Tim
Yeah, that's awesome. That's kind of the cool part of open source – there are so many people out there with so many different projects and we get to collaborate with them and kind of build the best collaborations and combinations of these tools."
538	How do you handle categorical variables with several options in the dictionary vectorizer, since it consumes a lot of memory when training it?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
539	How do you handle categorical variables with several options in the dictionary vectorizer, since it consumes a lot of memory when training it?	I'm not sure I understand the question. The question is “Why test_train_split splits in two, but here, we split into three parts? For that there is a video in week one that explains why we need three datasets, not two. So you can go check it out. There is a problem called “multiple comparison problem” and that's why we use test/split to account for that. You will see later, in module three – there we can also use SciKit Learn train_test_split to split our original dataset into three parts and this is exactly what we will do.
540	I started self-paced learning with ML Zoomcamp. I'm thinking I should combine this Data Engineering Zoomcamp and do it in parallel. Any advice?	"Alexey
Yeah, that's not going to happen. I think ChatGPT will help us do our job, but it will not replace people, at least in my opinion. But I use it in my work and it's helpful. You should try using it too – if you want, of course."
541	I started self-paced learning with ML Zoomcamp. I'm thinking I should combine this Data Engineering Zoomcamp and do it in parallel. Any advice?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
542	I started self-paced learning with ML Zoomcamp. I'm thinking I should combine this Data Engineering Zoomcamp and do it in parallel. Any advice?	If it's a question for me, specifically – in Windows, I use a tool called Windows Terminal. This is the tool I use. In the settings, they have Git Bash, the usual command prompt, then they have multiple subsystems for Linux terminals. There also used to be PowerShell. I don't use PowerShell, so I don't have it here. This is what it looks like. I really like this. It’s just called Windows terminal. Check it out.
543	I started self-paced learning with ML Zoomcamp. I'm thinking I should combine this Data Engineering Zoomcamp and do it in parallel. Any advice?	If you have time, then why not? But as I said, this course requires some commitment from many people lately. Doing two courses in parallel might be ambitious, but if you have time, then why not?
544	I started self-paced learning with ML Zoomcamp. I'm thinking I should combine this Data Engineering Zoomcamp and do it in parallel. Any advice?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
545	What do you mean by “decision tree and XGBoost probabilities are not real probabilities”? What are they then? They are relativities at least, right?	The way the decision tree computes “probabilities” i.e. scores is that it looks at the leaf and in the leaf, it sees “What is the proportion of the customers, let's say, who defaulted?” And then this is the score. But the score does not necessarily represent a good probability from the theoretical point of view. From a practical one, it's fine. But from the theoretical point of view, there are some issues, let's say. Then please refer to this calibration stuff I referred to earlier, because I'm not ready to talk in more details about this right now. For simple projects, I think you will do fine with just using the output of this. Sometimes, as I said, your business requirements might require collaboration – that the output is consistent and that it's real probabilities. In terms of “real probability” remember that the definition of probability from probability theory is “How likely a thing is to happen?” “If I take this client, how likely is this client to leave?” The output of random forest might not be exactly this likelihood. It might be a score that you can use to understand the relative risk of this client to default or not. But it's not exactly a probability in the sense from the probability theory point of view.
546	What do you mean by “decision tree and XGBoost probabilities are not real probabilities”? What are they then? They are relativities at least, right?	"You can check out a few projects in this playlist. In the playlist, you can see some of the videos here, for example, there's a project from Alvaro. And it's not just Alvaro, there’s one from Alvaro and Ninad. You can just check out these projects – there’s a project from Carolina and Hamad, a project from Lisa, and a project from Timur. The project from Timor may actually fit what you want. Timur has done a lot of work and it's very interesting. So check it out. Actually, Timur’s project is not a midterm project. It's more like a capstone project – there is some deep learning there, he uses Karis. So maybe focus on the other three. 
You can also go to our course web page, and then go to midterm project (you need to use the 2021 folder) and then in the readme there should be some assignments. There will be some form that says “To find the projects you need to review, use this table.” Here, you can see all the projects from previous students’ midterm projects. You can pick some of them and check if you like them or not."
547	What do you mean by “decision tree and XGBoost probabilities are not real probabilities”? What are they then? They are relativities at least, right?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
548	What do you mean by “decision tree and XGBoost probabilities are not real probabilities”? What are they then? They are relativities at least, right?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
549	What do you mean by “decision tree and XGBoost probabilities are not real probabilities”? What are they then? They are relativities at least, right?	I guess, use validation.
550	How many percent of people successfully completed the course last year?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
551	How many percent of people successfully completed the course last year?	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
552	How many percent of people successfully completed the course last year?	Percentage-wise, it's hard because a lot of people signed up, but very few finished. I think we had slightly less than a hundred graduates. But this course is tough time-wise as well. It's okay to take your time for some of the weeks. You might be a bit behind, but maybe what you should do is focus on learning and not on getting a certificate, especially if you have work, if you have family, if you have other commitments. In the feedback form, some people said that it was difficult for them to actually finish the course in addition to other commitments. But what some of them did was take the course and finished it at their own pace and thus still successfully completed the course. But they are not part of the statistics that I shared.
553	How many percent of people successfully completed the course last year?	"Alexey
From what I see (And Ankush, maybe you can talk more about this) this is a perfect profile for switching to data engineering. Am I right?
Ankush
Yeah, I think this is a good time to be an engineer. And if you have Python knowledge and some database knowledge, you are already ahead of the curve. It won't be too difficult. It will be challenging, of course. But I hope this course helps you out. And I hope that overall, it would be really worth the effort. Because I think data engineering is a rewarding career for that."
554	How many percent of people successfully completed the course last year?	Yes
555	Is Scikit Learn different from what you did in the video for linear regression? For homework 2, would we get different results?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
556	Is Scikit Learn different from what you did in the video for linear regression? For homework 2, would we get different results?	"You probably mean for logistic regression, right? For logistic regression, it's doing something very similar to what we saw in module 2. It is trying to minimize the… (I'm just trying to figure out how I can explain it without going into too many details and not to get lost myself and not to confuse you). It's probably a good idea to refer to some other course. I think I partly talked about this. Let me check. Here I talk a little bit about this, in ML Zoomcamp Office Hours week #4, where I explain a little bit how exactly it works. But it's a very superficial, let's say, overview of exactly what's happening there. 
If you want to learn in more detail how exactly the process works there, maybe you can just go to Google and search for “gradient descent, logistic regression,” or something like this. The first result will probably have a good explanation of what's happening under the hood. SciKit Learn does not use this exact method – it doesn't use gradient descent – it uses something more complex (more advanced) than that. But this is roughly what's happening under the hood. I hope you're satisfied by this answer. I don't think I can give you a better one with the time we have. It will be like a separate lecture. If you don't want to go into details, you can think of this as something that’s similar to logistic regression happening there but we add sigmoid on top of that."
557	Is Scikit Learn different from what you did in the video for linear regression? For homework 2, would we get different results?	It was this above average thing. That was the y_train variable.
558	Is Scikit Learn different from what you did in the video for linear regression? For homework 2, would we get different results?	"Scikit Learn is not very different. This is not exactly how Scikit Learn is implemented. It's a little bit different, but the idea is very similar. In Scikit Learn, they use smarter ways of doing this. For example, in week two, we have a case when we need to apply regularization. When we have correlated or very close to correlated columns, then our matrix becomes very unstable, so we cannot easily invert it. 
If what I’m saying now doesn't make sense, it will make sense once you start watching the videos. Anyways, what I'm trying to say is that in Scikit Learn, they have a smart way of working around this, such that your weights – your vector with weights, the W vector – does not contain insanely large values. They have smarter methods for doing this. But apart from that, it's very similar and you should not get two different results. You can experiment, and I do recommend experimenting with Scikit Learn. We will actually do this in week three. In week three, we will use Scikit Learn for some of these things. 
In week two, we will implement things ourselves. But starting from week three, we will only use Scikit Learn and other libraries, so we will stop implementing things ourselves. Here's just to give you a taste of what machine learning is inside. It’s just a bunch of formulas – algorithms – there is no magic. Internally, Scikit Learn implements them too."
559	Is Scikit Learn different from what you did in the video for linear regression? For homework 2, would we get different results?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
560	I'm struggling with pandas. I am getting your lecture videos, but when I try to implement on my own, I don't know which method to use and how to use pandas.	"Welcome to the club. Every new library is difficult. You can replace the statement with “I am struggling with X,” and it will apply to pretty much any other library. The good news is that you can somehow find a solution to this problem and the solution is practice. You get stuck – try to get unstuck by Googling, by understanding what you need, and then try to formulate what the problem you’re facing right now is. What do you want to do with pandas, but don't know how? Then try to formulate it and put it into Google and you will find a solution. This is how you learn libraries. This is how I learned libraries. 
There is usually documentation, tutorials – try to go through these tutorials – but first, it starts with formulating what the problem you have is and you want to solve, and then trying to find the solution to this problem. Usually the solution is a piece of documentation, or a video, or a tutorial, or a project that somebody did. Then you try to make sense from the information you find and this is how you learn – by practicing. There are probably videos about pandas – tutorials that you can just watch to learn a lot of that. But you still need to put it into practice. If you don't, then you will simply forget what you learned in the courses."
561	I'm struggling with pandas. I am getting your lecture videos, but when I try to implement on my own, I don't know which method to use and how to use pandas.	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
562	I'm struggling with pandas. I am getting your lecture videos, but when I try to implement on my own, I don't know which method to use and how to use pandas.	"Yes or no. Accuracy can be used for multi-class classification – precision, recall and ROC curve, they're all for binary classification. But it's still possible to extend them for multi-class. First of all, there is a thing called one versus all classification. In this case, let's say we have three classes. [Image for reference] We have the green, blue and red classes. 
First, we actually see this problem as three binary classification problems and we train a separate model for each. For example, we can train a model for distinguishing green from the rest – maybe that will be our first logistic regression model. Then we can train a model for distinguishing the blue squares from the rest. And then we can have a third model for distinguishing the red crosses from the rest. This is called the “one vs all” approach. When we do this, we will end up with three binary models, and then we can compute precision, recall, F1 score, ROC, for each of these models separately and then we can merge them. We can take an average and see what the precision and recall are for this average. So that's one of the approaches. 
There are other approaches you can take. Let's say, ROC curve SciKit Learn, there is a way to extend it. [Image for reference] You see, for this roc_auc_score, they can actually do this for multi-class cases. There are different ways of how you can take average between these three classifiers. These micro/macro weighted samples – you can just read this description, this recommendation and see what the different ways of doing this are. I think this is a common approach. This is the one I just described – you compute them independently and then you kind of take the average."
563	I'm struggling with pandas. I am getting your lecture videos, but when I try to implement on my own, I don't know which method to use and how to use pandas.	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
564	I'm struggling with pandas. I am getting your lecture videos, but when I try to implement on my own, I don't know which method to use and how to use pandas.	"You will probably also need to learn how to pass interviews. This course does not teach that, but you will need to acquire the skill somehow. The best way to acquire these skills is by interviewing. But apart from that, I think this course covers let's say 80% of the skills that you will need for a job and the remaining 20% depends on the company. 
So you will need to do some research on the company where you want to work, see what kind of tools they use, what kind of problems they solve, how much emphasis they make in the interviews on the theoretical part. You will need to somehow figure this out and, based on that, see what you need to learn. I think with just this content, you should be able to get a machine learning-related job."
565	Can you revisit the ETA explanation? I do not understand what it means yet.	I think the easiest way to understand what it means is to just try different ETAs, and then see exactly how your learning curves – the plots where you show the performance – how exactly they look like. If it's too big, you will see that your model overfits very quickly – on the training set, it reaches 100% performance very quickly, but on the validation dataset, it becomes worse and worse after just a few trees (after a few iterations). So it happens very fast. If you set a learning rate that’s too small, then you will need a huge load of trees to actually have anything meaningful. It will simply take too much time to learn anything, which would be the case of underfitting. So selecting the right learning rate also takes some trial and error – you need to try different values to see what works best. What I usually do is try 0.3, I try 0.1, and then I try 0.01 0.05 – then I just look at these plots and based on that, I make my decision. Also, keep in mind that when you have a lot of trees, your model becomes slower. You don't want to have a slow model. Sometimes it's better to set the learning rate a bit higher so that you have fewer trees. So it's better that way.
566	Can you revisit the ETA explanation? I do not understand what it means yet.	"Scikit Learn is not very different. This is not exactly how Scikit Learn is implemented. It's a little bit different, but the idea is very similar. In Scikit Learn, they use smarter ways of doing this. For example, in week two, we have a case when we need to apply regularization. When we have correlated or very close to correlated columns, then our matrix becomes very unstable, so we cannot easily invert it. 
If what I’m saying now doesn't make sense, it will make sense once you start watching the videos. Anyways, what I'm trying to say is that in Scikit Learn, they have a smart way of working around this, such that your weights – your vector with weights, the W vector – does not contain insanely large values. They have smarter methods for doing this. But apart from that, it's very similar and you should not get two different results. You can experiment, and I do recommend experimenting with Scikit Learn. We will actually do this in week three. In week three, we will use Scikit Learn for some of these things. 
In week two, we will implement things ourselves. But starting from week three, we will only use Scikit Learn and other libraries, so we will stop implementing things ourselves. Here's just to give you a taste of what machine learning is inside. It’s just a bunch of formulas – algorithms – there is no magic. Internally, Scikit Learn implements them too."
567	Can you revisit the ETA explanation? I do not understand what it means yet.	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
568	Can you revisit the ETA explanation? I do not understand what it means yet.	I do sometimes. Not very often these days, but we have quite a few models that require deep learning. Usually, these models are related to images. We have some images on the platform, so when we need to understand what's happening on these images, we use deep learning.
569	Can you revisit the ETA explanation? I do not understand what it means yet.	"Tim 
Not really. We have users who use both Streamlit and Gradio. I think Streamlit and Gradio are a bit more about the presentation side. We do have a couple of users who have asked us about deeper Streamlit or Gradio support and I think that's something that's probably coming up in the next few months.
Alexey
For those who don't know what Streamlit or Gradio is – this is a way to create an interface for your services. Well, Gradio is focused on machine learning, meaning you create interfaces specifically for models. And then Streamlit is just a Python package for creating interfaces. We actually had a project at DataTalks.Club about Streamlit. I actually even have a tweet about this. This is what Streamlit looks like. In the video, you can see that you have this interface, and then based on the toggles that you move the core of your model changes. It gives you an easy way to build an interface like that. Meanwhile Bento focuses on quite a different use case, right? It’s not building interfaces, but serving models.
Tim 
Right, yeah. I think one of our users was asking for a really easy way, so that once you have your model and your Streamlit UI to be able to create a Bento deployment with one line in that way, anybody that you gave your Streamlit app to would be calling to that service and it could work anywhere. Which is a nice feature, I think.
Alexey 
Yeah, I think it is. I imagine if you have a Pydentic class there, then you can easily generate a Streamlit app for that.
Tim 
Right. Oh, that's a good idea. I hadn't thought of that. Yeah.
Alexey
And then in addition to that, the Swagger UI that you have, could be like having the actual interface. 
Tim
Oh, yeah, that would be cool. [chuckles]
Alexey
That could be a good Hackathon project, right?
Tim 
Yeah, for sure.
Alexey
From what I see – let's say you’ve built a model. Then you deployed this model and you have this nice API. You can query it with CURL. But what if you want to demo this project to your manager, or a manager of your manager, or somebody who does not necessarily have a technical background? If you give them the URL and say, “Okay, you need to execute the CURL command with a post request,” they would be like, “CURL what? What do you want from me? Just show me how it looks.” This is when tools like Streamlit come in handy. Instead of giving them a command line interface to query it, you just give them “Okay, this is the link. Play with this.” If Bento could do something like that – automatically generate things like that – many data scientists would thank you.
Tim
Yeah, I was just looking because I figured there's a library that turns Pydantic models into Streamlit apps. It looks like there is one that does that. I wonder if we could just plug into that app and then have that same thing. 
Alexey
That's cool. 
Tim
Yeah, that's awesome. That's kind of the cool part of open source – there are so many people out there with so many different projects and we get to collaborate with them and kind of build the best collaborations and combinations of these tools."
570	What are your thoughts on IBM's data engineering course?	Yes, you will have to come up with your own individual projects. It's not easy, I know. But we'll help you. We will help you to come up with a project idea. That's actually on purpose because we want every project to be unique. Then it becomes a part of your portfolio when you look for a job.
571	What are your thoughts on IBM's data engineering course?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
572	What are your thoughts on IBM's data engineering course?	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
573	What are your thoughts on IBM's data engineering course?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
574	What are your thoughts on IBM's data engineering course?	I do not know this course. Maybe you could share the link in Slack and we can discuss it.
575	Do BigQuery run and query times change based on the format like CSV/Parquet? For example, can using Parquet lead to lesser data scans/query time for external tables?	This question was probably asked before the solution was published. Just in case, I will show you where the solutions are. Go to cohorts, 2023 cohort, then the week 1 homework and we have the solution here. We already recorded the solution, so just go through this and you'll see the answers. It’s the same for the other homework assignments and solutions. For the Terraform part, there is no solution. It's just simply applying everything you saw in the lectures. For workflow orchestration, we will also have the solution here after the deadline.
576	Do BigQuery run and query times change based on the format like CSV/Parquet? For example, can using Parquet lead to lesser data scans/query time for external tables?	"Alexey
I think it depends. If it's an external table, then yes. If it's internal, I don't really know how it works, but I think in this case (I might be wrong) the data is already stored internally in BigQuery and you can do partitioning, clustering on that to have fewer scans and faster query time."
577	Do BigQuery run and query times change based on the format like CSV/Parquet? For example, can using Parquet lead to lesser data scans/query time for external tables?	"Alexey
No. I mean, we kind of told you. Yes, you maybe didn't see the intro video. But, sorry. No, it's late. We will actually recalculate the points for the second homework (HW 1B) because in the forum, it said that you can get up to one point, but we calculated that the cap there was seven. We are going to recalculate that. Thus, people who got a lot of points will get just one. Sorry about that. But don't focus on points. If you go to the leaderboard, we know that the person with this hash (722366d8b29ece9be3a7605363562c7c60d6918e) did seven links. Do you know who that is? Probably you don't. 
These points are virtual. It's nice to have a good score, but remember that these points mean whatever you want them to mean – not more than not less. Maybe it's okay if you get a few points less than others for the first homework, but the important thing here is consistency. If you just do your homework, even without doing any Learning in Public stuff, which I think you should do (you should post about the course). If you don't do this and you just do all the homework, by the end, you will be somewhere in the top 20. If you look at the leaderboard from the previous iteration, you will see that not everyone stuck around till the end. 
If you just do your homework, don't worry about these points, and you'll be fine. Sorry about that. I know that it can be discouraging that you didn't get as many points as others, but don't worry about that. Focus on learning. Maybe Michael has some other suggestions or some other things that I didn't mention, because he took part last year. But I think he was one of the people who did a few posts every week, right?
Michael
Yes. Actually, when I went through it, I missed the whole… you can get up to seven points, but I was just doing one every week and I just stuck with that. I was still near the top. I think the real value in that isn't necessarily the points but just being comfortable sharing – writing articles and making videos. That's the real value there. But everyone likes the clout value, too. As Alexey said, it will fluctuate, so if you keep up with it going forward, you'll probably be just fine."
578	Do BigQuery run and query times change based on the format like CSV/Parquet? For example, can using Parquet lead to lesser data scans/query time for external tables?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
579	Do BigQuery run and query times change based on the format like CSV/Parquet? For example, can using Parquet lead to lesser data scans/query time for external tables?	No, you cannot.
580	How does the peer review work?	Everything you do here is individual. You don't form teams.
581	How does the peer review work?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
582	How does the peer review work?	"Alexey
You finish your projects and then after that, you get to review three projects of your peers. There will be a set of criteria. It should be here in the project section, if you search for “Peer review criteria”. You will need to follow this criteria to evaluate. That's how it works, roughly."
583	How does the peer review work?	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
584	How does the peer review work?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
585	Do you have any date for when the new Terraform videos will be available? I already finished week 1 a few days ago.	"Alexey
You can create a Python path variable in your .bashrc and it will be there permanently. Just in the same way as you do with PATH. I don't know why we do it like that. Maybe I just didn't mention that. But, of course, you can just put everything you want in .bashrc and this alliance (this code) will be executed every time you launch your virtual machine. This way you can execute these things automatically."
586	Do you have any date for when the new Terraform videos will be available? I already finished week 1 a few days ago.	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
587	Do you have any date for when the new Terraform videos will be available? I already finished week 1 a few days ago.	Everything you do here is individual. You don't form teams.
588	Do you have any date for when the new Terraform videos will be available? I already finished week 1 a few days ago.	"Alexey
No. It's just in one way, you start it manually by clicking a button, and in another (in CLI or in the code)  instead of clicking a button, you're using the CLI or you do it from code. I don't think there is any difference. Or maybe you have some concrete example in mind? If so please, ask in Slack, if you want to clarify."
589	Do you have any date for when the new Terraform videos will be available? I already finished week 1 a few days ago.	We’re still in progress (as of January 26, 2023). I thought we would do this earlier. But it's still not finished.
590	Can you explain the possible pipeline phase if we can use DBT in the final project?	Well, what you can always do is create a new account. Don't forget to unlink your credit card if you want to use the same credit card. Unlink it from the billing from the old account, and then link it with a new one. Or if you have a different credit card, then you can just use that. For example, I use Revolut as a bank and in Revolut, you can create one-time virtual cards. That could be an option, too.
591	Can you explain the possible pipeline phase if we can use DBT in the final project?	"Victoria
Going back to Spark versus DBT, it's really up to you. You can use whatever you want. The possible pipeline phase, if you wanted to use DBT, will look similar to how your project looked until week four. So you would use whatever scheduler you want. Something that you could use is Airflow – or for you could use Prefect – to load the data that you chose to the backups and then move them to BigQuery. Once you have them in BigQuery, you would use DBT to do the transformation like you’re using Web4 and then you would use any analytics tool of your choice. We saw a database in Looker studio."
592	Can you explain the possible pipeline phase if we can use DBT in the final project?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
593	Can you explain the possible pipeline phase if we can use DBT in the final project?	"Alexey
No. I mean, we kind of told you. Yes, you maybe didn't see the intro video. But, sorry. No, it's late. We will actually recalculate the points for the second homework (HW 1B) because in the forum, it said that you can get up to one point, but we calculated that the cap there was seven. We are going to recalculate that. Thus, people who got a lot of points will get just one. Sorry about that. But don't focus on points. If you go to the leaderboard, we know that the person with this hash (722366d8b29ece9be3a7605363562c7c60d6918e) did seven links. Do you know who that is? Probably you don't. 
These points are virtual. It's nice to have a good score, but remember that these points mean whatever you want them to mean – not more than not less. Maybe it's okay if you get a few points less than others for the first homework, but the important thing here is consistency. If you just do your homework, even without doing any Learning in Public stuff, which I think you should do (you should post about the course). If you don't do this and you just do all the homework, by the end, you will be somewhere in the top 20. If you look at the leaderboard from the previous iteration, you will see that not everyone stuck around till the end. 
If you just do your homework, don't worry about these points, and you'll be fine. Sorry about that. I know that it can be discouraging that you didn't get as many points as others, but don't worry about that. Focus on learning. Maybe Michael has some other suggestions or some other things that I didn't mention, because he took part last year. But I think he was one of the people who did a few posts every week, right?
Michael
Yes. Actually, when I went through it, I missed the whole… you can get up to seven points, but I was just doing one every week and I just stuck with that. I was still near the top. I think the real value in that isn't necessarily the points but just being comfortable sharing – writing articles and making videos. That's the real value there. But everyone likes the clout value, too. As Alexey said, it will fluctuate, so if you keep up with it going forward, you'll probably be just fine."
594	Can you explain the possible pipeline phase if we can use DBT in the final project?	No, you cannot.
595	Not able to install TerraForm on Windows.	"Alexey
Yeah, that's not going to happen. I think ChatGPT will help us do our job, but it will not replace people, at least in my opinion. But I use it in my work and it's helpful. You should try using it too – if you want, of course."
596	Not able to install TerraForm on Windows.	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
597	Not able to install TerraForm on Windows.	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
598	Not able to install TerraForm on Windows.	Both. You can do it both ways and there are pros and cons. It really depends on the use case. From what I understood, it’s usually cheaper to first transform the data, put it into a data lake, and then load it to Google BigQuery. For example, in cases of DBT, this is what you do with DBT. You transform the data that is already in the data warehouse. Thus, both options work.
599	Not able to install TerraForm on Windows.	Sorry, I think they need to know more to help you. But maybe what will actually help you is going to week one, TerraForm GCP and we have the windows.md file that explains how to install Google Cloud SDK and how to install TerraForm. If this doesn't work, then please let us know in Slack. I've already seen a few threads that talk about problems on Windows. I'm quite certain you will find a solution there. If not, ask your question and don't forget to check frequently asked questions first.
600	Will we cover Databricks at some point?	"Alexey
For this case, if you go to our data engineering page, and then go to the homework, you will see this note: “If the answer does not match exactly, then select the closest option.” With that, if you have an answer, but it's somewhat not the same one, you just go with the closest one and then you'll get it right."
601	Will we cover Databricks at some point?	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
602	Will we cover Databricks at some point?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
603	Will we cover Databricks at some point?	No, it's not in our plans
604	Will we cover Databricks at some point?	No, you cannot.
605	Do you need to know anything about Big Data before starting this course?	No.
606	Do you need to know anything about Big Data before starting this course?	"Alexey
Yeah, we talked about that in Slack, so please check it. I think this happens because some of the columns have missing values and when you use Pandas for converting CSV to Parquet, it reads columns with missing values as double. That's why this happens."
607	Do you need to know anything about Big Data before starting this course?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
608	Do you need to know anything about Big Data before starting this course?	For Spark, we use Python. So it's PySpark. So what Java aspects will be covered? Ankush already uploaded the videos for week 6, just go and check that out.
609	Do you need to know anything about Big Data before starting this course?	We currently do not have any course partners yet. If you work at a company and you want to partner with us, please reach out and we will see how we can do it. In one of our courses, which was our first iteration of the Machine Learning Zoomcamp, we partnered with a company called Delphi. They got two interns from our course and they were quite satisfied with the outcome. You can actually read more about the work of these interns in our articles – Interview with Valerii Chetvertakov and then another one, Interview with Ken Wu. You can learn more about the interns and if you think that, at your company, you need good interns or juniors or you want to partner in any other way, please reach out and we can see how to make it work.
610	I missed the last two weeks. What would you recommend I do in order to get up to speed with the course?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
611	I missed the last two weeks. What would you recommend I do in order to get up to speed with the course?	No, you cannot.
612	I missed the last two weeks. What would you recommend I do in order to get up to speed with the course?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
613	I missed the last two weeks. What would you recommend I do in order to get up to speed with the course?	"Alexey
It’s up to you. If you really want to get a certificate, then you can maybe put more time into watching the videos. If you don't care about the certificate, just take the course at your own pace. When you finish it, you finish it. There is no right or wrong answer. Just do it the way you feel doing it and focus on learning. With the certificate, there is actually a second chance to get it. For example, if you start taking the course now and you're not on time with the first project, it's not a big deal because there is another project attempt and you can use that. Before that comes around, you probably will finish watching all the videos."
614	I missed the last two weeks. What would you recommend I do in order to get up to speed with the course?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
615	When will the capstone project start? Today (November 28, 2022)?	I don't know much about this, to be honest. I know that their model for predicting the price of real estate went rogue. So it started predicting some… I won’t say more, because I'm not super sure about what exactly happened there. I wasn't really following. I know that the company lost a lot of money because of a rogue model. I think if you just Google that you will find an explanation.
616	When will the capstone project start? Today (November 28, 2022)?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
617	When will the capstone project start? Today (November 28, 2022)?	No. It was supposed to be today, but I changed it a little bit. If you go to the updated deadline calendar, you will see that the capstone project starts next week (December 5, 2022). For the evaluation, we give you a lot of time. During this time, depending on when you take your vacation, you will have some time to evaluate the project and then the capstone project starts next week. I think what happened is that when we added Bento ML here, it kind of messed up the whole schedule. The idea was to finish everything by Christmas but, unfortunately, we won't be able to finish everything by Christmas which probably means that if you do the midterm projects 1 and 2, you can get your certificate somewhere here or maybe at the end January. I don't know. Let's see. It will probably be easier to just give certificates at the end of January. Let's say if you finish everything by New Year’s, then you're free to stop taking the course. I know it was very difficult and time consuming. You might as well just finish everything and start the data engineering course or enjoy your life, or do many other things. We'll also have an article, which starts pretty soon. During the next Office Hours, I should probably talk more about that.
618	When will the capstone project start? Today (November 28, 2022)?	"It all depends on what you mean by “full stack ML engineer”. As you’ve noticed, data engineer Zoomcamp is not about machine learning – it's about data engineering. If in your opinion a “full stack ML engineer” needs to know data engineering, then you should do data engineering. If not, then no. From my point of view, there is a “full stack of data scientist”. What I put in the definition of a full stack data scientist is – a person who can do all the steps in the CRISP-DM process. They can help product managers with business understanding, they can work with analysts in data understanding, they can help data engineers in the data preparation step, they can do the modeling, and they can deploy the models. If you want to be that kind of person, then, of course, you also need to learn a bit of product management and analytics, which we don't have courses for. But data engineering – preparing data before it goes to a model – will certainly be helpful. 
Also, machine learning ops Zoomcamp (MLOps Zoomcamp) will be helpful as well. I would suggest, if you really want to focus on machine learning engineering, then probably doing MLOps Zoomcamp will make more sense for you. But data engineering Zoomcamp will be quite useful in the future, because data scientists and ML engineers tend to work on data pipelines as well. 
Maybe not all the content will be useful for you. For example, the content about data warehousing – as a data scientist, I don't find it super useful in my work to me personally. Also, the module about analytics engineering – it's nice, but it's not something I do day to day. But the modules about batch, the modules about Spark, the modules about streaming – this is something that can be useful, because this is something we could use in building machine learning pipelines as well. 
To put data in a machine learning model, we need to prepare it – we need to create data pipelines – and this is pretty relevant. But you can just check it out. All the content is already there. You can just go and check it out. If you like it, do it – if you don't, don’t. I would suggest maybe going with this one (ML Zoomcamp)."
619	When will the capstone project start? Today (November 28, 2022)?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
620	For the second project submission, will there be a peer review as well?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
621	For the second project submission, will there be a peer review as well?	If it's a question for me, specifically – in Windows, I use a tool called Windows Terminal. This is the tool I use. In the settings, they have Git Bash, the usual command prompt, then they have multiple subsystems for Linux terminals. There also used to be PowerShell. I don't use PowerShell, so I don't have it here. This is what it looks like. I really like this. It’s just called Windows terminal. Check it out.
622	For the second project submission, will there be a peer review as well?	Yes, there will be peer review. For both projects attempts, there will be a peer review phase.
623	For the second project submission, will there be a peer review as well?	I would say to focus on projects, not on tools. In this course, you will get a chance to do this. If you open a job description for data engineers, you will see a lot, a lot, a lot of different technologies. Maybe think of a project that you can do for this particular company and do this project using, for example, the tools you learn here or the tools you want to learn, and build a portfolio. I guess this is more of a tip for getting a data engineering job, rather than for somebody who's starting their data engineering career. But if you already have a job, then maybe think of the business impact of what you need. I think I will not spend too much time talking about that because this requires a one hour long discussion. Perhaps we should have a podcast interview about that. If you can think of any potential guests for this podcast interview, you can let me know and we can try to invite these people and talk about this in more detail. One thing I want to share with you is that in our YouTube channel, we had a few interesting talks. We had this Getting a Data Engineering Job with Jeff talk. It’s a very nice presentation. Then we also had an interview with Jeff. So these two podcasts are quite interesting. You can also check out other ones. I think you'll find a lot of interesting content here.
624	For the second project submission, will there be a peer review as well?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
625	How are the take-home assignments that we get while applying for a data scientist position? Are they like what we have done in the course’s project?	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
626	How are the take-home assignments that we get while applying for a data scientist position? Are they like what we have done in the course’s project?	"Tim
The best way to reach out to me is in the Bento ML Slack. Usually, you can join our Slack group and then you can just directly message me"
627	How are the take-home assignments that we get while applying for a data scientist position? Are they like what we have done in the course’s project?	"It all depends on what you mean by “full stack ML engineer”. As you’ve noticed, data engineer Zoomcamp is not about machine learning – it's about data engineering. If in your opinion a “full stack ML engineer” needs to know data engineering, then you should do data engineering. If not, then no. From my point of view, there is a “full stack of data scientist”. What I put in the definition of a full stack data scientist is – a person who can do all the steps in the CRISP-DM process. They can help product managers with business understanding, they can work with analysts in data understanding, they can help data engineers in the data preparation step, they can do the modeling, and they can deploy the models. If you want to be that kind of person, then, of course, you also need to learn a bit of product management and analytics, which we don't have courses for. But data engineering – preparing data before it goes to a model – will certainly be helpful. 
Also, machine learning ops Zoomcamp (MLOps Zoomcamp) will be helpful as well. I would suggest, if you really want to focus on machine learning engineering, then probably doing MLOps Zoomcamp will make more sense for you. But data engineering Zoomcamp will be quite useful in the future, because data scientists and ML engineers tend to work on data pipelines as well. 
Maybe not all the content will be useful for you. For example, the content about data warehousing – as a data scientist, I don't find it super useful in my work to me personally. Also, the module about analytics engineering – it's nice, but it's not something I do day to day. But the modules about batch, the modules about Spark, the modules about streaming – this is something that can be useful, because this is something we could use in building machine learning pipelines as well. 
To put data in a machine learning model, we need to prepare it – we need to create data pipelines – and this is pretty relevant. But you can just check it out. All the content is already there. You can just go and check it out. If you like it, do it – if you don't, don’t. I would suggest maybe going with this one (ML Zoomcamp)."
628	How are the take-home assignments that we get while applying for a data scientist position? Are they like what we have done in the course’s project?	Yeah, they're similar. Not all of them will ask you to deploy stuff, but for data science, it's often necessary to do simple exploratory data analysis then train the model and write some conclusions.
629	How are the take-home assignments that we get while applying for a data scientist position? Are they like what we have done in the course’s project?	Yeah. If it works for you, use it. I don't mind.
630	I can see that there are meetings scheduled for the next six weeks – all of the resources are already on YouTube. What will we be discussing during those meetings?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
631	I can see that there are meetings scheduled for the next six weeks – all of the resources are already on YouTube. What will we be discussing during those meetings?	"Alexey
Yeah, that's not going to happen. I think ChatGPT will help us do our job, but it will not replace people, at least in my opinion. But I use it in my work and it's helpful. You should try using it too – if you want, of course."
632	I can see that there are meetings scheduled for the next six weeks – all of the resources are already on YouTube. What will we be discussing during those meetings?	It will be a Q&A like this one. You ask your questions and we answer. And that's pretty much it. Typically, it happens live. Sometimes it does not happen live. But the idea is to have a more or less live interaction.
633	I can see that there are meetings scheduled for the next six weeks – all of the resources are already on YouTube. What will we be discussing during those meetings?	No, you cannot.
634	I can see that there are meetings scheduled for the next six weeks – all of the resources are already on YouTube. What will we be discussing during those meetings?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
635	I want to be a full stack ML engineer. Do I need data engineering Zoomcamp or MLOps Zoomcamp after this?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
636	I want to be a full stack ML engineer. Do I need data engineering Zoomcamp or MLOps Zoomcamp after this?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
637	I want to be a full stack ML engineer. Do I need data engineering Zoomcamp or MLOps Zoomcamp after this?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
638	I want to be a full stack ML engineer. Do I need data engineering Zoomcamp or MLOps Zoomcamp after this?	"It all depends on what you mean by “full stack ML engineer”. As you’ve noticed, data engineer Zoomcamp is not about machine learning – it's about data engineering. If in your opinion a “full stack ML engineer” needs to know data engineering, then you should do data engineering. If not, then no. From my point of view, there is a “full stack of data scientist”. What I put in the definition of a full stack data scientist is – a person who can do all the steps in the CRISP-DM process. They can help product managers with business understanding, they can work with analysts in data understanding, they can help data engineers in the data preparation step, they can do the modeling, and they can deploy the models. If you want to be that kind of person, then, of course, you also need to learn a bit of product management and analytics, which we don't have courses for. But data engineering – preparing data before it goes to a model – will certainly be helpful. 
Also, machine learning ops Zoomcamp (MLOps Zoomcamp) will be helpful as well. I would suggest, if you really want to focus on machine learning engineering, then probably doing MLOps Zoomcamp will make more sense for you. But data engineering Zoomcamp will be quite useful in the future, because data scientists and ML engineers tend to work on data pipelines as well. 
Maybe not all the content will be useful for you. For example, the content about data warehousing – as a data scientist, I don't find it super useful in my work to me personally. Also, the module about analytics engineering – it's nice, but it's not something I do day to day. But the modules about batch, the modules about Spark, the modules about streaming – this is something that can be useful, because this is something we could use in building machine learning pipelines as well. 
To put data in a machine learning model, we need to prepare it – we need to create data pipelines – and this is pretty relevant. But you can just check it out. All the content is already there. You can just go and check it out. If you like it, do it – if you don't, don’t. I would suggest maybe going with this one (ML Zoomcamp)."
639	I want to be a full stack ML engineer. Do I need data engineering Zoomcamp or MLOps Zoomcamp after this?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
640	Since you mentioned MLflow, how does BentoML integrate with MLflow? Because with MLflow, you do something similar – you finish training your model and then at the end, you say, “MLflow.XGBoost.save_model”. Then you save the model to the MLflow registry. With Bento, it’s very similar, right? So how do these two work together?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
641	Since you mentioned MLflow, how does BentoML integrate with MLflow? Because with MLflow, you do something similar – you finish training your model and then at the end, you say, “MLflow.XGBoost.save_model”. Then you save the model to the MLflow registry. With Bento, it’s very similar, right? So how do these two work together?	I don't think I understand the question. Is the question about what we do when we have duplicates in data and we split it and then there are duplicates? Some of the datasets in week one and then one split and then another one, or…? Oh, okay. Yeah, the way we do it is, we generate random numbers. I don't remember, to be honest, how exactly we do this. Let me quickly take a look. If we go to regression, notebook, and split – I think we shuffle data, not draw random numbers. Yeah, we take a range from zero to n exclusive. So n is not included and then we shuffle it. This way, we make sure that the same number does not end up in the different splits. I hope that answers your question.
642	Since you mentioned MLflow, how does BentoML integrate with MLflow? Because with MLflow, you do something similar – you finish training your model and then at the end, you say, “MLflow.XGBoost.save_model”. Then you save the model to the MLflow registry. With Bento, it’s very similar, right? So how do these two work together?	Not always. For logistic regression, it does. For decision trees and random forest and XGBoost – not really. Then there is a thing called “calibration”. SciKit Learn calibration. You will need to check this out. I think there are also examples of how to do this with random forest. If you can't find examples here, there are examples on Kaggle. But practically speaking, I don't remember the last time I needed to use something like this. It's helpful when you want to train multiple models. Let's say you've been using logistic regression in production for some time and then you make some decisions based on this threshold. But then you want to deploy a new model (XGBoost) and you want to make sure that the ranges you have in the previous model are the same in the new model so the distribution looks kind of similar. This is where calibration helps. It's a useful thing, but not every application needs it.
643	Since you mentioned MLflow, how does BentoML integrate with MLflow? Because with MLflow, you do something similar – you finish training your model and then at the end, you say, “MLflow.XGBoost.save_model”. Then you save the model to the MLflow registry. With Bento, it’s very similar, right? So how do these two work together?	"Tim
Yeah, we have a lot of users who use MLflow and BentoML together. Once you have a model that’s sort of your “finalized” model – you save it to your MLflow registry and then (it depends on what your CI/CD pipeline looks like) but BentoML has a command to import from an MLflow registry. You can tell which MLflow model is the one that you want to deploy. It's very similar to save_model, except instead of pulling from your local environment, you're pulling from an MLflow registry. So you import the model, it automatically gets pulled into your Bento and then you can deploy it. On the BentoML documentation site, there's a big framework document on how to integrate with MLflow. 
Alexey
When searching, one of the suggestions was BentoML vs MLflow. I guess there are also some similarities because with MLflow, you can serve models. 
Tim
The thing about serving models is that it's a part of the pipeline that you just have to do. Every single framework out there has some way to serve a model – you train the model and then it just puts it up there to be able to serve. Typically, frameworks don't specialize in serving those. BentoML specializes in making the serving part really, really fast and really, really easy. What you find a lot of the time is, when you have serving logic in a really large end-to-end pipeline, it just puts the model there and then the only thing that you could submit to the model is the data. But we know in practice that there's usually business logic around this – pre-transformation logic, post-transformation logic, there's the version of the model that was saved. There are a lot of these other components that you want around the model, not just the model for inference. And of course there's the performance layer underneath. The BentoML architecture kind of brings that all together into one deployable, rather than just giving you one point to call inference for your model. 
Alexey
That is a comment in the live chat that says “There is PyCaret, too.” This is how you use PyCaret with Bento. 
Tim
Right. BentoML has lots and lots of integrations and we're only adding to them every day. The idea is to get the model from wherever you built it, bring it into your Bento, and then you can deploy it anywhere. Once it's in the Bento – in this standard packaging – it's nice because then we've got lots of different tools to deploy to Lambda, SageMaker, and lots of different places. And the places to deploy, we're adding to every week and month as well.
Alexey  
Since you can have a Docker container at the end – you do “bento_containerize” right? That's the command? And then you have a Docker container (Docker image) that you can deploy everywhere where you can deploy a Docker container, which is pretty much every place on the internet, right? 
Tim
Right. Some places, though, require specialized… 
Alexey
Like Lambda, right? 
Tim
Yeah, like Lambda won't accept just a normal Docker container. You have to actually create special endpoints. When we deploy to Lambda, we create those endpoints for you and make sure that it's behind an API gateway and things like that. For certain services, there are a lot of nuances that we kind of take care of for you – as long as you're in the Bento standard format.
Alexey 
For those who have no idea what MLflow is, and why we are talking about this – you don't need to worry about this right now. But at some point, this tool and other similar tools will be quite handy. You can check out our MLOps Zoomcamp course, after you finish this one. Don't try to do multiple courses at the same time. That could be too much. But of course, if you're curious, go check it out."
644	Since you mentioned MLflow, how does BentoML integrate with MLflow? Because with MLflow, you do something similar – you finish training your model and then at the end, you say, “MLflow.XGBoost.save_model”. Then you save the model to the MLflow registry. With Bento, it’s very similar, right? So how do these two work together?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
645	I see people referring to “Applied Linear Statistical Models” (Is it a book? I don't know this one) that says collinearity does not affect predictions. Is it true? Only if not severe? Depends on the model?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
646	I see people referring to “Applied Linear Statistical Models” (Is it a book? I don't know this one) that says collinearity does not affect predictions. Is it true? Only if not severe? Depends on the model?	"Alexey
CI stands for continuous integration. This is a way to – let's say you wrote some code, and then you push it to GitHub. Then on GitHub, there's GitHub actions, which is a way to automatically run some checks on your code – run tests, deploy things somewhere. Actually, again, I will do a shameless plug – in our MLOps Zoomcamp, we talked about GitHub actions in the best practices module. Right now, don't worry about this. Focus on your projects. But after you do your projects, it's really worth spending some time learning about best practices. These best practices include things like writing tests, using make files and also tools like CI/CD, infrastructure as code – we cover all that in the module. But if you are interested in these things, maybe it's actually worth taking that course too. We'll have another iteration in May. Not super soon – I’m just telling you that to get you excited."
647	I see people referring to “Applied Linear Statistical Models” (Is it a book? I don't know this one) that says collinearity does not affect predictions. Is it true? Only if not severe? Depends on the model?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
648	I see people referring to “Applied Linear Statistical Models” (Is it a book? I don't know this one) that says collinearity does not affect predictions. Is it true? Only if not severe? Depends on the model?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
649	I see people referring to “Applied Linear Statistical Models” (Is it a book? I don't know this one) that says collinearity does not affect predictions. Is it true? Only if not severe? Depends on the model?	"Yeah, it is true. If not severe? Yes, it does depend on the model. For example, for linear regression that we implemented ourselves in week 2, you saw that it does affect it. Our model simply could not work unless we added regularization. But when we added regularization, we fixed this. Collinearity was no longer an issue. So it is true and not true at the same time – it depends on the model. 
For tree-based models that we are covering right now in this week's materials, collinearity can also affect how we train our team models. If one feature and another feature are exact replicas of the same, then the tree would randomly select one of them. At the end, the model will probably be the same, more or less, but there are nuances. Sometimes it doesn't play well. 
For example, for random forest, when we select only a part of the feature set, the column that is a duplicate of another column – this feature simply will have more chances to get in a random set, which will affect the quality of the model, but probably not significantly. So I wouldn't say it's a big problem, because we have regularization, because we have models like trees that aren't really affected that strongly. But again, I don't think it's a good idea to have collinear features that correlate. Simply because – why do you need them? Your model will be simpler if you don't include them, so don't include them."
650	I think this question comes from week two, which is about RMSE. Can you give us a feeling of what is a good/acceptable RMSE in an industry project? Or does it depend project to project?	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
651	I think this question comes from week two, which is about RMSE. Can you give us a feeling of what is a good/acceptable RMSE in an industry project? Or does it depend project to project?	Yes. We will actually cover cross validation this week. Sometimes, if your dataset is small, (this was one of the questions today) then using cross validation is a very good idea.
652	I think this question comes from week two, which is about RMSE. Can you give us a feeling of what is a good/acceptable RMSE in an industry project? Or does it depend project to project?	"Indeed, it depends. It really depends on your data. Also, when it comes to RMSE, it depends on your target. If the values of your target are large, then your RMSE will also be large. So it really depends. For classification, usually the metrics we use are 80% (or something along those lines). So they are in percent, and are kind of not absolute, honestly. They don't depend on what you have in your data. But when it comes to RMSE, it depends on the values in your target variable. Here, it can be anything. There is no industry standard. 
Actually, when it comes to other evaluation metrics that we will study in this week – like accuracy, precision, recall – again, there is no good industry standard for this. In some cases, 90% is bad accuracy and in some cases, it's good accuracy. Precision of 90% is good sometimes, in some cases it’s bad. It really depends on the project, on what kind of data you have, how good your features are, and things like this. I would say that if your RMSE or any other performance metric is too good – for example, we will cover AUC as a metric – if it's 95%, then it's suspicious, usually. 95% or more. If I see a very good number, or RMSE that is close to zero, then it should raise suspicion and you should go and check and investigate and figure out why the performance is so good. Quite often, it's because you have some sort of data problems – data leakage, for example, or things like this. Usually, very good performance is an indicator of something being wrong."
653	I think this question comes from week two, which is about RMSE. Can you give us a feeling of what is a good/acceptable RMSE in an industry project? Or does it depend project to project?	"It all depends on what you mean by “full stack ML engineer”. As you’ve noticed, data engineer Zoomcamp is not about machine learning – it's about data engineering. If in your opinion a “full stack ML engineer” needs to know data engineering, then you should do data engineering. If not, then no. From my point of view, there is a “full stack of data scientist”. What I put in the definition of a full stack data scientist is – a person who can do all the steps in the CRISP-DM process. They can help product managers with business understanding, they can work with analysts in data understanding, they can help data engineers in the data preparation step, they can do the modeling, and they can deploy the models. If you want to be that kind of person, then, of course, you also need to learn a bit of product management and analytics, which we don't have courses for. But data engineering – preparing data before it goes to a model – will certainly be helpful. 
Also, machine learning ops Zoomcamp (MLOps Zoomcamp) will be helpful as well. I would suggest, if you really want to focus on machine learning engineering, then probably doing MLOps Zoomcamp will make more sense for you. But data engineering Zoomcamp will be quite useful in the future, because data scientists and ML engineers tend to work on data pipelines as well. 
Maybe not all the content will be useful for you. For example, the content about data warehousing – as a data scientist, I don't find it super useful in my work to me personally. Also, the module about analytics engineering – it's nice, but it's not something I do day to day. But the modules about batch, the modules about Spark, the modules about streaming – this is something that can be useful, because this is something we could use in building machine learning pipelines as well. 
To put data in a machine learning model, we need to prepare it – we need to create data pipelines – and this is pretty relevant. But you can just check it out. All the content is already there. You can just go and check it out. If you like it, do it – if you don't, don’t. I would suggest maybe going with this one (ML Zoomcamp)."
654	I think this question comes from week two, which is about RMSE. Can you give us a feeling of what is a good/acceptable RMSE in an industry project? Or does it depend project to project?	I do sometimes. Not very often these days, but we have quite a few models that require deep learning. Usually, these models are related to images. We have some images on the platform, so when we need to understand what's happening on these images, we use deep learning.
655	How much SQL do I need to know?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
656	How much SQL do I need to know?	For everything we do in this course, it's possible to run things locally. In some cases, when you need to create a Google Cloud Storage bucket or when you need to create a BigQuery table, you cannot do this locally. But for many things, you can. You can set up your Postgres instance, and we show you how to do this. We showed this in the first week’s video. In the Prefect videos, we also show how to do things locally. Then, you will basically need to skip week 3 because you will not be able to use BigQuery. And then week 4, we show how to use DBT with Postgres. Then with Spark, we show how to run things locally. And for week 6, with Confluent, I think you can still get access to Confluent Cloud, but you can also run things locally – run Kafka locally.
657	How much SQL do I need to know?	"Alexey
Yeah, we talked about that in Slack, so please check it. I think this happens because some of the columns have missing values and when you use Pandas for converting CSV to Parquet, it reads columns with missing values as double. That's why this happens."
658	How much SQL do I need to know?	Yes, you will have to come up with your own individual projects. It's not easy, I know. But we'll help you. We will help you to come up with a project idea. That's actually on purpose because we want every project to be unique. Then it becomes a part of your portfolio when you look for a job.
659	How much SQL do I need to know?	Enough to complete the first homework. If you can't complete the first homework, you should probably take a SQL course. Or maybe you can quickly take it now and finish the course. I know students who did that. If it's not too difficult, or not too unfamiliar regarding what we do in the SQL refresher video and for the homework, then you're fine.
660	I could not meet the registration deadline. Do I have a chance to register later and submit the assignments?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
661	I could not meet the registration deadline. Do I have a chance to register later and submit the assignments?	"There was no registration deadline at all for this course. So you can still register. You can still sign up and you can follow the course. You will not be able to submit the homework assignments where the due date is already over because there are already solutions posted, so it doesn't make much sense to submit homework when the solutions are already there. You can follow the course but you will not be able to submit the homework. You can check our frequently asked questions and I do recommend doing this. Please remember the rule – if you have a question, first go check the frequently asked questions, and if you don't find the question there, go and ask in Slack. 
For this particular question, it’s already there. “Don't worry, you can take the course.” Maybe right now it's becoming a little bit more challenging to catch up with everything, but note that you can skip the midterm project. Right now, if you want to catch up with everything, if you skip the midterm project, it's fine. You will be able to do two capstone projects and still get the certificate. Also, you can just take the course in self-paced mode. There is nothing wrong with that either."
662	I could not meet the registration deadline. Do I have a chance to register later and submit the assignments?	"Scikit Learn is not very different. This is not exactly how Scikit Learn is implemented. It's a little bit different, but the idea is very similar. In Scikit Learn, they use smarter ways of doing this. For example, in week two, we have a case when we need to apply regularization. When we have correlated or very close to correlated columns, then our matrix becomes very unstable, so we cannot easily invert it. 
If what I’m saying now doesn't make sense, it will make sense once you start watching the videos. Anyways, what I'm trying to say is that in Scikit Learn, they have a smart way of working around this, such that your weights – your vector with weights, the W vector – does not contain insanely large values. They have smarter methods for doing this. But apart from that, it's very similar and you should not get two different results. You can experiment, and I do recommend experimenting with Scikit Learn. We will actually do this in week three. In week three, we will use Scikit Learn for some of these things. 
In week two, we will implement things ourselves. But starting from week three, we will only use Scikit Learn and other libraries, so we will stop implementing things ourselves. Here's just to give you a taste of what machine learning is inside. It’s just a bunch of formulas – algorithms – there is no magic. Internally, Scikit Learn implements them too."
663	I could not meet the registration deadline. Do I have a chance to register later and submit the assignments?	I do sometimes. Not very often these days, but we have quite a few models that require deep learning. Usually, these models are related to images. We have some images on the platform, so when we need to understand what's happening on these images, we use deep learning.
664	I could not meet the registration deadline. Do I have a chance to register later and submit the assignments?	I think the easiest way to understand what it means is to just try different ETAs, and then see exactly how your learning curves – the plots where you show the performance – how exactly they look like. If it's too big, you will see that your model overfits very quickly – on the training set, it reaches 100% performance very quickly, but on the validation dataset, it becomes worse and worse after just a few trees (after a few iterations). So it happens very fast. If you set a learning rate that’s too small, then you will need a huge load of trees to actually have anything meaningful. It will simply take too much time to learn anything, which would be the case of underfitting. So selecting the right learning rate also takes some trial and error – you need to try different values to see what works best. What I usually do is try 0.3, I try 0.1, and then I try 0.01 0.05 – then I just look at these plots and based on that, I make my decision. Also, keep in mind that when you have a lot of trees, your model becomes slower. You don't want to have a slow model. Sometimes it's better to set the learning rate a bit higher so that you have fewer trees. So it's better that way.
665	What is the difference between Prefect and other ETL tools, like DBT and Matillion?	"There are lots of different varieties of approaches here. One thing that you might do is use some kind of data ingestion tool, as I mentioned in the live FAQ discussion – something like Airbyte or Fivetran – if you have lots of data that you're ingesting, that can be a common way to stay on top of that. Or you could do it through a script, kind of like we do here. 
But what you might do is run your agent on a virtual environment in Google Cloud VM, for example, or in AWS, or somewhere else. You could run it in Kubernetes. There are a number of places where maybe you would have that running and have your infrastructure there, so that you were just pulling down the code not to your local machine, but it would just be in your VM. 
If you're actually going and using Pandas, you're putting the data into memory, so you do need to have some scripts that will do that. And you can go directly between Google Cloud Storage and BigQuery with the Prefect GCP library. So that Prefect GCP library that you installed, that you've been using, does have some options in there. I think it's in the BigQuery module to actually get data directly into BigQuery from GCS. So there are a lot of options there. Check out Prefect GCP to see how to do it."
666	What is the difference between Prefect and other ETL tools, like DBT and Matillion?	I would say to focus on projects, not on tools. In this course, you will get a chance to do this. If you open a job description for data engineers, you will see a lot, a lot, a lot of different technologies. Maybe think of a project that you can do for this particular company and do this project using, for example, the tools you learn here or the tools you want to learn, and build a portfolio. I guess this is more of a tip for getting a data engineering job, rather than for somebody who's starting their data engineering career. But if you already have a job, then maybe think of the business impact of what you need. I think I will not spend too much time talking about that because this requires a one hour long discussion. Perhaps we should have a podcast interview about that. If you can think of any potential guests for this podcast interview, you can let me know and we can try to invite these people and talk about this in more detail. One thing I want to share with you is that in our YouTube channel, we had a few interesting talks. We had this Getting a Data Engineering Job with Jeff talk. It’s a very nice presentation. Then we also had an interview with Jeff. So these two podcasts are quite interesting. You can also check out other ones. I think you'll find a lot of interesting content here.
667	What is the difference between Prefect and other ETL tools, like DBT and Matillion?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
668	What is the difference between Prefect and other ETL tools, like DBT and Matillion?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
669	What is the difference between Prefect and other ETL tools, like DBT and Matillion?	"Alexey
I wouldn’t call DBT necessarily... It is kind of an ETL tool, but more like a transform tool. It doesn't do orchestration. Matillion – I have no idea what that is. What would be interesting is to compare Prefect to things like Airflow, Dagter, Flyte and other similar things.
Jeff
Yeah, I think people will learn about DBT here in week 4. You're gonna get to see how DBT works and get more experienced with that. Things like Airflow are potentially interesting to compare to. Kalise, you probably give that spiel more often than I do, if you want to do it.
Kalise
Yeah. Concerning Prefect, our founder was actually a main contributor to Airflow. It was really the vision of Prefect as an improvement of Airflow on workflow orchestration. Having data passing between tasks and being Pythonic as a first-class citizen feature is really important. I really suggest the article, Why Not Airflow? It was written quite a few years ago, but it actually touches on a lot of points of what Prefect does versus Airflow as well. In terms of just a lot of ETL tools, workflow orchestration can do more than just ETL as a whole. If all you are really doing is ETL, or transformations, or something very particular, definitely use the tool that's right for you.
Alexey
I think that there was a blog post from your colleague recently, from Anna, about something similar. Was there?"
670	Can you say something about the required computing power to be successful in the course?	"Alexey
For this case, if you go to our data engineering page, and then go to the homework, you will see this note: “If the answer does not match exactly, then select the closest option.” With that, if you have an answer, but it's somewhat not the same one, you just go with the closest one and then you'll get it right."
671	Can you say something about the required computing power to be successful in the course?	Yeah. Last year, we needed at least 16 GB of RAM to run Airflow. Here, we don't use Airflow, so probably 8 GB of RAM should be enough. But I will still go with a VM – I would still take 16 GB of RAM. It will just be better. Let's say you have a laptop with 8 GB of RAM and you already have Chrome running there and VSL and other things, there will really be no space for running stuff.
672	Can you say something about the required computing power to be successful in the course?	If it's a question for me, specifically – in Windows, I use a tool called Windows Terminal. This is the tool I use. In the settings, they have Git Bash, the usual command prompt, then they have multiple subsystems for Linux terminals. There also used to be PowerShell. I don't use PowerShell, so I don't have it here. This is what it looks like. I really like this. It’s just called Windows terminal. Check it out.
673	Can you say something about the required computing power to be successful in the course?	No, you cannot.
674	Can you say something about the required computing power to be successful in the course?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
675	How can I add Python ingest data to the compose file as a service for everything to work together by only typing “compose up”?	Yes, you can.
676	How can I add Python ingest data to the compose file as a service for everything to work together by only typing “compose up”?	"Alexey
I guess you refer to week 1, right? You do it in the same way as you do other things. There is one nuance,  one thing that you need to keep in mind, is that your service needs to wait till other things are ready. Perhaps you will need to add a bit of code there for that. If anyone knows a good example, please let us know and share it in Slack."
677	How can I add Python ingest data to the compose file as a service for everything to work together by only typing “compose up”?	Since we had more signups this year than last year, I hope it will be more than 100. But let's see.
678	How can I add Python ingest data to the compose file as a service for everything to work together by only typing “compose up”?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
679	How can I add Python ingest data to the compose file as a service for everything to work together by only typing “compose up”?	I would say to focus on projects, not on tools. In this course, you will get a chance to do this. If you open a job description for data engineers, you will see a lot, a lot, a lot of different technologies. Maybe think of a project that you can do for this particular company and do this project using, for example, the tools you learn here or the tools you want to learn, and build a portfolio. I guess this is more of a tip for getting a data engineering job, rather than for somebody who's starting their data engineering career. But if you already have a job, then maybe think of the business impact of what you need. I think I will not spend too much time talking about that because this requires a one hour long discussion. Perhaps we should have a podcast interview about that. If you can think of any potential guests for this podcast interview, you can let me know and we can try to invite these people and talk about this in more detail. One thing I want to share with you is that in our YouTube channel, we had a few interesting talks. We had this Getting a Data Engineering Job with Jeff talk. It’s a very nice presentation. Then we also had an interview with Jeff. So these two podcasts are quite interesting. You can also check out other ones. I think you'll find a lot of interesting content here.
680	What is an excellent midterm, capstone, or any ML project? Modeling lots of data, how features engineering is done, how it is presented, or the accuracy?	"Tim
Yeah, we have a lot of users who use MLflow and BentoML together. Once you have a model that’s sort of your “finalized” model – you save it to your MLflow registry and then (it depends on what your CI/CD pipeline looks like) but BentoML has a command to import from an MLflow registry. You can tell which MLflow model is the one that you want to deploy. It's very similar to save_model, except instead of pulling from your local environment, you're pulling from an MLflow registry. So you import the model, it automatically gets pulled into your Bento and then you can deploy it. On the BentoML documentation site, there's a big framework document on how to integrate with MLflow. 
Alexey
When searching, one of the suggestions was BentoML vs MLflow. I guess there are also some similarities because with MLflow, you can serve models. 
Tim
The thing about serving models is that it's a part of the pipeline that you just have to do. Every single framework out there has some way to serve a model – you train the model and then it just puts it up there to be able to serve. Typically, frameworks don't specialize in serving those. BentoML specializes in making the serving part really, really fast and really, really easy. What you find a lot of the time is, when you have serving logic in a really large end-to-end pipeline, it just puts the model there and then the only thing that you could submit to the model is the data. But we know in practice that there's usually business logic around this – pre-transformation logic, post-transformation logic, there's the version of the model that was saved. There are a lot of these other components that you want around the model, not just the model for inference. And of course there's the performance layer underneath. The BentoML architecture kind of brings that all together into one deployable, rather than just giving you one point to call inference for your model. 
Alexey
That is a comment in the live chat that says “There is PyCaret, too.” This is how you use PyCaret with Bento. 
Tim
Right. BentoML has lots and lots of integrations and we're only adding to them every day. The idea is to get the model from wherever you built it, bring it into your Bento, and then you can deploy it anywhere. Once it's in the Bento – in this standard packaging – it's nice because then we've got lots of different tools to deploy to Lambda, SageMaker, and lots of different places. And the places to deploy, we're adding to every week and month as well.
Alexey  
Since you can have a Docker container at the end – you do “bento_containerize” right? That's the command? And then you have a Docker container (Docker image) that you can deploy everywhere where you can deploy a Docker container, which is pretty much every place on the internet, right? 
Tim
Right. Some places, though, require specialized… 
Alexey
Like Lambda, right? 
Tim
Yeah, like Lambda won't accept just a normal Docker container. You have to actually create special endpoints. When we deploy to Lambda, we create those endpoints for you and make sure that it's behind an API gateway and things like that. For certain services, there are a lot of nuances that we kind of take care of for you – as long as you're in the Bento standard format.
Alexey 
For those who have no idea what MLflow is, and why we are talking about this – you don't need to worry about this right now. But at some point, this tool and other similar tools will be quite handy. You can check out our MLOps Zoomcamp course, after you finish this one. Don't try to do multiple courses at the same time. That could be too much. But of course, if you're curious, go check it out."
681	What is an excellent midterm, capstone, or any ML project? Modeling lots of data, how features engineering is done, how it is presented, or the accuracy?	Yeah. If it works for you, use it. I don't mind.
682	What is an excellent midterm, capstone, or any ML project? Modeling lots of data, how features engineering is done, how it is presented, or the accuracy?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
683	What is an excellent midterm, capstone, or any ML project? Modeling lots of data, how features engineering is done, how it is presented, or the accuracy?	"You can check out a few projects in this playlist. In the playlist, you can see some of the videos here, for example, there's a project from Alvaro. And it's not just Alvaro, there’s one from Alvaro and Ninad. You can just check out these projects – there’s a project from Carolina and Hamad, a project from Lisa, and a project from Timur. The project from Timor may actually fit what you want. Timur has done a lot of work and it's very interesting. So check it out. Actually, Timur’s project is not a midterm project. It's more like a capstone project – there is some deep learning there, he uses Karis. So maybe focus on the other three. 
You can also go to our course web page, and then go to midterm project (you need to use the 2021 folder) and then in the readme there should be some assignments. There will be some form that says “To find the projects you need to review, use this table.” Here, you can see all the projects from previous students’ midterm projects. You can pick some of them and check if you like them or not."
684	What is an excellent midterm, capstone, or any ML project? Modeling lots of data, how features engineering is done, how it is presented, or the accuracy?	That's a very similar question to when we should use the lambda and when we should use Kubernetes. If you want to use Kubernetes, then go with TensorFlow Serving. Otherwise, TensorFlow Lite. There is one thing where you have to use TensorFlow Lite – where you cannot use TensorFlow Serving – and this is when you want to deploy your model on a mobile device. I have a phone here. If you want to create an app with a model, then you will have to use TensorFlow Lite because you will not be able to use TensorFlow Serving. TensorFlow Serving also adds a bit of complexity. You will need to have this gateway service, you will need to have GRPC service, so that becomes more complex. If you have a lot of load, then you need to do this otherwise, TensorFlow Lite.
685	Does creating resources (like we have created Google Storage Bucket and BigQuery using Terraform) have any charges?	"There are lots of different varieties of approaches here. One thing that you might do is use some kind of data ingestion tool, as I mentioned in the live FAQ discussion – something like Airbyte or Fivetran – if you have lots of data that you're ingesting, that can be a common way to stay on top of that. Or you could do it through a script, kind of like we do here. 
But what you might do is run your agent on a virtual environment in Google Cloud VM, for example, or in AWS, or somewhere else. You could run it in Kubernetes. There are a number of places where maybe you would have that running and have your infrastructure there, so that you were just pulling down the code not to your local machine, but it would just be in your VM. 
If you're actually going and using Pandas, you're putting the data into memory, so you do need to have some scripts that will do that. And you can go directly between Google Cloud Storage and BigQuery with the Prefect GCP library. So that Prefect GCP library that you installed, that you've been using, does have some options in there. I think it's in the BigQuery module to actually get data directly into BigQuery from GCS. So there are a lot of options there. Check out Prefect GCP to see how to do it."
686	Does creating resources (like we have created Google Storage Bucket and BigQuery using Terraform) have any charges?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
687	Does creating resources (like we have created Google Storage Bucket and BigQuery using Terraform) have any charges?	Just one. It's either the first attempt or the second. You should not try both.
688	Does creating resources (like we have created Google Storage Bucket and BigQuery using Terraform) have any charges?	"Alexey
Yeah, we talked about that in Slack, so please check it. I think this happens because some of the columns have missing values and when you use Pandas for converting CSV to Parquet, it reads columns with missing values as double. That's why this happens."
689	Does creating resources (like we have created Google Storage Bucket and BigQuery using Terraform) have any charges?	I think that once you start using them. You also pay a little bit for requests – every time you make a request, you pay a little bit. That’s to my knowledge. I don't really know how it works in GCP. I usually use AWS. In AWS, every time you send a request, you have to pay a little bit. But I think you will get charged money, like fractions of cents, when you start using it.
690	Why do we have to add PySpark to the Python path every time on a VM? Why can't we add this to the path permanently like how we added Java and Spark?	"Jeff
I'm not sure exactly what's being referenced here. But I'm guessing maybe the GitHub question from the homework – the GitHub block question turned out to be a little trickier than anticipated for folks. This is a great question. I got a couple of things on it here. One is that there is some stuff in the documentation, but maybe not a complete handhold of things. The storage doc does talk about code storage and flow storage in general here, and a little bit about how to do it. 
If you would like more detailed information, it’s always a great idea to look and think about me opening an issue. If you go to the GitHub repository for Prefect, you can click on “new issue”. If you want, you can propose a feature enhancement, if you want to call it that, or report a bug if you think you should have more information, and then put in what you would like to see. 
If you want to go one step further at something I really recommend, even like the next extension of Learning in Public is contributing to projects. This is an open source project here. You're welcome to contribute to it. You could go ahead and fork the repository, clone it down, and make a change to the documentation once you've figured things out there. Lots of lots of folks I know did get there with some support, through the FAQ and through Slack. It's partly what the videos are also for, in the course, to help you out as you're going. That's what I would suggest there. 
But I will also suggest to our docs folks, that we perhaps enhance the GitHub section in particular."
691	Why do we have to add PySpark to the Python path every time on a VM? Why can't we add this to the path permanently like how we added Java and Spark?	I would say to focus on projects, not on tools. In this course, you will get a chance to do this. If you open a job description for data engineers, you will see a lot, a lot, a lot of different technologies. Maybe think of a project that you can do for this particular company and do this project using, for example, the tools you learn here or the tools you want to learn, and build a portfolio. I guess this is more of a tip for getting a data engineering job, rather than for somebody who's starting their data engineering career. But if you already have a job, then maybe think of the business impact of what you need. I think I will not spend too much time talking about that because this requires a one hour long discussion. Perhaps we should have a podcast interview about that. If you can think of any potential guests for this podcast interview, you can let me know and we can try to invite these people and talk about this in more detail. One thing I want to share with you is that in our YouTube channel, we had a few interesting talks. We had this Getting a Data Engineering Job with Jeff talk. It’s a very nice presentation. Then we also had an interview with Jeff. So these two podcasts are quite interesting. You can also check out other ones. I think you'll find a lot of interesting content here.
692	Why do we have to add PySpark to the Python path every time on a VM? Why can't we add this to the path permanently like how we added Java and Spark?	"Alexey
Yeah, that's not going to happen. I think ChatGPT will help us do our job, but it will not replace people, at least in my opinion. But I use it in my work and it's helpful. You should try using it too – if you want, of course."
693	Why do we have to add PySpark to the Python path every time on a VM? Why can't we add this to the path permanently like how we added Java and Spark?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
694	Why do we have to add PySpark to the Python path every time on a VM? Why can't we add this to the path permanently like how we added Java and Spark?	"Alexey
You can create a Python path variable in your .bashrc and it will be there permanently. Just in the same way as you do with PATH. I don't know why we do it like that. Maybe I just didn't mention that. But, of course, you can just put everything you want in .bashrc and this alliance (this code) will be executed every time you launch your virtual machine. This way you can execute these things automatically."
695	Since waitress-serve works on both Windows and Linux, isn't it a better idea to use it instead of Gunicorn?	Yes. We will actually cover cross validation this week. Sometimes, if your dataset is small, (this was one of the questions today) then using cross validation is a very good idea.
696	Since waitress-serve works on both Windows and Linux, isn't it a better idea to use it instead of Gunicorn?	Yes, there is a thing called the validation dataset that you can use to guide your decision. That's the best thing you can do. Another thing you should take into consideration is time. Let's say that we have a lot of categorical variables, then fitting a decision will take more time than a logistic regression. Then applying it will also take more time. So this is something you should also consider in your experiments. But in the end, you probably want to use the validation set to guide you.
697	Since waitress-serve works on both Windows and Linux, isn't it a better idea to use it instead of Gunicorn?	Well, I’m glad you asked. We have a data engineering Zoomcamp, which starts in January and we have the MLOps Zoomcamp, which starts in May. The only thing that is missing here to become a full stack data scientist is the business understanding part. Remember, when we talked about the CRISP DM process, the first part was business understanding. This is business domain knowledge and things like that. So the only thing that is missing to become a full stack data scientist is picking up this part. For that, I don't know what the best way to learn it is, apart from just joining a company and then talking to stakeholders (to the users of your model) and trying to understand more and more from them. I do recommend doing this, but let's say if you’re not working yet and you want to be a generalist, taking all these three courses is fine. You don't have to do everything there. For example, in the data engineering course, maybe data warehousing is not as important for data scientists as for data engineers, so maybe you can skip that part. But chances are that, as a data scientist, you will need to work with data warehouses too. So you might as well just watch the whole thing.
698	Since waitress-serve works on both Windows and Linux, isn't it a better idea to use it instead of Gunicorn?	Yeah. If it works for you, use it. I don't mind.
699	Since waitress-serve works on both Windows and Linux, isn't it a better idea to use it instead of Gunicorn?	Rescale is already preprocessing. Actually, in exception, this is what they do. It's actually the scaling there. There are two operations, I think. I don't remember exactly, but it's rescaling plus something else. So rescaling is preprocessing.
700	What's the best way to start a project in a real-world scenario?	In deep learning, if you saw the lectures already, you might have noticed that we don't use cross-validation there, simply because it's too time consuming. We set aside a validation dataset and we use this to guide us – to select the best parameter. That's the usual approach I follow, because with proper cross-validation, with creating three folds, it’s simply too time-consuming. That's why I follow a simpler approach. I guess that is the alternative.
701	What's the best way to start a project in a real-world scenario?	Yeah. If it works for you, use it. I don't mind.
702	What's the best way to start a project in a real-world scenario?	"Maybe you remember in module one, we talked about CRISP DM. This is a very good way to follow a project. There, you need to start with a goal, so “What kind of goal do you have in mind?” Then you need to think about how you will measure the success of this project. And then, based on that, you go through the rest of the steps. You're probably looking for a more detailed answer here. We can discuss again, but it all depends on your specific project. What do you already have in place? What do you want to do? 
It's all different, but the structure of how you approach the project and how you approach problems is very similar to CRISP DM. Usually, you start with the goal and then you find data that you need for this, then you build the pipelines, you take care of the data engineering part, and then you train your models. Then you evaluate this model, you deploy it. Maybe I missed a step here. But this is how you do this."
703	What's the best way to start a project in a real-world scenario?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
704	What's the best way to start a project in a real-world scenario?	I think for binary features, just one and zero makes more sense than OneHotEncoding. OneHotEncoding will take care of that but you will have two columns that are the exact opposite of each other. So that is a bit redundant. I think it's better just to convert it to a Boolean column with ones and zeros.
705	Who will review homework?	We will, or rather a script will review it.
706	Who will review homework?	"Alexey
No. I mean, we kind of told you. Yes, you maybe didn't see the intro video. But, sorry. No, it's late. We will actually recalculate the points for the second homework (HW 1B) because in the forum, it said that you can get up to one point, but we calculated that the cap there was seven. We are going to recalculate that. Thus, people who got a lot of points will get just one. Sorry about that. But don't focus on points. If you go to the leaderboard, we know that the person with this hash (722366d8b29ece9be3a7605363562c7c60d6918e) did seven links. Do you know who that is? Probably you don't. 
These points are virtual. It's nice to have a good score, but remember that these points mean whatever you want them to mean – not more than not less. Maybe it's okay if you get a few points less than others for the first homework, but the important thing here is consistency. If you just do your homework, even without doing any Learning in Public stuff, which I think you should do (you should post about the course). If you don't do this and you just do all the homework, by the end, you will be somewhere in the top 20. If you look at the leaderboard from the previous iteration, you will see that not everyone stuck around till the end. 
If you just do your homework, don't worry about these points, and you'll be fine. Sorry about that. I know that it can be discouraging that you didn't get as many points as others, but don't worry about that. Focus on learning. Maybe Michael has some other suggestions or some other things that I didn't mention, because he took part last year. But I think he was one of the people who did a few posts every week, right?
Michael
Yes. Actually, when I went through it, I missed the whole… you can get up to seven points, but I was just doing one every week and I just stuck with that. I was still near the top. I think the real value in that isn't necessarily the points but just being comfortable sharing – writing articles and making videos. That's the real value there. But everyone likes the clout value, too. As Alexey said, it will fluctuate, so if you keep up with it going forward, you'll probably be just fine."
707	Who will review homework?	"Jeff
I can try. I like Black a lot. If you just Google “Python Black” and go to the GitHub readme, it’s become super popular in the last couple of years for automatically formatting your code. It is really nice. Mine is set up in Visual Studio Code, so when I hit “save,” it automatically formats. As it says, it’s an uncompromising code formatter. I think it says here about asking Henry Ford for color back in the day. Is that right? It's colored cars or color telephones from Alexander Graham Bell or somebody, and it's like, “They can have any color they want as long as it's black.” He wasn't going to compromise. You don't have to think about it now. It's easy. Just go right ahead and get your gear code formatted automatically. So it’s things like two lines after function or before – that kind of stuff just happens automatically with it. Getting it set up in VS Code can be a little tricky sometimes. But there are guides to that. Googling is what I do for that, usually.
Alexey
I just wanted to do a shameless plug, because we have another course called MLOps Zoomcamp. By the way, there is also a Prefect model there. One of the things there is best practices. In best practices, we have this video called Code quality: linting and formatting. It does not show how to integrate Black with VS Code, I think. I don't remember if we do this. But if you want to learn more about testing, and Black, and other things like pre-commit hooks, make files, and so on – you can check this out.
Jeff
Yeah, it's not too tricky to set up. It's like pip install a package in your environment and then you have to give it a path maybe or set one setting in VS code. If all goes well, cross your fingers, it should just work after you reload it. Not always the case. If you have trouble, a lot of people use it, so there are a lot of good resources online. Black is great. This is an awesome set of some resources here on best practices. Type hints are just getting more and more popular. They're very helpful, so that people know what kind – especially with autocomplete and little pop-up type – things like that in Visual Studio Code and other code editors. You can see what kind of argument type you should put in and then Prefect uses that information as well, in our flows, for example, to make sure that if it's a block in the UI or a parameter in the UI – it'll be smart. It'll be like, “Oh, is this a number? Okay.” It will give you options to put in numbers. “Is this a different kind of form field?” It'll have different options. It also can then do some validation to make sure that people actually put in something that conforms to that type-in. Python is slowly getting more and more smart about how it handles typing and newer versions keep adding more functionality. Type hints are nice to use. It takes a little bit of writing, but it makes your docstring shorter. The last thing that was asked about here was docstrings. It's great to have in every function to tell people what it's about. It's something that maybe you don't always do if you're in a hurry, but you should do it, especially if other people are going to read the code. Code is read like 20 times more often than it's written, or however you translate that – some stat. So do it. It's so helpful for you in the future and it's helpful for other people in the future, who are going to read your code to see, “What were you thinking? What is the purpose of this function?” Keep your function small, explain it in your doc string – it's good stuff. Then it shows up in your code editor, if you're lucky (if you have a good code editor). That's all to say about that.
Alexey
Do you know any resources where people can learn about setting up? Or learning more about these things, like good Python coding standards? What I showed is obviously a good resource, but it does not cover all these things that this question asks about.
Jeff
It's a good question. I do have a link to Google Style, or there are a couple different styles of docstrings. It seems like they're a little bit much these days, maybe. But there are links for different ways to do type hinting. I do have a few things if I look around for them. I don't have them at my fingertips right now. But Michael looks like maybe he's got one he shared there.
Michael
Yes, this one is a little bit older, but it is great. It goes into using virtual environments, Poetry… there's a lot to unpack, but I think that's still pretty much the standard best practice at the moment."
708	Who will review homework?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
709	Who will review homework?	"Alexey
Google Cloud Platform is not mandatory. You can do most of the course without a cloud. You will not be able to use BigQuery, obviously, because it's a cloud offering. But you can use local postgres instead of that and I think you can run almost everything locally. For example, for streaming, instead of using Confluent Cloud, you can use Kafka. You can set it up locally.
Ankush
We also provide Docker images for everything, but the videos will be more on Confluent Cloud. But you can always change that.
Alexey  
I know that there are people from Nigeria, Iran, who cannot register in Google Cloud Platform. You can do things locally."
710	Why not use Ansible Playbook for setup preparations?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
711	Why not use Ansible Playbook for setup preparations?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
712	Why not use Ansible Playbook for setup preparations?	Contributions are welcome. If you know how to do this, then please create a guide and then share it with us. You can also create a pull request and include the link in the course repo.
713	Why not use Ansible Playbook for setup preparations?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
714	Why not use Ansible Playbook for setup preparations?	Everything you do here is individual. You don't form teams.
715	Why is the Prefect register block used? As I can see the GCS block in Prefect before as well?	"Alexey
You will see links soon. I think we already have the first one, but we just need to put it on GitHub. We will communicate over everything in Telegram and then the automator will post the link from Telegram to Slack. So you will see if you're in Slack."
716	Why is the Prefect register block used? As I can see the GCS block in Prefect before as well?	"Jeff
This is getting at different servers. When we start out in the course, you're using the Prefect Orion server. It has a bunch of blocks ready to go with it – they’re built in and it knows about this. But then there are bunches of modules – we have over 40 different collections now on our integrations. There are all kinds of different things out there. If someone has a piece of software and they want it to integrate, they make a module for it, you can install it with pip, but we need to somehow let the server know that it has some blocks in it that could be used by Prefect. So that's what Prefect block register does, as the name of the module suggests. If you connect to a different server, like Prefect Cloud, for example, or a different workspace there, the server doesn't know that you have those blocks available. It doesn't have anything on that server. Those blocks live on that server, so you need to somehow let it know. That's what registering those blocks can do with the block types."
717	Why is the Prefect register block used? As I can see the GCS block in Prefect before as well?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
718	Why is the Prefect register block used? As I can see the GCS block in Prefect before as well?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
719	Why is the Prefect register block used? As I can see the GCS block in Prefect before as well?	No, you cannot.
720	Do I need to know cloud basics?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
721	Do I need to know cloud basics?	No. From what I understood, from what Ankush said, you don't need to know Java.
722	Do I need to know cloud basics?	It will be helpful, but you can learn this in the course as well.
723	Do I need to know cloud basics?	For Spark, we use Python. So it's PySpark. So what Java aspects will be covered? Ankush already uploaded the videos for week 6, just go and check that out.
724	Do I need to know cloud basics?	We currently do not have any course partners yet. If you work at a company and you want to partner with us, please reach out and we will see how we can do it. In one of our courses, which was our first iteration of the Machine Learning Zoomcamp, we partnered with a company called Delphi. They got two interns from our course and they were quite satisfied with the outcome. You can actually read more about the work of these interns in our articles – Interview with Valerii Chetvertakov and then another one, Interview with Ken Wu. You can learn more about the interns and if you think that, at your company, you need good interns or juniors or you want to partner in any other way, please reach out and we can see how to make it work.
725	When we have a large dataset, how should we select the features? Should I use correlation? What if the relationship was not linear?	"Yeah. Here, the important thing is to use some validation framework. So set aside some data for testing – for validation – and then use it to select features. There are many things you can actually use for selecting features. If you go to our DataTalks.club YouTube channel, there’s a talk called Feature Selection in Machine Learning with Python. In this talk, Soledad explains what kind of feature selection models there are – algorithms. So please check it out. And I think if you just Google “feature selection,” you will find a lot of articles about this. 
Correlation is a good first step and we will see this in week three, when we look at feature importance. Sometimes you can just throw away unimportant features and that should be large enough. But the best way here is to use cross-validation or just validation and see if removing the feature doesn’t actually doesn't change the performance of your model."
726	When we have a large dataset, how should we select the features? Should I use correlation? What if the relationship was not linear?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
727	When we have a large dataset, how should we select the features? Should I use correlation? What if the relationship was not linear?	"Tim 
Not really. We have users who use both Streamlit and Gradio. I think Streamlit and Gradio are a bit more about the presentation side. We do have a couple of users who have asked us about deeper Streamlit or Gradio support and I think that's something that's probably coming up in the next few months.
Alexey
For those who don't know what Streamlit or Gradio is – this is a way to create an interface for your services. Well, Gradio is focused on machine learning, meaning you create interfaces specifically for models. And then Streamlit is just a Python package for creating interfaces. We actually had a project at DataTalks.Club about Streamlit. I actually even have a tweet about this. This is what Streamlit looks like. In the video, you can see that you have this interface, and then based on the toggles that you move the core of your model changes. It gives you an easy way to build an interface like that. Meanwhile Bento focuses on quite a different use case, right? It’s not building interfaces, but serving models.
Tim 
Right, yeah. I think one of our users was asking for a really easy way, so that once you have your model and your Streamlit UI to be able to create a Bento deployment with one line in that way, anybody that you gave your Streamlit app to would be calling to that service and it could work anywhere. Which is a nice feature, I think.
Alexey 
Yeah, I think it is. I imagine if you have a Pydentic class there, then you can easily generate a Streamlit app for that.
Tim 
Right. Oh, that's a good idea. I hadn't thought of that. Yeah.
Alexey
And then in addition to that, the Swagger UI that you have, could be like having the actual interface. 
Tim
Oh, yeah, that would be cool. [chuckles]
Alexey
That could be a good Hackathon project, right?
Tim 
Yeah, for sure.
Alexey
From what I see – let's say you’ve built a model. Then you deployed this model and you have this nice API. You can query it with CURL. But what if you want to demo this project to your manager, or a manager of your manager, or somebody who does not necessarily have a technical background? If you give them the URL and say, “Okay, you need to execute the CURL command with a post request,” they would be like, “CURL what? What do you want from me? Just show me how it looks.” This is when tools like Streamlit come in handy. Instead of giving them a command line interface to query it, you just give them “Okay, this is the link. Play with this.” If Bento could do something like that – automatically generate things like that – many data scientists would thank you.
Tim
Yeah, I was just looking because I figured there's a library that turns Pydantic models into Streamlit apps. It looks like there is one that does that. I wonder if we could just plug into that app and then have that same thing. 
Alexey
That's cool. 
Tim
Yeah, that's awesome. That's kind of the cool part of open source – there are so many people out there with so many different projects and we get to collaborate with them and kind of build the best collaborations and combinations of these tools."
728	When we have a large dataset, how should we select the features? Should I use correlation? What if the relationship was not linear?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
729	When we have a large dataset, how should we select the features? Should I use correlation? What if the relationship was not linear?	"You probably mean for logistic regression, right? For logistic regression, it's doing something very similar to what we saw in module 2. It is trying to minimize the… (I'm just trying to figure out how I can explain it without going into too many details and not to get lost myself and not to confuse you). It's probably a good idea to refer to some other course. I think I partly talked about this. Let me check. Here I talk a little bit about this, in ML Zoomcamp Office Hours week #4, where I explain a little bit how exactly it works. But it's a very superficial, let's say, overview of exactly what's happening there. 
If you want to learn in more detail how exactly the process works there, maybe you can just go to Google and search for “gradient descent, logistic regression,” or something like this. The first result will probably have a good explanation of what's happening under the hood. SciKit Learn does not use this exact method – it doesn't use gradient descent – it uses something more complex (more advanced) than that. But this is roughly what's happening under the hood. I hope you're satisfied by this answer. I don't think I can give you a better one with the time we have. It will be like a separate lecture. If you don't want to go into details, you can think of this as something that’s similar to logistic regression happening there but we add sigmoid on top of that."
730	Is it possible to submit a competition entry as a capstone project? I'd love to enter the competition, but I'm afraid I won't have time for both.	I guess not. That's the process of learning? Maybe more senior people don't… Yeah, I see where you're going with this. Senior people are not supposed to go bankrupt and default. They are probably supposed to pay back the debt. I guess these are the particularities of this dataset, I assume. Indeed, it's probably a data issue ,or not an issue, but a “characteristic” of this dataset.
731	Is it possible to submit a competition entry as a capstone project? I'd love to enter the competition, but I'm afraid I won't have time for both.	Yeah. If it works for you, use it. I don't mind.
732	Is it possible to submit a competition entry as a capstone project? I'd love to enter the competition, but I'm afraid I won't have time for both.	It is possible to use the competition as your capstone project. I actually would like to encourage that. So please do that. There’s one thing, though. In the competition, you only train a model – you do not deploy the model. That is something you will need to do extra for the capstone project. You can spend a bit of time taking part in the competition, then you will have a model, and then you can just do the rest of the stuff like deployment of that model for your capstone project. That is totally fine. Also, the competition will last for two months. I wanted to open it from the first of December, but somehow I didn't see how to keep it closed and open it on the first of December on Kaggle, so it got opened earlier. The official date would be the first of December and then from that, it's two months. So maybe during these two months you will be able to find a bit of time. Please, go ahead and use it as your capstone project.
733	Is it possible to submit a competition entry as a capstone project? I'd love to enter the competition, but I'm afraid I won't have time for both.	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
734	Is it possible to submit a competition entry as a capstone project? I'd love to enter the competition, but I'm afraid I won't have time for both.	"You probably mean for logistic regression, right? For logistic regression, it's doing something very similar to what we saw in module 2. It is trying to minimize the… (I'm just trying to figure out how I can explain it without going into too many details and not to get lost myself and not to confuse you). It's probably a good idea to refer to some other course. I think I partly talked about this. Let me check. Here I talk a little bit about this, in ML Zoomcamp Office Hours week #4, where I explain a little bit how exactly it works. But it's a very superficial, let's say, overview of exactly what's happening there. 
If you want to learn in more detail how exactly the process works there, maybe you can just go to Google and search for “gradient descent, logistic regression,” or something like this. The first result will probably have a good explanation of what's happening under the hood. SciKit Learn does not use this exact method – it doesn't use gradient descent – it uses something more complex (more advanced) than that. But this is roughly what's happening under the hood. I hope you're satisfied by this answer. I don't think I can give you a better one with the time we have. It will be like a separate lecture. If you don't want to go into details, you can think of this as something that’s similar to logistic regression happening there but we add sigmoid on top of that."
735	If I don't do public learning, is it going to be a problem?	Terraform is not for creating containers. It's for creating infrastructure – different services in the cloud. One of these things could be creating a Kubernetes cluster, for example, creating… I don't remember how these services are called in Google Cloud Platform, but there are some other container orchestrators. Then you deploy your Docker containers there.
736	If I don't do public learning, is it going to be a problem?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
737	If I don't do public learning, is it going to be a problem?	Not at all.
738	If I don't do public learning, is it going to be a problem?	No, you cannot.
739	If I don't do public learning, is it going to be a problem?	We’re still in progress (as of January 26, 2023). I thought we would do this earlier. But it's still not finished.
740	Do you have any recommendations for where to start with NLP? Project-based, hands-on, not full of theories – like ML Bookcamp or some other camp?	That's a very similar question to when we should use the lambda and when we should use Kubernetes. If you want to use Kubernetes, then go with TensorFlow Serving. Otherwise, TensorFlow Lite. There is one thing where you have to use TensorFlow Lite – where you cannot use TensorFlow Serving – and this is when you want to deploy your model on a mobile device. I have a phone here. If you want to create an app with a model, then you will have to use TensorFlow Lite because you will not be able to use TensorFlow Serving. TensorFlow Serving also adds a bit of complexity. You will need to have this gateway service, you will need to have GRPC service, so that becomes more complex. If you have a lot of load, then you need to do this otherwise, TensorFlow Lite.
741	Do you have any recommendations for where to start with NLP? Project-based, hands-on, not full of theories – like ML Bookcamp or some other camp?	I don't know much about this, to be honest. I know that their model for predicting the price of real estate went rogue. So it started predicting some… I won’t say more, because I'm not super sure about what exactly happened there. I wasn't really following. I know that the company lost a lot of money because of a rogue model. I think if you just Google that you will find an explanation.
742	Do you have any recommendations for where to start with NLP? Project-based, hands-on, not full of theories – like ML Bookcamp or some other camp?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
743	Do you have any recommendations for where to start with NLP? Project-based, hands-on, not full of theories – like ML Bookcamp or some other camp?	"That's also a very generic question. It depends on what kind of data you have. Can you take a sample from this data or do you need to use the entire dataset? Also, as I said, what kind of data is it? Is it images? Is it text? Is it tables? I would just read a part of this. I actually don't know if it's possible with the HDF format. Maybe you can just read the first couple of thousand rows? I don't know. The best practices could be, let's say – the first one is to take a sample from this. 
The second one is maybe seeing how you can parallelize it with something like Spark. Then again, I don't know exactly what HDF format is and if you can read with Spark. Maybe not. Maybe you will need to first turn it into something else and then read it with something like Spark. Then the third one would be to get a big machine with 500 megs of RAM, for example, and then just read it from there. The last option seems to be the easiest one if you actually need to read all the data. In some cases you might not. Is it related to neural networks? I think some of the data was saved in this format. 
I might be wrong, but I think you might iterate over this data and then read it in chunks. You don't need to read the entire thing. You just read one gigabyte at a time, for example. That's another strategy - chunking. This is actually something that Spark relies on. When dealing with large datasets, Spark chunks the dataset into small partitions, and then it processes each partition separately. Then it combines the result at the end."
744	Do you have any recommendations for where to start with NLP? Project-based, hands-on, not full of theories – like ML Bookcamp or some other camp?	"What I personally like is Kaggle. It's not a course, yes I know. But there, you can see a lot of different kernels, notebooks that you can follow and try to learn. I don't think this answers your question. I'll give two recommendations. The first recommendation is to go to our MLOps Zoomcamp. Then in cohorts 2021, in the midterm project, we had these three office hours. During these office hours, I showed some extra stuff. In office hours 8, I showed how to deal with texts. So, if for your project, you're going to use text, then check this notebook and check out the corresponding video on how you can quickly incorporate text in your machine learning models. This is a relatively simple approach. There is also a good book called “
Introduction to Information Retrieval. This is a very nice book. It's also available for free. It's relatively old – from 2006 – but it's a very good book. It's quite practical. It talks about search, but many of these things are applicable to machine learning. For example, “vector space classification” is for text classification and Naive Bayes. So it’s not always relevant for NLP, but some of the things are. For example, “index construction/compression” is more relevant if you want to learn about search. But some of the things like “How do you break down strings into tokens, into words? What are the methods there?” I think it's a good book for that. There is also a thing called Taming Text. But I think the examples are in Java, if I'm not mistaken. So maybe it's a bit old. Kaggle is probably good. If you saw, I recently asked the question in general, “What kind of course do you want me to work on?” Maybe you can write what exactly you want to know about NLP. I know some stuff about classical NLP, information NLP, and search. But I don't know much about “modern NLP” which is what I call all these transformers, GPT-3, BERT – this kind of stuff. All this, I don't know anything about, to be honest, so I won't be able to actually do a course about that. But please go to the thread and write what you have in mind."
745	How to know what metric is useful with use cases?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
746	How to know what metric is useful with use cases?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
747	How to know what metric is useful with use cases?	"For each use case, indeed, you need to think about what kind of business problem you’re solving and then based on that, come up with a metric. In most cases, precision and recall and F1 score are useful. Accuracy – most of them are good. Maybe what you can do is go to Kaggle where there are a lot of competitions. You can go through these competitions and see what kind of evaluation metric they use. 
First you can try to understand what the problem is about and then you can see what kind of evaluation metric they use for this competition. This way, over time, you will build your intuition regarding what kind of metric to use in which situation? It's not always great. I think Kaggle has some limitations, at least it used to have them – it has a very limited set of metrics. Sometimes, competition organizers would use a metric that didn't make much sense. But it doesn't happen very often. I think right now, the platform supports very customized metrics for each use case. So check out Kaggle."
748	How to know what metric is useful with use cases?	"It all depends on what you mean by “full stack ML engineer”. As you’ve noticed, data engineer Zoomcamp is not about machine learning – it's about data engineering. If in your opinion a “full stack ML engineer” needs to know data engineering, then you should do data engineering. If not, then no. From my point of view, there is a “full stack of data scientist”. What I put in the definition of a full stack data scientist is – a person who can do all the steps in the CRISP-DM process. They can help product managers with business understanding, they can work with analysts in data understanding, they can help data engineers in the data preparation step, they can do the modeling, and they can deploy the models. If you want to be that kind of person, then, of course, you also need to learn a bit of product management and analytics, which we don't have courses for. But data engineering – preparing data before it goes to a model – will certainly be helpful. 
Also, machine learning ops Zoomcamp (MLOps Zoomcamp) will be helpful as well. I would suggest, if you really want to focus on machine learning engineering, then probably doing MLOps Zoomcamp will make more sense for you. But data engineering Zoomcamp will be quite useful in the future, because data scientists and ML engineers tend to work on data pipelines as well. 
Maybe not all the content will be useful for you. For example, the content about data warehousing – as a data scientist, I don't find it super useful in my work to me personally. Also, the module about analytics engineering – it's nice, but it's not something I do day to day. But the modules about batch, the modules about Spark, the modules about streaming – this is something that can be useful, because this is something we could use in building machine learning pipelines as well. 
To put data in a machine learning model, we need to prepare it – we need to create data pipelines – and this is pretty relevant. But you can just check it out. All the content is already there. You can just go and check it out. If you like it, do it – if you don't, don’t. I would suggest maybe going with this one (ML Zoomcamp)."
749	How to know what metric is useful with use cases?	"Tim
The best way to reach out to me is in the Bento ML Slack. Usually, you can join our Slack group and then you can just directly message me"
750	If my target variable is continuous, can it affect the fact that it only has categorical variables for its prediction?	"Actually, I don't remember exactly how I do this. The right approach would be to use only train data and not use validation data or test dataset. This is done on purpose for exactly this reason that you mentioned. We want to model a situation in which there is some unseen data – it's truly unseen, we do not see that all – and anything can happen to this unseen data, including the situation that you described, such as there is a value that is only present in the test data. 
Imagine that there is a new iPhone – when you train your model, this new iPhone didn't exist (you didn't have this category) but you apply this model, and then, all of a sudden, there is a new iPhone. How will your model react to this? You need to somehow model this situation. This is totally normal. This can and will happen in practice. We just want to understand what would be the effect of that. That's why we need to see this in training data. This is good and this is how it should be. I hope that is a good answer."
751	If my target variable is continuous, can it affect the fact that it only has categorical variables for its prediction?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
752	If my target variable is continuous, can it affect the fact that it only has categorical variables for its prediction?	Basically, you only have categorical variables as features and your target variable is continuous. Yeah, there are no problems with that. Just train a linear regression, use one hot encoding, and the job is done – more or less. [chuckles]
753	If my target variable is continuous, can it affect the fact that it only has categorical variables for its prediction?	"You will probably also need to learn how to pass interviews. This course does not teach that, but you will need to acquire the skill somehow. The best way to acquire these skills is by interviewing. But apart from that, I think this course covers let's say 80% of the skills that you will need for a job and the remaining 20% depends on the company. 
So you will need to do some research on the company where you want to work, see what kind of tools they use, what kind of problems they solve, how much emphasis they make in the interviews on the theoretical part. You will need to somehow figure this out and, based on that, see what you need to learn. I think with just this content, you should be able to get a machine learning-related job."
754	If my target variable is continuous, can it affect the fact that it only has categorical variables for its prediction?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
755	How are you using Ubuntu with Windows and why are you using Windows? I am thinking of dual boot Windows with Pop!OS.	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
756	How are you using Ubuntu with Windows and why are you using Windows? I am thinking of dual boot Windows with Pop!OS.	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
757	How are you using Ubuntu with Windows and why are you using Windows? I am thinking of dual boot Windows with Pop!OS.	"I have no idea what Pop!OS is. The reason I am using Windows is because I like it. I have been using Ubuntu since 2010, I think. I used another Linux OS even earlier. But I switched to Windows recently (last year) when recording this course, actually (ML Zoomcamp course) because I needed to edit videos and editing videos on Linux is super annoying – I don't know if you have ever attempted to do this. Ubuntu or any Linux is just not the operational system people use for editing videos or any other thing – pictures, also forget about this. So I thought “Okay, how about I try windows?” I gave it a try and a funny thing happened. 
There is this thing called WSL – Windows Subsystem for Linux – which works exactly like my Ubuntu. And I thought, “If I can have Ubuntu on my Windows, then why do I need Ubuntu?” [chuckles] You see what I mean? On Windows, I could edit videos, I could do many, many other things. So some of the software I use right now, for example, Loom for recording videos, or I use a tool called Krisp for noise reduction, and many, many other tools. They simply don't work on Linux, because the developers don't care about Linux users – they care about Windows users and they care about MacOS users. 
Now I get the best of both worlds. I can use tools that work on Windows, but don't work on Linux and I can still use Linux and do all the development stuff I've been doing on Ubuntu since 2010. And it's been quite great. I really enjoy Windows. I have Windows terminal, which I can use to either do things in Windows or in Ubuntu. And I can always just SSH to a remote computer and have my Ubuntu terminal there. I am not coming back to Ubuntu anytime soon, at least “full” Ubuntu, let’s say."
758	How are you using Ubuntu with Windows and why are you using Windows? I am thinking of dual boot Windows with Pop!OS.	It is possible to do everything locally. You will need to do it by… I think you will need to do it in module five when we deploy something. Or actually, you will probably need to do it when we do your project. Yeah. It's fine to do everything locally. But if you have access to cloud and you want to practice with this, I do recommend practicing, because cloud is one of the skills that employers are looking for. They really need it. So if you want to be more employable, I do recommend learning about cloud.
759	How are you using Ubuntu with Windows and why are you using Windows? I am thinking of dual boot Windows with Pop!OS.	Okay. Yeah
760	Any production model that is really important for industry?	Well, I'm glad you asked, because we have a data engineering course. If you take it, you will understand a bit more – we do not go into details about how exactly it connects with this course. If you remember, when we talked about CRISP DM, one of the steps there is data preparation and this is what data engineering covers. Once the data is prepared, once the data is in a data lake or data warehouse, then we, data scientists, machine learning engineers, get the data, train the model, deploy this model, and take it to the other steps of the process.
761	Any production model that is really important for industry?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
762	Any production model that is really important for industry?	No. It was supposed to be today, but I changed it a little bit. If you go to the updated deadline calendar, you will see that the capstone project starts next week (December 5, 2022). For the evaluation, we give you a lot of time. During this time, depending on when you take your vacation, you will have some time to evaluate the project and then the capstone project starts next week. I think what happened is that when we added Bento ML here, it kind of messed up the whole schedule. The idea was to finish everything by Christmas but, unfortunately, we won't be able to finish everything by Christmas which probably means that if you do the midterm projects 1 and 2, you can get your certificate somewhere here or maybe at the end January. I don't know. Let's see. It will probably be easier to just give certificates at the end of January. Let's say if you finish everything by New Year’s, then you're free to stop taking the course. I know it was very difficult and time consuming. You might as well just finish everything and start the data engineering course or enjoy your life, or do many other things. We'll also have an article, which starts pretty soon. During the next Office Hours, I should probably talk more about that.
763	Any production model that is really important for industry?	I don't know what this question means. Everything we study here in this course is important for industry.
764	Any production model that is really important for industry?	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
765	Is there any plan to include a block for IBM Cloud Storage?	"Jeff
I'm not sure exactly what's being referenced here. But I'm guessing maybe the GitHub question from the homework – the GitHub block question turned out to be a little trickier than anticipated for folks. This is a great question. I got a couple of things on it here. One is that there is some stuff in the documentation, but maybe not a complete handhold of things. The storage doc does talk about code storage and flow storage in general here, and a little bit about how to do it. 
If you would like more detailed information, it’s always a great idea to look and think about me opening an issue. If you go to the GitHub repository for Prefect, you can click on “new issue”. If you want, you can propose a feature enhancement, if you want to call it that, or report a bug if you think you should have more information, and then put in what you would like to see. 
If you want to go one step further at something I really recommend, even like the next extension of Learning in Public is contributing to projects. This is an open source project here. You're welcome to contribute to it. You could go ahead and fork the repository, clone it down, and make a change to the documentation once you've figured things out there. Lots of lots of folks I know did get there with some support, through the FAQ and through Slack. It's partly what the videos are also for, in the course, to help you out as you're going. That's what I would suggest there. 
But I will also suggest to our docs folks, that we perhaps enhance the GitHub section in particular."
766	Is there any plan to include a block for IBM Cloud Storage?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
767	Is there any plan to include a block for IBM Cloud Storage?	"Jeff
We have not had a request for that, I don't believe. Not any of that I know of in our catalog of requests. I will note that there is interest in IBM Cloud Storage. But the best move here – you could probably take something that's very similar to GCP and add that as a collection, potentially. Our collections, again, are all here and you can look at contributing your own if you'd like, so then we would have that available for other people to use. I encourage you to check that out. You can also just make your own custom block and use that. That’s kind of an easier step. There's a section on custom blocks in the docs. You can check that out here and use those as a guide. That can be something that's used. But we haven't had a lot of requests for that, so it's not something I expected soon."
768	Is there any plan to include a block for IBM Cloud Storage?	Yes, you will have to come up with your own individual projects. It's not easy, I know. But we'll help you. We will help you to come up with a project idea. That's actually on purpose because we want every project to be unique. Then it becomes a part of your portfolio when you look for a job.
769	Is there any plan to include a block for IBM Cloud Storage?	This question was probably asked before the solution was published. Just in case, I will show you where the solutions are. Go to cohorts, 2023 cohort, then the week 1 homework and we have the solution here. We already recorded the solution, so just go through this and you'll see the answers. It’s the same for the other homework assignments and solutions. For the Terraform part, there is no solution. It's just simply applying everything you saw in the lectures. For workflow orchestration, we will also have the solution here after the deadline.
770	Besides the 30 hours of GPU offered in Saturn cloud, is Colab (free plan) a possible solution to execute the deep learning tasks in the session with GPUs?	Yeah. If it works for you, use it. I don't mind.
771	Besides the 30 hours of GPU offered in Saturn cloud, is Colab (free plan) a possible solution to execute the deep learning tasks in the session with GPUs?	"Tim
The best way to reach out to me is in the Bento ML Slack. Usually, you can join our Slack group and then you can just directly message me"
772	Besides the 30 hours of GPU offered in Saturn cloud, is Colab (free plan) a possible solution to execute the deep learning tasks in the session with GPUs?	You'll have to come up with a project idea yourself. We will help you, of course. We will share some datasets with you, but will need to go through these datasets and you will need to find a project yourself. Then if you're not sure if this is a good project or not, you can just, again, ask that in Slack. You can say “Okay, I found this project. Will this dataset be a good dataset for the project?” I can already tell you that datasets like Iris, Titanic, Buying Quality, MNIST – these datasets that you see in every tutorial – are not good for the project. Try to find something a little bit more unique.
773	Besides the 30 hours of GPU offered in Saturn cloud, is Colab (free plan) a possible solution to execute the deep learning tasks in the session with GPUs?	I think for binary features, just one and zero makes more sense than OneHotEncoding. OneHotEncoding will take care of that but you will have two columns that are the exact opposite of each other. So that is a bit redundant. I think it's better just to convert it to a Boolean column with ones and zeros.
774	Besides the 30 hours of GPU offered in Saturn cloud, is Colab (free plan) a possible solution to execute the deep learning tasks in the session with GPUs?	"Colab is an option, yes. You can use that. Colab is a little bit unpredictable when it comes to… working. [chuckles] Sometimes it just stops working and then you have to restart and start training the model from scratch. Overall, it's good. It works. It's just that they might turn off your instance after one hour. It's not like there’s a rule that after one hour, your instance will die. It usually happens sporadically. If you want to have some guarantees with Colab, you probably have to pay. 
If you just want to play around with Colab with the free plan, there is another downside. I think if you want to save something in Colab, and your session dies, I'm not sure if you can recover the files. You need to save them in your Google drive somewhere, or I don't know how it works. So there is a bit of overhead when it comes to the free version of Colab. If you're happy to do this, by all means do it. I actually don't know how much the paid version costs. I think 30 hours will be enough – and remember that it's per month. Next month, you will get more hours."
775	Are the tools used for the course free (such as GCP) or do we need to use any premium stuff?	No, all the tools are free. That's actually why we use GCP here, because they can give free credits.
776	Are the tools used for the course free (such as GCP) or do we need to use any premium stuff?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
777	Are the tools used for the course free (such as GCP) or do we need to use any premium stuff?	"Alexey
It's possible to apply to jobs right now. You don't have to take the course. Just start applying. Why do you need to take the course to start applying, right? It was possible to do it last year. Nobody is stopping you from doing this. But this course will give you some knowledge and add projects to your portfolio. That will be helpful in your job search. 
But I would encourage you to start applying to jobs right now to see what they actually want from you – what kind of topics come up, what kind of test assignments you get. You don't need to wait until the course is finished to start doing this. You can already learn a lot about the job market in your area or in the domain where you want to work.
Luis
I just want to add that when I did the 2022 cohort, I was in another company and the process of data engineering in that company was really similar to the Zoomcamp. What I started to notice was that the companies mostly use this architecture in engineering. So it's good for you to show yourself that you're starting to understand all this data engineering stuff. I think it's important.
Ankush
I just wanted to add. I don't know about applying for jobs, but once you have a job, if you want to get an increment, contact Michael. [chuckles]

Victoria
Michael moneymaker. [chuckles]"
778	Are the tools used for the course free (such as GCP) or do we need to use any premium stuff?	No, you cannot.
779	Are the tools used for the course free (such as GCP) or do we need to use any premium stuff?	Everything you do here is individual. You don't form teams.
780	Can I submit the final project past the due date?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
781	Can I submit the final project past the due date?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
782	Can I submit the final project past the due date?	This question was probably asked before the solution was published. Just in case, I will show you where the solutions are. Go to cohorts, 2023 cohort, then the week 1 homework and we have the solution here. We already recorded the solution, so just go through this and you'll see the answers. It’s the same for the other homework assignments and solutions. For the Terraform part, there is no solution. It's just simply applying everything you saw in the lectures. For workflow orchestration, we will also have the solution here after the deadline.
783	Can I submit the final project past the due date?	Everything you do here is individual. You don't form teams.
784	Can I submit the final project past the due date?	No, you cannot.
785	Can we use the datasets from homework to make a project portfolio with more work on them? Or are they too easy datasets?	"You probably mean for logistic regression, right? For logistic regression, it's doing something very similar to what we saw in module 2. It is trying to minimize the… (I'm just trying to figure out how I can explain it without going into too many details and not to get lost myself and not to confuse you). It's probably a good idea to refer to some other course. I think I partly talked about this. Let me check. Here I talk a little bit about this, in ML Zoomcamp Office Hours week #4, where I explain a little bit how exactly it works. But it's a very superficial, let's say, overview of exactly what's happening there. 
If you want to learn in more detail how exactly the process works there, maybe you can just go to Google and search for “gradient descent, logistic regression,” or something like this. The first result will probably have a good explanation of what's happening under the hood. SciKit Learn does not use this exact method – it doesn't use gradient descent – it uses something more complex (more advanced) than that. But this is roughly what's happening under the hood. I hope you're satisfied by this answer. I don't think I can give you a better one with the time we have. It will be like a separate lecture. If you don't want to go into details, you can think of this as something that’s similar to logistic regression happening there but we add sigmoid on top of that."
786	Can we use the datasets from homework to make a project portfolio with more work on them? Or are they too easy datasets?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
787	Can we use the datasets from homework to make a project portfolio with more work on them? Or are they too easy datasets?	I don't think it's a good idea to use the same datasets. I would go with something different. But if you really want to do this, I don't think I should say “no”. Please write in Slack about this – why you want to use this particular dataset and how it’s different from the homework we do. If it looks fine – yeah, why not? But don't restrict yourself to just these datasets. Try to find something, go to Kaggle, explore what’s there and maybe you will find something better.
788	Can we use the datasets from homework to make a project portfolio with more work on them? Or are they too easy datasets?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
789	Can we use the datasets from homework to make a project portfolio with more work on them? Or are they too easy datasets?	I wish Tim was here and would help me answer that because I don't know. I assume if you wanted to use Bento, you would probably use TensorFlow Lite – you don't want to use the usual TensorFlow for the same reasons that we talked about in the serverless module, because it's simply too large. Probably for Bento, you would use something like TensorFlow Lite and it will still work. Maybe another thing you can check – in our channel, we had a talk from one of the Bento folks, Building an ML Service Platform from the Ground Up. Here, Sean talks about the differences between TensorFlow Serving and Bento. With TensorFlow Serving, it's good – it's quite optimized for deep learning. There are some problems. First of all, it's written in C++ and expects your model in a certain format – this “saved model format”. Adding anything extra on top of that (like pre-processing, post-processing) becomes difficult, because you will need to have an extra step there. Bento makes it easier. But, again, I don't know. It's very difficult to give you a concrete recommendation without knowing what exactly you want to do. So it's case dependent. If you already use TensorFlow Serving in your company, then maybe it makes sense to stick to that. If we have Tim on Office Hours next week, maybe you can ask him that. He will be a better person to answer this question.
790	Is the course Windows friendly?	Yes, you will have to come up with your own individual projects. It's not easy, I know. But we'll help you. We will help you to come up with a project idea. That's actually on purpose because we want every project to be unique. Then it becomes a part of your portfolio when you look for a job.
791	Is the course Windows friendly?	No, but you can suggest some stuff in Slack. If we see that this is useful, we can also include this in the project repo
792	Is the course Windows friendly?	It is, yes.
793	Is the course Windows friendly?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
794	Is the course Windows friendly?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
795	Do you often use interaction/polynomial terms when applying linear regression?	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
796	Do you often use interaction/polynomial terms when applying linear regression?	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
797	Do you often use interaction/polynomial terms when applying linear regression?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
798	Do you often use interaction/polynomial terms when applying linear regression?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
799	Do you often use interaction/polynomial terms when applying linear regression?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
800	Is it OK to make capstone project 1 similar to midterm project (without deep learning, serverless, etc.)?	"Imagine that you're running an ecommerce company – simple ecommerce. Our CEO comes and says, “Our users have big problems discovering items. How can we help them?” Then you need to understand “Okay, what does it mean ‘they have problems discovering items’?” You need to understand what problems they have and how we can help solve these problems. Once you understand the problems, then you can generate possible solutions. One example is coming up with a good search. Another solution could be coming up with good recommender systems. It also depends on a business goal. What do we want to optimize? Do we want to make sure that our users buy more or so that it's actually easier for them to find what they're looking for when they are just exploring? This is all input that comes from the business people – the stakeholders in the company – and the users. Taking all this input into account and thinking, “Okay, now I know what I need to work on to solve this problem. What can it look like?” And the solution could be designing or coming up with a recommender system that is on the item page that shows similar items. Or, for example, showing what people buy in addition to this item. So that's one example. 
Another example. Let's say we have a video hosting coming – like YouTube. All of a sudden, people start uploading some content that we don't want to have on the platform. We want to get rid of this, right? Business comes to us, the data scientists or software engineers, and says, “Can we somehow make sure this doesn't happen?” And then we can think, “Okay, what are the ways that we can make the bad content go away?” It could be introducing a content moderation platform, for example, where we apply some different machine learning algorithms to a video in order to understand that this video should not be on the platform. Instead of a video, it can be anything. 
So this process of listening to the stakeholders, understanding what the problem is, and then up to the point where you have an idea of how this could be implemented – this is what I call “converting a business problem into a machine learning problem.” I hope it makes sense now."
801	Is it OK to make capstone project 1 similar to midterm project (without deep learning, serverless, etc.)?	Yeah, it's okay. You don't have to use deep learning for the capstone project. As I said, if you just use the material for the first seven modules for your capstone project, you'll be fine. You don't have to use deep learning.
802	Is it OK to make capstone project 1 similar to midterm project (without deep learning, serverless, etc.)?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
803	Is it OK to make capstone project 1 similar to midterm project (without deep learning, serverless, etc.)?	Okay, what I will do is I will copy this question and paste it into Google and see what it answers. I don't know, is this good enough? Or do you want to know more about that? I usually go with ENTRYPOINT. In lambda, we use CMD because ENTRYPOINT is already specified in the base Docker image. Then using CMD, you can kind of overwrite, but only partly. CMD is like a part of ENTRYPOINT. As far as I remember, that's the main difference between them. Please look it up. I don't remember too much about it.
804	Is it OK to make capstone project 1 similar to midterm project (without deep learning, serverless, etc.)?	"You will probably also need to learn how to pass interviews. This course does not teach that, but you will need to acquire the skill somehow. The best way to acquire these skills is by interviewing. But apart from that, I think this course covers let's say 80% of the skills that you will need for a job and the remaining 20% depends on the company. 
So you will need to do some research on the company where you want to work, see what kind of tools they use, what kind of problems they solve, how much emphasis they make in the interviews on the theoretical part. You will need to somehow figure this out and, based on that, see what you need to learn. I think with just this content, you should be able to get a machine learning-related job."
805	What are the reasons why we would just use Spark instead of DBT? Do both Spark and DBT do transformations? What are the differences? Is it possible to combine DBT and Spark? Before feeding the data into Spark, can we perform the transformations needed via SQL using BigQuery or DBT rather than Athena?	We currently do not have any course partners yet. If you work at a company and you want to partner with us, please reach out and we will see how we can do it. In one of our courses, which was our first iteration of the Machine Learning Zoomcamp, we partnered with a company called Delphi. They got two interns from our course and they were quite satisfied with the outcome. You can actually read more about the work of these interns in our articles – Interview with Valerii Chetvertakov and then another one, Interview with Ken Wu. You can learn more about the interns and if you think that, at your company, you need good interns or juniors or you want to partner in any other way, please reach out and we can see how to make it work.
806	What are the reasons why we would just use Spark instead of DBT? Do both Spark and DBT do transformations? What are the differences? Is it possible to combine DBT and Spark? Before feeding the data into Spark, can we perform the transformations needed via SQL using BigQuery or DBT rather than Athena?	Everything you do here is individual. You don't form teams.
807	What are the reasons why we would just use Spark instead of DBT? Do both Spark and DBT do transformations? What are the differences? Is it possible to combine DBT and Spark? Before feeding the data into Spark, can we perform the transformations needed via SQL using BigQuery or DBT rather than Athena?	"Victoria
The first thing to note is that you can use DBT with Spark. There's an adapter for Spark that you can use. You probably saw it when you created your project in DBT cloud and then for Core, you need to install a different adapter – instead of DBT BigQuery, you do DBT Spark. This means that the limits that DBT has for the transformation are really the limits of the data platform that you're connecting it to. The question is not whether it's more Spark versus BigQuery, rather than DBT versus Spark. 
The other question is: Is Spark with DBT or not? That one can also sync with any of these platforms friends. Why we chose to include both. These are alternative ways of transforming and, as a data engineer, you're going to find both setups in a company. There are companies that you may work at that have all transformations that are all Spark, while other companies have all of the transformations in BigQuery. The same – or just Redshift or just Snowflake. 
Some companies have both BigQuery and Spark, for example. The main difference really will depend on the use case of your data. Spark is usually used for higher loads of data, or a need for streaming as well. That's where normally one would use Spark. It also depends on the data engineering team. And then BigQuery is going to perform better in big data sets, but still is more than Spark in batch processing. I would still recommend reading more about both."
808	What are the reasons why we would just use Spark instead of DBT? Do both Spark and DBT do transformations? What are the differences? Is it possible to combine DBT and Spark? Before feeding the data into Spark, can we perform the transformations needed via SQL using BigQuery or DBT rather than Athena?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
809	What are the reasons why we would just use Spark instead of DBT? Do both Spark and DBT do transformations? What are the differences? Is it possible to combine DBT and Spark? Before feeding the data into Spark, can we perform the transformations needed via SQL using BigQuery or DBT rather than Athena?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
810	Can you show how to run DBT from the subfolder of the repo?	Yes
811	Can you show how to run DBT from the subfolder of the repo?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
812	Can you show how to run DBT from the subfolder of the repo?	"Victoria
I think this one was solved this already. [inaudible] I know that was in one of those threads. But yes, this is this is my account. I am here under settings on the projects. You're going to see that I actually have the same account as you'll have. It just the videos are one year old and DBT changed a lot since then. You're going to see that when I go through one of my projects, I have this project SAP directory, where you can write the directory. If you don't write anything, then your project that you’re going to develop, is going to start in here. The same when you're writing, it’s going to start always in the root. 
This is my repository – it has nothing but week 4. I have this other one here, If instead you have something like the full repository of the data engineering Zoomcamp, then I can put the project’s directory here and my DBT project will start after all of this. That is where it's going to go and look for my code. If you're using the terminal, then it's as simple as locating yourself where your project yml is. This is the same for here. 
Here is my project yml, under this directory. You locate yourself with either C/D, if you're using Linux or Mac, or dir, if you're using Windows and you execute DBT from there. It's going to go and look through their project yml and then execute from there."
813	Can you show how to run DBT from the subfolder of the repo?	"Jeff
I can try. I like Black a lot. If you just Google “Python Black” and go to the GitHub readme, it’s become super popular in the last couple of years for automatically formatting your code. It is really nice. Mine is set up in Visual Studio Code, so when I hit “save,” it automatically formats. As it says, it’s an uncompromising code formatter. I think it says here about asking Henry Ford for color back in the day. Is that right? It's colored cars or color telephones from Alexander Graham Bell or somebody, and it's like, “They can have any color they want as long as it's black.” He wasn't going to compromise. You don't have to think about it now. It's easy. Just go right ahead and get your gear code formatted automatically. So it’s things like two lines after function or before – that kind of stuff just happens automatically with it. Getting it set up in VS Code can be a little tricky sometimes. But there are guides to that. Googling is what I do for that, usually.
Alexey
I just wanted to do a shameless plug, because we have another course called MLOps Zoomcamp. By the way, there is also a Prefect model there. One of the things there is best practices. In best practices, we have this video called Code quality: linting and formatting. It does not show how to integrate Black with VS Code, I think. I don't remember if we do this. But if you want to learn more about testing, and Black, and other things like pre-commit hooks, make files, and so on – you can check this out.
Jeff
Yeah, it's not too tricky to set up. It's like pip install a package in your environment and then you have to give it a path maybe or set one setting in VS code. If all goes well, cross your fingers, it should just work after you reload it. Not always the case. If you have trouble, a lot of people use it, so there are a lot of good resources online. Black is great. This is an awesome set of some resources here on best practices. Type hints are just getting more and more popular. They're very helpful, so that people know what kind – especially with autocomplete and little pop-up type – things like that in Visual Studio Code and other code editors. You can see what kind of argument type you should put in and then Prefect uses that information as well, in our flows, for example, to make sure that if it's a block in the UI or a parameter in the UI – it'll be smart. It'll be like, “Oh, is this a number? Okay.” It will give you options to put in numbers. “Is this a different kind of form field?” It'll have different options. It also can then do some validation to make sure that people actually put in something that conforms to that type-in. Python is slowly getting more and more smart about how it handles typing and newer versions keep adding more functionality. Type hints are nice to use. It takes a little bit of writing, but it makes your docstring shorter. The last thing that was asked about here was docstrings. It's great to have in every function to tell people what it's about. It's something that maybe you don't always do if you're in a hurry, but you should do it, especially if other people are going to read the code. Code is read like 20 times more often than it's written, or however you translate that – some stat. So do it. It's so helpful for you in the future and it's helpful for other people in the future, who are going to read your code to see, “What were you thinking? What is the purpose of this function?” Keep your function small, explain it in your doc string – it's good stuff. Then it shows up in your code editor, if you're lucky (if you have a good code editor). That's all to say about that.
Alexey
Do you know any resources where people can learn about setting up? Or learning more about these things, like good Python coding standards? What I showed is obviously a good resource, but it does not cover all these things that this question asks about.
Jeff
It's a good question. I do have a link to Google Style, or there are a couple different styles of docstrings. It seems like they're a little bit much these days, maybe. But there are links for different ways to do type hinting. I do have a few things if I look around for them. I don't have them at my fingertips right now. But Michael looks like maybe he's got one he shared there.
Michael
Yes, this one is a little bit older, but it is great. It goes into using virtual environments, Poetry… there's a lot to unpack, but I think that's still pretty much the standard best practice at the moment."
814	Can you show how to run DBT from the subfolder of the repo?	"Alexey
Google Cloud Platform is not mandatory. You can do most of the course without a cloud. You will not be able to use BigQuery, obviously, because it's a cloud offering. But you can use local postgres instead of that and I think you can run almost everything locally. For example, for streaming, instead of using Confluent Cloud, you can use Kafka. You can set it up locally.
Ankush
We also provide Docker images for everything, but the videos will be more on Confluent Cloud. But you can always change that.
Alexey  
I know that there are people from Nigeria, Iran, who cannot register in Google Cloud Platform. You can do things locally."
815	How powerful should your local machine be?	It's like 10 weeks or something like that. If you count the second attempt of the project, it will be like 13 weeks. Quite long, but not so long as, let's say our machine learning engineering course.
816	How powerful should your local machine be?	Everything you do here is individual. You don't form teams.
817	How powerful should your local machine be?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
818	How powerful should your local machine be?	No, you cannot.
819	How powerful should your local machine be?	"Alexey
I think 16 gigs of RAM should be sufficient. But again, this year, we are not doing Airflow. Airflow was the most problematic week when it came to local machines. When we tried to run Airflow in Docker, my computer was basically heating up and was going to fly away on the fence. It was too much for it. In the end, I did most of the stuff in a remote virtual machine. This is what you should do as well – use a remote virtual machine because you have this $300 credit. But 16 gigs should be enough, 8 gigs will not be enough. 
Michael
Just in case they didn't catch that. If you don't have enough RAM or CPU, you can just start from the beginning in a virtual machine in GCP – you've got more than enough credits to make it through the course and be running that machine from the start.
Alexey
That's the best thing. Then if everyone has the same platform, same operating system (which is Ubuntu) it just makes it a lot easier to help you. You just say, “Okay, I'm using this virtual machine. It's Ubuntu. This is the problem I have. And then anyone can reproduce this problem, hopefully. Meanwhile, if you’re running this on Windows or MacOS, then everyone has a slightly different version of Windows and sometimes it's just impossible to produce."
820	What are your thoughts on Naive Bayes?	"You can check out a few projects in this playlist. In the playlist, you can see some of the videos here, for example, there's a project from Alvaro. And it's not just Alvaro, there’s one from Alvaro and Ninad. You can just check out these projects – there’s a project from Carolina and Hamad, a project from Lisa, and a project from Timur. The project from Timor may actually fit what you want. Timur has done a lot of work and it's very interesting. So check it out. Actually, Timur’s project is not a midterm project. It's more like a capstone project – there is some deep learning there, he uses Karis. So maybe focus on the other three. 
You can also go to our course web page, and then go to midterm project (you need to use the 2021 folder) and then in the readme there should be some assignments. There will be some form that says “To find the projects you need to review, use this table.” Here, you can see all the projects from previous students’ midterm projects. You can pick some of them and check if you like them or not."
821	What are your thoughts on Naive Bayes?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
822	What are your thoughts on Naive Bayes?	Naive Bayes is a really good model, but I find it very similar to logistic regression. In the end, when you compare the performance of Naive Bayes with logistic regression, it's very similar. It's a very nice model, but since we're covering logistic regression here anyway, I didn't see a lot of sense in including Naive Bayes as well. It has a very nice and elegant theory. You can check it out. It's really beautiful. But in the end, for all practical purposes, it's very similar to logistic regression.
823	What are your thoughts on Naive Bayes?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
824	What are your thoughts on Naive Bayes?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
825	Do you run a private mentorship program (probably paid) that one can join?	I do not. I was actually thinking about doing this, but it will probably be quite expensive. If any of you are interested in this, maybe write to me in Slack and then I can think about how to organize that. But so far, there is no “official” program for that.
826	Do you run a private mentorship program (probably paid) that one can join?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
827	Do you run a private mentorship program (probably paid) that one can join?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
828	Do you run a private mentorship program (probably paid) that one can join?	Yeah. If it works for you, use it. I don't mind.
829	Do you run a private mentorship program (probably paid) that one can join?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
830	What teams do you interact with at your job as an ML engineer or data scientist? What does a team usually consist of? I mean, according to roles and responsibilities.	"Well, there are multiple ways you can deal with this. First – get more data, if possible. If not, try to reduce the number of features you have. Sometimes, when the data set is small, if you have a lot of features, it might be confusing for the model. Or try to have a model with a lot of regularization. This should help. Here, the main thing is that you need to come up with a reliable way of evaluating the performance of your model. 
You really need to think about how you can set up your validation strategy in such a way that it's reliable. Once you have that, you can start experimenting with all these regularization things, with dropping some columns, with techniques like upsampling and downsampling and things like this. The important thing here is setting up your validation strategy. Once you have that, everything else will come. You will just need to experiment and the experiments will show what the best way is."
831	What teams do you interact with at your job as an ML engineer or data scientist? What does a team usually consist of? I mean, according to roles and responsibilities.	Yeah. If it works for you, use it. I don't mind.
832	What teams do you interact with at your job as an ML engineer or data scientist? What does a team usually consist of? I mean, according to roles and responsibilities.	Instead of answering this question right now, I will refer to this article called Roles in a Data Team. I wrote it some time ago, but you're basically asking for this information. What the people in the teams are, how they work together and things like this. When it comes to the setup I have at OLX, it's a little bit too complex to explain in the remaining 10 minutes, so maybe I will not do it right now. But please do check out this article and if you have questions, I'll be happy to answer them.
833	What teams do you interact with at your job as an ML engineer or data scientist? What does a team usually consist of? I mean, according to roles and responsibilities.	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
834	What teams do you interact with at your job as an ML engineer or data scientist? What does a team usually consist of? I mean, according to roles and responsibilities.	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
835	Can you explain the difference between data engineering, data science, and data analysis?	No, you cannot.
836	Can you explain the difference between data engineering, data science, and data analysis?	"Alexey
The best way to contribute is to take notes and include them to the notes section. And tell your friends about the course. As a part of your Learning in Public, spread the word. It will help us. And don't forget to give us a star if you have."
837	Can you explain the difference between data engineering, data science, and data analysis?	Yes, I can. I already showed you an article, Roles in a Data Team. Just go through this article, where I cover all these roles. Of course, the opinion of what these roles do varies in the industry. This is my own experience, how I see these roles interact in many companies, specifically in Europe. But I think the ideas are more or less the same across different companies.
838	Can you explain the difference between data engineering, data science, and data analysis?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
839	Can you explain the difference between data engineering, data science, and data analysis?	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
840	When should we use TensorFlow Lite and when should we use TensorFlow Serving?	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
841	When should we use TensorFlow Lite and when should we use TensorFlow Serving?	"Well, there are multiple ways you can deal with this. First – get more data, if possible. If not, try to reduce the number of features you have. Sometimes, when the data set is small, if you have a lot of features, it might be confusing for the model. Or try to have a model with a lot of regularization. This should help. Here, the main thing is that you need to come up with a reliable way of evaluating the performance of your model. 
You really need to think about how you can set up your validation strategy in such a way that it's reliable. Once you have that, you can start experimenting with all these regularization things, with dropping some columns, with techniques like upsampling and downsampling and things like this. The important thing here is setting up your validation strategy. Once you have that, everything else will come. You will just need to experiment and the experiments will show what the best way is."
842	When should we use TensorFlow Lite and when should we use TensorFlow Serving?	"If you take statistics for machine learning education, the first lecture will be this – you'll probably derive the formula for logistic regression and there will be a slide with all these assumptions about the linear regression model. Here is the Google result: 
https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/assumptions-of-linear-regression/

Go through this. They are usually useful in practice, I guess. But what I typically do is train a model, and then rely on validation to tell me if my model is doing well or not. So if there is something wrong with my validation, I will see that the metric validation is bad. It's just a lot of trial and error, rather than figuring out all these theoretical assumptions. 
I don't want to say that all these assumptions are not relevant. If you work as a data scientist, they still are. But I also found that the more practical way to understand if you can apply linear regression to the data or not, is to just apply it and see if it works or not. If it works – if the results on the validation are good – then it means that you can apply your model to this dataset. 
Maybe some of the assumptions are violated. For example, multicollinearity – this is actually the case in the lectures, when we needed to regularize (add regularization to the model) this assumption was violated. You will see in the lectures what exactly happened. So check it out. I don't think I will be able to give a good answer, because I am not prepared to talk about this, but any statistics book or theoretical machine learning book explains this."
843	When should we use TensorFlow Lite and when should we use TensorFlow Serving?	Yeah. If it works for you, use it. I don't mind.
844	When should we use TensorFlow Lite and when should we use TensorFlow Serving?	That's a very similar question to when we should use the lambda and when we should use Kubernetes. If you want to use Kubernetes, then go with TensorFlow Serving. Otherwise, TensorFlow Lite. There is one thing where you have to use TensorFlow Lite – where you cannot use TensorFlow Serving – and this is when you want to deploy your model on a mobile device. I have a phone here. If you want to create an app with a model, then you will have to use TensorFlow Lite because you will not be able to use TensorFlow Serving. TensorFlow Serving also adds a bit of complexity. You will need to have this gateway service, you will need to have GRPC service, so that becomes more complex. If you have a lot of load, then you need to do this otherwise, TensorFlow Lite.
845	Why do we use Gunicorn and not just Flask?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
846	Why do we use Gunicorn and not just Flask?	Yeah. If it works for you, use it. I don't mind.
847	Why do we use Gunicorn and not just Flask?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
848	Why do we use Gunicorn and not just Flask?	If we use just Flask, then there is a warning that “Flask is not a production WSGI server.” So what you need to check is what WSGI is and read about this. I don't think I'll have the time or the knowledge to explain it here right now. But the thing with Flask is, it's not a WSGI server, but Gunicorn is. Well, Flask kind of is, but it's like a toy server, let's say – just for development. It will work on your computer only for testing, but it will not scale. It will not be able to process multiple requests coming in parallel. Gunicorn is much more scalable, parallelizable and just better to use. That's the main reason why Gunicorn is needed. For more details, maybe just check out this article – could be a good one. It explains what WSGI is, why we need this application framework (which is Flask) and a WSGI server, which is the actual server.
849	Why do we use Gunicorn and not just Flask?	That's a very similar question to when we should use the lambda and when we should use Kubernetes. If you want to use Kubernetes, then go with TensorFlow Serving. Otherwise, TensorFlow Lite. There is one thing where you have to use TensorFlow Lite – where you cannot use TensorFlow Serving – and this is when you want to deploy your model on a mobile device. I have a phone here. If you want to create an app with a model, then you will have to use TensorFlow Lite because you will not be able to use TensorFlow Serving. TensorFlow Serving also adds a bit of complexity. You will need to have this gateway service, you will need to have GRPC service, so that becomes more complex. If you have a lot of load, then you need to do this otherwise, TensorFlow Lite.
850	What was the y_train variable for the week 3 homework? I got to question 4 to use logistic regression and got an error that (x_train, y_train) were not the same size.	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
851	What was the y_train variable for the week 3 homework? I got to question 4 to use logistic regression and got an error that (x_train, y_train) were not the same size.	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
852	What was the y_train variable for the week 3 homework? I got to question 4 to use logistic regression and got an error that (x_train, y_train) were not the same size.	It was this above average thing. That was the y_train variable.
853	What was the y_train variable for the week 3 homework? I got to question 4 to use logistic regression and got an error that (x_train, y_train) were not the same size.	"That's like a pretty generic question, so the answer will also be generic. They will look at the skills you have, they will look at the match – how the skills you have match what is needed. I guess that's it. This is what recruiters usually look for in candidates. If you're interested, another thing you can check out is our podcasts. 
We have an interview, Recruiting Data Professionals with Alicja Notowska. Alicja is a recruiter. She was working as a recruiter at Zalando, which is a very large company in Germany and she shared some tips from your point of view of a recruiter and what she looks for when hiring people. So check it out. It’s quite insightful. You will probably find useful things there."
854	What was the y_train variable for the week 3 homework? I got to question 4 to use logistic regression and got an error that (x_train, y_train) were not the same size.	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
855	When are we talking about the solutions for the homework?	"Alexey
Please check the FAQ. If you don't find the answer there, ask in Slack. If you get an answer in Slack, please put it into the FAQ."
856	When are we talking about the solutions for the homework?	"Alexey
I don't know when this question was asked, but we did extend it."
857	When are we talking about the solutions for the homework?	This question was probably asked before the solution was published. Just in case, I will show you where the solutions are. Go to cohorts, 2023 cohort, then the week 1 homework and we have the solution here. We already recorded the solution, so just go through this and you'll see the answers. It’s the same for the other homework assignments and solutions. For the Terraform part, there is no solution. It's just simply applying everything you saw in the lectures. For workflow orchestration, we will also have the solution here after the deadline.
858	When are we talking about the solutions for the homework?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
859	When are we talking about the solutions for the homework?	"Alexey
Yeah, that's not going to happen. I think ChatGPT will help us do our job, but it will not replace people, at least in my opinion. But I use it in my work and it's helpful. You should try using it too – if you want, of course."
860	Does all real time machine learning model deployment follow the same procedure as we did in week 5 or are there other tools and procedures available to deploy it?	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
861	Does all real time machine learning model deployment follow the same procedure as we did in week 5 or are there other tools and procedures available to deploy it?	"When I was freelancing, it was through a website that’s similar to Upwork. You can try that. Or Fiver or something like this. But, depending on where you live, you can just use LinkedIn for that and if somebody wants to invite you for an interview, you can ask them, “Hey, do you consider freelancers?” Some of the recruiters who reach out to you will say, “Yes, we do consider freelancers.” And then you can just start doing this. I don't think I can give you a better recommendation, because I'm not a freelancer myself. 
Maybe what you can do is go to DataTalks.Club site’s podcasts, where we have two podcast episodes about freelancing. The first one is Freelancing and Consulting with Data Engineering and then the other one is Freelancing in Machine Learning. In both cases, the guests talk about finding your first client, how they started freelancing, what they do. They know more about this than me."
862	Does all real time machine learning model deployment follow the same procedure as we did in week 5 or are there other tools and procedures available to deploy it?	"It all depends on what you mean by “full stack ML engineer”. As you’ve noticed, data engineer Zoomcamp is not about machine learning – it's about data engineering. If in your opinion a “full stack ML engineer” needs to know data engineering, then you should do data engineering. If not, then no. From my point of view, there is a “full stack of data scientist”. What I put in the definition of a full stack data scientist is – a person who can do all the steps in the CRISP-DM process. They can help product managers with business understanding, they can work with analysts in data understanding, they can help data engineers in the data preparation step, they can do the modeling, and they can deploy the models. If you want to be that kind of person, then, of course, you also need to learn a bit of product management and analytics, which we don't have courses for. But data engineering – preparing data before it goes to a model – will certainly be helpful. 
Also, machine learning ops Zoomcamp (MLOps Zoomcamp) will be helpful as well. I would suggest, if you really want to focus on machine learning engineering, then probably doing MLOps Zoomcamp will make more sense for you. But data engineering Zoomcamp will be quite useful in the future, because data scientists and ML engineers tend to work on data pipelines as well. 
Maybe not all the content will be useful for you. For example, the content about data warehousing – as a data scientist, I don't find it super useful in my work to me personally. Also, the module about analytics engineering – it's nice, but it's not something I do day to day. But the modules about batch, the modules about Spark, the modules about streaming – this is something that can be useful, because this is something we could use in building machine learning pipelines as well. 
To put data in a machine learning model, we need to prepare it – we need to create data pipelines – and this is pretty relevant. But you can just check it out. All the content is already there. You can just go and check it out. If you like it, do it – if you don't, don’t. I would suggest maybe going with this one (ML Zoomcamp)."
863	Does all real time machine learning model deployment follow the same procedure as we did in week 5 or are there other tools and procedures available to deploy it?	"Tim
I think the way that it works is that you can always join the Slack community and ask. That's usually the best way to get in touch with one of us. Joining the community is always helpful. Anybody there can point you in the right direction. Then, depending on what you want to contribute, we'll kind of walk you through how to structure it. A lot of the time, people will need certain features that we don't have and we'll go ahead and fork the repository and open PRs themselves. That's typically the fastest way to contribute. If you need something, then you build it yourself and then open the PR, and then we'll review it and probably have a few comments and then commit it 
Alexey
A word of caution – sometimes just opening a PR without talking to any of the maintainers first may lead to frustration and a lack of understanding of why a PR was not accepted, like “Hey! I spent two weeks of my time contributing and they don't want to accept my project. I'm not going to contribute to open source anymore in my life.” This has happened to me. That was frustrating. What I learned from that experience is that, first, it's best to talk to the maintainers and ask how exactly they want this feature to be implemented. The way you might want to implement may not be the same way the maintainers see this feature being implemented. So it's worth discussing first. 
Tim
That's definitely true, Alexey. I think most of the features out there, we've probably heard of, and we already have some thoughts on how we want to do it. Also, it's likely that we've seen a lot more edge cases as well. That's why it's probably a good idea to run your approach past us before you start coding it. That’s if you want to contribute. If you don't want to contribute and you just want to solve the problem for yourself, you can do it however you want."
864	Does all real time machine learning model deployment follow the same procedure as we did in week 5 or are there other tools and procedures available to deploy it?	"If I look at the course content outline, we are here right now, which is deployment. Then we will have BentoML, which is deployment. Then we have neural networks, which is more about how models work. Then the rest of the content is about deployment. As you see, 3 of the models (plus one optional) are about deployment. There are other tools that you will learn about and how to apply them. The procedure is very similar – you train a model and then deploy it. That's more or less it. 
What we covered in week 5, are fundamentals – how to organize your virtual environments, what Docker is, how to create a web service, etc. You don't have to use Flask. Many people say that “Flask is no longer cool – FastAPI is cool. We should use FastAPI.” Please, go ahead and use FastAPI if you want, but the process is roughly the same no matter which tool you use. If you want to learn more about that, also check out our MLOps Zoomcamp where we go into more detail about how it should actually be organized. We also talk about different tools we need to use for training and so on. The current course is the foundation and then MLOps builds on top of that and goes more into all the engineering aspects of machine learning."
865	In ETL web to GCS, can we save the data as a parquet file directly to GCS instead of locally first?	No, you cannot.
866	In ETL web to GCS, can we save the data as a parquet file directly to GCS instead of locally first?	I would say to focus on projects, not on tools. In this course, you will get a chance to do this. If you open a job description for data engineers, you will see a lot, a lot, a lot of different technologies. Maybe think of a project that you can do for this particular company and do this project using, for example, the tools you learn here or the tools you want to learn, and build a portfolio. I guess this is more of a tip for getting a data engineering job, rather than for somebody who's starting their data engineering career. But if you already have a job, then maybe think of the business impact of what you need. I think I will not spend too much time talking about that because this requires a one hour long discussion. Perhaps we should have a podcast interview about that. If you can think of any potential guests for this podcast interview, you can let me know and we can try to invite these people and talk about this in more detail. One thing I want to share with you is that in our YouTube channel, we had a few interesting talks. We had this Getting a Data Engineering Job with Jeff talk. It’s a very nice presentation. Then we also had an interview with Jeff. So these two podcasts are quite interesting. You can also check out other ones. I think you'll find a lot of interesting content here.
867	In ETL web to GCS, can we save the data as a parquet file directly to GCS instead of locally first?	Yes. If Michael was here, he would probably confirm this because this is the background he has. He's an analyst. We also had other analysts who did pretty well in the course. So yeah, certainly you can do well. As a beginner in data engineering, I would say that we don't expect any data engineering knowledge for taking this course. If you're a beginner already – if you know some data engineering concepts – then it's more likely that you will do well in this course.
868	In ETL web to GCS, can we save the data as a parquet file directly to GCS instead of locally first?	"Alexey
No. I mean, we kind of told you. Yes, you maybe didn't see the intro video. But, sorry. No, it's late. We will actually recalculate the points for the second homework (HW 1B) because in the forum, it said that you can get up to one point, but we calculated that the cap there was seven. We are going to recalculate that. Thus, people who got a lot of points will get just one. Sorry about that. But don't focus on points. If you go to the leaderboard, we know that the person with this hash (722366d8b29ece9be3a7605363562c7c60d6918e) did seven links. Do you know who that is? Probably you don't. 
These points are virtual. It's nice to have a good score, but remember that these points mean whatever you want them to mean – not more than not less. Maybe it's okay if you get a few points less than others for the first homework, but the important thing here is consistency. If you just do your homework, even without doing any Learning in Public stuff, which I think you should do (you should post about the course). If you don't do this and you just do all the homework, by the end, you will be somewhere in the top 20. If you look at the leaderboard from the previous iteration, you will see that not everyone stuck around till the end. 
If you just do your homework, don't worry about these points, and you'll be fine. Sorry about that. I know that it can be discouraging that you didn't get as many points as others, but don't worry about that. Focus on learning. Maybe Michael has some other suggestions or some other things that I didn't mention, because he took part last year. But I think he was one of the people who did a few posts every week, right?
Michael
Yes. Actually, when I went through it, I missed the whole… you can get up to seven points, but I was just doing one every week and I just stuck with that. I was still near the top. I think the real value in that isn't necessarily the points but just being comfortable sharing – writing articles and making videos. That's the real value there. But everyone likes the clout value, too. As Alexey said, it will fluctuate, so if you keep up with it going forward, you'll probably be just fine."
869	In ETL web to GCS, can we save the data as a parquet file directly to GCS instead of locally first?	"Jeff
I think we answered this in Slack and a few other places too. We just want to do a few things. You could just go right in and ingest it as a CSV file, but we wanted to actually change it into a parquet file, do some transformation there and just go through a little example of cleaning up something. But you could go and read something directly in. That's fine. There are tools that are becoming more popular for doing ingestion in particular. Things like Airbyte and Fivetran and other places, especially if you have lots and lots of data in a big organization where you need to have ingests – that can be helpful.
Alexey
In the live chat, there is a comment that says “Getting a Data Engineering Job - Jeff Katz.” This is actually one of our videos. Let me show you how to find it. Go to our YouTube channel and then you just put “Getting a Data Engineering Job - Jeff Katz “ in the search and then this is the link. Check it out. It's also pretty useful."
870	As a junior data scientist, what do you consider to be the most important skill or skills (top 3 three)?	Yeah. If it works for you, use it. I don't mind.
871	As a junior data scientist, what do you consider to be the most important skill or skills (top 3 three)?	I mentioned cross-validation. Another thing I ask when I interview people is about projects. Usually, I ask, “Why did you do this project?” If I get a good answer to this question then it's a very good sign. Here, what I mean to say is – explaining why you did certain things is really helpful compared to a validation framework. So communication is the skill I mean when I say “explaining things. And I guess coding in general is the third one.
872	As a junior data scientist, what do you consider to be the most important skill or skills (top 3 three)?	That's a very similar question to when we should use the lambda and when we should use Kubernetes. If you want to use Kubernetes, then go with TensorFlow Serving. Otherwise, TensorFlow Lite. There is one thing where you have to use TensorFlow Lite – where you cannot use TensorFlow Serving – and this is when you want to deploy your model on a mobile device. I have a phone here. If you want to create an app with a model, then you will have to use TensorFlow Lite because you will not be able to use TensorFlow Serving. TensorFlow Serving also adds a bit of complexity. You will need to have this gateway service, you will need to have GRPC service, so that becomes more complex. If you have a lot of load, then you need to do this otherwise, TensorFlow Lite.
873	As a junior data scientist, what do you consider to be the most important skill or skills (top 3 three)?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
874	As a junior data scientist, what do you consider to be the most important skill or skills (top 3 three)?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
875	When it comes to a simple decision tree versus logistic regression, how can one decide which model to apply? Is there something in the data that tells us that one would work better?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
876	When it comes to a simple decision tree versus logistic regression, how can one decide which model to apply? Is there something in the data that tells us that one would work better?	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
877	When it comes to a simple decision tree versus logistic regression, how can one decide which model to apply? Is there something in the data that tells us that one would work better?	"That's also a very generic question. It depends on what kind of data you have. Can you take a sample from this data or do you need to use the entire dataset? Also, as I said, what kind of data is it? Is it images? Is it text? Is it tables? I would just read a part of this. I actually don't know if it's possible with the HDF format. Maybe you can just read the first couple of thousand rows? I don't know. The best practices could be, let's say – the first one is to take a sample from this. 
The second one is maybe seeing how you can parallelize it with something like Spark. Then again, I don't know exactly what HDF format is and if you can read with Spark. Maybe not. Maybe you will need to first turn it into something else and then read it with something like Spark. Then the third one would be to get a big machine with 500 megs of RAM, for example, and then just read it from there. The last option seems to be the easiest one if you actually need to read all the data. In some cases you might not. Is it related to neural networks? I think some of the data was saved in this format. 
I might be wrong, but I think you might iterate over this data and then read it in chunks. You don't need to read the entire thing. You just read one gigabyte at a time, for example. That's another strategy - chunking. This is actually something that Spark relies on. When dealing with large datasets, Spark chunks the dataset into small partitions, and then it processes each partition separately. Then it combines the result at the end."
878	When it comes to a simple decision tree versus logistic regression, how can one decide which model to apply? Is there something in the data that tells us that one would work better?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
879	When it comes to a simple decision tree versus logistic regression, how can one decide which model to apply? Is there something in the data that tells us that one would work better?	Yes, there is a thing called the validation dataset that you can use to guide your decision. That's the best thing you can do. Another thing you should take into consideration is time. Let's say that we have a lot of categorical variables, then fitting a decision will take more time than a logistic regression. Then applying it will also take more time. So this is something you should also consider in your experiments. But in the end, you probably want to use the validation set to guide you.
880	In this course, we use GCP. In the future, what is the best cloud to invest time in learning?	"Ankush
I think if it's about finding a job, spend time learning AWS, because… come on – everybody's using AWS and you have a higher probability of finding a company which will have AWS. After that choose GCS or Azure or whatever you feel. But I think the main point is to learn the concepts. For example, let's talk about data warehouses for partitioning and clustering. In Prefect, it’s how to create your DAGs in particular structure or paralyzing your flow. I think if you learn this concept, it doesn't really matter which cloud platform you use at the end of the day, because all of their services will be pretty similar. I think once you grab the basic concepts and your base layer is good, it's very easy to adjust to both of the cloud environments. I have personally worked on AWS and Google Cloud. I've also worked a bit with Snowflake. Once you can get the ideas, or the concepts, it's really easy to make that switch.
Alexey
I'll add that my experience with Cloud is mostly AWS. Not mostly, I think, it’s only AWS. For this course, I used GCP for the first time and it wasn't that difficult. The UI is different. I actually think GCP has a better UI, in my opinion. It’s more intuitive. The only thing I needed to figure out was, “In AWS this thing is called this way. How was it called in GCP?” Then I would just Google it, find it, and then just use it. Most of the time, it was like that.
Ankush
I have a follow up question. What if ChatGPT is only integrated in Azure? Then what do we do? 
Alexey
Well… I'll have to use Azure. [chuckles]
Ankush
So we’ll need to migrate? [laughs]"
881	In this course, we use GCP. In the future, what is the best cloud to invest time in learning?	"Alexey
The best way to contribute is to take notes and include them to the notes section. And tell your friends about the course. As a part of your Learning in Public, spread the word. It will help us. And don't forget to give us a star if you have."
882	In this course, we use GCP. In the future, what is the best cloud to invest time in learning?	Well, what you can always do is create a new account. Don't forget to unlink your credit card if you want to use the same credit card. Unlink it from the billing from the old account, and then link it with a new one. Or if you have a different credit card, then you can just use that. For example, I use Revolut as a bank and in Revolut, you can create one-time virtual cards. That could be an option, too.
883	In this course, we use GCP. In the future, what is the best cloud to invest time in learning?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
884	In this course, we use GCP. In the future, what is the best cloud to invest time in learning?	"There are lots of different varieties of approaches here. One thing that you might do is use some kind of data ingestion tool, as I mentioned in the live FAQ discussion – something like Airbyte or Fivetran – if you have lots of data that you're ingesting, that can be a common way to stay on top of that. Or you could do it through a script, kind of like we do here. 
But what you might do is run your agent on a virtual environment in Google Cloud VM, for example, or in AWS, or somewhere else. You could run it in Kubernetes. There are a number of places where maybe you would have that running and have your infrastructure there, so that you were just pulling down the code not to your local machine, but it would just be in your VM. 
If you're actually going and using Pandas, you're putting the data into memory, so you do need to have some scripts that will do that. And you can go directly between Google Cloud Storage and BigQuery with the Prefect GCP library. So that Prefect GCP library that you installed, that you've been using, does have some options in there. I think it's in the BigQuery module to actually get data directly into BigQuery from GCS. So there are a lot of options there. Check out Prefect GCP to see how to do it."
885	Can you explain the difference between setting regularization without limit on W0 and with? Andrew Ng shows a formula without the limit, but in the second week, we made with one.	That's a very similar question to when we should use the lambda and when we should use Kubernetes. If you want to use Kubernetes, then go with TensorFlow Serving. Otherwise, TensorFlow Lite. There is one thing where you have to use TensorFlow Lite – where you cannot use TensorFlow Serving – and this is when you want to deploy your model on a mobile device. I have a phone here. If you want to create an app with a model, then you will have to use TensorFlow Lite because you will not be able to use TensorFlow Serving. TensorFlow Serving also adds a bit of complexity. You will need to have this gateway service, you will need to have GRPC service, so that becomes more complex. If you have a lot of load, then you need to do this otherwise, TensorFlow Lite.
886	Can you explain the difference between setting regularization without limit on W0 and with? Andrew Ng shows a formula without the limit, but in the second week, we made with one.	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
887	Can you explain the difference between setting regularization without limit on W0 and with? Andrew Ng shows a formula without the limit, but in the second week, we made with one.	"Yeah, I honestly think that there is not much of a difference between this. In one case, if we don't put limits on the bias term, we say that we don't want to penalize it – it can be as large as possible or not. But in the case of Andrew Ng (in his machine learning lectures) they say “Okay, bias term can be anything. We only want to control weights for things.” Practically, I don't think there is a significant difference, but it becomes a little bit more difficult to implement it. 
When you add these to diagonal lines, for our case implementation is super easy. You just take this identity matrix and multiply it by something. But then in case we don't want to penalize for the bias term, it becomes a little bit more involved. Since I didn't notice any practical difference in these two approaches, I decided not to spend time explaining that it's also possible to do it this way. I hope that answers the question."
888	Can you explain the difference between setting regularization without limit on W0 and with? Andrew Ng shows a formula without the limit, but in the second week, we made with one.	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
889	Can you explain the difference between setting regularization without limit on W0 and with? Andrew Ng shows a formula without the limit, but in the second week, we made with one.	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
890	I would like to understand how we take the data engineering component and connect it to this course. For example, transform Big Query data to train a model.	[chuckles] I'm not prepared to answer that question. I don't have a top three for guidelines. Maybe this is something we can discuss next time. I don't know how to answer that. Maybe what you can do is go to our YouTube channel (I know I keep doing this) where we just had this Responsible and Explainable AI Interview about that. So maybe check it out. I will mark this as “answered” and then maybe this is something we can go through next time, like we did last time – some of the questions I didn't answer and we moved it to this week. So probably we’re going to keep doing it like this.
891	I would like to understand how we take the data engineering component and connect it to this course. For example, transform Big Query data to train a model.	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
892	I would like to understand how we take the data engineering component and connect it to this course. For example, transform Big Query data to train a model.	"There was no registration deadline at all for this course. So you can still register. You can still sign up and you can follow the course. You will not be able to submit the homework assignments where the due date is already over because there are already solutions posted, so it doesn't make much sense to submit homework when the solutions are already there. You can follow the course but you will not be able to submit the homework. You can check our frequently asked questions and I do recommend doing this. Please remember the rule – if you have a question, first go check the frequently asked questions, and if you don't find the question there, go and ask in Slack. 
For this particular question, it’s already there. “Don't worry, you can take the course.” Maybe right now it's becoming a little bit more challenging to catch up with everything, but note that you can skip the midterm project. Right now, if you want to catch up with everything, if you skip the midterm project, it's fine. You will be able to do two capstone projects and still get the certificate. Also, you can just take the course in self-paced mode. There is nothing wrong with that either."
893	I would like to understand how we take the data engineering component and connect it to this course. For example, transform Big Query data to train a model.	Well, I'm glad you asked, because we have a data engineering course. If you take it, you will understand a bit more – we do not go into details about how exactly it connects with this course. If you remember, when we talked about CRISP DM, one of the steps there is data preparation and this is what data engineering covers. Once the data is prepared, once the data is in a data lake or data warehouse, then we, data scientists, machine learning engineers, get the data, train the model, deploy this model, and take it to the other steps of the process.
894	I would like to understand how we take the data engineering component and connect it to this course. For example, transform Big Query data to train a model.	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
895	What are the most terrible errors that data scientists could make nowadays? What should we be informed about?	I'm not sure I understand the question. The question is “Why test_train_split splits in two, but here, we split into three parts? For that there is a video in week one that explains why we need three datasets, not two. So you can go check it out. There is a problem called “multiple comparison problem” and that's why we use test/split to account for that. You will see later, in module three – there we can also use SciKit Learn train_test_split to split our original dataset into three parts and this is exactly what we will do.
896	What are the most terrible errors that data scientists could make nowadays? What should we be informed about?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
897	What are the most terrible errors that data scientists could make nowadays? What should we be informed about?	"Tim
I think a couple people asked this in ML Zoomcamp Slack. I was thinking about this a little bit. I think it's sort of a philosophical thing [chuckles] where we think about models and Bento is sort of separate a little bit from code. So your project directory is where all the stuff that you'll commit to Git. I don't think we think that it's the right model to commit your models to Git, for example. We think of ourselves a little bit more like Docker. You don't have Docker images in your project directory, right? You just have your Docker file and that's what you commit to Git. In the same way, we store your models and your Bento in that dedicated directory, and then sort of leave it up to you to push it to different places where you might build or where you might need it
Alexey
Do you know why you decided to save it locally on your local file system? For example, what MLflow does – you can save it to S3 directly, but in the case of Bento, you always first save it to local storage and then you decide “This model I want to export, but for the rest, I don't care.”
Tim
Right. First, I think it’s because that's the easiest way to do it, but also because I think with BentoML, the workflow is that a lot of times, somebody's building a model on their laptop, or somebody's building a model in a slightly less distributed system and they're assembling their Bento there. So I think that the fastest way to access that model is locally. We do give you the ability to push that model and pull that model from other places. But ultimately, when you assemble your Bento and then you assemble your container, all that stuff has to be there. I don't think we wanted separate abstractions for when a person is training and saving, versus when a person is building a Bento and pulling in the model and assembling the container. It's just the same abstraction, which is just a local repository which contains models.
Alexey 
Maybe you don't want to save every model every single time you change one single parameter to the cloud. [Tim agrees] Maybe at the end, after all the experiments, you want to save the final one. Meanwhile, for MLflow, the use case is different – you actually want to save every single experiment because you want to see the results of these experiments.
Tim
Right."
898	What are the most terrible errors that data scientists could make nowadays? What should we be informed about?	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
899	What are the most terrible errors that data scientists could make nowadays? What should we be informed about?	"The most dangerous one… I can think of two. One is going for the most “exciting” and complex model, when a simple non-machine learning solution will work. For example, you can just do something like group_by and calculate_mean for each of the things. Let's say you want to predict the price of a car. What you can do is run a group_by_query from your data, and you can look at the model, make, and year, do group_by and then use this as a prediction. That's very simple. It doesn't need any machine learning at all. And it's already a good baseline. Then the logistic regression model that we covered is another good baseline that will probably improve over the previous baseline. Maybe that's enough for a start. You don't need a complex XGBoost model (maybe XGBoost is not that complex compared to deep learning). But it's very tempting to go with the fancy new solution and Skip the boring ones – the group_by one, linear regression, decision tree – skip them and go to the more “fun” ones. So that's something that data scientists sometimes do. 
Then another one is chasing new tools – again, trying new tools instead of focusing on the business problem. I think it's common across all engineers, not just data scientists, but also software engineers, frontend engineers, and so on. I would again, go to our YouTube channel, where there is a talk by Elena, How Your Machine Learning Project will Fail, where Elena talks about the many different reasons of how things can go wrong in a machine learning project. Then this one is somewhat similar, but here Doug talks about search projects, not just machine learning projects, but specifically search. The one with Elena is more general. So check that out. I think that covers the most “terrible” one."
900	Are we going to come up with our own individual projects?	Yes, you will have to come up with your own individual projects. It's not easy, I know. But we'll help you. We will help you to come up with a project idea. That's actually on purpose because we want every project to be unique. Then it becomes a part of your portfolio when you look for a job.
901	Are we going to come up with our own individual projects?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
902	Are we going to come up with our own individual projects?	I would say to focus on projects, not on tools. In this course, you will get a chance to do this. If you open a job description for data engineers, you will see a lot, a lot, a lot of different technologies. Maybe think of a project that you can do for this particular company and do this project using, for example, the tools you learn here or the tools you want to learn, and build a portfolio. I guess this is more of a tip for getting a data engineering job, rather than for somebody who's starting their data engineering career. But if you already have a job, then maybe think of the business impact of what you need. I think I will not spend too much time talking about that because this requires a one hour long discussion. Perhaps we should have a podcast interview about that. If you can think of any potential guests for this podcast interview, you can let me know and we can try to invite these people and talk about this in more detail. One thing I want to share with you is that in our YouTube channel, we had a few interesting talks. We had this Getting a Data Engineering Job with Jeff talk. It’s a very nice presentation. Then we also had an interview with Jeff. So these two podcasts are quite interesting. You can also check out other ones. I think you'll find a lot of interesting content here.
903	Are we going to come up with our own individual projects?	Yeah, we can pat you on your shoulder. [chuckles] There’s the position on the leaderboard. The leaderboard is anonymized so nobody will know that this hash belongs to you, but if you want, at the end we will have a forum where you can share your actual contact information – your actual information about you and you will be the first in this public leaderboard. This is how I can kind of give you a reward for that. But apart from that – eternal glory? Maybe that?
904	Are we going to come up with our own individual projects?	"Jeff
As Alexey said many times in the class, you can use whatever tool you want for things. So if you want to accomplish things with Workflows, that's fine. There are different tools out there. I'd say the big difference is that Prefect is agnostic to any cloud. You can easily drop in different storage blocks if you want to, or have just different options. Anyone who's using Python first wants to be able to use lots of different tools. That's a good option there. With Workflows, you're kind of tied to Google's infrastructure at that point, in a pretty significant way. But use a tool that works for you. That’s my advice there."
905	Will there eventually be instructions on how to deploy Prefect with Terraform?	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
906	Will there eventually be instructions on how to deploy Prefect with Terraform?	"Jeff
I don't know. When hosting meetups, I'll just draw on [a board]. It's great to have hybrid options for folks. People who can be remote, people can be in-person – it definitely adds complexity in how things are done. There is a lot more with logistics and things. The short answer is no. But if people are interested in some kind of online Prefect-specific course, send me a message in Slack. I'm there. Let me know about your interest level. It sounds like there's some interest.
Alexey
At least six people are interested. 
Jeff
[chuckles] At least six. 
Alexey
Maybe more, but they just didn’t know that they could vote."
907	Will there eventually be instructions on how to deploy Prefect with Terraform?	"There are lots of different varieties of approaches here. One thing that you might do is use some kind of data ingestion tool, as I mentioned in the live FAQ discussion – something like Airbyte or Fivetran – if you have lots of data that you're ingesting, that can be a common way to stay on top of that. Or you could do it through a script, kind of like we do here. 
But what you might do is run your agent on a virtual environment in Google Cloud VM, for example, or in AWS, or somewhere else. You could run it in Kubernetes. There are a number of places where maybe you would have that running and have your infrastructure there, so that you were just pulling down the code not to your local machine, but it would just be in your VM. 
If you're actually going and using Pandas, you're putting the data into memory, so you do need to have some scripts that will do that. And you can go directly between Google Cloud Storage and BigQuery with the Prefect GCP library. So that Prefect GCP library that you installed, that you've been using, does have some options in there. I think it's in the BigQuery module to actually get data directly into BigQuery from GCS. So there are a lot of options there. Check out Prefect GCP to see how to do it."
908	Will there eventually be instructions on how to deploy Prefect with Terraform?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
909	Will there eventually be instructions on how to deploy Prefect with Terraform?	I will not answer this right now. We did not plan anything like that. I think Anna prepared some materials about how to deploy Prefect, not with Terraform, but with some Google Cloud Platform stuff. I might be mistaken, but in practice, I think we use something like Kubernetes. Kubernetes is not managed through Terraform. I will leave this question answered and in the next Office Hours when we'll talk more about Prefect. Somebody from the Prefect team will answer this question. Probably Jeff will be in our next Office Hours.
910	What do you suggest after this? Date engineering Zoomcamp or MLOps Zoomcamp?	Yeah. If it works for you, use it. I don't mind.
911	What do you suggest after this? Date engineering Zoomcamp or MLOps Zoomcamp?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
912	What do you suggest after this? Date engineering Zoomcamp or MLOps Zoomcamp?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
913	What do you suggest after this? Date engineering Zoomcamp or MLOps Zoomcamp?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
914	What do you suggest after this? Date engineering Zoomcamp or MLOps Zoomcamp?	I can't make this decision for you. I don't know enough of your background to answer this question. It can be both, too. Why not? Probably, the more relevant one would be the MLOps Zoomcamp. Here, I am assuming that you are more interested in ML engineering for this course. And if you're interested more in ML engineering, then it's definitely MLOps Zoomcamp. In the end, it's really up to you. There is no right or wrong answer, so I would suggest doing both. [chuckles] My opinion is a little bit biased here. Also, you only have 24 hours in your day.
915	How can we handle wrong values in a dataset? For example, extra large numbers in numerical columns or a set of letters in categorical ones?	It is possible to do everything locally. You will need to do it by… I think you will need to do it in module five when we deploy something. Or actually, you will probably need to do it when we do your project. Yeah. It's fine to do everything locally. But if you have access to cloud and you want to practice with this, I do recommend practicing, because cloud is one of the skills that employers are looking for. They really need it. So if you want to be more employable, I do recommend learning about cloud.
916	How can we handle wrong values in a dataset? For example, extra large numbers in numerical columns or a set of letters in categorical ones?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
917	How can we handle wrong values in a dataset? For example, extra large numbers in numerical columns or a set of letters in categorical ones?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
918	How can we handle wrong values in a dataset? For example, extra large numbers in numerical columns or a set of letters in categorical ones?	You do exploratory data analysis – you look at the data, you see if anything is wrong there, and then you write some code for preparing this dataset. There is no silver bullet for that. You just need to look at the data to see if there are things that are wrong, and then see how you can programmatically fix these things with your code. For example, for extra large numbers, there are multiple things you can do. You can just throw away the observation completely, or you can do capping. Capping is when you say, “If this value is larger than X, you keep it at X.” There are many ways you can do it, but I think these two are the most typical ones.
919	How can we handle wrong values in a dataset? For example, extra large numbers in numerical columns or a set of letters in categorical ones?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
920	What is a good sample or size of the dataset?	It depends. In some problems, you can already build a good model with 100 examples. In other cases, 10,000 might not be enough.
921	What is a good sample or size of the dataset?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
922	What is a good sample or size of the dataset?	"You can check out a few projects in this playlist. In the playlist, you can see some of the videos here, for example, there's a project from Alvaro. And it's not just Alvaro, there’s one from Alvaro and Ninad. You can just check out these projects – there’s a project from Carolina and Hamad, a project from Lisa, and a project from Timur. The project from Timor may actually fit what you want. Timur has done a lot of work and it's very interesting. So check it out. Actually, Timur’s project is not a midterm project. It's more like a capstone project – there is some deep learning there, he uses Karis. So maybe focus on the other three. 
You can also go to our course web page, and then go to midterm project (you need to use the 2021 folder) and then in the readme there should be some assignments. There will be some form that says “To find the projects you need to review, use this table.” Here, you can see all the projects from previous students’ midterm projects. You can pick some of them and check if you like them or not."
923	What is a good sample or size of the dataset?	Yes. We will actually cover cross validation this week. Sometimes, if your dataset is small, (this was one of the questions today) then using cross validation is a very good idea.
924	What is a good sample or size of the dataset?	Yes, that's the main reason we have two capstone projects. This is for those who are catching up to be able to get the certificate. Now there are holidays, like Christmas holidays and then January holidays in some countries – like in Russia, for example, people usually take holidays not during the end of December, but at the beginning of January. No matter in which part of the world you are in, you still have one month to catch up (November) and then start working on the capstone. Actually, I’ve now realized that the capstone starts quite soon. But for capstone 1, you only need the first six weeks or so, and then you can finish this project. Then during the holidays, you can watch Kubernetes, neural networks and so on, if you want. Then you can do capstone 2. That's probably better. But yeah, it's fine. You should do 2 capstones.
925	To get the certificate, is it only required to submit the final project?	Yes, you don't need to do weekly homework assignments for the certificate.
926	To get the certificate, is it only required to submit the final project?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
927	To get the certificate, is it only required to submit the final project?	No, you cannot.
928	To get the certificate, is it only required to submit the final project?	"Alexey
You can create a Python path variable in your .bashrc and it will be there permanently. Just in the same way as you do with PATH. I don't know why we do it like that. Maybe I just didn't mention that. But, of course, you can just put everything you want in .bashrc and this alliance (this code) will be executed every time you launch your virtual machine. This way you can execute these things automatically."
929	To get the certificate, is it only required to submit the final project?	"Alexey
No. I mean, we kind of told you. Yes, you maybe didn't see the intro video. But, sorry. No, it's late. We will actually recalculate the points for the second homework (HW 1B) because in the forum, it said that you can get up to one point, but we calculated that the cap there was seven. We are going to recalculate that. Thus, people who got a lot of points will get just one. Sorry about that. But don't focus on points. If you go to the leaderboard, we know that the person with this hash (722366d8b29ece9be3a7605363562c7c60d6918e) did seven links. Do you know who that is? Probably you don't. 
These points are virtual. It's nice to have a good score, but remember that these points mean whatever you want them to mean – not more than not less. Maybe it's okay if you get a few points less than others for the first homework, but the important thing here is consistency. If you just do your homework, even without doing any Learning in Public stuff, which I think you should do (you should post about the course). If you don't do this and you just do all the homework, by the end, you will be somewhere in the top 20. If you look at the leaderboard from the previous iteration, you will see that not everyone stuck around till the end. 
If you just do your homework, don't worry about these points, and you'll be fine. Sorry about that. I know that it can be discouraging that you didn't get as many points as others, but don't worry about that. Focus on learning. Maybe Michael has some other suggestions or some other things that I didn't mention, because he took part last year. But I think he was one of the people who did a few posts every week, right?
Michael
Yes. Actually, when I went through it, I missed the whole… you can get up to seven points, but I was just doing one every week and I just stuck with that. I was still near the top. I think the real value in that isn't necessarily the points but just being comfortable sharing – writing articles and making videos. That's the real value there. But everyone likes the clout value, too. As Alexey said, it will fluctuate, so if you keep up with it going forward, you'll probably be just fine."
930	How many homework assignments will there be?	"Alexey
I don't know when this question was asked, but we did extend it."
931	How many homework assignments will there be?	Yes, definitely. Many people who did Python took the course and succeeded at the end.
932	How many homework assignments will there be?	Six and the first one kind of has two parts. So I guess, seven.
933	How many homework assignments will there be?	"Alexey
I don't know. Maybe I'll put this question to Ankush as well. But check the website. I think this is one of the good examples when you can just use Google to find the answer."
934	How many homework assignments will there be?	I would say to focus on projects, not on tools. In this course, you will get a chance to do this. If you open a job description for data engineers, you will see a lot, a lot, a lot of different technologies. Maybe think of a project that you can do for this particular company and do this project using, for example, the tools you learn here or the tools you want to learn, and build a portfolio. I guess this is more of a tip for getting a data engineering job, rather than for somebody who's starting their data engineering career. But if you already have a job, then maybe think of the business impact of what you need. I think I will not spend too much time talking about that because this requires a one hour long discussion. Perhaps we should have a podcast interview about that. If you can think of any potential guests for this podcast interview, you can let me know and we can try to invite these people and talk about this in more detail. One thing I want to share with you is that in our YouTube channel, we had a few interesting talks. We had this Getting a Data Engineering Job with Jeff talk. It’s a very nice presentation. Then we also had an interview with Jeff. So these two podcasts are quite interesting. You can also check out other ones. I think you'll find a lot of interesting content here.
935	How often do you use hypothesis tests and p-values in your real data science job?	Quite often. I don't use them personally. We use them for running A/B tests. Let's take a churn prediction example – churn detection. We have one model for detecting churn and then we roll out a new model. Our ultimate goal is increasing revenue. So “What is our revenue in one month?” From these experiments, we know that our model has this precision and has this recall, it has this AUC, it has blah, blah, blah, all these metrics. But we don't always know how exactly these numbers translate into real revenue at the end – how much money the company will have. What we can do is take these two models and then send some of the customers to the old model (variant A), and some other customers to the new model (variant B). Then you can measure what the revenue will be after one month, let's say. That would be a hypothesis test. In this case, I think, it's a t-test that checks that the means are significantly different between two groups. And then p-value will tell you if you should reject this hypothesis or not – whether this new model is actually better, or whether they are the same. But I don't sit with pen and paper and calculate all these p-values. There is a statistical engine that does that for us. Of course, t-tests don't always work. There are some assumptions with this, like there’s normal distributions and these statistical engines usually select the best test for a particular situation. Maybe I will recommend for you to check our YouTube channel, where we have three talks about A/B tests. I was thinking about this one – Setting up an A/B testing Framework from Agnes. Agnes is my ex-colleague – we used to work together at OLX. She describes the framework we used at OLX for running A/B tests. This conversation with Jakob is about how his company runs A/B tests and this one with Sadiq shows how to set up the infra for doing this. It was a few weeks ago, so many of you probably have seen it. So I was referring to the one with Agnes. And actually, this one about Growthbook is another interesting one. They don't use traditional statistical tests – they use Bayesian tests. It's an open source library for doing these Bayesian A/B Test. So you can check that out too. Yeah, check this out. You'll find a lot of useful stuff. So the answer to your question is ‘yes,’ that happens often. Sometimes these questions actually come up during the interview, but not for ML engineers. I think data analysts and data scientists need to do these kinds of things more often than ML engineers.
936	How often do you use hypothesis tests and p-values in your real data science job?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
937	How often do you use hypothesis tests and p-values in your real data science job?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
938	How often do you use hypothesis tests and p-values in your real data science job?	"Actually, I don't remember exactly how I do this. The right approach would be to use only train data and not use validation data or test dataset. This is done on purpose for exactly this reason that you mentioned. We want to model a situation in which there is some unseen data – it's truly unseen, we do not see that all – and anything can happen to this unseen data, including the situation that you described, such as there is a value that is only present in the test data. 
Imagine that there is a new iPhone – when you train your model, this new iPhone didn't exist (you didn't have this category) but you apply this model, and then, all of a sudden, there is a new iPhone. How will your model react to this? You need to somehow model this situation. This is totally normal. This can and will happen in practice. We just want to understand what would be the effect of that. That's why we need to see this in training data. This is good and this is how it should be. I hope that is a good answer."
939	How often do you use hypothesis tests and p-values in your real data science job?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
940	Is log transformation suitable for both right- and left-skewed distribution? Is there any other better transformation method available for skewed data?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
941	Is log transformation suitable for both right- and left-skewed distribution? Is there any other better transformation method available for skewed data?	"Tim 
I think, yes. We have a few examples on our website. I think with Colab, it becomes a little bit more difficult once you're hosting the Bento to access it, if it's running on a Google Colab server. But yeah, you can save it. What I would say is that you can save your model and then export your model to somewhere where you can then build your Bento there.
Alexey 
By default, Bento saves in a local file system (in your /home) so you need to have a way to save this model somewhere, let's say on the cloud, S3, whatever – somewhere externally. The workflow here is that you train your model in Colab, you save it, and then you deploy it somewhere. You don't deploy it on a Colab instance.
Tim
Right. When you save your model, I think it'll save it to the Colab instance. Then you have to use BentoML Export to push it to an S3 bucket or something where you can pick it up and deploy your Bento.
Alexey 
And there are options for deploying. We already discussed this in the course, when we looked at Elastic Beanstalk. I assume it can work with Bento. I haven't checked. [Tim agrees] But then what we saw this week, we saw how to deploy it with ECS, and then we can also deploy to Kubernetes EKS, we can also deploy to lambda – there are a ton of other options. Everywhere where you can deploy a Docker container (Docker image) you can deploy it there. Right? [Tim agrees]"
942	Is log transformation suitable for both right- and left-skewed distribution? Is there any other better transformation method available for skewed data?	"Tim
I think a couple people asked this in ML Zoomcamp Slack. I was thinking about this a little bit. I think it's sort of a philosophical thing [chuckles] where we think about models and Bento is sort of separate a little bit from code. So your project directory is where all the stuff that you'll commit to Git. I don't think we think that it's the right model to commit your models to Git, for example. We think of ourselves a little bit more like Docker. You don't have Docker images in your project directory, right? You just have your Docker file and that's what you commit to Git. In the same way, we store your models and your Bento in that dedicated directory, and then sort of leave it up to you to push it to different places where you might build or where you might need it
Alexey
Do you know why you decided to save it locally on your local file system? For example, what MLflow does – you can save it to S3 directly, but in the case of Bento, you always first save it to local storage and then you decide “This model I want to export, but for the rest, I don't care.”
Tim
Right. First, I think it’s because that's the easiest way to do it, but also because I think with BentoML, the workflow is that a lot of times, somebody's building a model on their laptop, or somebody's building a model in a slightly less distributed system and they're assembling their Bento there. So I think that the fastest way to access that model is locally. We do give you the ability to push that model and pull that model from other places. But ultimately, when you assemble your Bento and then you assemble your container, all that stuff has to be there. I don't think we wanted separate abstractions for when a person is training and saving, versus when a person is building a Bento and pulling in the model and assembling the container. It's just the same abstraction, which is just a local repository which contains models.
Alexey 
Maybe you don't want to save every model every single time you change one single parameter to the cloud. [Tim agrees] Maybe at the end, after all the experiments, you want to save the final one. Meanwhile, for MLflow, the use case is different – you actually want to save every single experiment because you want to see the results of these experiments.
Tim
Right."
943	Is log transformation suitable for both right- and left-skewed distribution? Is there any other better transformation method available for skewed data?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
944	Is log transformation suitable for both right- and left-skewed distribution? Is there any other better transformation method available for skewed data?	"Yeah, I think log transformation is suitable for right-skewed distributions. The same we had in the course. Because for the left-skewed, I don't actually know how it will work. You can just apply it and see what happens. I think that's the best approach. Maybe the answer is “it depends”. You can just try it and see what happens. Then based on what you observe, you can decide what to do next. If you see something that remotely reminds you of a bell-shaped curve, then you're on the right track? I hope I answered this question. 
I don't know about any better transformation methods available for skewed data. I think logarithmic transformation is a pretty standard one when it comes to pricing data, that's why we did include this in the course. If you work with prices, it's very likely that you will have to use this one. But when it comes to others, maybe it's less common. There are things like the Box Cox transformation – there are quite a few of them. But maybe they are not as widespread as this one."
945	Shouldn't R-Squared or MAPE free us from the scale issue suffered by RMSE in regression?	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
946	Shouldn't R-Squared or MAPE free us from the scale issue suffered by RMSE in regression?	"I wouldn't call it an issue, actually. It's more like a feature than a bug of RMSE. Let's say you have a model that predicts price and you want to know, on average, how wrong your model is in dollars. That's the purpose of RMSE. Or if you're predicting the age of somebody, then you want to know, on average, how wrong your model is in years. It gives us this understanding of how wrong the model is, on average, on the same scale as our target variable, while R-Squared and MAPE do not give us that. They serve different purposes. 
I never actually used R-Squared in practice. For me, it's always misleading. I cannot really interpret it quite well. But MAPE is a good one – it has its own problems and I think we talked about that in the last Office Hours. It's good if you look at multiple metrics at the same time – you look at RMSE, you look at MAPE, you perhaps look at mean absolute error (MAE) and then, based on that, you make some decisions whether to use this model or not."
947	Shouldn't R-Squared or MAPE free us from the scale issue suffered by RMSE in regression?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
948	Shouldn't R-Squared or MAPE free us from the scale issue suffered by RMSE in regression?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
949	Shouldn't R-Squared or MAPE free us from the scale issue suffered by RMSE in regression?	I guess not. That's the process of learning? Maybe more senior people don't… Yeah, I see where you're going with this. Senior people are not supposed to go bankrupt and default. They are probably supposed to pay back the debt. I guess these are the particularities of this dataset, I assume. Indeed, it's probably a data issue ,or not an issue, but a “characteristic” of this dataset.
950	If I use Airflow, will it be able to complete homework for week 2 and the following weeks?	Yes, more or less. Some parts will require Prefect, but I think you will still find a way of getting the answers without Prefect. I believe so. So technically, yes – it's possible. But then again, we do not officially support Airflow in this iteration. Other students who use Airflow for homework for week 2 can help you and most likely will help you. We also have some frequently asked questions from the previous iteration. Do this if you know what you're doing. If you're just learning right now and you don't feel very adventurous, then stick to Prefect.
951	If I use Airflow, will it be able to complete homework for week 2 and the following weeks?	"Jeff
There are lots of different things there. This is something I used to teach folks doing data science-type work and it applies for data engineering, too. I don't have a good checklist at the tip of my fingers, but you do want to look at things like “Are your data in the right type? Do you have missing values, libraries, rate expectations (which we have integration for)?” That can help with trying to make sure your data looks how you think it'll look, in terms of the statistical properties of it. Hypothesis can also generate some ideas there, or some other Python library. Pandera for everything being how you expect, values matching what you expect. So there are a lot of different tools in Python that you can use there. It's a good question. It can be a lot of different things."
952	If I use Airflow, will it be able to complete homework for week 2 and the following weeks?	For everything we do in this course, it's possible to run things locally. In some cases, when you need to create a Google Cloud Storage bucket or when you need to create a BigQuery table, you cannot do this locally. But for many things, you can. You can set up your Postgres instance, and we show you how to do this. We showed this in the first week’s video. In the Prefect videos, we also show how to do things locally. Then, you will basically need to skip week 3 because you will not be able to use BigQuery. And then week 4, we show how to use DBT with Postgres. Then with Spark, we show how to run things locally. And for week 6, with Confluent, I think you can still get access to Confluent Cloud, but you can also run things locally – run Kafka locally.
953	If I use Airflow, will it be able to complete homework for week 2 and the following weeks?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
954	If I use Airflow, will it be able to complete homework for week 2 and the following weeks?	"There are lots of different varieties of approaches here. One thing that you might do is use some kind of data ingestion tool, as I mentioned in the live FAQ discussion – something like Airbyte or Fivetran – if you have lots of data that you're ingesting, that can be a common way to stay on top of that. Or you could do it through a script, kind of like we do here. 
But what you might do is run your agent on a virtual environment in Google Cloud VM, for example, or in AWS, or somewhere else. You could run it in Kubernetes. There are a number of places where maybe you would have that running and have your infrastructure there, so that you were just pulling down the code not to your local machine, but it would just be in your VM. 
If you're actually going and using Pandas, you're putting the data into memory, so you do need to have some scripts that will do that. And you can go directly between Google Cloud Storage and BigQuery with the Prefect GCP library. So that Prefect GCP library that you installed, that you've been using, does have some options in there. I think it's in the BigQuery module to actually get data directly into BigQuery from GCS. So there are a lot of options there. Check out Prefect GCP to see how to do it."
955	I think this one is related to what we were talking about. “Should data scientists be able to deploy models?” Then the question is “ML engineers should be able to work on the whole machine learning pipeline, not only deployment.” What do you think, Tim?	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
956	I think this one is related to what we were talking about. “Should data scientists be able to deploy models?” Then the question is “ML engineers should be able to work on the whole machine learning pipeline, not only deployment.” What do you think, Tim?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
957	I think this one is related to what we were talking about. “Should data scientists be able to deploy models?” Then the question is “ML engineers should be able to work on the whole machine learning pipeline, not only deployment.” What do you think, Tim?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
958	I think this one is related to what we were talking about. “Should data scientists be able to deploy models?” Then the question is “ML engineers should be able to work on the whole machine learning pipeline, not only deployment.” What do you think, Tim?	"Tim
I think a couple people asked this in ML Zoomcamp Slack. I was thinking about this a little bit. I think it's sort of a philosophical thing [chuckles] where we think about models and Bento is sort of separate a little bit from code. So your project directory is where all the stuff that you'll commit to Git. I don't think we think that it's the right model to commit your models to Git, for example. We think of ourselves a little bit more like Docker. You don't have Docker images in your project directory, right? You just have your Docker file and that's what you commit to Git. In the same way, we store your models and your Bento in that dedicated directory, and then sort of leave it up to you to push it to different places where you might build or where you might need it
Alexey
Do you know why you decided to save it locally on your local file system? For example, what MLflow does – you can save it to S3 directly, but in the case of Bento, you always first save it to local storage and then you decide “This model I want to export, but for the rest, I don't care.”
Tim
Right. First, I think it’s because that's the easiest way to do it, but also because I think with BentoML, the workflow is that a lot of times, somebody's building a model on their laptop, or somebody's building a model in a slightly less distributed system and they're assembling their Bento there. So I think that the fastest way to access that model is locally. We do give you the ability to push that model and pull that model from other places. But ultimately, when you assemble your Bento and then you assemble your container, all that stuff has to be there. I don't think we wanted separate abstractions for when a person is training and saving, versus when a person is building a Bento and pulling in the model and assembling the container. It's just the same abstraction, which is just a local repository which contains models.
Alexey 
Maybe you don't want to save every model every single time you change one single parameter to the cloud. [Tim agrees] Maybe at the end, after all the experiments, you want to save the final one. Meanwhile, for MLflow, the use case is different – you actually want to save every single experiment because you want to see the results of these experiments.
Tim
Right."
959	I think this one is related to what we were talking about. “Should data scientists be able to deploy models?” Then the question is “ML engineers should be able to work on the whole machine learning pipeline, not only deployment.” What do you think, Tim?	"Tim 
I think “ML engineer” is a fairly new title. I do think it implies that they should work the whole pipeline. [chuckles] I am not sure that the industry is going to end up with these individual people who can do the whole thing and be really good at it. That doesn't seem like a scalable approach to me. I think eventually you'll find that the tools will get better, or good enough, where people can specialize and not have to know every part of the pipeline. Not know every part of the pipeline in a really deep way, but I think it's helpful for everyone to know the pipeline at a high and medium level to be able to collaborate better. Collaboration, especially in ML, is difficult because there's so many people involved.
Alexey 
What I see in practice is that often data pipelines are built by data engineers and then there is a bit of feature pre-processing, feature engineering – all that – and usually, the data scientists take care of that. And then ML engineers focus more on deployment and then scaling and then DevOps, MLOps people support more on the infrastructure side, like “How can we make sure that Kubernetes is running, that all the logs are saved, that we can monitor the whole thing?” At least this is what I see around me in the company where I work. I talked to a few other companies and they have a similar setup. Sometimes ML engineers work in detail on training pipelines as well, but I think it's more common that data scientists do this. At least from what I see. I haven’t interviewed many companies – I just talked to a bunch of people here in Berlin when I went to have lunch. I asked, “Okay, tell me about how your organization works. What kind of roles do you have? What kind of responsibilities?” And this is what they tell me.
Tim 
I think as the industry matures, the specializations will become clearer. 15 years ago in software development, DevOps didn't even exist and software engineers were supposed to be doing the builds and the testing – everything. Then we realized, “Okay, we need somebody who kind of bridges Ops and development.” That became DevOps. And then the software engineers, as we progressed in data, were doing all the transformations and the pipelining and everything else. But then we said, “Oh, actually, we need data engineers.” So that became another thing. I think as the industry matures, the specializations will become more clear. But there will always be the early people in the industry who benefit and companies who benefit from people who work the whole pipeline.
Alexey
I think there was a term “full stack data scientist,” right? A person who can do it all. I don't know if it’s still a thing. I think a couple of years ago it was, but now we have ML engineers. Before, it wasn't like a very prominent role but now we see more and more ML engineers."
960	In the real world, after churn, risk customers are found and measures are taken (discounts, for example) the model will become irrelevant, right? How is this handled?	In some domains, maybe the color doesn't matter much – on the shapes matter. If it's that domain, then yes. For example, I think for clothing, it might be the case. Maybe colors don't matter much, because you can have all different colors. But on the other hand, maybe it's actually still important because you want to distinguish the background from the foreground (from the actual item). This is where the color information could be important. If you think it will give you some performance increase in terms of making predictions faster – just use validation to find out if there is any predictive performance drop (drop in accuracy) when you switch to grayscale. If you see that there is no drop, then just stick to that.
961	In the real world, after churn, risk customers are found and measures are taken (discounts, for example) the model will become irrelevant, right? How is this handled?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
962	In the real world, after churn, risk customers are found and measures are taken (discounts, for example) the model will become irrelevant, right? How is this handled?	Why do you think it will be irrelevant? Do you think we'll just identify all churned users, bribe them with discounts, and live happily ever after? At some point, other users will consider leaving too, so we actually need to run this model regularly. We will need to update this model regularly – retrain – because the model will probably make mistakes. So it's probably a never-ending process here. We will always need to keep an eye on this model – probably in an automated way. Maybe we have a process that automatically retrains it every half a year or something. But I don't think just deploying this model and using it will solve the churn problem. Maybe I misunderstood your question. I don't know.
963	In the real world, after churn, risk customers are found and measures are taken (discounts, for example) the model will become irrelevant, right? How is this handled?	"Tim 
Not really. We have users who use both Streamlit and Gradio. I think Streamlit and Gradio are a bit more about the presentation side. We do have a couple of users who have asked us about deeper Streamlit or Gradio support and I think that's something that's probably coming up in the next few months.
Alexey
For those who don't know what Streamlit or Gradio is – this is a way to create an interface for your services. Well, Gradio is focused on machine learning, meaning you create interfaces specifically for models. And then Streamlit is just a Python package for creating interfaces. We actually had a project at DataTalks.Club about Streamlit. I actually even have a tweet about this. This is what Streamlit looks like. In the video, you can see that you have this interface, and then based on the toggles that you move the core of your model changes. It gives you an easy way to build an interface like that. Meanwhile Bento focuses on quite a different use case, right? It’s not building interfaces, but serving models.
Tim 
Right, yeah. I think one of our users was asking for a really easy way, so that once you have your model and your Streamlit UI to be able to create a Bento deployment with one line in that way, anybody that you gave your Streamlit app to would be calling to that service and it could work anywhere. Which is a nice feature, I think.
Alexey 
Yeah, I think it is. I imagine if you have a Pydentic class there, then you can easily generate a Streamlit app for that.
Tim 
Right. Oh, that's a good idea. I hadn't thought of that. Yeah.
Alexey
And then in addition to that, the Swagger UI that you have, could be like having the actual interface. 
Tim
Oh, yeah, that would be cool. [chuckles]
Alexey
That could be a good Hackathon project, right?
Tim 
Yeah, for sure.
Alexey
From what I see – let's say you’ve built a model. Then you deployed this model and you have this nice API. You can query it with CURL. But what if you want to demo this project to your manager, or a manager of your manager, or somebody who does not necessarily have a technical background? If you give them the URL and say, “Okay, you need to execute the CURL command with a post request,” they would be like, “CURL what? What do you want from me? Just show me how it looks.” This is when tools like Streamlit come in handy. Instead of giving them a command line interface to query it, you just give them “Okay, this is the link. Play with this.” If Bento could do something like that – automatically generate things like that – many data scientists would thank you.
Tim
Yeah, I was just looking because I figured there's a library that turns Pydantic models into Streamlit apps. It looks like there is one that does that. I wonder if we could just plug into that app and then have that same thing. 
Alexey
That's cool. 
Tim
Yeah, that's awesome. That's kind of the cool part of open source – there are so many people out there with so many different projects and we get to collaborate with them and kind of build the best collaborations and combinations of these tools."
964	In the real world, after churn, risk customers are found and measures are taken (discounts, for example) the model will become irrelevant, right? How is this handled?	Just take your time. Go through the materials at your own pace. If we talk about the syllabus, you will only need the second, third, fourth – up to the midterm project. This should be sufficient – up to the seventh module. This should be sufficient to finish the midterm project and the capstone project. You can treat the modules after 7 as extra. They are useful – they are very useful. But if you're really short on time, you can just skip them and use the materials from the first seven modules to complete your capstone project. Once you complete your capstone project, once you must submit everything you did, then you can take the modules after 8 at your own pace. That could be one approach. Another approach could be just taking them without caring about the certificate. Just focus on learning and do things at your own pace. Don't rush.
965	In your experience, Tim, do you think data scientists usually take care of model deployment or is it somebody else?	"Scikit Learn is not very different. This is not exactly how Scikit Learn is implemented. It's a little bit different, but the idea is very similar. In Scikit Learn, they use smarter ways of doing this. For example, in week two, we have a case when we need to apply regularization. When we have correlated or very close to correlated columns, then our matrix becomes very unstable, so we cannot easily invert it. 
If what I’m saying now doesn't make sense, it will make sense once you start watching the videos. Anyways, what I'm trying to say is that in Scikit Learn, they have a smart way of working around this, such that your weights – your vector with weights, the W vector – does not contain insanely large values. They have smarter methods for doing this. But apart from that, it's very similar and you should not get two different results. You can experiment, and I do recommend experimenting with Scikit Learn. We will actually do this in week three. In week three, we will use Scikit Learn for some of these things. 
In week two, we will implement things ourselves. But starting from week three, we will only use Scikit Learn and other libraries, so we will stop implementing things ourselves. Here's just to give you a taste of what machine learning is inside. It’s just a bunch of formulas – algorithms – there is no magic. Internally, Scikit Learn implements them too."
966	In your experience, Tim, do you think data scientists usually take care of model deployment or is it somebody else?	Yes. We will actually cover cross validation this week. Sometimes, if your dataset is small, (this was one of the questions today) then using cross validation is a very good idea.
967	In your experience, Tim, do you think data scientists usually take care of model deployment or is it somebody else?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
968	In your experience, Tim, do you think data scientists usually take care of model deployment or is it somebody else?	Not always. For logistic regression, it does. For decision trees and random forest and XGBoost – not really. Then there is a thing called “calibration”. SciKit Learn calibration. You will need to check this out. I think there are also examples of how to do this with random forest. If you can't find examples here, there are examples on Kaggle. But practically speaking, I don't remember the last time I needed to use something like this. It's helpful when you want to train multiple models. Let's say you've been using logistic regression in production for some time and then you make some decisions based on this threshold. But then you want to deploy a new model (XGBoost) and you want to make sure that the ranges you have in the previous model are the same in the new model so the distribution looks kind of similar. This is where calibration helps. It's a useful thing, but not every application needs it.
969	In your experience, Tim, do you think data scientists usually take care of model deployment or is it somebody else?	"Tim
This is a really great question and I don't have a good answer for it, I think, because the industry is evolving so quickly. I've definitely seen data scientists being in charge of deploying models, probably where that data scientist is one of the only engineering resources, or the engineering resources that are there are too busy. If you're a data scientist and if you build a model, the way that you scale the business value of that model is having that model deployed as an API. If somebody has to do it and there are no resources anywhere else, then yes – the data scientists can do that and we try to make that as easy as possible. In larger teams, I think, typically not. You'll probably have at least some type of DevOps or ML engineer that will help you deploy your model. But knowing how the model deployment pipeline works, I think, is pretty important. That way, you can work better with your engineers. I think it's always nice to know at least the high and medium level of how it works and how to do it end-to-end in fairly simple scenarios. But if it becomes more complicated, I think it does take somebody with a little bit more specialized knowledge."
970	Can you please explain why the Parquet format is better than CSV?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
971	Can you please explain why the Parquet format is better than CSV?	We’re still in progress (as of January 26, 2023). I thought we would do this earlier. But it's still not finished.
972	Can you please explain why the Parquet format is better than CSV?	"Jeff
Yeah, I can talk about this a little bit. I actually was just reading about this yesterday. I had a book in front of me and I was like, “Oh, I want to read it right from the book. Maybe I'll copy/paste it when I go grab it.” I like the Fundamentals of Data Engineering book by Reis and Housley that just came out recently. I recommend that. It is a great book that gives you a lot of background in data engineering. It has a good description of Parquet, in particular, and why it's useful. It can be compressed a lot more than a CSV file. CSV is just kind of this not very standardized way of doing things and they take up a lot of space, even if you do compress them. Parquet is a columnar format and it's very useful for more efficiently storing data, and then you can compress it, oftentimes with Snappy compression and make it even smaller. So it's a fast way to read and write data and not take up tons of space. There are other file storage methods that are even more efficient for reading fast and writing fast. But it's very good for a balance of ways to store things and it has some metadata about the columns in it that goes along with it. That's just really useful for a lot of things like putting it into a database, for example. So it's a winner. I feel like it's pretty much the standard way, if you're just going to store something in a file persisted to disk and just gonna be sitting on a disk. Parquet has become the most popular way to do that with data. But other people can feel free to jump in and add on or disagree.
Alexey
Which chapter was it? Sorry. 
Jeff
It's in the appendix at the end.
Michael
I would also add that with Parquet, you can also do partitioning to decide where the files are written to. When you read the files back in, it's not going to read all of them like you would with a CSV. Also, Apache Delta Lake is built with Parquet files. It just uses extra JSON files to track the changes, so you can just take object storage like S3 or a GCP bucket, and instead of making a full database, it just writes the Parquet files, but then you can interact with them like it's a database, which I think is just really powerful.
Jeff
Parquet is cool. Use Parquet. [Michael agrees]
Alexey
We had one issue though, with Parquet. Michael, you should know about that, because we came across this issue again recently. You have to be careful with the schemas there, because once you say that a particular column is flawed, and then if in another Parquet file, it's not flawed, but it's integer – then you might run into problems. So you have to be careful with the schema. Meanwhile, in CSV, it's just text and then you kind of convert to the proper type as you read, not as you write.
Michael
Yes, so I would say that you need to be very explicit with your schema definitions when saving your files. If anyone is interested, the NYC data – I know they updated the Parquet and there are many files with many issues in there, which is great to practice your data wrangling (if you want some punishment [chuckles])."
973	Can you please explain why the Parquet format is better than CSV?	"Alexey
You can create a Python path variable in your .bashrc and it will be there permanently. Just in the same way as you do with PATH. I don't know why we do it like that. Maybe I just didn't mention that. But, of course, you can just put everything you want in .bashrc and this alliance (this code) will be executed every time you launch your virtual machine. This way you can execute these things automatically."
974	Can you please explain why the Parquet format is better than CSV?	Yes, you will have to come up with your own individual projects. It's not easy, I know. But we'll help you. We will help you to come up with a project idea. That's actually on purpose because we want every project to be unique. Then it becomes a part of your portfolio when you look for a job.
975	Please briefly explain what to expect with the recommender system coming up in Project of the Week this week? How does it work? I want to follow along and observe.	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
976	Please briefly explain what to expect with the recommender system coming up in Project of the Week this week? How does it work? I want to follow along and observe.	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
977	Please briefly explain what to expect with the recommender system coming up in Project of the Week this week? How does it work? I want to follow along and observe.	"I am glad you asked. Project of the Week has nothing to do with the course to the extent that it's just different initiatives that are run in our community. We have a course and we also have Project of the Week. We also have webinars, we also have podcasts. They are all different activities. For the Project of the Week, this week, we'll have a project about recommender systems. The idea there is that every day, you get a set of tasks. On Wednesday, you get this set of tasks. You need to come up with an idea. You need to find a dataset for this idea, and you need to share your progress. Then on day two, you will need to go through the suggested materials and also find some materials on your own and do this thing – learn about the basics, do exploratory data analysis, understand this data that you found and share your progress. As you can see on the project page, every day, you get a set of tasks and you need to do them. So it's not like we're telling you exactly what to do like in the Zoomcamp (in the videos we tell you that “This is the exact sequence of actions you need to execute and this is the result you get.”) With the Project of the Week, it's more independent. We just give you four bullet points and then it's up to you to actually do these things. And then, of course, you share your progress. The idea behind this project is that, at the end, you have a project that you can add to your portfolio. This is not a project that you just took from the tutorial and followed along, but this is a project that you did with some guidance. These are two very different things. That's the idea behind Project of the Week. For the project that we'll have as a part of the course, the idea is somewhat similar, except you will get less guidance – you will have more independence, let's say. The guidance we have for the project in the course is the form of a bunch of criteria that you need to satisfy. Then it's up to you to come up with a plan. But here, for Project the Week, we have a suggested plan that you can follow and hopefully, at the end, come up with a result. Keep in mind that for Project of the Week, some things might not go according to plan and that’s fine. This is just a suggested plan. We don't know if it will actually work exactly like we outlined day by day. It's more just to give you some guidance, but it’s up to you how exactly to approach that. 
There is a comment that says “I did not know about this.” Yes, we have a channel in our slack. It's called #project-of-the-week and it starts this week Wednesday. You'll notice that there's the digit one in the name “2022-10-19-recommenders-1.md”. We'll also have a follow-up project about recommender systems. If you want to take part in this (by “take part” I mean, propose your own topic) for example, some of you were asking about time series, some of you were asking about NLP, some of you were asking about other things in the comments in live chat. Daniel mentioned audio – If you want to learn any of these libraries, we can do a Project of the Week to learn these libraries or methods or approaches or whatever."
978	Please briefly explain what to expect with the recommender system coming up in Project of the Week this week? How does it work? I want to follow along and observe.	"If you take statistics for machine learning education, the first lecture will be this – you'll probably derive the formula for logistic regression and there will be a slide with all these assumptions about the linear regression model. Here is the Google result: 
https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/assumptions-of-linear-regression/

Go through this. They are usually useful in practice, I guess. But what I typically do is train a model, and then rely on validation to tell me if my model is doing well or not. So if there is something wrong with my validation, I will see that the metric validation is bad. It's just a lot of trial and error, rather than figuring out all these theoretical assumptions. 
I don't want to say that all these assumptions are not relevant. If you work as a data scientist, they still are. But I also found that the more practical way to understand if you can apply linear regression to the data or not, is to just apply it and see if it works or not. If it works – if the results on the validation are good – then it means that you can apply your model to this dataset. 
Maybe some of the assumptions are violated. For example, multicollinearity – this is actually the case in the lectures, when we needed to regularize (add regularization to the model) this assumption was violated. You will see in the lectures what exactly happened. So check it out. I don't think I will be able to give a good answer, because I am not prepared to talk about this, but any statistics book or theoretical machine learning book explains this."
979	Please briefly explain what to expect with the recommender system coming up in Project of the Week this week? How does it work? I want to follow along and observe.	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
980	I have a very big dataset (almost 100 gigabytes) in HDF format. Even loading it is becoming a headache. What is the best practice? Can you suggest something?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
981	I have a very big dataset (almost 100 gigabytes) in HDF format. Even loading it is becoming a headache. What is the best practice? Can you suggest something?	"Tim
Yeah, we have a lot of users who use MLflow and BentoML together. Once you have a model that’s sort of your “finalized” model – you save it to your MLflow registry and then (it depends on what your CI/CD pipeline looks like) but BentoML has a command to import from an MLflow registry. You can tell which MLflow model is the one that you want to deploy. It's very similar to save_model, except instead of pulling from your local environment, you're pulling from an MLflow registry. So you import the model, it automatically gets pulled into your Bento and then you can deploy it. On the BentoML documentation site, there's a big framework document on how to integrate with MLflow. 
Alexey
When searching, one of the suggestions was BentoML vs MLflow. I guess there are also some similarities because with MLflow, you can serve models. 
Tim
The thing about serving models is that it's a part of the pipeline that you just have to do. Every single framework out there has some way to serve a model – you train the model and then it just puts it up there to be able to serve. Typically, frameworks don't specialize in serving those. BentoML specializes in making the serving part really, really fast and really, really easy. What you find a lot of the time is, when you have serving logic in a really large end-to-end pipeline, it just puts the model there and then the only thing that you could submit to the model is the data. But we know in practice that there's usually business logic around this – pre-transformation logic, post-transformation logic, there's the version of the model that was saved. There are a lot of these other components that you want around the model, not just the model for inference. And of course there's the performance layer underneath. The BentoML architecture kind of brings that all together into one deployable, rather than just giving you one point to call inference for your model. 
Alexey
That is a comment in the live chat that says “There is PyCaret, too.” This is how you use PyCaret with Bento. 
Tim
Right. BentoML has lots and lots of integrations and we're only adding to them every day. The idea is to get the model from wherever you built it, bring it into your Bento, and then you can deploy it anywhere. Once it's in the Bento – in this standard packaging – it's nice because then we've got lots of different tools to deploy to Lambda, SageMaker, and lots of different places. And the places to deploy, we're adding to every week and month as well.
Alexey  
Since you can have a Docker container at the end – you do “bento_containerize” right? That's the command? And then you have a Docker container (Docker image) that you can deploy everywhere where you can deploy a Docker container, which is pretty much every place on the internet, right? 
Tim
Right. Some places, though, require specialized… 
Alexey
Like Lambda, right? 
Tim
Yeah, like Lambda won't accept just a normal Docker container. You have to actually create special endpoints. When we deploy to Lambda, we create those endpoints for you and make sure that it's behind an API gateway and things like that. For certain services, there are a lot of nuances that we kind of take care of for you – as long as you're in the Bento standard format.
Alexey 
For those who have no idea what MLflow is, and why we are talking about this – you don't need to worry about this right now. But at some point, this tool and other similar tools will be quite handy. You can check out our MLOps Zoomcamp course, after you finish this one. Don't try to do multiple courses at the same time. That could be too much. But of course, if you're curious, go check it out."
982	I have a very big dataset (almost 100 gigabytes) in HDF format. Even loading it is becoming a headache. What is the best practice? Can you suggest something?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
983	I have a very big dataset (almost 100 gigabytes) in HDF format. Even loading it is becoming a headache. What is the best practice? Can you suggest something?	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
984	I have a very big dataset (almost 100 gigabytes) in HDF format. Even loading it is becoming a headache. What is the best practice? Can you suggest something?	"That's also a very generic question. It depends on what kind of data you have. Can you take a sample from this data or do you need to use the entire dataset? Also, as I said, what kind of data is it? Is it images? Is it text? Is it tables? I would just read a part of this. I actually don't know if it's possible with the HDF format. Maybe you can just read the first couple of thousand rows? I don't know. The best practices could be, let's say – the first one is to take a sample from this. 
The second one is maybe seeing how you can parallelize it with something like Spark. Then again, I don't know exactly what HDF format is and if you can read with Spark. Maybe not. Maybe you will need to first turn it into something else and then read it with something like Spark. Then the third one would be to get a big machine with 500 megs of RAM, for example, and then just read it from there. The last option seems to be the easiest one if you actually need to read all the data. In some cases you might not. Is it related to neural networks? I think some of the data was saved in this format. 
I might be wrong, but I think you might iterate over this data and then read it in chunks. You don't need to read the entire thing. You just read one gigabyte at a time, for example. That's another strategy - chunking. This is actually something that Spark relies on. When dealing with large datasets, Spark chunks the dataset into small partitions, and then it processes each partition separately. Then it combines the result at the end."
985	How should I choose the number of layers and networks within a layer?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
986	How should I choose the number of layers and networks within a layer?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
987	How should I choose the number of layers and networks within a layer?	Not always. For logistic regression, it does. For decision trees and random forest and XGBoost – not really. Then there is a thing called “calibration”. SciKit Learn calibration. You will need to check this out. I think there are also examples of how to do this with random forest. If you can't find examples here, there are examples on Kaggle. But practically speaking, I don't remember the last time I needed to use something like this. It's helpful when you want to train multiple models. Let's say you've been using logistic regression in production for some time and then you make some decisions based on this threshold. But then you want to deploy a new model (XGBoost) and you want to make sure that the ranges you have in the previous model are the same in the new model so the distribution looks kind of similar. This is where calibration helps. It's a useful thing, but not every application needs it.
988	How should I choose the number of layers and networks within a layer?	I guess, use validation.
989	How should I choose the number of layers and networks within a layer?	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
990	There’s lots of discussion these days around analytics engineering. What are your thoughts about this exciting area?	No, you cannot.
991	There’s lots of discussion these days around analytics engineering. What are your thoughts about this exciting area?	"Alexey
Yeah, that's not going to happen. I think ChatGPT will help us do our job, but it will not replace people, at least in my opinion. But I use it in my work and it's helpful. You should try using it too – if you want, of course."
992	There’s lots of discussion these days around analytics engineering. What are your thoughts about this exciting area?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
993	There’s lots of discussion these days around analytics engineering. What are your thoughts about this exciting area?	Our thoughts are that this is indeed an exciting area. This is an in-demand skill and that's why we have a section about analytics engineering in this course.
994	There’s lots of discussion these days around analytics engineering. What are your thoughts about this exciting area?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
995	Is pandas useful in dealing with a lot of data or do you use other faster libraries like datatable?	"Tim
I think a couple people asked this in ML Zoomcamp Slack. I was thinking about this a little bit. I think it's sort of a philosophical thing [chuckles] where we think about models and Bento is sort of separate a little bit from code. So your project directory is where all the stuff that you'll commit to Git. I don't think we think that it's the right model to commit your models to Git, for example. We think of ourselves a little bit more like Docker. You don't have Docker images in your project directory, right? You just have your Docker file and that's what you commit to Git. In the same way, we store your models and your Bento in that dedicated directory, and then sort of leave it up to you to push it to different places where you might build or where you might need it
Alexey
Do you know why you decided to save it locally on your local file system? For example, what MLflow does – you can save it to S3 directly, but in the case of Bento, you always first save it to local storage and then you decide “This model I want to export, but for the rest, I don't care.”
Tim
Right. First, I think it’s because that's the easiest way to do it, but also because I think with BentoML, the workflow is that a lot of times, somebody's building a model on their laptop, or somebody's building a model in a slightly less distributed system and they're assembling their Bento there. So I think that the fastest way to access that model is locally. We do give you the ability to push that model and pull that model from other places. But ultimately, when you assemble your Bento and then you assemble your container, all that stuff has to be there. I don't think we wanted separate abstractions for when a person is training and saving, versus when a person is building a Bento and pulling in the model and assembling the container. It's just the same abstraction, which is just a local repository which contains models.
Alexey 
Maybe you don't want to save every model every single time you change one single parameter to the cloud. [Tim agrees] Maybe at the end, after all the experiments, you want to save the final one. Meanwhile, for MLflow, the use case is different – you actually want to save every single experiment because you want to see the results of these experiments.
Tim
Right."
996	Is pandas useful in dealing with a lot of data or do you use other faster libraries like datatable?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
997	Is pandas useful in dealing with a lot of data or do you use other faster libraries like datatable?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
998	Is pandas useful in dealing with a lot of data or do you use other faster libraries like datatable?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
999	Is pandas useful in dealing with a lot of data or do you use other faster libraries like datatable?	"It depends, really. I use Pandas for pretty large data sets. But, it depends on how large your machine is. With pandas, you can always take your data and chunk it into multiple pieces. There is the pandas read_csv iterator – you can iterate over the rows of your CSV file or whatever. Yeah, just use pandas. Usually, in practice, more often than not, we use a database. Yes. For example, for the setup we have at OLX, we do have a database – I would call it a data lake, because at the end it's just a bunch of parquet files in S3. 
If what I say makes no sense for you, maybe check out our data engineering course, eventually (after this one is over) where we explain what a data lake is, what a data warehouse is, and things like this. 
At OLX, we have some sort of database (some sort of storage) and we can run some queries on top of this storage. Then, the result is often saved as a CSV file or as a parquet file. Then what I do often, for example, is just fetch this file, download it on the machine where my jobs are executed, and read it with pandas, and then apply the function that I need (like a model that predicts to this pandas data frame) and then save the result somewhere. 
It's also possible to do it with a database. Actually there is a very nice library that I will show you right now. We will have a workshop called Effective Machine Learning Inside Your Database (September 21st 2022) There are tools that allow you to run machine learning inside your database. If you're watching/reading this past the 21st of September 2022, there will be a recording of this workshop on our channel. This shows how to actually apply machine learning in a database without leaving the database at all. So there are many, many options that you can do here. Again, there is no right or wrong answer – it depends on your case – but pandas is totally fine. 
There's also the common one – Spark. Spark is also used quite often. Again, maybe after this course, you can check our data engineering course. There, we show how to use Spark in one of the videos there. I am the instructor for the Spark module. In one of the videos there, I show how to apply a machine learning model to a Spark data frame. But maybe not right now – after the course – because right now you already have a lot on your plate, so don't get distracted. It's important to keep focus. If you intend to go through the course one-by-one, maybe it's better to focus on the course. Make a “to watch” list to come back to after finishing the course."
1000	If you have previously signed up with GCP and the free credits expired or used up, can you still use the free tier with the course?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1001	If you have previously signed up with GCP and the free credits expired or used up, can you still use the free tier with the course?	"Jeff
There are lots of different things there. This is something I used to teach folks doing data science-type work and it applies for data engineering, too. I don't have a good checklist at the tip of my fingers, but you do want to look at things like “Are your data in the right type? Do you have missing values, libraries, rate expectations (which we have integration for)?” That can help with trying to make sure your data looks how you think it'll look, in terms of the statistical properties of it. Hypothesis can also generate some ideas there, or some other Python library. Pandera for everything being how you expect, values matching what you expect. So there are a lot of different tools in Python that you can use there. It's a good question. It can be a lot of different things."
1002	If you have previously signed up with GCP and the free credits expired or used up, can you still use the free tier with the course?	We’re still in progress (as of January 26, 2023). I thought we would do this earlier. But it's still not finished.
1003	If you have previously signed up with GCP and the free credits expired or used up, can you still use the free tier with the course?	Well, what you can always do is create a new account. Don't forget to unlink your credit card if you want to use the same credit card. Unlink it from the billing from the old account, and then link it with a new one. Or if you have a different credit card, then you can just use that. For example, I use Revolut as a bank and in Revolut, you can create one-time virtual cards. That could be an option, too.
1004	If you have previously signed up with GCP and the free credits expired or used up, can you still use the free tier with the course?	Just one. It's either the first attempt or the second. You should not try both.
1005	Is Prefect limited to the number of cores that Python can use or is it more flexible, like solutions such as Argo Workflows that work on top of Kubernetes?	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
1006	Is Prefect limited to the number of cores that Python can use or is it more flexible, like solutions such as Argo Workflows that work on top of Kubernetes?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
1007	Is Prefect limited to the number of cores that Python can use or is it more flexible, like solutions such as Argo Workflows that work on top of Kubernetes?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1008	Is Prefect limited to the number of cores that Python can use or is it more flexible, like solutions such as Argo Workflows that work on top of Kubernetes?	"Jeff
There, really, the sky's the limit in terms of the ability to paralyze work as you go. We have integrations with Prefect and Dask and the ability to use Kubernetes to scale things out. Absolutely. Check out the collections catalog to see what we have right now. You can also make your own custom block if you want to, from a custom collection, if you want to contribute back to everyone else. So, Ray and Dask are two task runners that we have – two libraries that are popular, that we integrate with, where you can spread out your operations over multiple cores and over multiple machines. Those can be popular for those kinds of use cases.
We saw how to use a Docker block. We also have a Kubernetes block. So that is something you can pull. We use the infrastructure of the Docker container. You can use Kubernetes Job. That’s kind of the easiest way to go with things. That's totally available right here, and then things will kick off in your cluster."
1009	Is Prefect limited to the number of cores that Python can use or is it more flexible, like solutions such as Argo Workflows that work on top of Kubernetes?	Yes, you will have to come up with your own individual projects. It's not easy, I know. But we'll help you. We will help you to come up with a project idea. That's actually on purpose because we want every project to be unique. Then it becomes a part of your portfolio when you look for a job.
1010	Do we need to know the basics of Java as well, not only Python?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
1011	Do we need to know the basics of Java as well, not only Python?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
1012	Do we need to know the basics of Java as well, not only Python?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1013	Do we need to know the basics of Java as well, not only Python?	No. From what I understood, from what Ankush said, you don't need to know Java.
1014	Do we need to know the basics of Java as well, not only Python?	No, you cannot.
1015	Can you give a general overview of what skills a junior data analyst should know. Job postings are confusing, as companies seem to have different ideas.	I mean, I can give you my idea on what data analysts do, but it doesn't mean that everyone follows this idea. You should probably actually start with job postings, and then think, “Okay, these are the companies I want to work for. What kind of skills do they require?” And then go from there. That would probably be more useful than just my opinion. That's, “in my humble opinion”. But I'll try to answer it anyway. Data analysts should know SQL very well. That's their main tool – SQL. By SQL, I don't mean just select star from a table, you should also know “joins,” you should know “group by,” “having statement,” “Window functions”. If you know window functions, that's great. Every time I need to use a window function, I need to Google it, but analysts usually know them much better than data scientists, because they need to do SQL a lot more often. Another tool that analysts need is Python or R. I think Python makes more sense these days. But again, it depends on the company. In some companies, the analysts use R. There, it's the usual. Let's take Python –you need to know tools like pandas, some visualization libraries like Seaborn, matplotlib. I don't remember the name of the interactive library, but if you Google, you will find it (In the comments, people suggest Plotly, which is an interactive plotting tool in Python. It’s quite cool. I’ve seen some data analysts at OLX use it). So doing simple analytics in Python and then plotting the results is useful. What I also see our data analysts at OLX do (we call them product analysts) is also use Tableau a lot – Tableau or any other dashboarding tool. Again, it depends on the company. Many data analysts spent quite a lot of time building dashboards in Tableau. Sometimes, it can also be an ad hoc analysis in Excel, or first in SQL and then putting data in Excel and then saving it over. SQL, the most important thing, then Python and data analytics tools in Python such as pandas, and then dashboarding. That probably covers like 90% of the work. But also, it's important for data analysts to be able to communicate. I hope that's helpful. It's a bit off-topic for this course and I am not an analyst. I work with analysts quite often, but I'm not an analyst, so please treat this with a grain of salt and maybe talk to real data analysts about that.
1016	Can you give a general overview of what skills a junior data analyst should know. Job postings are confusing, as companies seem to have different ideas.	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1017	Can you give a general overview of what skills a junior data analyst should know. Job postings are confusing, as companies seem to have different ideas.	"It all depends on what you mean by “full stack ML engineer”. As you’ve noticed, data engineer Zoomcamp is not about machine learning – it's about data engineering. If in your opinion a “full stack ML engineer” needs to know data engineering, then you should do data engineering. If not, then no. From my point of view, there is a “full stack of data scientist”. What I put in the definition of a full stack data scientist is – a person who can do all the steps in the CRISP-DM process. They can help product managers with business understanding, they can work with analysts in data understanding, they can help data engineers in the data preparation step, they can do the modeling, and they can deploy the models. If you want to be that kind of person, then, of course, you also need to learn a bit of product management and analytics, which we don't have courses for. But data engineering – preparing data before it goes to a model – will certainly be helpful. 
Also, machine learning ops Zoomcamp (MLOps Zoomcamp) will be helpful as well. I would suggest, if you really want to focus on machine learning engineering, then probably doing MLOps Zoomcamp will make more sense for you. But data engineering Zoomcamp will be quite useful in the future, because data scientists and ML engineers tend to work on data pipelines as well. 
Maybe not all the content will be useful for you. For example, the content about data warehousing – as a data scientist, I don't find it super useful in my work to me personally. Also, the module about analytics engineering – it's nice, but it's not something I do day to day. But the modules about batch, the modules about Spark, the modules about streaming – this is something that can be useful, because this is something we could use in building machine learning pipelines as well. 
To put data in a machine learning model, we need to prepare it – we need to create data pipelines – and this is pretty relevant. But you can just check it out. All the content is already there. You can just go and check it out. If you like it, do it – if you don't, don’t. I would suggest maybe going with this one (ML Zoomcamp)."
1018	Can you give a general overview of what skills a junior data analyst should know. Job postings are confusing, as companies seem to have different ideas.	"Scikit Learn is not very different. This is not exactly how Scikit Learn is implemented. It's a little bit different, but the idea is very similar. In Scikit Learn, they use smarter ways of doing this. For example, in week two, we have a case when we need to apply regularization. When we have correlated or very close to correlated columns, then our matrix becomes very unstable, so we cannot easily invert it. 
If what I’m saying now doesn't make sense, it will make sense once you start watching the videos. Anyways, what I'm trying to say is that in Scikit Learn, they have a smart way of working around this, such that your weights – your vector with weights, the W vector – does not contain insanely large values. They have smarter methods for doing this. But apart from that, it's very similar and you should not get two different results. You can experiment, and I do recommend experimenting with Scikit Learn. We will actually do this in week three. In week three, we will use Scikit Learn for some of these things. 
In week two, we will implement things ourselves. But starting from week three, we will only use Scikit Learn and other libraries, so we will stop implementing things ourselves. Here's just to give you a taste of what machine learning is inside. It’s just a bunch of formulas – algorithms – there is no magic. Internally, Scikit Learn implements them too."
1019	Can you give a general overview of what skills a junior data analyst should know. Job postings are confusing, as companies seem to have different ideas.	"You probably mean for logistic regression, right? For logistic regression, it's doing something very similar to what we saw in module 2. It is trying to minimize the… (I'm just trying to figure out how I can explain it without going into too many details and not to get lost myself and not to confuse you). It's probably a good idea to refer to some other course. I think I partly talked about this. Let me check. Here I talk a little bit about this, in ML Zoomcamp Office Hours week #4, where I explain a little bit how exactly it works. But it's a very superficial, let's say, overview of exactly what's happening there. 
If you want to learn in more detail how exactly the process works there, maybe you can just go to Google and search for “gradient descent, logistic regression,” or something like this. The first result will probably have a good explanation of what's happening under the hood. SciKit Learn does not use this exact method – it doesn't use gradient descent – it uses something more complex (more advanced) than that. But this is roughly what's happening under the hood. I hope you're satisfied by this answer. I don't think I can give you a better one with the time we have. It will be like a separate lecture. If you don't want to go into details, you can think of this as something that’s similar to logistic regression happening there but we add sigmoid on top of that."
1020	JeffIn the exercises, you make us copy data locally and then later uploaded to GCS / BQ. In an actual production environment, what would be the real approach?	No, you cannot.
1021	JeffIn the exercises, you make us copy data locally and then later uploaded to GCS / BQ. In an actual production environment, what would be the real approach?	"Alexey
You will see links soon. I think we already have the first one, but we just need to put it on GitHub. We will communicate over everything in Telegram and then the automator will post the link from Telegram to Slack. So you will see if you're in Slack."
1022	JeffIn the exercises, you make us copy data locally and then later uploaded to GCS / BQ. In an actual production environment, what would be the real approach?	"There are lots of different varieties of approaches here. One thing that you might do is use some kind of data ingestion tool, as I mentioned in the live FAQ discussion – something like Airbyte or Fivetran – if you have lots of data that you're ingesting, that can be a common way to stay on top of that. Or you could do it through a script, kind of like we do here. 
But what you might do is run your agent on a virtual environment in Google Cloud VM, for example, or in AWS, or somewhere else. You could run it in Kubernetes. There are a number of places where maybe you would have that running and have your infrastructure there, so that you were just pulling down the code not to your local machine, but it would just be in your VM. 
If you're actually going and using Pandas, you're putting the data into memory, so you do need to have some scripts that will do that. And you can go directly between Google Cloud Storage and BigQuery with the Prefect GCP library. So that Prefect GCP library that you installed, that you've been using, does have some options in there. I think it's in the BigQuery module to actually get data directly into BigQuery from GCS. So there are a lot of options there. Check out Prefect GCP to see how to do it."
1023	JeffIn the exercises, you make us copy data locally and then later uploaded to GCS / BQ. In an actual production environment, what would be the real approach?	"Alexey
No. I mean, we kind of told you. Yes, you maybe didn't see the intro video. But, sorry. No, it's late. We will actually recalculate the points for the second homework (HW 1B) because in the forum, it said that you can get up to one point, but we calculated that the cap there was seven. We are going to recalculate that. Thus, people who got a lot of points will get just one. Sorry about that. But don't focus on points. If you go to the leaderboard, we know that the person with this hash (722366d8b29ece9be3a7605363562c7c60d6918e) did seven links. Do you know who that is? Probably you don't. 
These points are virtual. It's nice to have a good score, but remember that these points mean whatever you want them to mean – not more than not less. Maybe it's okay if you get a few points less than others for the first homework, but the important thing here is consistency. If you just do your homework, even without doing any Learning in Public stuff, which I think you should do (you should post about the course). If you don't do this and you just do all the homework, by the end, you will be somewhere in the top 20. If you look at the leaderboard from the previous iteration, you will see that not everyone stuck around till the end. 
If you just do your homework, don't worry about these points, and you'll be fine. Sorry about that. I know that it can be discouraging that you didn't get as many points as others, but don't worry about that. Focus on learning. Maybe Michael has some other suggestions or some other things that I didn't mention, because he took part last year. But I think he was one of the people who did a few posts every week, right?
Michael
Yes. Actually, when I went through it, I missed the whole… you can get up to seven points, but I was just doing one every week and I just stuck with that. I was still near the top. I think the real value in that isn't necessarily the points but just being comfortable sharing – writing articles and making videos. That's the real value there. But everyone likes the clout value, too. As Alexey said, it will fluctuate, so if you keep up with it going forward, you'll probably be just fine."
1024	JeffIn the exercises, you make us copy data locally and then later uploaded to GCS / BQ. In an actual production environment, what would be the real approach?	I would say to focus on projects, not on tools. In this course, you will get a chance to do this. If you open a job description for data engineers, you will see a lot, a lot, a lot of different technologies. Maybe think of a project that you can do for this particular company and do this project using, for example, the tools you learn here or the tools you want to learn, and build a portfolio. I guess this is more of a tip for getting a data engineering job, rather than for somebody who's starting their data engineering career. But if you already have a job, then maybe think of the business impact of what you need. I think I will not spend too much time talking about that because this requires a one hour long discussion. Perhaps we should have a podcast interview about that. If you can think of any potential guests for this podcast interview, you can let me know and we can try to invite these people and talk about this in more detail. One thing I want to share with you is that in our YouTube channel, we had a few interesting talks. We had this Getting a Data Engineering Job with Jeff talk. It’s a very nice presentation. Then we also had an interview with Jeff. So these two podcasts are quite interesting. You can also check out other ones. I think you'll find a lot of interesting content here.
1025	I don't quite understand which playlist I should use. Could you repeat in more detail?	Building projects. Maybe build another one, where instead of batch, you use streaming, maybe. If you're interested in a particular employer, try to build a project that is similar to what they’re working on. For example, if you want to apply for Spotify, try to find some music data and build a project for that.
1026	I don't quite understand which playlist I should use. Could you repeat in more detail?	Okay, I can. All the course videos you will need are here, Data Engineering Zoomcamp. And all the live videos, all the homework, everything that is specifically related to this cohort, will be in the Data Engineering Zoomcamp 2023. But you don't actually need to use a playlist because all the videos are linked here. But if it's more convenient for you to use a playlist, then this is the playlist to use. And this one is just materials for live streams, homeworks, and so on.
1027	I don't quite understand which playlist I should use. Could you repeat in more detail?	"Alexey
Google Cloud Platform is not mandatory. You can do most of the course without a cloud. You will not be able to use BigQuery, obviously, because it's a cloud offering. But you can use local postgres instead of that and I think you can run almost everything locally. For example, for streaming, instead of using Confluent Cloud, you can use Kafka. You can set it up locally.
Ankush
We also provide Docker images for everything, but the videos will be more on Confluent Cloud. But you can always change that.
Alexey  
I know that there are people from Nigeria, Iran, who cannot register in Google Cloud Platform. You can do things locally."
1028	I don't quite understand which playlist I should use. Could you repeat in more detail?	"Alexey
The best way to contribute is to take notes and include them to the notes section. And tell your friends about the course. As a part of your Learning in Public, spread the word. It will help us. And don't forget to give us a star if you have."
1029	I don't quite understand which playlist I should use. Could you repeat in more detail?	"Michael
It's not necessary. We do have recipes for using Terraform with Prefect with different things. We talked about the Prefect recipes repo, but I'll put a link to it here. 
Jeff
This is in a docs and we have a GitHub repo where we have lots of different recipes. Some of them here explain things like, say you want to run an agent – how can you do that? There are so many different cloud environments. You can put things on Kubernetes, you can put things on something like a more serverless platform like Fargate. For example, on the right there, if you were wanting to use AWS with Prefect and ECS (elastic container service) then with Terraform – we have some recipes for that. So there's just so many different permutations of possible ways to do things. A lot of these are helpful ways to go and look at options, but it depends on what you're using in the real world (in a use case). I don't think we have anything specific for this course that requires us doing this, so we don't have anything further for it.
Alexey
I guess, the usual setup that I saw (not necessarily Prefect-related) you typically use Terraform for setting up infrastructure or things like a Kubernetes cluster and configure it there. But then there is a separate repo with configuration for Kubernetes that is not maintained/managed through Terraform but through something else. For that, you don't really need to Terraform – and then Prefect could live in this Kubernetes cluster.
Jeff 
Yeah, we do have some guys working with Kubernetes in here, for sure. I see some of them here. We do have a Helm chart that we can use. If you're into the Kubernetes world, there are lots of things there. It's a whole rabbit hole.
Alexey
Here are the Prefect Helm charts.
Kalise
Also, there are a lot of YouTube videos we have that can help if you're trying to use some of these resources – how to go about deploying the Helm chart with Kubernetes and stuff, like step-by-step tutorials as well. Generally, if you just go to the Prefect YouTube channel, you'll see a lot of those tutorials. There should be some playlists there that have various things.
Alexey
Is this a duck in the cover or what?
Jeff
That is. It's the top of a duck. Blue ducks!"
1030	Data lakes are like folders. We save data before/after transformation. Is this understanding correct? Then why save transformed data in a data lake rather than just a data warehouse?	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
1031	Data lakes are like folders. We save data before/after transformation. Is this understanding correct? Then why save transformed data in a data lake rather than just a data warehouse?	"Alexey
Yeah. They are kind of like folders and there are files in these folders. They can be stored in S3 or HDFS or Google Cloud Storage. Indeed, we save the data before transformation and if we want, we can also save the data after transformation in a data lake. As to why we save transformed data in a data lake rather than just a data warehouse – sometimes it's just cheaper. In a data warehouse like BigQuery or Snowflake, you need to pay a lot for storing the data there and for accessing the data. If you just save the data in a data lake, your indexing – the way how you can speed up the queries – is limited, so you will probably need to read all the files in a folder, but usually it's cheaper, especially if you are doing something like a full scan. When you need to access all the data for a day or inside a partition, then data lakes are usually much cheaper. Usually, that's why we save transformed data in the data lake – mostly because of that."
1032	Data lakes are like folders. We save data before/after transformation. Is this understanding correct? Then why save transformed data in a data lake rather than just a data warehouse?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1033	Data lakes are like folders. We save data before/after transformation. Is this understanding correct? Then why save transformed data in a data lake rather than just a data warehouse?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
1034	Data lakes are like folders. We save data before/after transformation. Is this understanding correct? Then why save transformed data in a data lake rather than just a data warehouse?	"Alexey
The best way to contribute is to take notes and include them to the notes section. And tell your friends about the course. As a part of your Learning in Public, spread the word. It will help us. And don't forget to give us a star if you have."
1035	Best practices on how to handle heavy Pickl models (maybe 2-3 GB)?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1036	Best practices on how to handle heavy Pickl models (maybe 2-3 GB)?	"There was no registration deadline at all for this course. So you can still register. You can still sign up and you can follow the course. You will not be able to submit the homework assignments where the due date is already over because there are already solutions posted, so it doesn't make much sense to submit homework when the solutions are already there. You can follow the course but you will not be able to submit the homework. You can check our frequently asked questions and I do recommend doing this. Please remember the rule – if you have a question, first go check the frequently asked questions, and if you don't find the question there, go and ask in Slack. 
For this particular question, it’s already there. “Don't worry, you can take the course.” Maybe right now it's becoming a little bit more challenging to catch up with everything, but note that you can skip the midterm project. Right now, if you want to catch up with everything, if you skip the midterm project, it's fine. You will be able to do two capstone projects and still get the certificate. Also, you can just take the course in self-paced mode. There is nothing wrong with that either."
1037	Best practices on how to handle heavy Pickl models (maybe 2-3 GB)?	"I would ask myself, “Why is my model that large?” Is it because it has like 5000 trees in my XGBoost model? Or 10,000? And if the answer is yes, I will try to cut the number of trees. Usually, in situations when I find myself having very large trees, it is when my learning rate is very low, so each three is only slightly improving the previous three. Here, what you can do is just increase the learning rate by three times (for example) and then reduce the number of trees by three times. Then you will have roughly the same performance or maybe a bit worse, but then your model will be one gigabyte instead of three. 
Another thing you can do is reduce the number of the depth of the model. And you can play with this more – so maybe increasing the learning rate and making the tree shallower. That could also help. That's typically how I would do it. Otherwise, if this is a linear model – the only reason your linear regression or logistic regression model could be that huge is because you have a lot of categorical variables. You have a weight for each of them, so maybe you can reduce the number of variables you have by using the techniques we talked about half an hour ago. Maybe you can talk about your specific case in Slack and we can figure out what the best way for you is."
1038	Best practices on how to handle heavy Pickl models (maybe 2-3 GB)?	"Alexey
CI stands for continuous integration. This is a way to – let's say you wrote some code, and then you push it to GitHub. Then on GitHub, there's GitHub actions, which is a way to automatically run some checks on your code – run tests, deploy things somewhere. Actually, again, I will do a shameless plug – in our MLOps Zoomcamp, we talked about GitHub actions in the best practices module. Right now, don't worry about this. Focus on your projects. But after you do your projects, it's really worth spending some time learning about best practices. These best practices include things like writing tests, using make files and also tools like CI/CD, infrastructure as code – we cover all that in the module. But if you are interested in these things, maybe it's actually worth taking that course too. We'll have another iteration in May. Not super soon – I’m just telling you that to get you excited."
1039	Best practices on how to handle heavy Pickl models (maybe 2-3 GB)?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1040	Is getting into data science or data engineering harder than the other or about the same?	Working, I guess. So yeah – work after courses. That's where I see you.
1041	Is getting into data science or data engineering harder than the other or about the same?	It really depends on your background. If you're already a software engineer, then maybe data engineering will be easier because you don't need to spend a lot of time studying machine learning. But it also depends on what you like more. It depends, as I said – Let's say if you have a PhD in physics, maybe data engineering will be more difficult for you than data science. But if you like data engineering more than data science, just go for it and do data engineering. If you like it more then it will not matter for you if it's harder or not.
1042	Is getting into data science or data engineering harder than the other or about the same?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
1043	Is getting into data science or data engineering harder than the other or about the same?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1044	Is getting into data science or data engineering harder than the other or about the same?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
1045	What is the relationship between Prefect agent and Prefect deployment? What is Prefect Profile? Could you explain it in more detail?	"Alexey
No. I mean, we kind of told you. Yes, you maybe didn't see the intro video. But, sorry. No, it's late. We will actually recalculate the points for the second homework (HW 1B) because in the forum, it said that you can get up to one point, but we calculated that the cap there was seven. We are going to recalculate that. Thus, people who got a lot of points will get just one. Sorry about that. But don't focus on points. If you go to the leaderboard, we know that the person with this hash (722366d8b29ece9be3a7605363562c7c60d6918e) did seven links. Do you know who that is? Probably you don't. 
These points are virtual. It's nice to have a good score, but remember that these points mean whatever you want them to mean – not more than not less. Maybe it's okay if you get a few points less than others for the first homework, but the important thing here is consistency. If you just do your homework, even without doing any Learning in Public stuff, which I think you should do (you should post about the course). If you don't do this and you just do all the homework, by the end, you will be somewhere in the top 20. If you look at the leaderboard from the previous iteration, you will see that not everyone stuck around till the end. 
If you just do your homework, don't worry about these points, and you'll be fine. Sorry about that. I know that it can be discouraging that you didn't get as many points as others, but don't worry about that. Focus on learning. Maybe Michael has some other suggestions or some other things that I didn't mention, because he took part last year. But I think he was one of the people who did a few posts every week, right?
Michael
Yes. Actually, when I went through it, I missed the whole… you can get up to seven points, but I was just doing one every week and I just stuck with that. I was still near the top. I think the real value in that isn't necessarily the points but just being comfortable sharing – writing articles and making videos. That's the real value there. But everyone likes the clout value, too. As Alexey said, it will fluctuate, so if you keep up with it going forward, you'll probably be just fine."
1046	What is the relationship between Prefect agent and Prefect deployment? What is Prefect Profile? Could you explain it in more detail?	"Alexey
Well, I can suggest a project for this course. At the end, after you complete this course, after you complete all the modules, as a part of this course, you will have to work on the project. This is how you will get a certificate at the end – if you pass the project. This is a good project to include in your portfolio. 
Michael
I don't know if I have much more to add other than being active in online communities. This course is a great place to start. There are a lot of large cities that have meetup groups, which are always great. One of the good side effects of the whole COVID pandemic is that a lot of those are remote now. If you Google “San Francisco Big Data meetup,” meet people, talk with them, get their insights. I think that'd be a good way to approach that.
Jeff
Plus one just for getting involved in communities, whether it's conferences, online or in-person is great. Volunteer, if you can, at those. That's often a great way to help out and work with people. Go to meetups. I run a meetup in DC on data science. If anyone's in the DC area, there are data engineering meetups in town too. If there isn’t one somewhere, and if there's a few people around, I encourage you to start one. That's something you can do, regardless of your level of experience.
30:06  
Alexey
Then I'm going to do another shameless plug. We have a podcast and the topic of career changing comes up pretty often there. One interesting one is from Juan Pablo. Here he's actually not talking about… well, it's kind of related to analytics engineering. He was a math teacher, which doesn't really qualify as an IT person. He was also working as an Uber driver. In the podcast, he tells his story of how he actually did this. He talks exactly about that. He talks about going to meetups. Instead of not talking about this, just go and check it out. That's not the only relevant podcast episode. You can just go through everything we have and see if anything catches your interest. I'm sure you'll find a lot of interesting stuff."
1047	What is the relationship between Prefect agent and Prefect deployment? What is Prefect Profile? Could you explain it in more detail?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
1048	What is the relationship between Prefect agent and Prefect deployment? What is Prefect Profile? Could you explain it in more detail?	"Jeff
The Prefect agent is what kicks off your workflows. It says here, “Go use the infrastructure as specified in the deployment.” So it's constantly there, it's pulling for work. The deployment is this concept that has all the information that is needed in order to actually run a workflow. It will have information such as a way to find your code on your own infrastructure, so your flow code has to be available somehow. Again, it's not stored on Prefects servers if using Prefect Cloud, it's not stored in Orion – it is stored on your infrastructure. 
And then in the deployment, you’re also going to have your storage, you're gonna have your infrastructure, and you're going to have just some other metadata that could be needed there. But the deployment gets put into a work queue or now a “worker pool,” as we're calling them. And the agent pulls that queue, looking for work. So when there's a scheduled flow run, then an agent picks that up and says, “Alright, now go run the code that’s specified in the storage in this infrastructure that’s specified.” 
The Prefect Profile is a nice way to just flip back and forth between different workspaces, if you're working locally. With a Prefect server, that's great if you're hosting that. Then you can also use the profile to switch to the cloud workspace. And if you have multiple different cloud workspaces, you can specify those with a Prefect Profile. I'm just gonna go to the docs here and search for profiles, which is under Settings. This is where you want to go. You can check out information about Prefect Profiles, and how to use those, where to switch back and forth. There’s profile files down there, so a number of things. The two most common things I use are Prefect Profile LS Prefect, in order to list the profiles and Prefect Profile use in the name of the profile I want to use to switch back and forth between the profiles."
1049	What is the relationship between Prefect agent and Prefect deployment? What is Prefect Profile? Could you explain it in more detail?	Yeah, I think one project can host several things. I don't know exactly, to be honest. I think there is probably an official recommendation from GCP. Check that out.
1050	Maybe we will see time series in some module? Or could you recommend a book for this topic?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1051	Maybe we will see time series in some module? Or could you recommend a book for this topic?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1052	Maybe we will see time series in some module? Or could you recommend a book for this topic?	This comes up pretty often. First of all, I will recommend going to our YouTube channel where you will see this talk from Kishan. Actually, in this talk, I already saw a book I was about to recommend. This book is called Forecasting Principles and Practice. This is the book you want to read if you want to learn about time series. It's in R, but that's fine. If you're doing time series, you probably want to stick to R anyway. But many things work in Python as well – for example, this exponential smoothing. You can implement it in Python yourself. This is like the easiest, in my opinion, the most simple method for doing time series forecasting. You can implement this and it's actually quite fun to implement and tweak it to see how it works. So if you have some time and want to learn more about time series, try to implement this exponential smoothing.
1053	Maybe we will see time series in some module? Or could you recommend a book for this topic?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
1054	Maybe we will see time series in some module? Or could you recommend a book for this topic?	"Alexey
CI stands for continuous integration. This is a way to – let's say you wrote some code, and then you push it to GitHub. Then on GitHub, there's GitHub actions, which is a way to automatically run some checks on your code – run tests, deploy things somewhere. Actually, again, I will do a shameless plug – in our MLOps Zoomcamp, we talked about GitHub actions in the best practices module. Right now, don't worry about this. Focus on your projects. But after you do your projects, it's really worth spending some time learning about best practices. These best practices include things like writing tests, using make files and also tools like CI/CD, infrastructure as code – we cover all that in the module. But if you are interested in these things, maybe it's actually worth taking that course too. We'll have another iteration in May. Not super soon – I’m just telling you that to get you excited."
1055	If we are going to be late on the midterm project, is there an extension or second submission date?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
1056	If we are going to be late on the midterm project, is there an extension or second submission date?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1057	If we are going to be late on the midterm project, is there an extension or second submission date?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1058	If we are going to be late on the midterm project, is there an extension or second submission date?	I think for binary features, just one and zero makes more sense than OneHotEncoding. OneHotEncoding will take care of that but you will have two columns that are the exact opposite of each other. So that is a bit redundant. I think it's better just to convert it to a Boolean column with ones and zeros.
1059	If we are going to be late on the midterm project, is there an extension or second submission date?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1060	The exploratory data analysis should not be performed before the data split? In case we need to handle null values, for example.	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1061	The exploratory data analysis should not be performed before the data split? In case we need to handle null values, for example.	"This negation makes it a bit difficult for me to understand the question, “exploratory data analysis should not be performed before the split.” Technically, again, coming back to what I said, you set aside your test data and you pretend it's not there. You just forget about it. The only reason why you want to use your test dataset is to test your model, which happens once in a blue moon – very infrequently. Then you can set this aside, you pretend it was never there – you don't know about its existence – and then you can do whatever you want with the remaining data. You can do exploratory data analysis on the train and validation combined, you can do exploratory data analysis only on train, you can do pretty much everything. 
Typically, I think the right approach would be: you set aside data for test, you forget about it, then you take what is left, you split it, you do exploratory data analysis on train. This would be the right textbook approach, I would say. Then if you need to handle null values, you handle null values. You explore your full train dataset and you see, indeed, there are some null values. This means that for your preprocessing step, you need to do the same thing for the test dataset. Basically, everything you do to prepare your data for your full train – you need to reproduce it to repeat it on test as well."
1062	The exploratory data analysis should not be performed before the data split? In case we need to handle null values, for example.	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
1063	The exploratory data analysis should not be performed before the data split? In case we need to handle null values, for example.	Not always. For logistic regression, it does. For decision trees and random forest and XGBoost – not really. Then there is a thing called “calibration”. SciKit Learn calibration. You will need to check this out. I think there are also examples of how to do this with random forest. If you can't find examples here, there are examples on Kaggle. But practically speaking, I don't remember the last time I needed to use something like this. It's helpful when you want to train multiple models. Let's say you've been using logistic regression in production for some time and then you make some decisions based on this threshold. But then you want to deploy a new model (XGBoost) and you want to make sure that the ranges you have in the previous model are the same in the new model so the distribution looks kind of similar. This is where calibration helps. It's a useful thing, but not every application needs it.
1064	The exploratory data analysis should not be performed before the data split? In case we need to handle null values, for example.	I think the easiest way to understand what it means is to just try different ETAs, and then see exactly how your learning curves – the plots where you show the performance – how exactly they look like. If it's too big, you will see that your model overfits very quickly – on the training set, it reaches 100% performance very quickly, but on the validation dataset, it becomes worse and worse after just a few trees (after a few iterations). So it happens very fast. If you set a learning rate that’s too small, then you will need a huge load of trees to actually have anything meaningful. It will simply take too much time to learn anything, which would be the case of underfitting. So selecting the right learning rate also takes some trial and error – you need to try different values to see what works best. What I usually do is try 0.3, I try 0.1, and then I try 0.01 0.05 – then I just look at these plots and based on that, I make my decision. Also, keep in mind that when you have a lot of trees, your model becomes slower. You don't want to have a slow model. Sometimes it's better to set the learning rate a bit higher so that you have fewer trees. So it's better that way.
1065	Can you provide the link to the Docker video that has been removed for setting up Docker? Thank you.	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1066	Can you provide the link to the Docker video that has been removed for setting up Docker? Thank you.	"I think it's confusing because there is no video 1.6 about setting up the environment. It's just a page. There was no video – that's why it wasn't removed, it just never existed. But if you actually want to find this Docker video, then you go here and here is the video. This video is not from this course – it is from a different course. But the steps you need to do here are very similar to what we need to do in this course. 
You can just go through this course and this video shows how you configure a remote machine on AWS. I hope that answers your question. It was never a part of this course. Maybe if somebody wants to contribute – maybe not everyone likes watching these videos – so if you want, you can just maybe go through this video and contribute a step-by-step guide for this. If you're interested – if somebody might need this."
1067	Can you provide the link to the Docker video that has been removed for setting up Docker? Thank you.	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1068	Can you provide the link to the Docker video that has been removed for setting up Docker? Thank you.	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1069	Can you provide the link to the Docker video that has been removed for setting up Docker? Thank you.	"You can check out a few projects in this playlist. In the playlist, you can see some of the videos here, for example, there's a project from Alvaro. And it's not just Alvaro, there’s one from Alvaro and Ninad. You can just check out these projects – there’s a project from Carolina and Hamad, a project from Lisa, and a project from Timur. The project from Timor may actually fit what you want. Timur has done a lot of work and it's very interesting. So check it out. Actually, Timur’s project is not a midterm project. It's more like a capstone project – there is some deep learning there, he uses Karis. So maybe focus on the other three. 
You can also go to our course web page, and then go to midterm project (you need to use the 2021 folder) and then in the readme there should be some assignments. There will be some form that says “To find the projects you need to review, use this table.” Here, you can see all the projects from previous students’ midterm projects. You can pick some of them and check if you like them or not."
1070	Increasing n_estimators gives better AUC, but also increasing the AUC range between X_train and X_val. Can you give your opinion please?	By “increasing the AUC range,” you mean that the gap between X_train and X_val becomes wider. I would not worry about this too much, as long as your score on validation is good and that you're certain that your model is not overfitting. Then it's fine. Oftentimes, I don't even look at the training score. So that's fine. As long as your relation is good and you trust your validation, then it's fine. If you're uncertain about your validation, what you can do is run cross-validation. Then if you have five scores instead of one, and you can see the standard deviation of this course, you can be more certain that the model is behaving or not behaving well. Pay more attention to validation rather than train.
1071	Increasing n_estimators gives better AUC, but also increasing the AUC range between X_train and X_val. Can you give your opinion please?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1072	Increasing n_estimators gives better AUC, but also increasing the AUC range between X_train and X_val. Can you give your opinion please?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1073	Increasing n_estimators gives better AUC, but also increasing the AUC range between X_train and X_val. Can you give your opinion please?	"There was no registration deadline at all for this course. So you can still register. You can still sign up and you can follow the course. You will not be able to submit the homework assignments where the due date is already over because there are already solutions posted, so it doesn't make much sense to submit homework when the solutions are already there. You can follow the course but you will not be able to submit the homework. You can check our frequently asked questions and I do recommend doing this. Please remember the rule – if you have a question, first go check the frequently asked questions, and if you don't find the question there, go and ask in Slack. 
For this particular question, it’s already there. “Don't worry, you can take the course.” Maybe right now it's becoming a little bit more challenging to catch up with everything, but note that you can skip the midterm project. Right now, if you want to catch up with everything, if you skip the midterm project, it's fine. You will be able to do two capstone projects and still get the certificate. Also, you can just take the course in self-paced mode. There is nothing wrong with that either."
1074	Increasing n_estimators gives better AUC, but also increasing the AUC range between X_train and X_val. Can you give your opinion please?	I do sometimes. Not very often these days, but we have quite a few models that require deep learning. Usually, these models are related to images. We have some images on the platform, so when we need to understand what's happening on these images, we use deep learning.
1075	Will there be live events?	Well, I'm glad you asked, because we have a data engineering course. If you take it, you will understand a bit more – we do not go into details about how exactly it connects with this course. If you remember, when we talked about CRISP DM, one of the steps there is data preparation and this is what data engineering covers. Once the data is prepared, once the data is in a data lake or data warehouse, then we, data scientists, machine learning engineers, get the data, train the model, deploy this model, and take it to the other steps of the process.
1076	Will there be live events?	This event is a live event. So, yes, there were live events and there will be others, if this is what you meant. If you meant something else, please let me know what exactly you have in mind.
1077	Will there be live events?	Working, I guess. So yeah – work after courses. That's where I see you.
1078	Will there be live events?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1079	Will there be live events?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1080	Is linear regression used more for prediction or inference purposes (in the ML community)?	What is the difference between prediction and inference? I don't think I know. To me, they both sound synonymous. They're used for both, I guess.
1081	Is linear regression used more for prediction or inference purposes (in the ML community)?	"You will probably also need to learn how to pass interviews. This course does not teach that, but you will need to acquire the skill somehow. The best way to acquire these skills is by interviewing. But apart from that, I think this course covers let's say 80% of the skills that you will need for a job and the remaining 20% depends on the company. 
So you will need to do some research on the company where you want to work, see what kind of tools they use, what kind of problems they solve, how much emphasis they make in the interviews on the theoretical part. You will need to somehow figure this out and, based on that, see what you need to learn. I think with just this content, you should be able to get a machine learning-related job."
1082	Is linear regression used more for prediction or inference purposes (in the ML community)?	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
1083	Is linear regression used more for prediction or inference purposes (in the ML community)?	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
1084	Is linear regression used more for prediction or inference purposes (in the ML community)?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1085	Week 5 question: need a recipe to check items connected my predict.py file run in VSCode says no module flask referring to code “from flask import Flask. Please help.	I'm trying to parse this question. If your VSCode says “no module flask” then what you need to do is look up how you connect your Visual Studio Code to your Python Interpreter. Probably something like this. What you end up doing at the end is – select an interpreter, the one in your virtual environment, and then this interpreter knows about flask and all these things.
1086	Week 5 question: need a recipe to check items connected my predict.py file run in VSCode says no module flask referring to code “from flask import Flask. Please help.	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1087	Week 5 question: need a recipe to check items connected my predict.py file run in VSCode says no module flask referring to code “from flask import Flask. Please help.	I think the easiest way to understand what it means is to just try different ETAs, and then see exactly how your learning curves – the plots where you show the performance – how exactly they look like. If it's too big, you will see that your model overfits very quickly – on the training set, it reaches 100% performance very quickly, but on the validation dataset, it becomes worse and worse after just a few trees (after a few iterations). So it happens very fast. If you set a learning rate that’s too small, then you will need a huge load of trees to actually have anything meaningful. It will simply take too much time to learn anything, which would be the case of underfitting. So selecting the right learning rate also takes some trial and error – you need to try different values to see what works best. What I usually do is try 0.3, I try 0.1, and then I try 0.01 0.05 – then I just look at these plots and based on that, I make my decision. Also, keep in mind that when you have a lot of trees, your model becomes slower. You don't want to have a slow model. Sometimes it's better to set the learning rate a bit higher so that you have fewer trees. So it's better that way.
1088	Week 5 question: need a recipe to check items connected my predict.py file run in VSCode says no module flask referring to code “from flask import Flask. Please help.	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
1089	Week 5 question: need a recipe to check items connected my predict.py file run in VSCode says no module flask referring to code “from flask import Flask. Please help.	I think for binary features, just one and zero makes more sense than OneHotEncoding. OneHotEncoding will take care of that but you will have two columns that are the exact opposite of each other. So that is a bit redundant. I think it's better just to convert it to a Boolean column with ones and zeros.
1090	How many finishers do you expect at the end of this Zoomcamp?	Our thoughts are that this is indeed an exciting area. This is an in-demand skill and that's why we have a section about analytics engineering in this course.
1091	How many finishers do you expect at the end of this Zoomcamp?	"Michael
Actually, I do have a resource – Real Python. They do have some paid content, but they also have a lot of free content that I look at frequently. Also, if you go to sites like LeetCode, or do the Advent of Code – not necessarily to solve the problems, but looking at others’ solutions – you can see some very good examples of clean code. Type Hinting is something that I’ve picked up a few years back from Advent of Code. I still don't do it all the time, but when you see more experienced people write code, you kind of know what you're aiming for. And I think that gives you a lot of direction.
Alexey
I think in this video, Getting a Data Engineering Job, Jeff also talks about learning good coding practices. I remember that the project he recommended to look at, if you want to learn more about good coding, was Prefect. He said, “Yeah, go check out Prefect. It's a great way to learn how to write good Python code.”I guess it's a good plug, right? [chuckles] 
Then there is also a book called Clean Code. It's about Java. It's a very nice book. It's actually not about Java, it's about clean code, but the examples there are in Java. For me, it was very useful when I was starting coding. But I was a Java developer. What I know is there are examples in this book in Python. Even though the book is about Java, there are GitHub repos where the same concepts are illustrated with Python. You can check that out, too.
Jeff
Yeah, I haven't read this book. But there's also a Clean Code in Python book that is out there. It has good reviews on Amazon. It could be worth checking out."
1092	How many finishers do you expect at the end of this Zoomcamp?	We’re still in progress (as of January 26, 2023). I thought we would do this earlier. But it's still not finished.
1093	How many finishers do you expect at the end of this Zoomcamp?	Since we had more signups this year than last year, I hope it will be more than 100. But let's see.
1094	How many finishers do you expect at the end of this Zoomcamp?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1095	What part of data science/ML work do you work on every day?	Yes, that's the main reason we have two capstone projects. This is for those who are catching up to be able to get the certificate. Now there are holidays, like Christmas holidays and then January holidays in some countries – like in Russia, for example, people usually take holidays not during the end of December, but at the beginning of January. No matter in which part of the world you are in, you still have one month to catch up (November) and then start working on the capstone. Actually, I’ve now realized that the capstone starts quite soon. But for capstone 1, you only need the first six weeks or so, and then you can finish this project. Then during the holidays, you can watch Kubernetes, neural networks and so on, if you want. Then you can do capstone 2. That's probably better. But yeah, it's fine. You should do 2 capstones.
1096	What part of data science/ML work do you work on every day?	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
1097	What part of data science/ML work do you work on every day?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1098	What part of data science/ML work do you work on every day?	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
1099	What part of data science/ML work do you work on every day?	"I don't know how to answer that, because I work as a principal data scientist. I work on pretty much everything that the company needs me to work with. This means that all the models are related, to some extent, to online classifieds – to online marketplaces. I need to deal more with domain expertise, maybe, rather than ML knowledge. I guess that's the answer. 
We have a lot of different use cases – we have recommender systems, we have search, we have moderation. We actually have an article about this. For example, Learning-To-Rank: Sorting Search Results. My colleague wrote this one. I think we have an article about data science, Data Science at OLX which happens to be written by me. You can see what kind of use cases we solve. Maybe it could be interesting for you."
1100	What is a good resource for practicing Python?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1101	What is a good resource for practicing Python?	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
1102	What is a good resource for practicing Python?	Yeah. If it works for you, use it. I don't mind.
1103	What is a good resource for practicing Python?	That's a very similar question to when we should use the lambda and when we should use Kubernetes. If you want to use Kubernetes, then go with TensorFlow Serving. Otherwise, TensorFlow Lite. There is one thing where you have to use TensorFlow Lite – where you cannot use TensorFlow Serving – and this is when you want to deploy your model on a mobile device. I have a phone here. If you want to create an app with a model, then you will have to use TensorFlow Lite because you will not be able to use TensorFlow Serving. TensorFlow Serving also adds a bit of complexity. You will need to have this gateway service, you will need to have GRPC service, so that becomes more complex. If you have a lot of load, then you need to do this otherwise, TensorFlow Lite.
1104	What is a good resource for practicing Python?	"The best resource for practicing Python is your terminal. Go to your terminal, open iPython and start practicing it there – or maybe not iPython but Jupyter Notebook. I guess this is not the answer you were looking for. Maybe try to think about what kind of simple things you can automate and automate them. There must be some things that you do manually every week (for example). Try to write a Python script to automate them. Maybe something similar might be… I don't know, I am running out of ideas right now. 
But if you just sit down for about 20 minutes and think “What kind of things can I automate? What kind of things can I do?” And then try to use Python for that. That's the best resource. Of course, maybe you want me to suggest a website where you can practice Python. Maybe LeetCode could be good. They have different problems. I don't even know how it works these days. You have some problem description and you try to solve it with Python. That could be useful, let's say, if you want to work at a company like Amazon because they will torture you with these kinds of problems during the interview. At the same time, you can also practice Python. 
Another resource could be Code Academy. I think they have some Python courses, so maybe you can check that out. I see that it says “pro” so you will probably need to pay for this one. They might have free ones. Anyway, I think the best way you can practice Python is by doing little projects – writing little scripts."
1105	Does predict_proba give out real probabilities? If not, how to convert them?	Not always. For logistic regression, it does. For decision trees and random forest and XGBoost – not really. Then there is a thing called “calibration”. SciKit Learn calibration. You will need to check this out. I think there are also examples of how to do this with random forest. If you can't find examples here, there are examples on Kaggle. But practically speaking, I don't remember the last time I needed to use something like this. It's helpful when you want to train multiple models. Let's say you've been using logistic regression in production for some time and then you make some decisions based on this threshold. But then you want to deploy a new model (XGBoost) and you want to make sure that the ranges you have in the previous model are the same in the new model so the distribution looks kind of similar. This is where calibration helps. It's a useful thing, but not every application needs it.
1106	Does predict_proba give out real probabilities? If not, how to convert them?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1107	Does predict_proba give out real probabilities? If not, how to convert them?	"You can check out a few projects in this playlist. In the playlist, you can see some of the videos here, for example, there's a project from Alvaro. And it's not just Alvaro, there’s one from Alvaro and Ninad. You can just check out these projects – there’s a project from Carolina and Hamad, a project from Lisa, and a project from Timur. The project from Timor may actually fit what you want. Timur has done a lot of work and it's very interesting. So check it out. Actually, Timur’s project is not a midterm project. It's more like a capstone project – there is some deep learning there, he uses Karis. So maybe focus on the other three. 
You can also go to our course web page, and then go to midterm project (you need to use the 2021 folder) and then in the readme there should be some assignments. There will be some form that says “To find the projects you need to review, use this table.” Here, you can see all the projects from previous students’ midterm projects. You can pick some of them and check if you like them or not."
1108	Does predict_proba give out real probabilities? If not, how to convert them?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1109	Does predict_proba give out real probabilities? If not, how to convert them?	"Tim 
I think, yes. We have a few examples on our website. I think with Colab, it becomes a little bit more difficult once you're hosting the Bento to access it, if it's running on a Google Colab server. But yeah, you can save it. What I would say is that you can save your model and then export your model to somewhere where you can then build your Bento there.
Alexey 
By default, Bento saves in a local file system (in your /home) so you need to have a way to save this model somewhere, let's say on the cloud, S3, whatever – somewhere externally. The workflow here is that you train your model in Colab, you save it, and then you deploy it somewhere. You don't deploy it on a Colab instance.
Tim
Right. When you save your model, I think it'll save it to the Colab instance. Then you have to use BentoML Export to push it to an S3 bucket or something where you can pick it up and deploy your Bento.
Alexey 
And there are options for deploying. We already discussed this in the course, when we looked at Elastic Beanstalk. I assume it can work with Bento. I haven't checked. [Tim agrees] But then what we saw this week, we saw how to deploy it with ECS, and then we can also deploy to Kubernetes EKS, we can also deploy to lambda – there are a ton of other options. Everywhere where you can deploy a Docker container (Docker image) you can deploy it there. Right? [Tim agrees]"
1110	In the homework for week 3, we created the above_average with the whole dataset. This is a bad practice, right?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1111	In the homework for week 3, we created the above_average with the whole dataset. This is a bad practice, right?	That's a very similar question to when we should use the lambda and when we should use Kubernetes. If you want to use Kubernetes, then go with TensorFlow Serving. Otherwise, TensorFlow Lite. There is one thing where you have to use TensorFlow Lite – where you cannot use TensorFlow Serving – and this is when you want to deploy your model on a mobile device. I have a phone here. If you want to create an app with a model, then you will have to use TensorFlow Lite because you will not be able to use TensorFlow Serving. TensorFlow Serving also adds a bit of complexity. You will need to have this gateway service, you will need to have GRPC service, so that becomes more complex. If you have a lot of load, then you need to do this otherwise, TensorFlow Lite.
1112	In the homework for week 3, we created the above_average with the whole dataset. This is a bad practice, right?	Yeah, it's probably not the best practice. I wouldn't do this in a real-life situation. In a real-life situation, we simply will not have unseen data. But the idea here was to get a dataset that is already familiar to you – the dataset we used in the previous homework – and just convert it to a binary problem. That was the goal we had in mind – how can we make it easier for you to learn these things without overloading you with a new dataset? That was the simplest approach we decided to take to actually do it here. Another alternative could have been, instead of asking you to compute the mean, just give you the number, but with the mean, it's a little bit more interesting, I think.
1113	In the homework for week 3, we created the above_average with the whole dataset. This is a bad practice, right?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1114	In the homework for week 3, we created the above_average with the whole dataset. This is a bad practice, right?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
1115	Can you share with us some real-work data cleaning strategies? Is there a good practices checklist regarding what to look out for when you’re at the “transform” step?	We’re still in progress (as of January 26, 2023). I thought we would do this earlier. But it's still not finished.
1116	Can you share with us some real-work data cleaning strategies? Is there a good practices checklist regarding what to look out for when you’re at the “transform” step?	For Spark, we use Python. So it's PySpark. So what Java aspects will be covered? Ankush already uploaded the videos for week 6, just go and check that out.
1117	Can you share with us some real-work data cleaning strategies? Is there a good practices checklist regarding what to look out for when you’re at the “transform” step?	Yes
1118	Can you share with us some real-work data cleaning strategies? Is there a good practices checklist regarding what to look out for when you’re at the “transform” step?	"Alexey
Well, let me try to see. Google “data cleaning checklist”. What do we have here? Yeah, maybe you can go through this thing. Seems quite good. There are also tools like, for example, Soda, or Great Expectations. Tools like that. There are quite a few of them. They look at data quality and they have an existing set of checks that you can just use. I think the checks that they have by default, or the checks they have in the libraries, make a good checklist."
1119	Can you share with us some real-work data cleaning strategies? Is there a good practices checklist regarding what to look out for when you’re at the “transform” step?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1120	How can we contribute to the course?	Since we had more signups this year than last year, I hope it will be more than 100. But let's see.
1121	How can we contribute to the course?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1122	How can we contribute to the course?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
1123	How can we contribute to the course?	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
1124	How can we contribute to the course?	I will defer this question to Ankush. He's unfortunately not here right now, so maybe ask him in Slack. From what I understood, it better describes the internals of Kafka. You don't need to know any Java. He explains everything you need. No, you will not need to use Java for your project. You can, but you don't have to.
1125	You have a dataset with two numerical independent targets. The question is, “Do you process it as two different datasets?”	"Scikit Learn is not very different. This is not exactly how Scikit Learn is implemented. It's a little bit different, but the idea is very similar. In Scikit Learn, they use smarter ways of doing this. For example, in week two, we have a case when we need to apply regularization. When we have correlated or very close to correlated columns, then our matrix becomes very unstable, so we cannot easily invert it. 
If what I’m saying now doesn't make sense, it will make sense once you start watching the videos. Anyways, what I'm trying to say is that in Scikit Learn, they have a smart way of working around this, such that your weights – your vector with weights, the W vector – does not contain insanely large values. They have smarter methods for doing this. But apart from that, it's very similar and you should not get two different results. You can experiment, and I do recommend experimenting with Scikit Learn. We will actually do this in week three. In week three, we will use Scikit Learn for some of these things. 
In week two, we will implement things ourselves. But starting from week three, we will only use Scikit Learn and other libraries, so we will stop implementing things ourselves. Here's just to give you a taste of what machine learning is inside. It’s just a bunch of formulas – algorithms – there is no magic. Internally, Scikit Learn implements them too."
1126	You have a dataset with two numerical independent targets. The question is, “Do you process it as two different datasets?”	I guess not. That's the process of learning? Maybe more senior people don't… Yeah, I see where you're going with this. Senior people are not supposed to go bankrupt and default. They are probably supposed to pay back the debt. I guess these are the particularities of this dataset, I assume. Indeed, it's probably a data issue ,or not an issue, but a “characteristic” of this dataset.
1127	You have a dataset with two numerical independent targets. The question is, “Do you process it as two different datasets?”	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
1128	You have a dataset with two numerical independent targets. The question is, “Do you process it as two different datasets?”	I wish Tim was here and would help me answer that because I don't know. I assume if you wanted to use Bento, you would probably use TensorFlow Lite – you don't want to use the usual TensorFlow for the same reasons that we talked about in the serverless module, because it's simply too large. Probably for Bento, you would use something like TensorFlow Lite and it will still work. Maybe another thing you can check – in our channel, we had a talk from one of the Bento folks, Building an ML Service Platform from the Ground Up. Here, Sean talks about the differences between TensorFlow Serving and Bento. With TensorFlow Serving, it's good – it's quite optimized for deep learning. There are some problems. First of all, it's written in C++ and expects your model in a certain format – this “saved model format”. Adding anything extra on top of that (like pre-processing, post-processing) becomes difficult, because you will need to have an extra step there. Bento makes it easier. But, again, I don't know. It's very difficult to give you a concrete recommendation without knowing what exactly you want to do. So it's case dependent. If you already use TensorFlow Serving in your company, then maybe it makes sense to stick to that. If we have Tim on Office Hours next week, maybe you can ask him that. He will be a better person to answer this question.
1129	You have a dataset with two numerical independent targets. The question is, “Do you process it as two different datasets?”	"For your project, what you can do is just select one of them – you don't have to do two. Another option would be to train two models. Let's say you have one XGBoost model for predicting target one and then you use the same features for predicting target two. Then you have two models. This is a perfectly viable approach. Then another approach would be to use a deep learning model or a neural network. With a neural network, the output that you have could be two numbers – your model will output two numbers, two targets. I guess that's what you referred to when you said “double”. 
By the way, we don't cover a use case like that in the course. We don't show how to create a model with two predictions. But if you do some research, you will find it. With Keras, it's actually relatively easy to do that. You can just Google or use your favorite search engine to find this. I think both of these options are good. It really depends. I don't know what the best way is. 
The upside for having a neural network is that you have just one model. But the downside is, let's say you want to improve the accuracy of predicting one value, but not the other – how do you do this without affecting the other prediction? Sometimes you might want to have this separation and you want to have independent models. I would actually go with two models."
1130	I love the testimonials of those who completed the course. If you can get graduates on the videos to speak and share their experience. I think that will be very motivational.	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1131	I love the testimonials of those who completed the course. If you can get graduates on the videos to speak and share their experience. I think that will be very motivational.	Yeah, good idea. Thank you.
1132	I love the testimonials of those who completed the course. If you can get graduates on the videos to speak and share their experience. I think that will be very motivational.	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
1133	I love the testimonials of those who completed the course. If you can get graduates on the videos to speak and share their experience. I think that will be very motivational.	We currently do not have any course partners yet. If you work at a company and you want to partner with us, please reach out and we will see how we can do it. In one of our courses, which was our first iteration of the Machine Learning Zoomcamp, we partnered with a company called Delphi. They got two interns from our course and they were quite satisfied with the outcome. You can actually read more about the work of these interns in our articles – Interview with Valerii Chetvertakov and then another one, Interview with Ken Wu. You can learn more about the interns and if you think that, at your company, you need good interns or juniors or you want to partner in any other way, please reach out and we can see how to make it work.
1134	I love the testimonials of those who completed the course. If you can get graduates on the videos to speak and share their experience. I think that will be very motivational.	"Alexey
Yeah, we talked about that in Slack, so please check it. I think this happens because some of the columns have missing values and when you use Pandas for converting CSV to Parquet, it reads columns with missing values as double. That's why this happens."
1135	Is there an interpretation for W0 (bias term)? Some weights can be negative, so it's not the minimum of the target. Can we interpret it somehow or consider it an abstraction?	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
1136	Is there an interpretation for W0 (bias term)? Some weights can be negative, so it's not the minimum of the target. Can we interpret it somehow or consider it an abstraction?	That's a very similar question to when we should use the lambda and when we should use Kubernetes. If you want to use Kubernetes, then go with TensorFlow Serving. Otherwise, TensorFlow Lite. There is one thing where you have to use TensorFlow Lite – where you cannot use TensorFlow Serving – and this is when you want to deploy your model on a mobile device. I have a phone here. If you want to create an app with a model, then you will have to use TensorFlow Lite because you will not be able to use TensorFlow Serving. TensorFlow Serving also adds a bit of complexity. You will need to have this gateway service, you will need to have GRPC service, so that becomes more complex. If you have a lot of load, then you need to do this otherwise, TensorFlow Lite.
1137	Is there an interpretation for W0 (bias term)? Some weights can be negative, so it's not the minimum of the target. Can we interpret it somehow or consider it an abstraction?	"If you take statistics for machine learning education, the first lecture will be this – you'll probably derive the formula for logistic regression and there will be a slide with all these assumptions about the linear regression model. Here is the Google result: 
https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/assumptions-of-linear-regression/

Go through this. They are usually useful in practice, I guess. But what I typically do is train a model, and then rely on validation to tell me if my model is doing well or not. So if there is something wrong with my validation, I will see that the metric validation is bad. It's just a lot of trial and error, rather than figuring out all these theoretical assumptions. 
I don't want to say that all these assumptions are not relevant. If you work as a data scientist, they still are. But I also found that the more practical way to understand if you can apply linear regression to the data or not, is to just apply it and see if it works or not. If it works – if the results on the validation are good – then it means that you can apply your model to this dataset. 
Maybe some of the assumptions are violated. For example, multicollinearity – this is actually the case in the lectures, when we needed to regularize (add regularization to the model) this assumption was violated. You will see in the lectures what exactly happened. So check it out. I don't think I will be able to give a good answer, because I am not prepared to talk about this, but any statistics book or theoretical machine learning book explains this."
1138	Is there an interpretation for W0 (bias term)? Some weights can be negative, so it's not the minimum of the target. Can we interpret it somehow or consider it an abstraction?	Well, in case of classification, if the bias term is negative (for example, in case of churn) then it means that the client is more likely to not churn than churn. It’s similar in linear regression. Again, it depends. If your values are only positive, then negative bias terms can be suspicious. But if you sometimes have negative values and positive values in your data, then nothing is particularly wrong with that. So, it really depends on your problem.
1139	Is there an interpretation for W0 (bias term)? Some weights can be negative, so it's not the minimum of the target. Can we interpret it somehow or consider it an abstraction?	Yeah. If it works for you, use it. I don't mind.
1140	What is the difference between epochs and iterations?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1141	What is the difference between epochs and iterations?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1142	What is the difference between epochs and iterations?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1143	What is the difference between epochs and iterations?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1144	What is the difference between epochs and iterations?	One epoch consists of multiple iterations. Remember, in the code – if you go to the course and open deep learning and open a notebook. Go to where we start training a model. This thing (batch_size=32) batch size. When we specify batch size, it means that we take our entire dataset, and we chunk this dataset into batches of 32 images. It's actually called step, not iteration. Then each step would be fitting a model on that batch, specifically. And when you go over the entire set of chunks, then you finish your epoch, usually. Usually, an epoch is going over the entire dataset once. It's not strictly correct. You can define epochs differently. You can define epoch as 100 steps or 1000 steps – it’s up to you. But usually, in practice, you say that one epoch is one iteration over the entire dataset. I guess when I say iteration/epoch, to me, it seems like I can use them interchangeably. But the difference between epoch and step is what I just described. Maybe if you find a different definition, please share it in Slack.
1145	Tim, are you taking part in Hacktoberfest? You personally, or BentoML.	I do sometimes. Not very often these days, but we have quite a few models that require deep learning. Usually, these models are related to images. We have some images on the platform, so when we need to understand what's happening on these images, we use deep learning.
1146	Tim, are you taking part in Hacktoberfest? You personally, or BentoML.	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1147	Tim, are you taking part in Hacktoberfest? You personally, or BentoML.	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
1148	Tim, are you taking part in Hacktoberfest? You personally, or BentoML.	"Tim 
Yes, actually. One of our developers was asking about that the other day. I think we'll try to do something like that.
Alexey
Because I think the idea behind Hacktoberfest – it's very similar to the “good first issue” that we saw. In Hacktoberfest, you need to add an issue, but then there is also an idea that you give some sort of guidance to the person who is contributing. You can tell them how they start, what they should do – it's a bit of hand-holding, but that is my understanding. I think to take part you just need to add an issue. I don't know, maybe you need to register somewhere explicitly."
1149	Tim, are you taking part in Hacktoberfest? You personally, or BentoML.	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
1150	What happens if we give Vectorizer unsubmitted information?	In deep learning, if you saw the lectures already, you might have noticed that we don't use cross-validation there, simply because it's too time consuming. We set aside a validation dataset and we use this to guide us – to select the best parameter. That's the usual approach I follow, because with proper cross-validation, with creating three folds, it’s simply too time-consuming. That's why I follow a simpler approach. I guess that is the alternative.
1151	What happens if we give Vectorizer unsubmitted information?	Naive Bayes is a really good model, but I find it very similar to logistic regression. In the end, when you compare the performance of Naive Bayes with logistic regression, it's very similar. It's a very nice model, but since we're covering logistic regression here anyway, I didn't see a lot of sense in including Naive Bayes as well. It has a very nice and elegant theory. You can check it out. It's really beautiful. But in the end, for all practical purposes, it's very similar to logistic regression.
1152	What happens if we give Vectorizer unsubmitted information?	I do sometimes. Not very often these days, but we have quite a few models that require deep learning. Usually, these models are related to images. We have some images on the platform, so when we need to understand what's happening on these images, we use deep learning.
1153	What happens if we give Vectorizer unsubmitted information?	"If you look at the criteria, you will see that model performance is not one of the criteria. If your model does not give you great performance, it will not affect the score that you will get at the end. This is intentional. The reason it's like that is because it's very hard to control this. The performance on the dataset of a model is very dataset-specific. For some datasets, this is how it is. In some cases, for example, for click prediction – if you want to predict if somebody will click on an ad or not, the performance of the model is usually bad. You cannot predict with very high probability if a person will click on an ad or not. But it doesn't mean that these models aren't useful. But the performance that you will observe on the validation dataset will be bad. And that's okay. 
This is just a characteristic (a feature) of this particular dataset – of this particular problem. That's why it's not a criterion, but you do need to try to train multiple models, as you see here (in the criteria for model training), and then try to tune their parameters. In the end, maybe you will not have a great model – you will not have great performance for this model – but at least you tried and this is what we evaluate here. We don't evaluate the performance, but whether you tune the parameters."
1154	What happens if we give Vectorizer unsubmitted information?	By “unsubmitted” you mean “unseen” information? One thing you can do is ask the Vectorizer to raise an error. It will see, “Okay, I have not seen this value before. Let's throw an error out.” And this is what it will do. A better thing would be to use Pydantic and BentoML. In BentoML, you can include this. One of the modules this week talks about that. It's called validation. Here, Tim shows how to add a Pydantic class. Pydantic is a special framework in Python that can validate your data. In this one here, we show how to add this validation to the input data. Actually, in the example, he just removes one of the fields from the request and the model still works – it's not throwing any exceptions. But this is not always what you want. In that module, we discuss how to deal with this.
1155	Will we learn how to productionize the setup of Docker to a server instead of locally?	I would say to focus on projects, not on tools. In this course, you will get a chance to do this. If you open a job description for data engineers, you will see a lot, a lot, a lot of different technologies. Maybe think of a project that you can do for this particular company and do this project using, for example, the tools you learn here or the tools you want to learn, and build a portfolio. I guess this is more of a tip for getting a data engineering job, rather than for somebody who's starting their data engineering career. But if you already have a job, then maybe think of the business impact of what you need. I think I will not spend too much time talking about that because this requires a one hour long discussion. Perhaps we should have a podcast interview about that. If you can think of any potential guests for this podcast interview, you can let me know and we can try to invite these people and talk about this in more detail. One thing I want to share with you is that in our YouTube channel, we had a few interesting talks. We had this Getting a Data Engineering Job with Jeff talk. It’s a very nice presentation. Then we also had an interview with Jeff. So these two podcasts are quite interesting. You can also check out other ones. I think you'll find a lot of interesting content here.
1156	Will we learn how to productionize the setup of Docker to a server instead of locally?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
1157	Will we learn how to productionize the setup of Docker to a server instead of locally?	"Alexey
Yes. Well, maybe. Nothing concrete. If you have some ideas, again, please share them with us. I know that many people asked for a course in deep learning. I unfortunately don't know deep learning that well to make the course about that. But maybe there are some other areas that you have ideas about – share them with us. Again, for deep learning – I think maybe we will eventually do this. I just need to find people who know it a lot better than me and I will just help with the process. So nothing concrete, but if you have ideas – share them."
1158	Will we learn how to productionize the setup of Docker to a server instead of locally?	I'm not sure what exactly you mean here. Probably, you’re talking about container orchestration platforms like Kubernetes. We did not cover it in this course. We do cover Kubernetes in our Machine Learning Zoomcamp. If you want to learn it, check it out.
1159	Will we learn how to productionize the setup of Docker to a server instead of locally?	No, you cannot.
1160	Why did we save our model to bin? How about Pickle and other formats? What is the difference between them?	I think for binary features, just one and zero makes more sense than OneHotEncoding. OneHotEncoding will take care of that but you will have two columns that are the exact opposite of each other. So that is a bit redundant. I think it's better just to convert it to a Boolean column with ones and zeros.
1161	Why did we save our model to bin? How about Pickle and other formats? What is the difference between them?	Yes. For a time series problem, you should never shuffle. For a time series problem, you should do a time-based split. Let's say you have data for one year, you take, for example, the first nine months for training, then two months for validation and one month for test. Or something like that. You need to do a time-based split here and shuffling is wrong. Because if you shuffle, what can happen is that you can accidentally kind of see into the future. In your training dataset, you might have a data point from December – your model will see into the future, and it's not supposed to do this.
1162	Why did we save our model to bin? How about Pickle and other formats? What is the difference between them?	"The extension of our file was bin simply because we decided to give it this name. It's actually Pickle, but it’s just when saving, I put “.bin” there. You could put anything there. You can put the “.pickle” you can put “.txt” – anything – it doesn't change the content of the file, which is a Pickle file. Other formats include – there's a bunch of different formats. There is Joblib, which is almost Pickle – there is PMML which is a special format for machine learning models. You can use them between different languages. 
For example, you create a model in SciKit Learn, and then you can save it and import it from a Java application. I don't use it, to be honest, because I usually write the training jobs in Python and then the serving scripts are also written in Python. So we don't have this problem. The difference is in the nuances and details. With Pickle – I don't remember if I mentioned that in the videos – there are security issues. For example, if somebody with malicious intent wants to execute some code on your computer, they can insert this malicious code into the Pickle file and when you load the pickle file, the code will be executed. Then potentially, the person (hacker) can overtake your computer. So load Pickle files only from the sources you trust. Other formats have similar pros and cons. PMML does not have this problem. But maybe it does the Py adoption. With Pickle, it's very simple – it's a built-in library in Python. You just import it, you don't need to install anything, – you just do pickle dump and then you have your file."
1163	Why did we save our model to bin? How about Pickle and other formats? What is the difference between them?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1164	Why did we save our model to bin? How about Pickle and other formats? What is the difference between them?	I do not understand this question, to be honest. I need an example. Maybe ask in Slack with… I usually don't like screenshots [chuckles] when you post them in Slack, because when I open a screenshot with code on my mobile, I cannot see anything. But maybe this is a case, when you can actually include a screenshot of what you mean and let's discuss it there.
1165	What are LSTM, RNN, and transformers?	I'll show you a trick. You can thank me later. You take this question. You go to your favorite search engine, you put this question to search and you see the answers. You're welcome. [laughs] But honestly, I don't know much about these things. I know a little bit, but I might tell you something that does not make any sense, so I will not even attempt to do that. I'm not really an expert in NLP. Secondly, with Google, you can find a lot of good resources. Maybe I will show you one good resource which is CS224N. This is natural language processing with deep learning, which talks about all these things. Word vectors, recurrent neural networks are RNNs. So go through this thing if you're interested in learning more about that.
1166	What are LSTM, RNN, and transformers?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1167	What are LSTM, RNN, and transformers?	"Tim
Yeah, we have a lot of users who use MLflow and BentoML together. Once you have a model that’s sort of your “finalized” model – you save it to your MLflow registry and then (it depends on what your CI/CD pipeline looks like) but BentoML has a command to import from an MLflow registry. You can tell which MLflow model is the one that you want to deploy. It's very similar to save_model, except instead of pulling from your local environment, you're pulling from an MLflow registry. So you import the model, it automatically gets pulled into your Bento and then you can deploy it. On the BentoML documentation site, there's a big framework document on how to integrate with MLflow. 
Alexey
When searching, one of the suggestions was BentoML vs MLflow. I guess there are also some similarities because with MLflow, you can serve models. 
Tim
The thing about serving models is that it's a part of the pipeline that you just have to do. Every single framework out there has some way to serve a model – you train the model and then it just puts it up there to be able to serve. Typically, frameworks don't specialize in serving those. BentoML specializes in making the serving part really, really fast and really, really easy. What you find a lot of the time is, when you have serving logic in a really large end-to-end pipeline, it just puts the model there and then the only thing that you could submit to the model is the data. But we know in practice that there's usually business logic around this – pre-transformation logic, post-transformation logic, there's the version of the model that was saved. There are a lot of these other components that you want around the model, not just the model for inference. And of course there's the performance layer underneath. The BentoML architecture kind of brings that all together into one deployable, rather than just giving you one point to call inference for your model. 
Alexey
That is a comment in the live chat that says “There is PyCaret, too.” This is how you use PyCaret with Bento. 
Tim
Right. BentoML has lots and lots of integrations and we're only adding to them every day. The idea is to get the model from wherever you built it, bring it into your Bento, and then you can deploy it anywhere. Once it's in the Bento – in this standard packaging – it's nice because then we've got lots of different tools to deploy to Lambda, SageMaker, and lots of different places. And the places to deploy, we're adding to every week and month as well.
Alexey  
Since you can have a Docker container at the end – you do “bento_containerize” right? That's the command? And then you have a Docker container (Docker image) that you can deploy everywhere where you can deploy a Docker container, which is pretty much every place on the internet, right? 
Tim
Right. Some places, though, require specialized… 
Alexey
Like Lambda, right? 
Tim
Yeah, like Lambda won't accept just a normal Docker container. You have to actually create special endpoints. When we deploy to Lambda, we create those endpoints for you and make sure that it's behind an API gateway and things like that. For certain services, there are a lot of nuances that we kind of take care of for you – as long as you're in the Bento standard format.
Alexey 
For those who have no idea what MLflow is, and why we are talking about this – you don't need to worry about this right now. But at some point, this tool and other similar tools will be quite handy. You can check out our MLOps Zoomcamp course, after you finish this one. Don't try to do multiple courses at the same time. That could be too much. But of course, if you're curious, go check it out."
1168	What are LSTM, RNN, and transformers?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1169	What are LSTM, RNN, and transformers?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1170	I'm a little confused about data pipelines. I'm curious which part of ETL is week 1? Extract or load?	No, you cannot.
1171	I'm a little confused about data pipelines. I'm curious which part of ETL is week 1? Extract or load?	The load part is when the data is already processed and you load it to a data warehouse. We don't touch a data warehouse in week 1. So I guess that may make it extract, or transform, or both. I guess that's the extract and transform part. The transformation we do is not really a transformation. We don't really do anything with the data. But if we did, then yeah, it would transform as well. I don't think it's really important. The important part is that we do something with the data and the data ends up in our database. Actually, we do load it to the database, so it's also load. I confused myself. So… [chuckles] I think it's all the parts. Anyways, let's discuss it in Slack.
1172	I'm a little confused about data pipelines. I'm curious which part of ETL is week 1? Extract or load?	"Jeff
There are lots of different things there. This is something I used to teach folks doing data science-type work and it applies for data engineering, too. I don't have a good checklist at the tip of my fingers, but you do want to look at things like “Are your data in the right type? Do you have missing values, libraries, rate expectations (which we have integration for)?” That can help with trying to make sure your data looks how you think it'll look, in terms of the statistical properties of it. Hypothesis can also generate some ideas there, or some other Python library. Pandera for everything being how you expect, values matching what you expect. So there are a lot of different tools in Python that you can use there. It's a good question. It can be a lot of different things."
1173	I'm a little confused about data pipelines. I'm curious which part of ETL is week 1? Extract or load?	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
1174	I'm a little confused about data pipelines. I'm curious which part of ETL is week 1? Extract or load?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1175	How can scaling be implemented, several servers? But also with several GPUs? Batch inference?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1176	How can scaling be implemented, several servers? But also with several GPUs? Batch inference?	"Actually, I don't remember exactly how I do this. The right approach would be to use only train data and not use validation data or test dataset. This is done on purpose for exactly this reason that you mentioned. We want to model a situation in which there is some unseen data – it's truly unseen, we do not see that all – and anything can happen to this unseen data, including the situation that you described, such as there is a value that is only present in the test data. 
Imagine that there is a new iPhone – when you train your model, this new iPhone didn't exist (you didn't have this category) but you apply this model, and then, all of a sudden, there is a new iPhone. How will your model react to this? You need to somehow model this situation. This is totally normal. This can and will happen in practice. We just want to understand what would be the effect of that. That's why we need to see this in training data. This is good and this is how it should be. I hope that is a good answer."
1177	How can scaling be implemented, several servers? But also with several GPUs? Batch inference?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1178	How can scaling be implemented, several servers? But also with several GPUs? Batch inference?	Yeah, it's probably not the best practice. I wouldn't do this in a real-life situation. In a real-life situation, we simply will not have unseen data. But the idea here was to get a dataset that is already familiar to you – the dataset we used in the previous homework – and just convert it to a binary problem. That was the goal we had in mind – how can we make it easier for you to learn these things without overloading you with a new dataset? That was the simplest approach we decided to take to actually do it here. Another alternative could have been, instead of asking you to compute the mean, just give you the number, but with the mean, it's a little bit more interesting, I think.
1179	How can scaling be implemented, several servers? But also with several GPUs? Batch inference?	"When it comes to scaling, for example, when we talk about AWS lambda, you don't need to worry about that. When lambda gets a lot of traffic, a lot of requests, and one lambda cannot handle them, it automatically starts another lambda function in parallel – another instance of a lambda function. So you don't need to worry about any of that, lambda is doing its own thing in the background. But as a result, what you get is it scales – it can handle more traffic. If you want to maybe have control over this, if you want to scale it yourself up and down, then in Kubernetes, there is the Auto Scaling group. You will see in the Kubernetes homework how to actually do this. Also in module five, we talked about Elastic Beanstalk, it can also automatically scale. When it sees a lot of requests coming in, it creates another instance and there is basically a load balancer. I think it's called Elastic Load Balancer, which forwards the request to multiple instances. When it comes to GPUs – in the course, we do not show how to use GPUs with Kubernetes. But if you Google it, you will see how to add nodes with GPUs to your Kubernetes cluster (to eks) and then, based on that, you can scale – you can create more machines with GPUs. If you need that, of course.
As for batch inference, what we talked about here is what I call “online inference”. In this course, we only covered online inference, and by “online,” I mean we create a web service and it's like an API. You get the request, the service processes this request and replies with a prediction – a response. This is what I call an online web service. There is another one which is called offline and this is what batch inference is, usually. There is also what we saw in Bento. In bento, there is a different kind of batching – it's still online, it just takes all the requests, puts them together in a batch and then it's still a web service. There is another way of serving, which I usually call “batch serving,” or “offline serving”. This is when we have all the data in our, let's say, S3 location or somewhere and then we just load this data with something like pandas, we apply the model, and we save predictions. That's also an option if your use case allows it. If you want to learn more about that, we actually discuss this topic in more detail in the MLOps Zoomcamp. There is a module about deployment, where we talk about three ways of deploying: web service, the one we use in this course (ML Zoomcamp), then there’s streaming, when you have a stream of data and you react to events in that stream. Then the third one is batching. So if you're interested in learning more about deployment, definitely check out MLOps Zoomcamp."
1180	When working with time series data for a regression problem, is it incorrect to shuffle during the train validation test split?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1181	When working with time series data for a regression problem, is it incorrect to shuffle during the train validation test split?	Yes. For a time series problem, you should never shuffle. For a time series problem, you should do a time-based split. Let's say you have data for one year, you take, for example, the first nine months for training, then two months for validation and one month for test. Or something like that. You need to do a time-based split here and shuffling is wrong. Because if you shuffle, what can happen is that you can accidentally kind of see into the future. In your training dataset, you might have a data point from December – your model will see into the future, and it's not supposed to do this.
1182	When working with time series data for a regression problem, is it incorrect to shuffle during the train validation test split?	Yeah, it's probably not the best practice. I wouldn't do this in a real-life situation. In a real-life situation, we simply will not have unseen data. But the idea here was to get a dataset that is already familiar to you – the dataset we used in the previous homework – and just convert it to a binary problem. That was the goal we had in mind – how can we make it easier for you to learn these things without overloading you with a new dataset? That was the simplest approach we decided to take to actually do it here. Another alternative could have been, instead of asking you to compute the mean, just give you the number, but with the mean, it's a little bit more interesting, I think.
1183	When working with time series data for a regression problem, is it incorrect to shuffle during the train validation test split?	Yeah, if it doesn't look like we're using it – we are not using it. Maybe we just want to set it aside and forget about it.
1184	When working with time series data for a regression problem, is it incorrect to shuffle during the train validation test split?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1185	Can you please explain the second project? Does this mean that we'll get a second attempt or it's a second project entirely?	Yeah, you can think of these as two attempts. Not two projects, but two attempts. If for some reason, you cannot take part – you're busy, you have a vacation, something happened, you get sick – and you cannot deliver the first project in time, then you get another chance. This is why we have the second attempt. Or for example, if you fail your project, then you also get another chance. If you don't do the first project and you fail the second attempt, then unfortunately, there is no way to resubmit it. It will be two attempts, but one project.
1186	Can you please explain the second project? Does this mean that we'll get a second attempt or it's a second project entirely?	"Alexey
No. I mean, we kind of told you. Yes, you maybe didn't see the intro video. But, sorry. No, it's late. We will actually recalculate the points for the second homework (HW 1B) because in the forum, it said that you can get up to one point, but we calculated that the cap there was seven. We are going to recalculate that. Thus, people who got a lot of points will get just one. Sorry about that. But don't focus on points. If you go to the leaderboard, we know that the person with this hash (722366d8b29ece9be3a7605363562c7c60d6918e) did seven links. Do you know who that is? Probably you don't. 
These points are virtual. It's nice to have a good score, but remember that these points mean whatever you want them to mean – not more than not less. Maybe it's okay if you get a few points less than others for the first homework, but the important thing here is consistency. If you just do your homework, even without doing any Learning in Public stuff, which I think you should do (you should post about the course). If you don't do this and you just do all the homework, by the end, you will be somewhere in the top 20. If you look at the leaderboard from the previous iteration, you will see that not everyone stuck around till the end. 
If you just do your homework, don't worry about these points, and you'll be fine. Sorry about that. I know that it can be discouraging that you didn't get as many points as others, but don't worry about that. Focus on learning. Maybe Michael has some other suggestions or some other things that I didn't mention, because he took part last year. But I think he was one of the people who did a few posts every week, right?
Michael
Yes. Actually, when I went through it, I missed the whole… you can get up to seven points, but I was just doing one every week and I just stuck with that. I was still near the top. I think the real value in that isn't necessarily the points but just being comfortable sharing – writing articles and making videos. That's the real value there. But everyone likes the clout value, too. As Alexey said, it will fluctuate, so if you keep up with it going forward, you'll probably be just fine."
1187	Can you please explain the second project? Does this mean that we'll get a second attempt or it's a second project entirely?	No, you cannot.
1188	Can you please explain the second project? Does this mean that we'll get a second attempt or it's a second project entirely?	"Jeff
There is a link here, as I shared in Slack. If you click on the Blocks page, anywhere in the UI here, there’s Prefect docs right here on the bottom left, if you're logged into Prefect Cloud."
1189	Can you please explain the second project? Does this mean that we'll get a second attempt or it's a second project entirely?	No, it's not in our plans
1190	Please consider adding links to video 1.2/06-environment.md in the Deployment model as a prerequisite. Also please add that video 5.5 will explain more.	Maybe you can just go ahead to our repo and create a pull request. That will help. That is certainly useful and you will help immensely if you just go and create a pull request with that and we will just accept it. Great idea. Thank you.
1191	Please consider adding links to video 1.2/06-environment.md in the Deployment model as a prerequisite. Also please add that video 5.5 will explain more.	"Alexey
It is quite important. I wouldn't say it's a must for all the projects. For example, if you use something like a tree-based model, then you shouldn't worry about scaling. I think I shared links to our Office Hours from the previous year. If you go to cohorts, and the previous year, and then you go to midterm project – in week ten, we show you how to use SciKit Learn pipelines. This is an amazing concept. So far, what we've been doing is Dictionary Vectorizer (dict_vectorizer) and then it was followed by a model. In the case of Bento, we needed to save this Dictionary Vectorizer separately, as an artifact (as a binary) and then we needed to load this and then we needed to apply the model. But with a pipeline, you can actually train a pipeline and just use BentoML to save the pipeline. It can be saved, right Tim? You can just take a pipeline and do BentoML.sklean.save_model and then the entire pipeline will be saved. Right? [Tim agrees] This is a cool thing. I don't have an example, but you can include this feature scaler here in your pipeline. There should be an example somewhere. I think we did that last year. There is an example without the pipeline. It should be here – classification, and then notebooks scaling this one. In this notebook, we show how to scale your features with standard scaling, or min/max scaler. If you check Office Hours #3 from last year I spoke more about that. It is an important topic. We intentionally did not include this because we cannot include everything, but this is an important topic. I recommend that you check it out. And using pipelines is cool because you will not need to save all this Scaler, Dictionary Vectorizer – all these things separately. Let's say with Bento, you can just take one pipeline, save it with Bento, and then you will have one binary on model. It makes things easier.
Tim 
And if you do have any kind of custom Tokenizer, or Scaler, or Vectorizer – you can always add it to that “custom objects” field.
Alexey
We talked about MLflow. It's very annoying, to be honest. With MLflow, I need to choose to save this in a file. Then I have a Pickle file and I tell MLflow, “Okay, there is an artifact that is already in this file that I want to upload together with the model.” So I upload it together with the model and then when I want to use it, I need to download this from the registry, then I need again to use Pickle to load this into the memory. There is too much overhead. With Bento, I really like how easy it is to save extra stuff.
Tim
Right. In our first version of BentoML, we were just saving models but then we had so many users saying “Well, how do I save my Tokenizer along with the model?” A lot of the time, the Tokenizer, the Vectorizer, is sort of one-to-one with the model. You may have a version of that as well. So it's nice to be able to version it with the model."
1192	Please consider adding links to video 1.2/06-environment.md in the Deployment model as a prerequisite. Also please add that video 5.5 will explain more.	"There was no registration deadline at all for this course. So you can still register. You can still sign up and you can follow the course. You will not be able to submit the homework assignments where the due date is already over because there are already solutions posted, so it doesn't make much sense to submit homework when the solutions are already there. You can follow the course but you will not be able to submit the homework. You can check our frequently asked questions and I do recommend doing this. Please remember the rule – if you have a question, first go check the frequently asked questions, and if you don't find the question there, go and ask in Slack. 
For this particular question, it’s already there. “Don't worry, you can take the course.” Maybe right now it's becoming a little bit more challenging to catch up with everything, but note that you can skip the midterm project. Right now, if you want to catch up with everything, if you skip the midterm project, it's fine. You will be able to do two capstone projects and still get the certificate. Also, you can just take the course in self-paced mode. There is nothing wrong with that either."
1193	Please consider adding links to video 1.2/06-environment.md in the Deployment model as a prerequisite. Also please add that video 5.5 will explain more.	"I don't know how to answer that, because I work as a principal data scientist. I work on pretty much everything that the company needs me to work with. This means that all the models are related, to some extent, to online classifieds – to online marketplaces. I need to deal more with domain expertise, maybe, rather than ML knowledge. I guess that's the answer. 
We have a lot of different use cases – we have recommender systems, we have search, we have moderation. We actually have an article about this. For example, Learning-To-Rank: Sorting Search Results. My colleague wrote this one. I think we have an article about data science, Data Science at OLX which happens to be written by me. You can see what kind of use cases we solve. Maybe it could be interesting for you."
1194	Please consider adding links to video 1.2/06-environment.md in the Deployment model as a prerequisite. Also please add that video 5.5 will explain more.	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1195	What to do when the data is too large and comes in real time. Any pointers?	"Tim
I think a couple people asked this in ML Zoomcamp Slack. I was thinking about this a little bit. I think it's sort of a philosophical thing [chuckles] where we think about models and Bento is sort of separate a little bit from code. So your project directory is where all the stuff that you'll commit to Git. I don't think we think that it's the right model to commit your models to Git, for example. We think of ourselves a little bit more like Docker. You don't have Docker images in your project directory, right? You just have your Docker file and that's what you commit to Git. In the same way, we store your models and your Bento in that dedicated directory, and then sort of leave it up to you to push it to different places where you might build or where you might need it
Alexey
Do you know why you decided to save it locally on your local file system? For example, what MLflow does – you can save it to S3 directly, but in the case of Bento, you always first save it to local storage and then you decide “This model I want to export, but for the rest, I don't care.”
Tim
Right. First, I think it’s because that's the easiest way to do it, but also because I think with BentoML, the workflow is that a lot of times, somebody's building a model on their laptop, or somebody's building a model in a slightly less distributed system and they're assembling their Bento there. So I think that the fastest way to access that model is locally. We do give you the ability to push that model and pull that model from other places. But ultimately, when you assemble your Bento and then you assemble your container, all that stuff has to be there. I don't think we wanted separate abstractions for when a person is training and saving, versus when a person is building a Bento and pulling in the model and assembling the container. It's just the same abstraction, which is just a local repository which contains models.
Alexey 
Maybe you don't want to save every model every single time you change one single parameter to the cloud. [Tim agrees] Maybe at the end, after all the experiments, you want to save the final one. Meanwhile, for MLflow, the use case is different – you actually want to save every single experiment because you want to see the results of these experiments.
Tim
Right."
1196	What to do when the data is too large and comes in real time. Any pointers?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1197	What to do when the data is too large and comes in real time. Any pointers?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1198	What to do when the data is too large and comes in real time. Any pointers?	"I believe we have a course that is just about that and that course is called Data Engineering Zoomcamp. So if you want to know what to do with cases when the data is large, check out our Data Engineering course – Data Engineering Zoomcamp. Maybe I’ll answer this quickly. So what to do when the data is too large? There are multiple things you can do. First, you can sample your data – meaning that you just take a part of this data and discard the rest, and you train your model on that. That is fine. Then you can go distributed –you can use distributed computing for accessing all the data, which is more complex. You should have a good reason for doing this because, oftentimes, training a model on a sample is sufficient. But if you know that you will get better accuracy, better performance, if you train on the entire dataset, then you should consider distributed computing – things like Spark and so on that we cover in the data engineering course. 
If data comes in real time – I don't really know what you mean here. But again, if data comes in real time and you want to score it in real time, then this is something we talk about in the data engineering course. We cover it in the streaming lecture and also in our MLOps Zoomcamp. In our MLOps Zoomcamp, we cover streaming, which shows a case when there is a stream of data, and how we apply the model to this stream. This is a course I recommend taking after the ML Zoomcamp – the one we're doing now. So you can finish this course and then go to MLOps Zoomcamp. When it comes to web service – I don't know if I'm answering your question right now, but we will cover this in deployment. This is for applying the model to your data, for predictions. If your training data comes in S3, in real time, then you just need to collect this data, save it somewhere, and then train. This is where Data Engineering Zoomcamp comes in."
1199	What to do when the data is too large and comes in real time. Any pointers?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1200	In which model will we need to make a decision about using cloud platform or local? Is it possible to do the entire course locally?	Yeah. If it works for you, use it. I don't mind.
1201	In which model will we need to make a decision about using cloud platform or local? Is it possible to do the entire course locally?	"Well, there are multiple ways you can deal with this. First – get more data, if possible. If not, try to reduce the number of features you have. Sometimes, when the data set is small, if you have a lot of features, it might be confusing for the model. Or try to have a model with a lot of regularization. This should help. Here, the main thing is that you need to come up with a reliable way of evaluating the performance of your model. 
You really need to think about how you can set up your validation strategy in such a way that it's reliable. Once you have that, you can start experimenting with all these regularization things, with dropping some columns, with techniques like upsampling and downsampling and things like this. The important thing here is setting up your validation strategy. Once you have that, everything else will come. You will just need to experiment and the experiments will show what the best way is."
1202	In which model will we need to make a decision about using cloud platform or local? Is it possible to do the entire course locally?	It is possible to do everything locally. You will need to do it by… I think you will need to do it in module five when we deploy something. Or actually, you will probably need to do it when we do your project. Yeah. It's fine to do everything locally. But if you have access to cloud and you want to practice with this, I do recommend practicing, because cloud is one of the skills that employers are looking for. They really need it. So if you want to be more employable, I do recommend learning about cloud.
1203	In which model will we need to make a decision about using cloud platform or local? Is it possible to do the entire course locally?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1204	In which model will we need to make a decision about using cloud platform or local? Is it possible to do the entire course locally?	"Tim
Yeah, we have a lot of users who use MLflow and BentoML together. Once you have a model that’s sort of your “finalized” model – you save it to your MLflow registry and then (it depends on what your CI/CD pipeline looks like) but BentoML has a command to import from an MLflow registry. You can tell which MLflow model is the one that you want to deploy. It's very similar to save_model, except instead of pulling from your local environment, you're pulling from an MLflow registry. So you import the model, it automatically gets pulled into your Bento and then you can deploy it. On the BentoML documentation site, there's a big framework document on how to integrate with MLflow. 
Alexey
When searching, one of the suggestions was BentoML vs MLflow. I guess there are also some similarities because with MLflow, you can serve models. 
Tim
The thing about serving models is that it's a part of the pipeline that you just have to do. Every single framework out there has some way to serve a model – you train the model and then it just puts it up there to be able to serve. Typically, frameworks don't specialize in serving those. BentoML specializes in making the serving part really, really fast and really, really easy. What you find a lot of the time is, when you have serving logic in a really large end-to-end pipeline, it just puts the model there and then the only thing that you could submit to the model is the data. But we know in practice that there's usually business logic around this – pre-transformation logic, post-transformation logic, there's the version of the model that was saved. There are a lot of these other components that you want around the model, not just the model for inference. And of course there's the performance layer underneath. The BentoML architecture kind of brings that all together into one deployable, rather than just giving you one point to call inference for your model. 
Alexey
That is a comment in the live chat that says “There is PyCaret, too.” This is how you use PyCaret with Bento. 
Tim
Right. BentoML has lots and lots of integrations and we're only adding to them every day. The idea is to get the model from wherever you built it, bring it into your Bento, and then you can deploy it anywhere. Once it's in the Bento – in this standard packaging – it's nice because then we've got lots of different tools to deploy to Lambda, SageMaker, and lots of different places. And the places to deploy, we're adding to every week and month as well.
Alexey  
Since you can have a Docker container at the end – you do “bento_containerize” right? That's the command? And then you have a Docker container (Docker image) that you can deploy everywhere where you can deploy a Docker container, which is pretty much every place on the internet, right? 
Tim
Right. Some places, though, require specialized… 
Alexey
Like Lambda, right? 
Tim
Yeah, like Lambda won't accept just a normal Docker container. You have to actually create special endpoints. When we deploy to Lambda, we create those endpoints for you and make sure that it's behind an API gateway and things like that. For certain services, there are a lot of nuances that we kind of take care of for you – as long as you're in the Bento standard format.
Alexey 
For those who have no idea what MLflow is, and why we are talking about this – you don't need to worry about this right now. But at some point, this tool and other similar tools will be quite handy. You can check out our MLOps Zoomcamp course, after you finish this one. Don't try to do multiple courses at the same time. That could be too much. But of course, if you're curious, go check it out."
1205	When I started, I followed chapter 1.6 and used Conda all throughout. Later, we used Pipenv and it had some problems due to multiple env managers. What is your usual environment manager?	Yeah. If it works for you, use it. I don't mind.
1206	When I started, I followed chapter 1.6 and used Conda all throughout. Later, we used Pipenv and it had some problems due to multiple env managers. What is your usual environment manager?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1207	When I started, I followed chapter 1.6 and used Conda all throughout. Later, we used Pipenv and it had some problems due to multiple env managers. What is your usual environment manager?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
1208	When I started, I followed chapter 1.6 and used Conda all throughout. Later, we used Pipenv and it had some problems due to multiple env managers. What is your usual environment manager?	I don't know much about this, to be honest. I know that their model for predicting the price of real estate went rogue. So it started predicting some… I won’t say more, because I'm not super sure about what exactly happened there. I wasn't really following. I know that the company lost a lot of money because of a rogue model. I think if you just Google that you will find an explanation.
1209	When I started, I followed chapter 1.6 and used Conda all throughout. Later, we used Pipenv and it had some problems due to multiple env managers. What is your usual environment manager?	"I use Anaconda for prototypes. I first create a prototype, just starting my Jupyter Notebook with the base environment from Anaconda. When the prototype is ready, the notebook is ready – I create the virtual environment. Of course, in theory, I should have first started creating the virtual environment, but I don't always know all the packages that I will need in advance – installing them one by one, I’m too lazy for that. So what I do is just use the base environment with all the packages I potentially need. But then, when this kind of proof of concept prototype is finished and I know exactly what I want to do – I want to turn this notebook into a script and this is when I create a Pipenv environment. 
So I start with Anaconda, I do everything for the prototype in Anaconda, and once the prototype is ready, I create a Pipenv environment ( a Pipenv file) and then put things there. Basically, the setup we had in this course is what I follow in practice as well. This is my usual environment manager – it's Anaconda for everything and for specific projects, I create Pipenv files. Again, in theory, maybe the recommended approach would be – when you start a project, you start with a clean virtual environment and then you first install NumPy, pandas, and whatnot (what you need) and then you add package by package when you create a prototype. I find it time consuming and I'm a lazy person, so that's why I cut corners a little bit."
1210	How relevant do you think books are when everything changes so fast in the tech world?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
1211	How relevant do you think books are when everything changes so fast in the tech world?	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
1212	How relevant do you think books are when everything changes so fast in the tech world?	"Well, I'll tell you a story about my book. In the book that I wrote Machine Learning Bookcamp, I thought that it would be a good idea to include a chapter about Kubeflow Serving. So I did that. It was actually at the end. I wanted to write a chapter only about Kubernetes, but then I thought, “Oh, it could be a good idea to also talk about Kubeflow Serving.” So I included that and it took a couple of months for the book to go to production, meaning they turned all these documents with chapters into a book. 
By the time the book was published, Kubeflow Serving no longer existed because it got renamed to K Serve and most of the stuff I was talking about in the book no longer worked. Books on new technologies become pretty… not irrelevant, and maybe not obsolete, but maybe slightly outdated. Therefore, the last part of chapter 10 in my book is outdated now – you can only use it for this old version of Kubeflow Serving. 
It’s the same with even this course. We did this K Serve model and by the time it finished, there was a new version and some things stopped working there, so we needed to update that. I want to thank Max Payne (I know it's not his real name). He was very helpful preparing things and he left a few notes there. So thanks a lot. Because of these notes, now it's actually up to date. The video might not be, but if you look at the notes, it is. I recently checked it and it's still working. Books do become outdated. 
But the rest of the stuff I put in the book is actually not outdated. Kubernetes still works. Flask still works. Maybe the version of Python we used (I think it was 3.7 in the book) I wouldn't use now. I would use 3.9. But most of the concepts are fine. So yeah, some things changed, but not so fast. Some things change fast. There are books, for example, Elements of Statistical Learning, that even though most of the content is pretty old, if not everything, it’s still relevant. But because it's theory – theory maybe doesn't change – fundamentals maybe don't change as fast as new technologies."
1213	How relevant do you think books are when everything changes so fast in the tech world?	"If you take statistics for machine learning education, the first lecture will be this – you'll probably derive the formula for logistic regression and there will be a slide with all these assumptions about the linear regression model. Here is the Google result: 
https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/assumptions-of-linear-regression/

Go through this. They are usually useful in practice, I guess. But what I typically do is train a model, and then rely on validation to tell me if my model is doing well or not. So if there is something wrong with my validation, I will see that the metric validation is bad. It's just a lot of trial and error, rather than figuring out all these theoretical assumptions. 
I don't want to say that all these assumptions are not relevant. If you work as a data scientist, they still are. But I also found that the more practical way to understand if you can apply linear regression to the data or not, is to just apply it and see if it works or not. If it works – if the results on the validation are good – then it means that you can apply your model to this dataset. 
Maybe some of the assumptions are violated. For example, multicollinearity – this is actually the case in the lectures, when we needed to regularize (add regularization to the model) this assumption was violated. You will see in the lectures what exactly happened. So check it out. I don't think I will be able to give a good answer, because I am not prepared to talk about this, but any statistics book or theoretical machine learning book explains this."
1214	How relevant do you think books are when everything changes so fast in the tech world?	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
1215	How should I choose the most efficient number of epochs? And what does it benefit me if it's less or more?	"You can check out a few projects in this playlist. In the playlist, you can see some of the videos here, for example, there's a project from Alvaro. And it's not just Alvaro, there’s one from Alvaro and Ninad. You can just check out these projects – there’s a project from Carolina and Hamad, a project from Lisa, and a project from Timur. The project from Timor may actually fit what you want. Timur has done a lot of work and it's very interesting. So check it out. Actually, Timur’s project is not a midterm project. It's more like a capstone project – there is some deep learning there, he uses Karis. So maybe focus on the other three. 
You can also go to our course web page, and then go to midterm project (you need to use the 2021 folder) and then in the readme there should be some assignments. There will be some form that says “To find the projects you need to review, use this table.” Here, you can see all the projects from previous students’ midterm projects. You can pick some of them and check if you like them or not."
1216	How should I choose the most efficient number of epochs? And what does it benefit me if it's less or more?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
1217	How should I choose the most efficient number of epochs? And what does it benefit me if it's less or more?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1218	How should I choose the most efficient number of epochs? And what does it benefit me if it's less or more?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
1219	How should I choose the most efficient number of epochs? And what does it benefit me if it's less or more?	I guess you're talking about how to select the number of epochs. The way I do it is just run it for 100-200 epochs, and I leave it training. Then I add checkpointing. When I come back, I see what's happening. The problem sometimes with more epochs is the discrepancy you have between the training set, where it often overfits to the training set. You might have 100% accuracy on the training data, but then it's let's say, 70% accuracy on validation. That might be problematic. If I compare this to a case... I'll take some notes so I don't get confused. Let's say you have two models. You have Model A, which you trained for, something like 100 epochs and it has 100% accuracy on train and it has 60% accuracy on validation. That's your Model A. Then you have Model B, which you trained for 10 epochs. It has 70% accuracy on train and 60% accuracy on validation. Here, the difference between train and validation performance is much smaller for the second model – for Model B. It's only a 10% difference. But for the first one, it's 40%. I would go with Model B. I would be more sure that it’s not overfitting – it will not have any surprises on the test dataset. So that's the only concern. But if you add some dropout, if you add some data augmentation, then probably by epoch number 100, you will not have 100% error on train. Usually you don't, because of all this extra regularization stuff. Then it could be that the model that you trained for 100 epochs is more robust because it has seen more variations of your data. It's all case-dependent, I would say. But the main thing I would look at is the learning curve and how far apart the curve for the training error and the validation error are.
1220	Why is shuffle=False in validation generation dataset but not in training, for deep learning?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
1221	Why is shuffle=False in validation generation dataset but not in training, for deep learning?	"If you look at the notebook, yes, we use shuffle=false. For train, the default shuffle is true. Actually, if you think about this, when you go over a dataset once to evaluate it, it probably shouldn't matter in which order you go over it. That's what shuffling is controlling – when you go over a dataset, you want to shuffle it so the order is random. I think the less randomization you have in the validation dataset, the better. I think that's the reason I shuffle=false here. 
You can experiment with this – set it to true and to false – and then see if the scores you get are the same ones or not. But I think for validation, it's important that we try to stay away from randomization, just in case. I don't know what can go wrong here, but maybe something can go wrong. You’d rather be careful. That's the reason, I think."
1222	Why is shuffle=False in validation generation dataset but not in training, for deep learning?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1223	Why is shuffle=False in validation generation dataset but not in training, for deep learning?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
1224	Why is shuffle=False in validation generation dataset but not in training, for deep learning?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1225	After creating deployment and having an agent pick it up, what caused this error? ``` This run is scheduled and hasn't generated logs``` any way to solve this?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
1226	After creating deployment and having an agent pick it up, what caused this error? ``` This run is scheduled and hasn't generated logs``` any way to solve this?	Yes, there is such a thing. If you go here and scroll down, you will see the Tools section. If you go to week 1, there is a video, Setting up the environment on cloud VM, which explains everything you need to do. Then you can also check out the notes from the students. For example, notes from Alvaro are quite comprehensive. I remember that some students also took a lot of notes about the environment preparation setup, so you can take a look at the notes too.
1227	After creating deployment and having an agent pick it up, what caused this error? ``` This run is scheduled and hasn't generated logs``` any way to solve this?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
1228	After creating deployment and having an agent pick it up, what caused this error? ``` This run is scheduled and hasn't generated logs``` any way to solve this?	"Jeff
I’m not sure, exactly. It might be the case that you made the deployment and it's scheduled and it's just waiting, but you are not connected to that workspace with the space where your agent is running. Agents are looking for work maybe locally, and you're connected to Prefect cloud, for example. That's one possibility. Just make sure that your agent is pulling for work from the workspace where you actually have scheduled the work to run. You can check that when you start your agent – it'll tell you what it's connected to, up at the top of the message that comes right back when you start the agent, in the CLI."
1229	After creating deployment and having an agent pick it up, what caused this error? ``` This run is scheduled and hasn't generated logs``` any way to solve this?	"Alexey
It's possible to apply to jobs right now. You don't have to take the course. Just start applying. Why do you need to take the course to start applying, right? It was possible to do it last year. Nobody is stopping you from doing this. But this course will give you some knowledge and add projects to your portfolio. That will be helpful in your job search. 
But I would encourage you to start applying to jobs right now to see what they actually want from you – what kind of topics come up, what kind of test assignments you get. You don't need to wait until the course is finished to start doing this. You can already learn a lot about the job market in your area or in the domain where you want to work.
Luis
I just want to add that when I did the 2022 cohort, I was in another company and the process of data engineering in that company was really similar to the Zoomcamp. What I started to notice was that the companies mostly use this architecture in engineering. So it's good for you to show yourself that you're starting to understand all this data engineering stuff. I think it's important.
Ankush
I just wanted to add. I don't know about applying for jobs, but once you have a job, if you want to get an increment, contact Michael. [chuckles]

Victoria
Michael moneymaker. [chuckles]"
1230	When will the next courses of DTC be released?	Yes. As I’ve already said during the launch stream, you can start applying for jobs right now. You don't actually have to complete the course to find a job. Maybe you already know enough for you to convince an employer to hire you. By that, I mean you already know how to program, you know the command line, and things like that. It will be enough – it will be possible – and many people from the previous iteration of the course did that. They found a job. Some of them, not even as juniors.
1231	When will the next courses of DTC be released?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1232	When will the next courses of DTC be released?	Well, what you can always do is create a new account. Don't forget to unlink your credit card if you want to use the same credit card. Unlink it from the billing from the old account, and then link it with a new one. Or if you have a different credit card, then you can just use that. For example, I use Revolut as a bank and in Revolut, you can create one-time virtual cards. That could be an option, too.
1233	When will the next courses of DTC be released?	"Alexey
I don’t know what you mean by “the next courses”. We have three courses right now: Machine Learning Engineering course (ML Zoomcamp) which is in September. Then we have this one (DE Zoomcamp) in January. And the MLOps Zoomcamp starts in May. We plan to reiterate them every year, for now. These three courses take the entire year to run and it makes it very difficult to think about if we can add another course to the courses we already have – how to find the time and how to do this. Right now, I'm figuring this out –I am often the bottleneck when running these courses, so I'm thinking about how I can just step out of the course and then let it run by itself without me. Then maybe we can focus on other courses. When this is figured out, then maybe there will be new courses from DataTalks.Club. But for now, you can enjoy the courses we already have."
1234	When will the next courses of DTC be released?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
1235	Is it okay to use my own Git repo and not the auto-created one?	I mean, if there is already a model in Hugging Face for your problem, that's good for you. But I guess that's only like 1% of problems that you solve in the industry. Data scientists wouldn't be working at companies if that was the case. [chuckles] We still need to get data and we still need to train these models. And then if you think about it, somebody actually put these models on Hugging Face, right? I suspect that it was done by data scientists. Right? [chuckles] I don't know any other model hub apart from that. Maybe Kaggle. I would say that’s also a good source for models – more for data, but in data, you have a dataset and you can also train a model there. There are many notebooks that show how to do this.
1236	Is it okay to use my own Git repo and not the auto-created one?	Yeah, they're similar. Not all of them will ask you to deploy stuff, but for data science, it's often necessary to do simple exploratory data analysis then train the model and write some conclusions.
1237	Is it okay to use my own Git repo and not the auto-created one?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1238	Is it okay to use my own Git repo and not the auto-created one?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1239	Is it okay to use my own Git repo and not the auto-created one?	Yes, it’s definitely okay. We actually had a problem with the script that assigns auto-generated ones. That's why that part was removed from the course instructions. Now you should create your own repo and use that for submitting the work. Sorry for the confusion. That happened before my vacation, so I thought, “Okay, I don't want to spend time debugging and figuring out what's wrong with the script. Let's just stick to the same approach we used previously – everyone creates a project in their own repo.” There are some downsides to that, but I wasn't in the mood of going through debugging. So please – it's okay. Please do that.
1240	When should I use a model with grayscale images?	In some domains, maybe the color doesn't matter much – on the shapes matter. If it's that domain, then yes. For example, I think for clothing, it might be the case. Maybe colors don't matter much, because you can have all different colors. But on the other hand, maybe it's actually still important because you want to distinguish the background from the foreground (from the actual item). This is where the color information could be important. If you think it will give you some performance increase in terms of making predictions faster – just use validation to find out if there is any predictive performance drop (drop in accuracy) when you switch to grayscale. If you see that there is no drop, then just stick to that.
1241	When should I use a model with grayscale images?	Rescale is already preprocessing. Actually, in exception, this is what they do. It's actually the scaling there. There are two operations, I think. I don't remember exactly, but it's rescaling plus something else. So rescaling is preprocessing.
1242	When should I use a model with grayscale images?	Not always. For logistic regression, it does. For decision trees and random forest and XGBoost – not really. Then there is a thing called “calibration”. SciKit Learn calibration. You will need to check this out. I think there are also examples of how to do this with random forest. If you can't find examples here, there are examples on Kaggle. But practically speaking, I don't remember the last time I needed to use something like this. It's helpful when you want to train multiple models. Let's say you've been using logistic regression in production for some time and then you make some decisions based on this threshold. But then you want to deploy a new model (XGBoost) and you want to make sure that the ranges you have in the previous model are the same in the new model so the distribution looks kind of similar. This is where calibration helps. It's a useful thing, but not every application needs it.
1243	When should I use a model with grayscale images?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1244	When should I use a model with grayscale images?	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
1245	You have consistently used .predict_proba instead of .predict for the models from SciKit Learn. Is it because you wanted to use auc-roc?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
1246	You have consistently used .predict_proba instead of .predict for the models from SciKit Learn. Is it because you wanted to use auc-roc?	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
1247	You have consistently used .predict_proba instead of .predict for the models from SciKit Learn. Is it because you wanted to use auc-roc?	"Alexey
CI stands for continuous integration. This is a way to – let's say you wrote some code, and then you push it to GitHub. Then on GitHub, there's GitHub actions, which is a way to automatically run some checks on your code – run tests, deploy things somewhere. Actually, again, I will do a shameless plug – in our MLOps Zoomcamp, we talked about GitHub actions in the best practices module. Right now, don't worry about this. Focus on your projects. But after you do your projects, it's really worth spending some time learning about best practices. These best practices include things like writing tests, using make files and also tools like CI/CD, infrastructure as code – we cover all that in the module. But if you are interested in these things, maybe it's actually worth taking that course too. We'll have another iteration in May. Not super soon – I’m just telling you that to get you excited."
1248	You have consistently used .predict_proba instead of .predict for the models from SciKit Learn. Is it because you wanted to use auc-roc?	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
1249	You have consistently used .predict_proba instead of .predict for the models from SciKit Learn. Is it because you wanted to use auc-roc?	Yes, also because of that. But usually, typically, I'm more interested in probabilities than in hard scores. For example, this model that we just studied – the risk score. Instead of just saying, “Okay, this user (client) is going to default,” it's much, much more useful to know, “What's the probability that this user is going to default? How risky is this customer?” We have a number that’s not just binary – risky or not risky. There are multiple gradations of risk, let's say. It could be completely not risky, somewhat not risky, moderate, somewhat risky, and risky. It could be five gradations or something like this. Then it's more granular for people who make the final decision and decide if they should give the credit or not.
1250	Does DataTalks.Club have career services?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1251	Does DataTalks.Club have career services?	For Spark, we use Python. So it's PySpark. So what Java aspects will be covered? Ankush already uploaded the videos for week 6, just go and check that out.
1252	Does DataTalks.Club have career services?	"Jeff
I don't know. When hosting meetups, I'll just draw on [a board]. It's great to have hybrid options for folks. People who can be remote, people can be in-person – it definitely adds complexity in how things are done. There is a lot more with logistics and things. The short answer is no. But if people are interested in some kind of online Prefect-specific course, send me a message in Slack. I'm there. Let me know about your interest level. It sounds like there's some interest.
Alexey
At least six people are interested. 
Jeff
[chuckles] At least six. 
Alexey
Maybe more, but they just didn’t know that they could vote."
1253	Does DataTalks.Club have career services?	Not really. But I'm not sure what you mean by that. What kind of services do you need? It's a community. If you need some help from the community, you can ask the community for help. It could be a review of your CV, a review of your project, or just general career advice. For example, you could say, “I am a data analyst. I want to become a data engineer. I'm taking this course. What else can I do to be more attractive to potential employers?” For example, ask a question like that and then we have a career questions channel where you can ask this kind of stuff.
1254	Does DataTalks.Club have career services?	"Ankush
[corrupted sound] talk about pricing when it comes to BigQuery. I don't think we're gonna compare it with other data warehouse solutions outside Snowflake because Snowflake costing is a bit more complicated. I don't think so. I think we’re going to stick to BigQuery for now. And just remember, the less you pay, the better it is. But the human cost is always more than machines. There is always a bit of a compromise. [chuckle] To answer it simply – no, we're not going to do it."
1255	I do not really understand why we need Spark. What can I do with Spark that I can’t do with Prefect?	"There are lots of different varieties of approaches here. One thing that you might do is use some kind of data ingestion tool, as I mentioned in the live FAQ discussion – something like Airbyte or Fivetran – if you have lots of data that you're ingesting, that can be a common way to stay on top of that. Or you could do it through a script, kind of like we do here. 
But what you might do is run your agent on a virtual environment in Google Cloud VM, for example, or in AWS, or somewhere else. You could run it in Kubernetes. There are a number of places where maybe you would have that running and have your infrastructure there, so that you were just pulling down the code not to your local machine, but it would just be in your VM. 
If you're actually going and using Pandas, you're putting the data into memory, so you do need to have some scripts that will do that. And you can go directly between Google Cloud Storage and BigQuery with the Prefect GCP library. So that Prefect GCP library that you installed, that you've been using, does have some options in there. I think it's in the BigQuery module to actually get data directly into BigQuery from GCS. So there are a lot of options there. Check out Prefect GCP to see how to do it."
1256	I do not really understand why we need Spark. What can I do with Spark that I can’t do with Prefect?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1257	I do not really understand why we need Spark. What can I do with Spark that I can’t do with Prefect?	"Alexey
The best way to contribute is to take notes and include them to the notes section. And tell your friends about the course. As a part of your Learning in Public, spread the word. It will help us. And don't forget to give us a star if you have."
1258	I do not really understand why we need Spark. What can I do with Spark that I can’t do with Prefect?	"Alexey
Prefect is an orchestrator. If you need to produce a lot of data, Prefect might not be the best tool. Prefect really shines when it needs to execute tasks in a specific order, one after another. Usually the approach many people use at work, from what I see, is they use an orchestrator for orchestrating, and then you delegate all the compute to something external, like Kubernetes, AWS Batch, or Spark. Spark is an external thing. You submit a Spark job from your Prefect graph – from your Prefect DAG. Of course, you can do many things with Prefect, but when it comes to large datasets, then Prefect executors might simply not be able to handle that. This is when you need to use Spark. One does not exclude the other. You use Prefect for scaling Spark jobs, because maybe after this Spark job, you have another job that is not Spark – and Prefect knows how to execute them and in which order."
1259	I do not really understand why we need Spark. What can I do with Spark that I can’t do with Prefect?	"Jeff
I don't know. When hosting meetups, I'll just draw on [a board]. It's great to have hybrid options for folks. People who can be remote, people can be in-person – it definitely adds complexity in how things are done. There is a lot more with logistics and things. The short answer is no. But if people are interested in some kind of online Prefect-specific course, send me a message in Slack. I'm there. Let me know about your interest level. It sounds like there's some interest.
Alexey
At least six people are interested. 
Jeff
[chuckles] At least six. 
Alexey
Maybe more, but they just didn’t know that they could vote."
1260	I am a physician and would like to work in data science. What is the best path for me? What would you advise me?	I always confuse the two – is a physician a doctor or somebody who does physics? Because I guess the answer would be completely different. Either way, coming from physics (from STEM) you probably already know how to program. But as a physician, as a doctor, you might not have the same skills, so this is where you need to start. Maybe learn the basics of Python. I think this advice actually applies to anyone who wants to start in data science – get comfortable with the command: things like navigating the file system like CD, LS, CP, MV and commands like that. Get familiar with using Git as well – configuring GitHub, that's pretty useful, too. Pushing code to GitHub, pulling code from GitHub – I think this is a must. After that, you can already take the course (this one). Maybe the chapters about deployment will be too difficult for you, but the rest of the chapters, where it's a more general introduction to machine learning, could be quite good. So just start with that and if it's difficult, we can think about that. But more importantly, you need to improve your Python. But since you're asking it here and it's already week nine, I assume your Python is fine. So just stick to the course and you'll be fine.
1261	I am a physician and would like to work in data science. What is the best path for me? What would you advise me?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1262	I am a physician and would like to work in data science. What is the best path for me? What would you advise me?	It is possible to do everything locally. You will need to do it by… I think you will need to do it in module five when we deploy something. Or actually, you will probably need to do it when we do your project. Yeah. It's fine to do everything locally. But if you have access to cloud and you want to practice with this, I do recommend practicing, because cloud is one of the skills that employers are looking for. They really need it. So if you want to be more employable, I do recommend learning about cloud.
1263	I am a physician and would like to work in data science. What is the best path for me? What would you advise me?	Okay. Yeah
1264	I am a physician and would like to work in data science. What is the best path for me? What would you advise me?	I have not used it, so I don't know.
1265	I like to contribute and help everyone in the Slack channel. Can I achieve being a member of the staff?	We’re still in progress (as of January 26, 2023). I thought we would do this earlier. But it's still not finished.
1266	I like to contribute and help everyone in the Slack channel. Can I achieve being a member of the staff?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1267	I like to contribute and help everyone in the Slack channel. Can I achieve being a member of the staff?	I don't know what you mean by that. If you want to help everyone in Slack, please do this. Nobody will say no. We're a community. We help each other. Please just go ahead and do this.
1268	I like to contribute and help everyone in the Slack channel. Can I achieve being a member of the staff?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
1269	I like to contribute and help everyone in the Slack channel. Can I achieve being a member of the staff?	No, you cannot.
1270	However can a beginner contributor add value to a BentoML project? The project is exciting.	Yeah. If it works for you, use it. I don't mind.
1271	However can a beginner contributor add value to a BentoML project? The project is exciting.	Well, I'm glad you asked, because we have a data engineering course. If you take it, you will understand a bit more – we do not go into details about how exactly it connects with this course. If you remember, when we talked about CRISP DM, one of the steps there is data preparation and this is what data engineering covers. Once the data is prepared, once the data is in a data lake or data warehouse, then we, data scientists, machine learning engineers, get the data, train the model, deploy this model, and take it to the other steps of the process.
1272	However can a beginner contributor add value to a BentoML project? The project is exciting.	I don't think I understand your question, to be honest. Maybe I'll try to rephrase it… Or ask in Slack, maybe?
1273	However can a beginner contributor add value to a BentoML project? The project is exciting.	"Tim 
Awesome question. Honestly, just in the last few days, you guys have all added a lot of value. I know you could feel frustrated sometimes when you find bugs – just little things. I think a few people found that “—reload” wasn't working on Windows systems. Just finding things like that and reporting them is actually a huge help to us. That way, we can find all the different scenarios where it doesn't work. That makes the library so much better. So I think filing bug reports helps and if you want to contribute to the codebase itself, I think there are a couple of different easy ways to get started. Certainly, if you've built a project with BentoML, we're always accepting more projects in our example folder. But there's also the tag in GitHub “good-first-issue” as well. Those are all issues that, if you comment on them and ask to contribute to build those issues, you can. A good first time issue is the good thing for your first time contributing. Then “help- wanted” is just a tag anywhere we see a need, we tag it with that. Although it may be a more advanced feature."
1274	However can a beginner contributor add value to a BentoML project? The project is exciting.	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1275	How should I find out whether my prediction model is working well or not using RMSE? Is there any way to change it to percentage?	"Well, there are multiple ways you can deal with this. First – get more data, if possible. If not, try to reduce the number of features you have. Sometimes, when the data set is small, if you have a lot of features, it might be confusing for the model. Or try to have a model with a lot of regularization. This should help. Here, the main thing is that you need to come up with a reliable way of evaluating the performance of your model. 
You really need to think about how you can set up your validation strategy in such a way that it's reliable. Once you have that, you can start experimenting with all these regularization things, with dropping some columns, with techniques like upsampling and downsampling and things like this. The important thing here is setting up your validation strategy. Once you have that, everything else will come. You will just need to experiment and the experiments will show what the best way is."
1276	How should I find out whether my prediction model is working well or not using RMSE? Is there any way to change it to percentage?	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
1277	How should I find out whether my prediction model is working well or not using RMSE? Is there any way to change it to percentage?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1278	How should I find out whether my prediction model is working well or not using RMSE? Is there any way to change it to percentage?	"To be honest, I don't know what the typical requirements for computer vision jobs are. They can vary from one company to another. I will give you a general piece of advice that I would give in any other case. You need to do some research about what the company is doing and understand what their problems are. What kinds of problems are they solving? Then try to solve a similar problem. For example, if it's a company like where I work – online classifieds – you can go to Kaggle and look at what kind of computer vision problems companies in the online classifieds domain are putting out there. 
Or just go through the tech blog and see what kind of articles that this company is publishing. Then try to build the project around that. For example, let's take OLX, the company where I work. You can find an article here, Fighting fraud with Triplet Loss. In this article, we show how we use computer vision to find duplicates – how to find image duplicates – and then this article describes that. So if you do something similar in your project, you can just add it to your CV and then talk about this during your interview. That's much better than just a random project. It will certainly help to pique attention. I guess that's the most important factor, try to do some research, find what kind of problems the company is interested in solving and then solve these problems. Then use it to build your portfolio."
1279	How should I find out whether my prediction model is working well or not using RMSE? Is there any way to change it to percentage?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1280	Which part of data science is trending now in terms of salary (computer vision, NLP and others)?	"If you take statistics for machine learning education, the first lecture will be this – you'll probably derive the formula for logistic regression and there will be a slide with all these assumptions about the linear regression model. Here is the Google result: 
https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/assumptions-of-linear-regression/

Go through this. They are usually useful in practice, I guess. But what I typically do is train a model, and then rely on validation to tell me if my model is doing well or not. So if there is something wrong with my validation, I will see that the metric validation is bad. It's just a lot of trial and error, rather than figuring out all these theoretical assumptions. 
I don't want to say that all these assumptions are not relevant. If you work as a data scientist, they still are. But I also found that the more practical way to understand if you can apply linear regression to the data or not, is to just apply it and see if it works or not. If it works – if the results on the validation are good – then it means that you can apply your model to this dataset. 
Maybe some of the assumptions are violated. For example, multicollinearity – this is actually the case in the lectures, when we needed to regularize (add regularization to the model) this assumption was violated. You will see in the lectures what exactly happened. So check it out. I don't think I will be able to give a good answer, because I am not prepared to talk about this, but any statistics book or theoretical machine learning book explains this."
1281	Which part of data science is trending now in terms of salary (computer vision, NLP and others)?	That's a very similar question to when we should use the lambda and when we should use Kubernetes. If you want to use Kubernetes, then go with TensorFlow Serving. Otherwise, TensorFlow Lite. There is one thing where you have to use TensorFlow Lite – where you cannot use TensorFlow Serving – and this is when you want to deploy your model on a mobile device. I have a phone here. If you want to create an app with a model, then you will have to use TensorFlow Lite because you will not be able to use TensorFlow Serving. TensorFlow Serving also adds a bit of complexity. You will need to have this gateway service, you will need to have GRPC service, so that becomes more complex. If you have a lot of load, then you need to do this otherwise, TensorFlow Lite.
1282	Which part of data science is trending now in terms of salary (computer vision, NLP and others)?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1283	Which part of data science is trending now in terms of salary (computer vision, NLP and others)?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1284	Which part of data science is trending now in terms of salary (computer vision, NLP and others)?	"I don't think there is a significant difference between different sub-parts of data science. Usually companies that pay higher salaries are American companies. Let's say if you don't live in the United States – in Europe, in Asia, or in some other parts of the world that are not in the States – if you get hired by an American company, they will pay you more simply because companies are usually richer in the States compared to the rest of the world. 
It's not a rule, of course. If a company is from Switzerland, they probably will pay similar money. But apart from that, I don't think there is any significant difference between different sub-parts of data science. Just pick what you like and focus on this, then you have a good salary. Again, I’m not saying that you have to work for an American company, but if you're after a high salary, then maybe consider American companies that are hiring remotely."
1285	I chose to work with a fraudulent transaction dataset. I see that there are a lot of similar projects online. I’m wondering if this will affect my project.	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1286	I chose to work with a fraudulent transaction dataset. I see that there are a lot of similar projects online. I’m wondering if this will affect my project.	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1287	I chose to work with a fraudulent transaction dataset. I see that there are a lot of similar projects online. I’m wondering if this will affect my project.	No, it should not. Just keep working on this project. It should not affect anything. Of course, if you decide to copy the project that somebody did, then it will, of course, affect it. If people who evaluate your project will accidentally find this out, you will receive zero points and you will have to rely on the other two projects to pass the course. You will not be disqualified for the course, but for the project, you will. So don't do that. Apart from that, I don't see any problems. As long as it's not a Titanic or Iris dataset – it's not one of these popular datasets, that's fine.
1288	I chose to work with a fraudulent transaction dataset. I see that there are a lot of similar projects online. I’m wondering if this will affect my project.	"That's like a pretty generic question, so the answer will also be generic. They will look at the skills you have, they will look at the match – how the skills you have match what is needed. I guess that's it. This is what recruiters usually look for in candidates. If you're interested, another thing you can check out is our podcasts. 
We have an interview, Recruiting Data Professionals with Alicja Notowska. Alicja is a recruiter. She was working as a recruiter at Zalando, which is a very large company in Germany and she shared some tips from your point of view of a recruiter and what she looks for when hiring people. So check it out. It’s quite insightful. You will probably find useful things there."
1289	I chose to work with a fraudulent transaction dataset. I see that there are a lot of similar projects online. I’m wondering if this will affect my project.	I do sometimes. Not very often these days, but we have quite a few models that require deep learning. Usually, these models are related to images. We have some images on the platform, so when we need to understand what's happening on these images, we use deep learning.
1290	How long is the course duration?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
1291	How long is the course duration?	"Jeff
I’m not sure, exactly. It might be the case that you made the deployment and it's scheduled and it's just waiting, but you are not connected to that workspace with the space where your agent is running. Agents are looking for work maybe locally, and you're connected to Prefect cloud, for example. That's one possibility. Just make sure that your agent is pulling for work from the workspace where you actually have scheduled the work to run. You can check that when you start your agent – it'll tell you what it's connected to, up at the top of the message that comes right back when you start the agent, in the CLI."
1292	How long is the course duration?	No, you cannot.
1293	How long is the course duration?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
1294	How long is the course duration?	It's like 10 weeks or something like that. If you count the second attempt of the project, it will be like 13 weeks. Quite long, but not so long as, let's say our machine learning engineering course.
1295	Is CUR decomposition a good way to achieve important feature selection?	I guess not. That's the process of learning? Maybe more senior people don't… Yeah, I see where you're going with this. Senior people are not supposed to go bankrupt and default. They are probably supposed to pay back the debt. I guess these are the particularities of this dataset, I assume. Indeed, it's probably a data issue ,or not an issue, but a “characteristic” of this dataset.
1296	Is CUR decomposition a good way to achieve important feature selection?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1297	Is CUR decomposition a good way to achieve important feature selection?	Not always. For logistic regression, it does. For decision trees and random forest and XGBoost – not really. Then there is a thing called “calibration”. SciKit Learn calibration. You will need to check this out. I think there are also examples of how to do this with random forest. If you can't find examples here, there are examples on Kaggle. But practically speaking, I don't remember the last time I needed to use something like this. It's helpful when you want to train multiple models. Let's say you've been using logistic regression in production for some time and then you make some decisions based on this threshold. But then you want to deploy a new model (XGBoost) and you want to make sure that the ranges you have in the previous model are the same in the new model so the distribution looks kind of similar. This is where calibration helps. It's a useful thing, but not every application needs it.
1298	Is CUR decomposition a good way to achieve important feature selection?	I wish Tim was here and would help me answer that because I don't know. I assume if you wanted to use Bento, you would probably use TensorFlow Lite – you don't want to use the usual TensorFlow for the same reasons that we talked about in the serverless module, because it's simply too large. Probably for Bento, you would use something like TensorFlow Lite and it will still work. Maybe another thing you can check – in our channel, we had a talk from one of the Bento folks, Building an ML Service Platform from the Ground Up. Here, Sean talks about the differences between TensorFlow Serving and Bento. With TensorFlow Serving, it's good – it's quite optimized for deep learning. There are some problems. First of all, it's written in C++ and expects your model in a certain format – this “saved model format”. Adding anything extra on top of that (like pre-processing, post-processing) becomes difficult, because you will need to have an extra step there. Bento makes it easier. But, again, I don't know. It's very difficult to give you a concrete recommendation without knowing what exactly you want to do. So it's case dependent. If you already use TensorFlow Serving in your company, then maybe it makes sense to stick to that. If we have Tim on Office Hours next week, maybe you can ask him that. He will be a better person to answer this question.
1299	Is CUR decomposition a good way to achieve important feature selection?	I am afraid this is the first time I hear about this. [Alexey Googles CUR feature decomposition] No, I don't really know what it is. Sorry. I can't answer this question.
1300	I just entered this Zoomcamp today and I wanted to ask if there is a problem that I'm late. Would submitting the homework late be a problem for you?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1301	I just entered this Zoomcamp today and I wanted to ask if there is a problem that I'm late. Would submitting the homework late be a problem for you?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
1302	I just entered this Zoomcamp today and I wanted to ask if there is a problem that I'm late. Would submitting the homework late be a problem for you?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1303	I just entered this Zoomcamp today and I wanted to ask if there is a problem that I'm late. Would submitting the homework late be a problem for you?	"In summary, “Yes, you can join late. Not a problem. If you care about the certificate, it will not impact your chances of receiving one. All we care about for that are projects. We will have three projects. If you complete two out of the three projects, then you will receive a certificate.” 
For this part “Would submitting the homework late be a problem?” It will be a problem in the sense that you will not be able to do this. Once we close the form, the form does not accept any submissions, so you will simply not be able to do this. There is little point in this, because once we close the form, we also publish the answers. Theoretically, after this, you can just look the answers up in the solutions and then put it in the form. Then it kind of defeats the purpose. 
Submitting homework assignments after the due date will not be possible. It doesn't mean you will not be able to get the certificate. Let's say if you're joining this course in November, then you can binge watch everything until the end (I know some people did this last year) and then you just focus on projects for the remaining time and then you get the certificate at the end anyway."
1304	I just entered this Zoomcamp today and I wanted to ask if there is a problem that I'm late. Would submitting the homework late be a problem for you?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1305	If we have a web deployment (for example, AWS Lambda) do we still have to create a Docker container? Or is that a specific task that we have to show we can do?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1306	If we have a web deployment (for example, AWS Lambda) do we still have to create a Docker container? Or is that a specific task that we have to show we can do?	"You saw that Docker is one of the criteria for the project. So if you don't create a Docker file, you will not get the points for that criterion. Let's say, if you deploy to PythonAnywhere – I think for PythonAnywhere, you don't need to create a Docker file. Maybe you do… I don't remember. But, indeed, there are environments where you don't need to create a Docker container. You can still do that – you will just not get points for the containerization part. 
I think it's quite important to know how to Dockerize your image, even if for your specific project, you don't need that. We want to make sure you also know how to do this. That's why it's part of the evaluation criteria. Knowing Docker is quite important, I think, in today's world. If you want to work as a data scientist, or an ML engineer or any kind of data engineer, you have to know about containerization. So that's why. 
Also, for Lambda, there are different options. Yes, you don't necessarily need Docker. But you can also do it with Docker. There is an option where you serve a container through Lambda – it's possible. If you want to use AWS Lambda, I would actually suggest following this approach."
1307	If we have a web deployment (for example, AWS Lambda) do we still have to create a Docker container? Or is that a specific task that we have to show we can do?	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
1308	If we have a web deployment (for example, AWS Lambda) do we still have to create a Docker container? Or is that a specific task that we have to show we can do?	I'm not sure I understand the question. The question is “Why test_train_split splits in two, but here, we split into three parts? For that there is a video in week one that explains why we need three datasets, not two. So you can go check it out. There is a problem called “multiple comparison problem” and that's why we use test/split to account for that. You will see later, in module three – there we can also use SciKit Learn train_test_split to split our original dataset into three parts and this is exactly what we will do.
1309	If we have a web deployment (for example, AWS Lambda) do we still have to create a Docker container? Or is that a specific task that we have to show we can do?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
1310	What is the difference between Airflow/Prefect and Google workflows? It seems like homework 2 can be done using Workflows too. When should we use Prefect and Workflows?	"Jeff
As Alexey said many times in the class, you can use whatever tool you want for things. So if you want to accomplish things with Workflows, that's fine. There are different tools out there. I'd say the big difference is that Prefect is agnostic to any cloud. You can easily drop in different storage blocks if you want to, or have just different options. Anyone who's using Python first wants to be able to use lots of different tools. That's a good option there. With Workflows, you're kind of tied to Google's infrastructure at that point, in a pretty significant way. But use a tool that works for you. That’s my advice there."
1311	What is the difference between Airflow/Prefect and Google workflows? It seems like homework 2 can be done using Workflows too. When should we use Prefect and Workflows?	Since we had more signups this year than last year, I hope it will be more than 100. But let's see.
1312	What is the difference between Airflow/Prefect and Google workflows? It seems like homework 2 can be done using Workflows too. When should we use Prefect and Workflows?	"Michael
It's not necessary. We do have recipes for using Terraform with Prefect with different things. We talked about the Prefect recipes repo, but I'll put a link to it here. 
Jeff
This is in a docs and we have a GitHub repo where we have lots of different recipes. Some of them here explain things like, say you want to run an agent – how can you do that? There are so many different cloud environments. You can put things on Kubernetes, you can put things on something like a more serverless platform like Fargate. For example, on the right there, if you were wanting to use AWS with Prefect and ECS (elastic container service) then with Terraform – we have some recipes for that. So there's just so many different permutations of possible ways to do things. A lot of these are helpful ways to go and look at options, but it depends on what you're using in the real world (in a use case). I don't think we have anything specific for this course that requires us doing this, so we don't have anything further for it.
Alexey
I guess, the usual setup that I saw (not necessarily Prefect-related) you typically use Terraform for setting up infrastructure or things like a Kubernetes cluster and configure it there. But then there is a separate repo with configuration for Kubernetes that is not maintained/managed through Terraform but through something else. For that, you don't really need to Terraform – and then Prefect could live in this Kubernetes cluster.
Jeff 
Yeah, we do have some guys working with Kubernetes in here, for sure. I see some of them here. We do have a Helm chart that we can use. If you're into the Kubernetes world, there are lots of things there. It's a whole rabbit hole.
Alexey
Here are the Prefect Helm charts.
Kalise
Also, there are a lot of YouTube videos we have that can help if you're trying to use some of these resources – how to go about deploying the Helm chart with Kubernetes and stuff, like step-by-step tutorials as well. Generally, if you just go to the Prefect YouTube channel, you'll see a lot of those tutorials. There should be some playlists there that have various things.
Alexey
Is this a duck in the cover or what?
Jeff
That is. It's the top of a duck. Blue ducks!"
1313	What is the difference between Airflow/Prefect and Google workflows? It seems like homework 2 can be done using Workflows too. When should we use Prefect and Workflows?	No, you cannot.
1314	What is the difference between Airflow/Prefect and Google workflows? It seems like homework 2 can be done using Workflows too. When should we use Prefect and Workflows?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1315	There is a question about when the deadline to finish the midterm project is.	"Alexey
I want to show you again how to find all the deadlines. You go to ML Zoomcamp – this is our 
course page. Then you go to cohorts, and then you go to 2022, which is this cohort, where you will find the deadline calendar. Click on this link and you will see that these are the deadlines. For the midterm project, the deadline is the 7th of November and then there will also be an evaluation phase and it's due on 14th of November. Then we will learn about neural networks, serverless deep learning, and Kubernetes. That's the plan right now. Things may change in the future. This is not set in stone right now. But this is to give you a rough understanding of where we will be. Most likely, we will stick to this – but things happen."
1316	There is a question about when the deadline to finish the midterm project is.	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
1317	There is a question about when the deadline to finish the midterm project is.	That's a very similar question to when we should use the lambda and when we should use Kubernetes. If you want to use Kubernetes, then go with TensorFlow Serving. Otherwise, TensorFlow Lite. There is one thing where you have to use TensorFlow Lite – where you cannot use TensorFlow Serving – and this is when you want to deploy your model on a mobile device. I have a phone here. If you want to create an app with a model, then you will have to use TensorFlow Lite because you will not be able to use TensorFlow Serving. TensorFlow Serving also adds a bit of complexity. You will need to have this gateway service, you will need to have GRPC service, so that becomes more complex. If you have a lot of load, then you need to do this otherwise, TensorFlow Lite.
1318	There is a question about when the deadline to finish the midterm project is.	I think for binary features, just one and zero makes more sense than OneHotEncoding. OneHotEncoding will take care of that but you will have two columns that are the exact opposite of each other. So that is a bit redundant. I think it's better just to convert it to a Boolean column with ones and zeros.
1319	There is a question about when the deadline to finish the midterm project is.	Yeah. If it works for you, use it. I don't mind.
1320	What is your favorite project from past 2021 Zoomcamp?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
1321	What is your favorite project from past 2021 Zoomcamp?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
1322	What is your favorite project from past 2021 Zoomcamp?	Yeah. If it works for you, use it. I don't mind.
1323	What is your favorite project from past 2021 Zoomcamp?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1324	What is your favorite project from past 2021 Zoomcamp?	I don't have a “favorite” project, but please check out the demos from the students of the previous cohort. And please check the midterm project link that I shared. You can also find it in the midterm page of the course. And maybe just come up with your own favorite. You can also share it in Slack, actually. So maybe you can see, “Okay, I went through all these projects. I really like this one,” and you can just share it with others.
1325	Can I change the order and study streaming after the basics and setup? Will it be more difficult than following all the course parts in order?	This question is tricky, because I haven't yet published the homework for streaming. I think it's in progress right now. You can study streaming right now, but you will need to do your homework and submit the homework later, when it's released. But let's say you don't feel like studying data warehousing. Then just don't. I think you should, but let's say that you don't want to study DBT and you just want to do streaming. Feel free to skip DBT Batch and focus on streaming. That's fine. You don't have to do some models that you don't think are relevant for you.
1326	Can I change the order and study streaming after the basics and setup? Will it be more difficult than following all the course parts in order?	"There are lots of different varieties of approaches here. One thing that you might do is use some kind of data ingestion tool, as I mentioned in the live FAQ discussion – something like Airbyte or Fivetran – if you have lots of data that you're ingesting, that can be a common way to stay on top of that. Or you could do it through a script, kind of like we do here. 
But what you might do is run your agent on a virtual environment in Google Cloud VM, for example, or in AWS, or somewhere else. You could run it in Kubernetes. There are a number of places where maybe you would have that running and have your infrastructure there, so that you were just pulling down the code not to your local machine, but it would just be in your VM. 
If you're actually going and using Pandas, you're putting the data into memory, so you do need to have some scripts that will do that. And you can go directly between Google Cloud Storage and BigQuery with the Prefect GCP library. So that Prefect GCP library that you installed, that you've been using, does have some options in there. I think it's in the BigQuery module to actually get data directly into BigQuery from GCS. So there are a lot of options there. Check out Prefect GCP to see how to do it."
1327	Can I change the order and study streaming after the basics and setup? Will it be more difficult than following all the course parts in order?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1328	Can I change the order and study streaming after the basics and setup? Will it be more difficult than following all the course parts in order?	We’re still in progress (as of January 26, 2023). I thought we would do this earlier. But it's still not finished.
1329	Can I change the order and study streaming after the basics and setup? Will it be more difficult than following all the course parts in order?	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
1330	In addition to image classification, what are some other use cases for neural networks? Can neural networks be used on text?	I don't think I understand your question, to be honest. Maybe I'll try to rephrase it… Or ask in Slack, maybe?
1331	In addition to image classification, what are some other use cases for neural networks? Can neural networks be used on text?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
1332	In addition to image classification, what are some other use cases for neural networks? Can neural networks be used on text?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1333	In addition to image classification, what are some other use cases for neural networks? Can neural networks be used on text?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1334	In addition to image classification, what are some other use cases for neural networks? Can neural networks be used on text?	They can definitely be used on text. I think if you just Google “neural networks for text”, or “deep learning for text”, you will find a lot of material. That’s a good way to start. There is also a Stanford course called Natural Language Processing with Deep Learning. It's very similar to the course I was referring to in the image classification module. But this one is more advanced – it's more difficult – and it focuses more on text. That is also a good resource. You can start learning about that by learning what text embedding is, and things like Word2vec, word embeddings, Word2vec growth and things like that. Then you can slowly progress to more complex models.
1335	Could I pass the project if I build the modeling part, but not deploy it?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1336	Could I pass the project if I build the modeling part, but not deploy it?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1337	Could I pass the project if I build the modeling part, but not deploy it?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1338	Could I pass the project if I build the modeling part, but not deploy it?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
1339	Could I pass the project if I build the modeling part, but not deploy it?	"Okay, well, let's take a look at the criteria. Model training, reproducibility, model deployment, dependency… You can see that four out of nine of the criteria are about deployment, right. This one (reproducibility) is also about that partly. Exporting notebook to a script. Hmm. Most of them are actually about productionizing your model, not about the modeling. If you do that, you will get seven points. I don't think that will be sufficient to pass the project. So you will need to work on this as well. But it doesn't have to be perfect. If you just create a Docker file, use BentoML, and you automatically have some points here and there. Just put in some effort and you pass it. Don’t worry. But if you just submit a Jupyter notebook, and that's it – I don't think it will work. You need to do a little bit more than that.
I want to add that this is a machine learning engineering course. This is not a course about data science. This is a course about engineering. If you want to be an engineer and receive a certificate showing that you learned the engineering part at the end, you have to learn the deployment part. There is no way around that."
1340	Cover letters get lost in the ATS (application tracking system). Cover letters are good for career changers. Use a cover letter in the email application and not on job board portals, unless asked.	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1341	Cover letters get lost in the ATS (application tracking system). Cover letters are good for career changers. Use a cover letter in the email application and not on job board portals, unless asked.	I think for binary features, just one and zero makes more sense than OneHotEncoding. OneHotEncoding will take care of that but you will have two columns that are the exact opposite of each other. So that is a bit redundant. I think it's better just to convert it to a Boolean column with ones and zeros.
1342	Cover letters get lost in the ATS (application tracking system). Cover letters are good for career changers. Use a cover letter in the email application and not on job board portals, unless asked.	That's a very similar question to when we should use the lambda and when we should use Kubernetes. If you want to use Kubernetes, then go with TensorFlow Serving. Otherwise, TensorFlow Lite. There is one thing where you have to use TensorFlow Lite – where you cannot use TensorFlow Serving – and this is when you want to deploy your model on a mobile device. I have a phone here. If you want to create an app with a model, then you will have to use TensorFlow Lite because you will not be able to use TensorFlow Serving. TensorFlow Serving also adds a bit of complexity. You will need to have this gateway service, you will need to have GRPC service, so that becomes more complex. If you have a lot of load, then you need to do this otherwise, TensorFlow Lite.
1343	Cover letters get lost in the ATS (application tracking system). Cover letters are good for career changers. Use a cover letter in the email application and not on job board portals, unless asked.	I do sometimes. Not very often these days, but we have quite a few models that require deep learning. Usually, these models are related to images. We have some images on the platform, so when we need to understand what's happening on these images, we use deep learning.
1344	Cover letters get lost in the ATS (application tracking system). Cover letters are good for career changers. Use a cover letter in the email application and not on job board portals, unless asked.	That's a great suggestion – I cannot answer it better. I can only confirm that I don't look at them. We have an applicant tracking system and I can actually look at the cover letter there. But I don't – because I don't care much, to be honest.
1345	Is there a checklist for the final project to make sure everything is applied?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1346	Is there a checklist for the final project to make sure everything is applied?	Yes, you will have to come up with your own individual projects. It's not easy, I know. But we'll help you. We will help you to come up with a project idea. That's actually on purpose because we want every project to be unique. Then it becomes a part of your portfolio when you look for a job.
1347	Is there a checklist for the final project to make sure everything is applied?	Our thoughts are that this is indeed an exciting area. This is an in-demand skill and that's why we have a section about analytics engineering in this course.
1348	Is there a checklist for the final project to make sure everything is applied?	"Alexey
Yeah, that's not going to happen. I think ChatGPT will help us do our job, but it will not replace people, at least in my opinion. But I use it in my work and it's helpful. You should try using it too – if you want, of course."
1349	Is there a checklist for the final project to make sure everything is applied?	Yes, there is. If you go to the project folder you will see the criteria. This criteria is like your checklist.
1350	How to automate the process of transforming a column with different descriptions (string) for each single product into an individual description?	I think for binary features, just one and zero makes more sense than OneHotEncoding. OneHotEncoding will take care of that but you will have two columns that are the exact opposite of each other. So that is a bit redundant. I think it's better just to convert it to a Boolean column with ones and zeros.
1351	How to automate the process of transforming a column with different descriptions (string) for each single product into an individual description?	"It all depends on what you mean by “full stack ML engineer”. As you’ve noticed, data engineer Zoomcamp is not about machine learning – it's about data engineering. If in your opinion a “full stack ML engineer” needs to know data engineering, then you should do data engineering. If not, then no. From my point of view, there is a “full stack of data scientist”. What I put in the definition of a full stack data scientist is – a person who can do all the steps in the CRISP-DM process. They can help product managers with business understanding, they can work with analysts in data understanding, they can help data engineers in the data preparation step, they can do the modeling, and they can deploy the models. If you want to be that kind of person, then, of course, you also need to learn a bit of product management and analytics, which we don't have courses for. But data engineering – preparing data before it goes to a model – will certainly be helpful. 
Also, machine learning ops Zoomcamp (MLOps Zoomcamp) will be helpful as well. I would suggest, if you really want to focus on machine learning engineering, then probably doing MLOps Zoomcamp will make more sense for you. But data engineering Zoomcamp will be quite useful in the future, because data scientists and ML engineers tend to work on data pipelines as well. 
Maybe not all the content will be useful for you. For example, the content about data warehousing – as a data scientist, I don't find it super useful in my work to me personally. Also, the module about analytics engineering – it's nice, but it's not something I do day to day. But the modules about batch, the modules about Spark, the modules about streaming – this is something that can be useful, because this is something we could use in building machine learning pipelines as well. 
To put data in a machine learning model, we need to prepare it – we need to create data pipelines – and this is pretty relevant. But you can just check it out. All the content is already there. You can just go and check it out. If you like it, do it – if you don't, don’t. I would suggest maybe going with this one (ML Zoomcamp)."
1352	How to automate the process of transforming a column with different descriptions (string) for each single product into an individual description?	"If you take statistics for machine learning education, the first lecture will be this – you'll probably derive the formula for logistic regression and there will be a slide with all these assumptions about the linear regression model. Here is the Google result: 
https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/assumptions-of-linear-regression/

Go through this. They are usually useful in practice, I guess. But what I typically do is train a model, and then rely on validation to tell me if my model is doing well or not. So if there is something wrong with my validation, I will see that the metric validation is bad. It's just a lot of trial and error, rather than figuring out all these theoretical assumptions. 
I don't want to say that all these assumptions are not relevant. If you work as a data scientist, they still are. But I also found that the more practical way to understand if you can apply linear regression to the data or not, is to just apply it and see if it works or not. If it works – if the results on the validation are good – then it means that you can apply your model to this dataset. 
Maybe some of the assumptions are violated. For example, multicollinearity – this is actually the case in the lectures, when we needed to regularize (add regularization to the model) this assumption was violated. You will see in the lectures what exactly happened. So check it out. I don't think I will be able to give a good answer, because I am not prepared to talk about this, but any statistics book or theoretical machine learning book explains this."
1353	How to automate the process of transforming a column with different descriptions (string) for each single product into an individual description?	Yeah. If it works for you, use it. I don't mind.
1354	How to automate the process of transforming a column with different descriptions (string) for each single product into an individual description?	I do not understand this question, to be honest. I need an example. Maybe ask in Slack with… I usually don't like screenshots [chuckles] when you post them in Slack, because when I open a screenshot with code on my mobile, I cannot see anything. But maybe this is a case, when you can actually include a screenshot of what you mean and let's discuss it there.
1355	I tried using parquet files from the source, but these contain a different number of rows than the backup CSV and the result is in the other answers.	"Jeff
I’m not sure, exactly. It might be the case that you made the deployment and it's scheduled and it's just waiting, but you are not connected to that workspace with the space where your agent is running. Agents are looking for work maybe locally, and you're connected to Prefect cloud, for example. That's one possibility. Just make sure that your agent is pulling for work from the workspace where you actually have scheduled the work to run. You can check that when you start your agent – it'll tell you what it's connected to, up at the top of the message that comes right back when you start the agent, in the CLI."
1356	I tried using parquet files from the source, but these contain a different number of rows than the backup CSV and the result is in the other answers.	BigQuery is a data warehouse and it's optimized. It's usually faster. AWS Athena is a data lake. You will see the difference in week 2 – there is a video that explains what a data lake is. Then in week 3, you will see what the data warehouse is. But AWS Athena is more like a data lake. You can still run all these queries, but maybe they will be slower and I also think they will be cheaper. For analytical queries that – for queries where you want to get results quickly, I usually use BigQuery. Again, like these are two different clouds, two different technologies. The counterpart of BigQuery would be Redshift. But I think Redshift is slower than BigQuery as well.
1357	I tried using parquet files from the source, but these contain a different number of rows than the backup CSV and the result is in the other answers.	"Alexey
I don't know. Maybe I'll put this question to Ankush as well. But check the website. I think this is one of the good examples when you can just use Google to find the answer."
1358	I tried using parquet files from the source, but these contain a different number of rows than the backup CSV and the result is in the other answers.	"Alexey
For this case, if you go to our data engineering page, and then go to the homework, you will see this note: “If the answer does not match exactly, then select the closest option.” With that, if you have an answer, but it's somewhat not the same one, you just go with the closest one and then you'll get it right."
1359	I tried using parquet files from the source, but these contain a different number of rows than the backup CSV and the result is in the other answers.	Yeah, I will not really check that. You are kind of expected to use the backup CSV. I don't know why the number of rows is different. Please use the backup CS
1360	What library do you use for data visualization?	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
1361	What library do you use for data visualization?	"Yes, indeed there is. You need to get something like 50% in order to pass the project. I don’t remember the exact score we used last year. This is how we did it last year – we looked at the distribution of the scores. We didn’t want to mark many projects as failed, so we looked at the distribution and we made sure that most of you passed the project. I think the passing score was pretty low. Most of the students who did the project – who did put some effort there in doing the project and who met at least half of the criteria or most of the criteria but partly – they passed the project. It wasn't a big problem, so don't worry about that too much. 
As long as you put in some effort, then you'll pass it. The people who didn't pass last year submitted empty projects, or projects that were copied from somewhere, or projects where they just added some readme file and maybe copied some Kaggle kernel and that was it. So nothing was done in addition to that. There were people who passed, but they just didn't put any effort into attempting to do the project. If you put in some genuine effort, don't worry, you will pass the project."
1362	What library do you use for data visualization?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1363	What library do you use for data visualization?	Seaborn.
1364	What library do you use for data visualization?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1365	How does BigQuery compare to something like Snowflake?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1366	How does BigQuery compare to something like Snowflake?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
1367	How does BigQuery compare to something like Snowflake?	"Victoria
I think it's also worth putting it as, it’s great, but why is it chosen as a cloud data platform? I saw that a lot of people were also talking about AWS and things like that. There’s even a channel of people that want to use Snowflake instead of GCP. The main reason why we chose GCP is because it has a very generous free tier that's $300. In the case of Snowflake, for example, it’s limited to one month. In the case of GCP it’s until you finish the money – you can use the backups and a lot of people use the virtual machines, and you can use BigQuery. It will cover all of that and you won’t spend anything from your money, which is the idea of the course. That's why we chose BigQuery. 
But the main goal that we have here is that you can apply the knowledge of a cloud data warehouse to any kind of data warehouse. The same with Prefect, you can also apply it to Airflow if you start working at a company that uses Airflow. Because at the end of the day, you should know how a cloud data warehouse and data warehousing works. In the case of specifics of BigQuery to something Snowflake – both our cloud data warehouses from my experience. BigQuery has a different way of connecting – it goes through an API – whereas in Snowflake you use the more well-known SQL. They both have their flavors or SQL. 
Big Query also has differences, for example, they call databases “projects,” they call schemas “datasets,” and things like that. So it changes terminology. They also have a different approach on cost. Something like, select, start, limit 10, for a BigQuery table could cost you a lot of money. But they tell you before that, how many bytes they'll scan and things like that. It’s just the smaller things, I would. Ankush, you're also very familiar with BigQuery. I've always used Snowflake or Redshift more, or Microsoft.
Ankush
I think the biggest reason for choosing BigQuery was that it just comes with Google Cloud Platform and it's free to use. That's one thing. I think, overall, Snowflake is generally more expensive than BigQuery, at least in my experience. But, of course, they’re super similar. If you look at Big Query optimizations, it's the same concept in Snowflake. 
The concepts, as Victoria said, are applicable to both the data warehouse solutions. It's just a flavor that we chose just because it's easy to have something like an inbuilt data warehouse where you actually start something on your own. This was one of the big reasons for choosing BigQuery.
Alexey
You're free to use AWS if you want, as well. I think that is also a channel where people want to use AWS for the course. We, unfortunately, will not be able to give you a lot of support there. Because with DBT, for example, I don't know how well it can connect to Redshift or Athena or whatever.
Victoria
Somebody is saying something in the chat. I think I was misunderstood. It says $300 is not a small amount. That's the amount of credits you get for free. You will not pay $300. The idea is that we chose GCP so you don't pay anything. You don't pay anything! [chuckles] 
Alexey
Yes, that's the main advantage. What you need is a Google account – you use it for registering at Google Cloud Platform. They will ask you for a credit card, but only to verify that you're a real human. This card is not used for a different Google Cloud Platform account. They will not charge you anything. To me, Google is trustworthy. I think you can trust it with your credit card. I mean, so far they didn’t violate the trust I have in them. It's a good deal, basically. You should take it. 
With AWS, most of the content we talk about – most of the things you will learn here about Google Cloud Platform – are easily transferable to AWS, like all these virtual machines, object storage, Spark. All these things work fine. In AWS, the buttons you need to click are different, or the Terraform script you need to write is different. But at the end, most of the concepts are still the same. You just need to map from Google Cloud Platform concept to AWS concept. But with AWS, you need to pay something."
1368	How does BigQuery compare to something like Snowflake?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
1369	How does BigQuery compare to something like Snowflake?	Everything you do here is individual. You don't form teams.
1370	Do you think that after finishing the ML Zoomcamp, it's possible to work on ML engineering freelance contracts or does one need years of professional experience?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1371	Do you think that after finishing the ML Zoomcamp, it's possible to work on ML engineering freelance contracts or does one need years of professional experience?	"It's possible. The sky's the limit. It's up to you. How do you sell yourself? I will tell you that I did that. I didn't finish ML Zoomcamp, of course – I finished Coursera courses and I did freelancing then. Before getting my full time job, I freelanced for a couple of years. I was also studying at university and then, when I got my first data science job, I already had a bunch of portfolio projects from freelancing. So it's certainly possible. One caveat, though – the market in 2012 was very different from 2022. But I think the need for freelancers is still there. 
I don't know how to find freelancer contracts now. Maybe it's like the usual ones, like Fiverr, UpWork, things like that. You need to have professional experience somewhere. In my case, I had experience in Java and I learned ML. So my profile was – I was helping companies with machine learning in Java. I already knew Java. Maybe that's an important piece of information that you also need to know. If you don't have professional experience at all, it might be a bit difficult, but you should probably focus on portfolio projects, not only the years of experience. If you can show the potential client that you can solve their problems, they wouldn't care if you have worked before or not. They will see your portfolio and they will understand, “Okay, this person can actually do the thing I need.” 
Maybe one more thing – I took part in a Kaggle competition that was about detecting duplicates. And then, because I was on the leaderboard, a company who had the same problem reached out to me saying “We have the same problem. Can you work with us and help solve it?” They didn't care if I had any professional experience. They only cared about my solution. They already knew from Kaggle that I could solve this problem and that's why they reached out to me. The idea there is – it's not like I was just working on this problem alone and didn't tell anyone. I was actively sharing what I was doing in public. 
That's why I encourage everyone – every single person who is taking this course – to share their progress. Because if you don't, then nobody will know that you actually have the skills. But if you do, for example, you share something about a project you did and this project could be about detecting duplicates, for example. I don't think any of you did a project like that, but just as an example. Then maybe somebody who needs to solve this problem will see this post and will think “Okay, this person actually did the project on that. Let me contact them and see if they would be up to helping us.”"
1372	Do you think that after finishing the ML Zoomcamp, it's possible to work on ML engineering freelance contracts or does one need years of professional experience?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
1373	Do you think that after finishing the ML Zoomcamp, it's possible to work on ML engineering freelance contracts or does one need years of professional experience?	"Actually, I don't remember exactly how I do this. The right approach would be to use only train data and not use validation data or test dataset. This is done on purpose for exactly this reason that you mentioned. We want to model a situation in which there is some unseen data – it's truly unseen, we do not see that all – and anything can happen to this unseen data, including the situation that you described, such as there is a value that is only present in the test data. 
Imagine that there is a new iPhone – when you train your model, this new iPhone didn't exist (you didn't have this category) but you apply this model, and then, all of a sudden, there is a new iPhone. How will your model react to this? You need to somehow model this situation. This is totally normal. This can and will happen in practice. We just want to understand what would be the effect of that. That's why we need to see this in training data. This is good and this is how it should be. I hope that is a good answer."
1374	Do you think that after finishing the ML Zoomcamp, it's possible to work on ML engineering freelance contracts or does one need years of professional experience?	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
1375	Are there any advantages or extra features when using Prefect Cloud versus the open source version, except that it's hosted?	Yes, you will have to come up with your own individual projects. It's not easy, I know. But we'll help you. We will help you to come up with a project idea. That's actually on purpose because we want every project to be unique. Then it becomes a part of your portfolio when you look for a job.
1376	Are there any advantages or extra features when using Prefect Cloud versus the open source version, except that it's hosted?	"Alexey
Yeah, that's not going to happen. I think ChatGPT will help us do our job, but it will not replace people, at least in my opinion. But I use it in my work and it's helpful. You should try using it too – if you want, of course."
1377	Are there any advantages or extra features when using Prefect Cloud versus the open source version, except that it's hosted?	"Kalise
Yeah. Prefect Cloud is open source and there's an entirely free version of it. It is obviously hosted so you don't have to manage the time of updating your UI and everything with new Prefect versions, if you use Prefect Cloud. It also has the ability that you can have collaborators. With the free version, you also get two other collaborators that you can invite to. This means sharing the blocks and reusing blocks within the different flows that you're building with your collaborators. Then there are also additional features such as not just notifications, but automation, which means being able to trigger an action when something happens, as well. For example, say your flow fails, or maybe your agent – the work queue is unhealthy –instead of just getting a notification, you could actually pause the deployment. You could trigger an action with that automation, as well.
Jeff
I’ll just add that there's a fifth homework question. People can use Slack and just in case you haven't been there yet, sometimes Slack is pretty limiting. It’s like, “Oh, too many people are hitting us with notifications for things.” You might have to make your own Slack workspace. There are instructions in the FAQ on that, so just check that out. Originally, I set it up with email, because email is nice and easy enough to deal with Slack. But it's just a cloud feature. So if you want that, you have to have an account on Prefect Cloud, which is free. That's our email server that's running that in the background. There are ways that you can set things up yourself, but it just gets pretty involved. A lot of things like that are just kind of easier with the cloud system.
Alexey
And I guess platform engineers are happy when they don't need to maintain stuff. 
Jeff
[chuckles] Yes. You don't have to employ people to do things. 
Alexey
Yeah. I mean, even if you do have to employ them, they're still happy that they don't need to worry about maintaining yet another server."
1378	Are there any advantages or extra features when using Prefect Cloud versus the open source version, except that it's hosted?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1379	Are there any advantages or extra features when using Prefect Cloud versus the open source version, except that it's hosted?	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
1380	Is it a good technique to train with small images and then use larger images for prediction?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1381	Is it a good technique to train with small images and then use larger images for prediction?	Yeah. If it works for you, use it. I don't mind.
1382	Is it a good technique to train with small images and then use larger images for prediction?	That's a very similar question to when we should use the lambda and when we should use Kubernetes. If you want to use Kubernetes, then go with TensorFlow Serving. Otherwise, TensorFlow Lite. There is one thing where you have to use TensorFlow Lite – where you cannot use TensorFlow Serving – and this is when you want to deploy your model on a mobile device. I have a phone here. If you want to create an app with a model, then you will have to use TensorFlow Lite because you will not be able to use TensorFlow Serving. TensorFlow Serving also adds a bit of complexity. You will need to have this gateway service, you will need to have GRPC service, so that becomes more complex. If you have a lot of load, then you need to do this otherwise, TensorFlow Lite.
1383	Is it a good technique to train with small images and then use larger images for prediction?	It's an okay technique. I would still train the model on big images at the end as well. But, again, check it with validation and then you will see how different the models are.
1384	Is it a good technique to train with small images and then use larger images for prediction?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
1385	Do we have to post about the project publicly like we do to get seven points weekly?	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
1386	Do we have to post about the project publicly like we do to get seven points weekly?	"To be honest, I don't know what the typical requirements for computer vision jobs are. They can vary from one company to another. I will give you a general piece of advice that I would give in any other case. You need to do some research about what the company is doing and understand what their problems are. What kinds of problems are they solving? Then try to solve a similar problem. For example, if it's a company like where I work – online classifieds – you can go to Kaggle and look at what kind of computer vision problems companies in the online classifieds domain are putting out there. 
Or just go through the tech blog and see what kind of articles that this company is publishing. Then try to build the project around that. For example, let's take OLX, the company where I work. You can find an article here, Fighting fraud with Triplet Loss. In this article, we show how we use computer vision to find duplicates – how to find image duplicates – and then this article describes that. So if you do something similar in your project, you can just add it to your CV and then talk about this during your interview. That's much better than just a random project. It will certainly help to pique attention. I guess that's the most important factor, try to do some research, find what kind of problems the company is interested in solving and then solve these problems. Then use it to build your portfolio."
1387	Do we have to post about the project publicly like we do to get seven points weekly?	"You don't have to. You can if you want. Remember, this is optional. This is something we talked about already. This is optional. If you want to share your progress publicly, you can do this. If you saw the introduction at the beginning when I was talking about the project – I was going through the forum. We have this thing called “Learning in public links.” The score is capped at 14 points. Every day, you share your progress and at the end, you have 14 links. 
By the way, I noticed that some people try to be smart and try to “game” the system by submitting non-relevant links. Please don't do that. I understand the desire to get extra points, but remember that this is actually anonymized. Nobody knows yet that this hash is you. That's one thing. Then the second thing is… why do you want to cheat? It's not clear to me. The person who I saw – they didn't cheat, but they tried to game the script that we have for evaluating. That person will lose all the learning in public links. They will not have any “learning in public” at all. Instead of seven points for each of the previous homeworks, they will get zero, and they will receive zero points for the rest of the course. So please don't do that. I don't see a point in this. I didn't think somebody would even try to do that. But life is full of surprises, right? Please, please don't do that. There is no point in doing this. Why do you want to do this? I don't understand. Yeah, but you've been warned."
1388	Do we have to post about the project publicly like we do to get seven points weekly?	"Actually, I don't remember exactly how I do this. The right approach would be to use only train data and not use validation data or test dataset. This is done on purpose for exactly this reason that you mentioned. We want to model a situation in which there is some unseen data – it's truly unseen, we do not see that all – and anything can happen to this unseen data, including the situation that you described, such as there is a value that is only present in the test data. 
Imagine that there is a new iPhone – when you train your model, this new iPhone didn't exist (you didn't have this category) but you apply this model, and then, all of a sudden, there is a new iPhone. How will your model react to this? You need to somehow model this situation. This is totally normal. This can and will happen in practice. We just want to understand what would be the effect of that. That's why we need to see this in training data. This is good and this is how it should be. I hope that is a good answer."
1389	Do we have to post about the project publicly like we do to get seven points weekly?	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
1390	Will you do a data engineering Zoomcamp in 2023?	Yeah. If it works for you, use it. I don't mind.
1391	Will you do a data engineering Zoomcamp in 2023?	You'll have to come up with a project idea yourself. We will help you, of course. We will share some datasets with you, but will need to go through these datasets and you will need to find a project yourself. Then if you're not sure if this is a good project or not, you can just, again, ask that in Slack. You can say “Okay, I found this project. Will this dataset be a good dataset for the project?” I can already tell you that datasets like Iris, Titanic, Buying Quality, MNIST – these datasets that you see in every tutorial – are not good for the project. Try to find something a little bit more unique.
1392	Will you do a data engineering Zoomcamp in 2023?	Yes, we will do this. I think I announced it last week. So yes, we will have it. I still have to make announcements on social media. But if you just want to register, you can go to the Data Engineering Zoomcamp on GitHub or just click here, and then you can sign up. You will get a notification closer to the starting date. For the ML Zoomcamp, we have a telegram channel. You can sign up for that too.
1393	Will you do a data engineering Zoomcamp in 2023?	"It all depends on what you mean by “full stack ML engineer”. As you’ve noticed, data engineer Zoomcamp is not about machine learning – it's about data engineering. If in your opinion a “full stack ML engineer” needs to know data engineering, then you should do data engineering. If not, then no. From my point of view, there is a “full stack of data scientist”. What I put in the definition of a full stack data scientist is – a person who can do all the steps in the CRISP-DM process. They can help product managers with business understanding, they can work with analysts in data understanding, they can help data engineers in the data preparation step, they can do the modeling, and they can deploy the models. If you want to be that kind of person, then, of course, you also need to learn a bit of product management and analytics, which we don't have courses for. But data engineering – preparing data before it goes to a model – will certainly be helpful. 
Also, machine learning ops Zoomcamp (MLOps Zoomcamp) will be helpful as well. I would suggest, if you really want to focus on machine learning engineering, then probably doing MLOps Zoomcamp will make more sense for you. But data engineering Zoomcamp will be quite useful in the future, because data scientists and ML engineers tend to work on data pipelines as well. 
Maybe not all the content will be useful for you. For example, the content about data warehousing – as a data scientist, I don't find it super useful in my work to me personally. Also, the module about analytics engineering – it's nice, but it's not something I do day to day. But the modules about batch, the modules about Spark, the modules about streaming – this is something that can be useful, because this is something we could use in building machine learning pipelines as well. 
To put data in a machine learning model, we need to prepare it – we need to create data pipelines – and this is pretty relevant. But you can just check it out. All the content is already there. You can just go and check it out. If you like it, do it – if you don't, don’t. I would suggest maybe going with this one (ML Zoomcamp)."
1394	Will you do a data engineering Zoomcamp in 2023?	I guess not. That's the process of learning? Maybe more senior people don't… Yeah, I see where you're going with this. Senior people are not supposed to go bankrupt and default. They are probably supposed to pay back the debt. I guess these are the particularities of this dataset, I assume. Indeed, it's probably a data issue ,or not an issue, but a “characteristic” of this dataset.
1395	What's the preferred hashtag?	"Jeff
There is a link here, as I shared in Slack. If you click on the Blocks page, anywhere in the UI here, there’s Prefect docs right here on the bottom left, if you're logged into Prefect Cloud."
1396	What's the preferred hashtag?	"Alexey
No. I mean, we kind of told you. Yes, you maybe didn't see the intro video. But, sorry. No, it's late. We will actually recalculate the points for the second homework (HW 1B) because in the forum, it said that you can get up to one point, but we calculated that the cap there was seven. We are going to recalculate that. Thus, people who got a lot of points will get just one. Sorry about that. But don't focus on points. If you go to the leaderboard, we know that the person with this hash (722366d8b29ece9be3a7605363562c7c60d6918e) did seven links. Do you know who that is? Probably you don't. 
These points are virtual. It's nice to have a good score, but remember that these points mean whatever you want them to mean – not more than not less. Maybe it's okay if you get a few points less than others for the first homework, but the important thing here is consistency. If you just do your homework, even without doing any Learning in Public stuff, which I think you should do (you should post about the course). If you don't do this and you just do all the homework, by the end, you will be somewhere in the top 20. If you look at the leaderboard from the previous iteration, you will see that not everyone stuck around till the end. 
If you just do your homework, don't worry about these points, and you'll be fine. Sorry about that. I know that it can be discouraging that you didn't get as many points as others, but don't worry about that. Focus on learning. Maybe Michael has some other suggestions or some other things that I didn't mention, because he took part last year. But I think he was one of the people who did a few posts every week, right?
Michael
Yes. Actually, when I went through it, I missed the whole… you can get up to seven points, but I was just doing one every week and I just stuck with that. I was still near the top. I think the real value in that isn't necessarily the points but just being comfortable sharing – writing articles and making videos. That's the real value there. But everyone likes the clout value, too. As Alexey said, it will fluctuate, so if you keep up with it going forward, you'll probably be just fine."
1397	What's the preferred hashtag?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
1398	What's the preferred hashtag?	In the homework forum, you will see that it's #dezoomcamp
1399	What's the preferred hashtag?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1400	What does CI mean?	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
1401	What does CI mean?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1402	What does CI mean?	"Alexey
CI stands for continuous integration. This is a way to – let's say you wrote some code, and then you push it to GitHub. Then on GitHub, there's GitHub actions, which is a way to automatically run some checks on your code – run tests, deploy things somewhere. Actually, again, I will do a shameless plug – in our MLOps Zoomcamp, we talked about GitHub actions in the best practices module. Right now, don't worry about this. Focus on your projects. But after you do your projects, it's really worth spending some time learning about best practices. These best practices include things like writing tests, using make files and also tools like CI/CD, infrastructure as code – we cover all that in the module. But if you are interested in these things, maybe it's actually worth taking that course too. We'll have another iteration in May. Not super soon – I’m just telling you that to get you excited."
1403	What does CI mean?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1404	What does CI mean?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1405	How many portfolio projects do we need to have before we can say, “Now I can apply for jobs.”?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1406	How many portfolio projects do we need to have before we can say, “Now I can apply for jobs.”?	"That's also a very generic question. It depends on what kind of data you have. Can you take a sample from this data or do you need to use the entire dataset? Also, as I said, what kind of data is it? Is it images? Is it text? Is it tables? I would just read a part of this. I actually don't know if it's possible with the HDF format. Maybe you can just read the first couple of thousand rows? I don't know. The best practices could be, let's say – the first one is to take a sample from this. 
The second one is maybe seeing how you can parallelize it with something like Spark. Then again, I don't know exactly what HDF format is and if you can read with Spark. Maybe not. Maybe you will need to first turn it into something else and then read it with something like Spark. Then the third one would be to get a big machine with 500 megs of RAM, for example, and then just read it from there. The last option seems to be the easiest one if you actually need to read all the data. In some cases you might not. Is it related to neural networks? I think some of the data was saved in this format. 
I might be wrong, but I think you might iterate over this data and then read it in chunks. You don't need to read the entire thing. You just read one gigabyte at a time, for example. That's another strategy - chunking. This is actually something that Spark relies on. When dealing with large datasets, Spark chunks the dataset into small partitions, and then it processes each partition separately. Then it combines the result at the end."
1407	How many portfolio projects do we need to have before we can say, “Now I can apply for jobs.”?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1408	How many portfolio projects do we need to have before we can say, “Now I can apply for jobs.”?	Zero. You can start applying for jobs right now. You don't have to have a portfolio for that. Maybe you can use your Zoomcamp projects as a portfolio. That's a very good idea. Maybe one-two should be good, but you can find a job without any projects in your portfolio at all. It also can happen that you don't find a job even with 10 projects in your portfolio. There are different situations – everyone has different backgrounds, everyone has different problems. So I don't think there’s a “one size fits all” answer to that. I'd say one or two is helpful to have.
1409	How many portfolio projects do we need to have before we can say, “Now I can apply for jobs.”?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1410	I don't have credit cards, how will I create a GCP account for free?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
1411	I don't have credit cards, how will I create a GCP account for free?	Everything you do here is individual. You don't form teams.
1412	I don't have credit cards, how will I create a GCP account for free?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1413	I don't have credit cards, how will I create a GCP account for free?	"Alexey
No. I mean, we kind of told you. Yes, you maybe didn't see the intro video. But, sorry. No, it's late. We will actually recalculate the points for the second homework (HW 1B) because in the forum, it said that you can get up to one point, but we calculated that the cap there was seven. We are going to recalculate that. Thus, people who got a lot of points will get just one. Sorry about that. But don't focus on points. If you go to the leaderboard, we know that the person with this hash (722366d8b29ece9be3a7605363562c7c60d6918e) did seven links. Do you know who that is? Probably you don't. 
These points are virtual. It's nice to have a good score, but remember that these points mean whatever you want them to mean – not more than not less. Maybe it's okay if you get a few points less than others for the first homework, but the important thing here is consistency. If you just do your homework, even without doing any Learning in Public stuff, which I think you should do (you should post about the course). If you don't do this and you just do all the homework, by the end, you will be somewhere in the top 20. If you look at the leaderboard from the previous iteration, you will see that not everyone stuck around till the end. 
If you just do your homework, don't worry about these points, and you'll be fine. Sorry about that. I know that it can be discouraging that you didn't get as many points as others, but don't worry about that. Focus on learning. Maybe Michael has some other suggestions or some other things that I didn't mention, because he took part last year. But I think he was one of the people who did a few posts every week, right?
Michael
Yes. Actually, when I went through it, I missed the whole… you can get up to seven points, but I was just doing one every week and I just stuck with that. I was still near the top. I think the real value in that isn't necessarily the points but just being comfortable sharing – writing articles and making videos. That's the real value there. But everyone likes the clout value, too. As Alexey said, it will fluctuate, so if you keep up with it going forward, you'll probably be just fine."
1414	I don't have credit cards, how will I create a GCP account for free?	You will not be able to do this because they require a credit card.
1415	I see another question in the live chat about Kedro. Have you used Kedro, Tim?	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
1416	I see another question in the live chat about Kedro. Have you used Kedro, Tim?	"Tim
No, I haven't. 
Alexey
Do you know what it is?
Tim  
No, actually.
Alexey
This is a tool for building machine learning pipelines. We had a workshop about Kedro here on our channel. You can check it out here. The idea behind Kedro – this is for training your models. This is not for serving. This is for splitting… for the projects for the course that you will need to do, you need to come up with a train.py file, but Kedro is a way to modularize (to break into separate chunks) this train.py file such that these chunks are more modular, I guess. You can have a separate function for preparing your dataset, for feature engineering, for doing all these things, and Kedro gives you an opinionated way of actually doing this. They give you a template, you follow this template, and then they run this. It's a cool thing.
Tim
Like a declarative ML pipeline. Got it.
Alexey 
Yeah, exactly. Eventually, at the end, it still runs on one machine. But it just gives you an easy way to split it into logical units, let's say, and also run it. It takes care of the pipeline orchestrator – it takes care of running things in the order unit.
Tim 
Right. I'd imagine there would be one final step once you've trained your model, if you like it – that would be a declarative BentoML deploy step.
Alexey
Exactly. For example, if you use MLflow, you save your model in the registry, and then another step would be to export it to Bento. Or immediately to Bento, skipping MLflow. Then, probably, there could be a conditional – if performance in validation is above certain thresholds, then do that. Otherwise, don't do anything. If you're interested in Kedro, check out this workshop with Merel."
1417	I see another question in the live chat about Kedro. Have you used Kedro, Tim?	Yes. For a time series problem, you should never shuffle. For a time series problem, you should do a time-based split. Let's say you have data for one year, you take, for example, the first nine months for training, then two months for validation and one month for test. Or something like that. You need to do a time-based split here and shuffling is wrong. Because if you shuffle, what can happen is that you can accidentally kind of see into the future. In your training dataset, you might have a data point from December – your model will see into the future, and it's not supposed to do this.
1418	I see another question in the live chat about Kedro. Have you used Kedro, Tim?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1419	I see another question in the live chat about Kedro. Have you used Kedro, Tim?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1420	How to check homework evaluation?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1421	How to check homework evaluation?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1422	How to check homework evaluation?	We haven't evaluated your first homework yet, because you still have some time to submit your homework. After it is done, you will see the evaluation.
1423	How to check homework evaluation?	I guess, use validation.
1424	How to check homework evaluation?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1425	I started ML Zoomcamp with Windows and it was difficult in deployment. What are the best system requirements for Section 7?	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
1426	I started ML Zoomcamp with Windows and it was difficult in deployment. What are the best system requirements for Section 7?	For Section 7, Windows should work. I tested it in, actually tested it in BSL, in Ubuntu. But on Windows, it should also work. If you want, I can test it, but I think that some students already did it and it worked on Windows. Remember that if you do this on Windows, (this is actually in our FAQ) 0.0.0.0. is not the same as local host. If you just run BentoML serve, and then the rest of the command, it will say that you need to open this URL (http://0.0.0.0:3000/) but this URL will not open – you need to replace the zeros with the local host. There is already a section in the FAQ about that. Apart from that, I don't expect to have any problems on Windows, so you can keep using Windows. As long as you have Docker installed, you should not have any problems.
1427	I started ML Zoomcamp with Windows and it was difficult in deployment. What are the best system requirements for Section 7?	Yeah. If it works for you, use it. I don't mind.
1428	I started ML Zoomcamp with Windows and it was difficult in deployment. What are the best system requirements for Section 7?	"If you take statistics for machine learning education, the first lecture will be this – you'll probably derive the formula for logistic regression and there will be a slide with all these assumptions about the linear regression model. Here is the Google result: 
https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/assumptions-of-linear-regression/

Go through this. They are usually useful in practice, I guess. But what I typically do is train a model, and then rely on validation to tell me if my model is doing well or not. So if there is something wrong with my validation, I will see that the metric validation is bad. It's just a lot of trial and error, rather than figuring out all these theoretical assumptions. 
I don't want to say that all these assumptions are not relevant. If you work as a data scientist, they still are. But I also found that the more practical way to understand if you can apply linear regression to the data or not, is to just apply it and see if it works or not. If it works – if the results on the validation are good – then it means that you can apply your model to this dataset. 
Maybe some of the assumptions are violated. For example, multicollinearity – this is actually the case in the lectures, when we needed to regularize (add regularization to the model) this assumption was violated. You will see in the lectures what exactly happened. So check it out. I don't think I will be able to give a good answer, because I am not prepared to talk about this, but any statistics book or theoretical machine learning book explains this."
1429	I started ML Zoomcamp with Windows and it was difficult in deployment. What are the best system requirements for Section 7?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
1430	Are there any recommended readings for every week?	No, but you can suggest some stuff in Slack. If we see that this is useful, we can also include this in the project repo
1431	Are there any recommended readings for every week?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
1432	Are there any recommended readings for every week?	I would say to focus on projects, not on tools. In this course, you will get a chance to do this. If you open a job description for data engineers, you will see a lot, a lot, a lot of different technologies. Maybe think of a project that you can do for this particular company and do this project using, for example, the tools you learn here or the tools you want to learn, and build a portfolio. I guess this is more of a tip for getting a data engineering job, rather than for somebody who's starting their data engineering career. But if you already have a job, then maybe think of the business impact of what you need. I think I will not spend too much time talking about that because this requires a one hour long discussion. Perhaps we should have a podcast interview about that. If you can think of any potential guests for this podcast interview, you can let me know and we can try to invite these people and talk about this in more detail. One thing I want to share with you is that in our YouTube channel, we had a few interesting talks. We had this Getting a Data Engineering Job with Jeff talk. It’s a very nice presentation. Then we also had an interview with Jeff. So these two podcasts are quite interesting. You can also check out other ones. I think you'll find a lot of interesting content here.
1433	Are there any recommended readings for every week?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1434	Are there any recommended readings for every week?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
1435	How can we submit homework assignments?	No, you cannot.
1436	How can we submit homework assignments?	Everything you do here is individual. You don't form teams.
1437	How can we submit homework assignments?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
1438	How can we submit homework assignments?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
1439	How can we submit homework assignments?	"Alexey
You will see links soon. I think we already have the first one, but we just need to put it on GitHub. We will communicate over everything in Telegram and then the automator will post the link from Telegram to Slack. So you will see if you're in Slack."
1440	Any tips for networking with people related to ML/data science? How to create an eye-catching profile for other people and companies?	"Tim
Yeah, we have a lot of users who use MLflow and BentoML together. Once you have a model that’s sort of your “finalized” model – you save it to your MLflow registry and then (it depends on what your CI/CD pipeline looks like) but BentoML has a command to import from an MLflow registry. You can tell which MLflow model is the one that you want to deploy. It's very similar to save_model, except instead of pulling from your local environment, you're pulling from an MLflow registry. So you import the model, it automatically gets pulled into your Bento and then you can deploy it. On the BentoML documentation site, there's a big framework document on how to integrate with MLflow. 
Alexey
When searching, one of the suggestions was BentoML vs MLflow. I guess there are also some similarities because with MLflow, you can serve models. 
Tim
The thing about serving models is that it's a part of the pipeline that you just have to do. Every single framework out there has some way to serve a model – you train the model and then it just puts it up there to be able to serve. Typically, frameworks don't specialize in serving those. BentoML specializes in making the serving part really, really fast and really, really easy. What you find a lot of the time is, when you have serving logic in a really large end-to-end pipeline, it just puts the model there and then the only thing that you could submit to the model is the data. But we know in practice that there's usually business logic around this – pre-transformation logic, post-transformation logic, there's the version of the model that was saved. There are a lot of these other components that you want around the model, not just the model for inference. And of course there's the performance layer underneath. The BentoML architecture kind of brings that all together into one deployable, rather than just giving you one point to call inference for your model. 
Alexey
That is a comment in the live chat that says “There is PyCaret, too.” This is how you use PyCaret with Bento. 
Tim
Right. BentoML has lots and lots of integrations and we're only adding to them every day. The idea is to get the model from wherever you built it, bring it into your Bento, and then you can deploy it anywhere. Once it's in the Bento – in this standard packaging – it's nice because then we've got lots of different tools to deploy to Lambda, SageMaker, and lots of different places. And the places to deploy, we're adding to every week and month as well.
Alexey  
Since you can have a Docker container at the end – you do “bento_containerize” right? That's the command? And then you have a Docker container (Docker image) that you can deploy everywhere where you can deploy a Docker container, which is pretty much every place on the internet, right? 
Tim
Right. Some places, though, require specialized… 
Alexey
Like Lambda, right? 
Tim
Yeah, like Lambda won't accept just a normal Docker container. You have to actually create special endpoints. When we deploy to Lambda, we create those endpoints for you and make sure that it's behind an API gateway and things like that. For certain services, there are a lot of nuances that we kind of take care of for you – as long as you're in the Bento standard format.
Alexey 
For those who have no idea what MLflow is, and why we are talking about this – you don't need to worry about this right now. But at some point, this tool and other similar tools will be quite handy. You can check out our MLOps Zoomcamp course, after you finish this one. Don't try to do multiple courses at the same time. That could be too much. But of course, if you're curious, go check it out."
1441	Any tips for networking with people related to ML/data science? How to create an eye-catching profile for other people and companies?	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
1442	Any tips for networking with people related to ML/data science? How to create an eye-catching profile for other people and companies?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1443	Any tips for networking with people related to ML/data science? How to create an eye-catching profile for other people and companies?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
1444	Any tips for networking with people related to ML/data science? How to create an eye-catching profile for other people and companies?	"I think this is actually two questions. How do you network with people? I think we even have a few podcast episodes about that. I like the one with Juan Pablo, From Math Teacher to Analytics Engineer, where he talks about networking – how to meet people at meetups and so on. So maybe check out this podcast. What you can also Google is a set of ice breaking questions. When you approach people at meetups, you can just ask them these ice breaking questions. I'm not sure if I'm very good at this to give you any good advice. But yeah, please check our podcasts. The one with Juan Pablo is the first one that comes to mind. Maybe we should have a special podcast episode that is for networking. 
Then how to create an eye-catching profile for other people and companies? We also had an episode about that. There are quite a few. I like this one: Standing Out as a Data Scientist with Luke. By the way, we will have another follow up episode with Luke in January, so check it out. Luke says (if I can give you a TLDR for this) you need to pick a niche and do a few projects in this niche. You also need to understand what companies in this niche you are really looking for. This will make you attractive to these companies in this niche."
1445	Should we research papers to stay up to date with recent advances in AI? Most are focused on deep learning, transformers, diffusion models, DALL-E, etc. How?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
1446	Should we research papers to stay up to date with recent advances in AI? Most are focused on deep learning, transformers, diffusion models, DALL-E, etc. How?	That's a very similar question to when we should use the lambda and when we should use Kubernetes. If you want to use Kubernetes, then go with TensorFlow Serving. Otherwise, TensorFlow Lite. There is one thing where you have to use TensorFlow Lite – where you cannot use TensorFlow Serving – and this is when you want to deploy your model on a mobile device. I have a phone here. If you want to create an app with a model, then you will have to use TensorFlow Lite because you will not be able to use TensorFlow Serving. TensorFlow Serving also adds a bit of complexity. You will need to have this gateway service, you will need to have GRPC service, so that becomes more complex. If you have a lot of load, then you need to do this otherwise, TensorFlow Lite.
1447	Should we research papers to stay up to date with recent advances in AI? Most are focused on deep learning, transformers, diffusion models, DALL-E, etc. How?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1448	Should we research papers to stay up to date with recent advances in AI? Most are focused on deep learning, transformers, diffusion models, DALL-E, etc. How?	"Depends. [chuckles] Again, sorry for answering this way. So, what is your end goal? Do you want to be a researcher or do you want to work as a machine learning engineer? Depending on what you answer, the answer to this question will be different. If your answer is “machine learning engineer,” then the answer is you don't stay up to date with recent advances in AI. You don't have to stay up to date, let's say. If you want, just go on Twitter and see what's trending and then read these articles. Or Reddit, r/machinelearning, for example. But if it's too much for you, then just don't stay up to date. Nobody is forcing you to do this. 
To know the basics well enough and for actual practical applications of machine learning, I recommend attending conferences – something like PyData, for example. I think PyData has chapters everywhere, so you can find a PyData conference in the area where you live, and then just attend it. Or watch PyData conferences online. And it's not just PyData. You can go to DataTalks.Club webinars, workshops, and also learn from them. Of course, these are not the only channels that you can find. YouTube is full of information that you can use for getting experience from people from the industry. This is how you stay up to date for machine learning engineering."
1449	Should we research papers to stay up to date with recent advances in AI? Most are focused on deep learning, transformers, diffusion models, DALL-E, etc. How?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1450	I chose a dataset problem above my skill level at this moment. The model is not giving good results. Will a bad model score affect my project’s evaluation?	"Tim
Yeah, we have a lot of users who use MLflow and BentoML together. Once you have a model that’s sort of your “finalized” model – you save it to your MLflow registry and then (it depends on what your CI/CD pipeline looks like) but BentoML has a command to import from an MLflow registry. You can tell which MLflow model is the one that you want to deploy. It's very similar to save_model, except instead of pulling from your local environment, you're pulling from an MLflow registry. So you import the model, it automatically gets pulled into your Bento and then you can deploy it. On the BentoML documentation site, there's a big framework document on how to integrate with MLflow. 
Alexey
When searching, one of the suggestions was BentoML vs MLflow. I guess there are also some similarities because with MLflow, you can serve models. 
Tim
The thing about serving models is that it's a part of the pipeline that you just have to do. Every single framework out there has some way to serve a model – you train the model and then it just puts it up there to be able to serve. Typically, frameworks don't specialize in serving those. BentoML specializes in making the serving part really, really fast and really, really easy. What you find a lot of the time is, when you have serving logic in a really large end-to-end pipeline, it just puts the model there and then the only thing that you could submit to the model is the data. But we know in practice that there's usually business logic around this – pre-transformation logic, post-transformation logic, there's the version of the model that was saved. There are a lot of these other components that you want around the model, not just the model for inference. And of course there's the performance layer underneath. The BentoML architecture kind of brings that all together into one deployable, rather than just giving you one point to call inference for your model. 
Alexey
That is a comment in the live chat that says “There is PyCaret, too.” This is how you use PyCaret with Bento. 
Tim
Right. BentoML has lots and lots of integrations and we're only adding to them every day. The idea is to get the model from wherever you built it, bring it into your Bento, and then you can deploy it anywhere. Once it's in the Bento – in this standard packaging – it's nice because then we've got lots of different tools to deploy to Lambda, SageMaker, and lots of different places. And the places to deploy, we're adding to every week and month as well.
Alexey  
Since you can have a Docker container at the end – you do “bento_containerize” right? That's the command? And then you have a Docker container (Docker image) that you can deploy everywhere where you can deploy a Docker container, which is pretty much every place on the internet, right? 
Tim
Right. Some places, though, require specialized… 
Alexey
Like Lambda, right? 
Tim
Yeah, like Lambda won't accept just a normal Docker container. You have to actually create special endpoints. When we deploy to Lambda, we create those endpoints for you and make sure that it's behind an API gateway and things like that. For certain services, there are a lot of nuances that we kind of take care of for you – as long as you're in the Bento standard format.
Alexey 
For those who have no idea what MLflow is, and why we are talking about this – you don't need to worry about this right now. But at some point, this tool and other similar tools will be quite handy. You can check out our MLOps Zoomcamp course, after you finish this one. Don't try to do multiple courses at the same time. That could be too much. But of course, if you're curious, go check it out."
1451	I chose a dataset problem above my skill level at this moment. The model is not giving good results. Will a bad model score affect my project’s evaluation?	"Tim 
Not really. We have users who use both Streamlit and Gradio. I think Streamlit and Gradio are a bit more about the presentation side. We do have a couple of users who have asked us about deeper Streamlit or Gradio support and I think that's something that's probably coming up in the next few months.
Alexey
For those who don't know what Streamlit or Gradio is – this is a way to create an interface for your services. Well, Gradio is focused on machine learning, meaning you create interfaces specifically for models. And then Streamlit is just a Python package for creating interfaces. We actually had a project at DataTalks.Club about Streamlit. I actually even have a tweet about this. This is what Streamlit looks like. In the video, you can see that you have this interface, and then based on the toggles that you move the core of your model changes. It gives you an easy way to build an interface like that. Meanwhile Bento focuses on quite a different use case, right? It’s not building interfaces, but serving models.
Tim 
Right, yeah. I think one of our users was asking for a really easy way, so that once you have your model and your Streamlit UI to be able to create a Bento deployment with one line in that way, anybody that you gave your Streamlit app to would be calling to that service and it could work anywhere. Which is a nice feature, I think.
Alexey 
Yeah, I think it is. I imagine if you have a Pydentic class there, then you can easily generate a Streamlit app for that.
Tim 
Right. Oh, that's a good idea. I hadn't thought of that. Yeah.
Alexey
And then in addition to that, the Swagger UI that you have, could be like having the actual interface. 
Tim
Oh, yeah, that would be cool. [chuckles]
Alexey
That could be a good Hackathon project, right?
Tim 
Yeah, for sure.
Alexey
From what I see – let's say you’ve built a model. Then you deployed this model and you have this nice API. You can query it with CURL. But what if you want to demo this project to your manager, or a manager of your manager, or somebody who does not necessarily have a technical background? If you give them the URL and say, “Okay, you need to execute the CURL command with a post request,” they would be like, “CURL what? What do you want from me? Just show me how it looks.” This is when tools like Streamlit come in handy. Instead of giving them a command line interface to query it, you just give them “Okay, this is the link. Play with this.” If Bento could do something like that – automatically generate things like that – many data scientists would thank you.
Tim
Yeah, I was just looking because I figured there's a library that turns Pydantic models into Streamlit apps. It looks like there is one that does that. I wonder if we could just plug into that app and then have that same thing. 
Alexey
That's cool. 
Tim
Yeah, that's awesome. That's kind of the cool part of open source – there are so many people out there with so many different projects and we get to collaborate with them and kind of build the best collaborations and combinations of these tools."
1452	I chose a dataset problem above my skill level at this moment. The model is not giving good results. Will a bad model score affect my project’s evaluation?	I guess not. That's the process of learning? Maybe more senior people don't… Yeah, I see where you're going with this. Senior people are not supposed to go bankrupt and default. They are probably supposed to pay back the debt. I guess these are the particularities of this dataset, I assume. Indeed, it's probably a data issue ,or not an issue, but a “characteristic” of this dataset.
1453	I chose a dataset problem above my skill level at this moment. The model is not giving good results. Will a bad model score affect my project’s evaluation?	"I don't know how to answer that, because I work as a principal data scientist. I work on pretty much everything that the company needs me to work with. This means that all the models are related, to some extent, to online classifieds – to online marketplaces. I need to deal more with domain expertise, maybe, rather than ML knowledge. I guess that's the answer. 
We have a lot of different use cases – we have recommender systems, we have search, we have moderation. We actually have an article about this. For example, Learning-To-Rank: Sorting Search Results. My colleague wrote this one. I think we have an article about data science, Data Science at OLX which happens to be written by me. You can see what kind of use cases we solve. Maybe it could be interesting for you."
1454	I chose a dataset problem above my skill level at this moment. The model is not giving good results. Will a bad model score affect my project’s evaluation?	"If you look at the criteria, you will see that model performance is not one of the criteria. If your model does not give you great performance, it will not affect the score that you will get at the end. This is intentional. The reason it's like that is because it's very hard to control this. The performance on the dataset of a model is very dataset-specific. For some datasets, this is how it is. In some cases, for example, for click prediction – if you want to predict if somebody will click on an ad or not, the performance of the model is usually bad. You cannot predict with very high probability if a person will click on an ad or not. But it doesn't mean that these models aren't useful. But the performance that you will observe on the validation dataset will be bad. And that's okay. 
This is just a characteristic (a feature) of this particular dataset – of this particular problem. That's why it's not a criterion, but you do need to try to train multiple models, as you see here (in the criteria for model training), and then try to tune their parameters. In the end, maybe you will not have a great model – you will not have great performance for this model – but at least you tried and this is what we evaluate here. We don't evaluate the performance, but whether you tune the parameters."
1455	Alexey, you should train some moderators to help you on Slack. Share the load – otherwise in the long run you might lose the joy of doing this.	I don't particularly enjoy moderating Slack, to be honest. [chuckles] So if you want to share the load and help me, please reach out. I will definitely share the joy of moderating Slack with you. That's a good suggestion. Thank you.
1456	Alexey, you should train some moderators to help you on Slack. Share the load – otherwise in the long run you might lose the joy of doing this.	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1457	Alexey, you should train some moderators to help you on Slack. Share the load – otherwise in the long run you might lose the joy of doing this.	"It all depends on what you mean by “full stack ML engineer”. As you’ve noticed, data engineer Zoomcamp is not about machine learning – it's about data engineering. If in your opinion a “full stack ML engineer” needs to know data engineering, then you should do data engineering. If not, then no. From my point of view, there is a “full stack of data scientist”. What I put in the definition of a full stack data scientist is – a person who can do all the steps in the CRISP-DM process. They can help product managers with business understanding, they can work with analysts in data understanding, they can help data engineers in the data preparation step, they can do the modeling, and they can deploy the models. If you want to be that kind of person, then, of course, you also need to learn a bit of product management and analytics, which we don't have courses for. But data engineering – preparing data before it goes to a model – will certainly be helpful. 
Also, machine learning ops Zoomcamp (MLOps Zoomcamp) will be helpful as well. I would suggest, if you really want to focus on machine learning engineering, then probably doing MLOps Zoomcamp will make more sense for you. But data engineering Zoomcamp will be quite useful in the future, because data scientists and ML engineers tend to work on data pipelines as well. 
Maybe not all the content will be useful for you. For example, the content about data warehousing – as a data scientist, I don't find it super useful in my work to me personally. Also, the module about analytics engineering – it's nice, but it's not something I do day to day. But the modules about batch, the modules about Spark, the modules about streaming – this is something that can be useful, because this is something we could use in building machine learning pipelines as well. 
To put data in a machine learning model, we need to prepare it – we need to create data pipelines – and this is pretty relevant. But you can just check it out. All the content is already there. You can just go and check it out. If you like it, do it – if you don't, don’t. I would suggest maybe going with this one (ML Zoomcamp)."
1458	Alexey, you should train some moderators to help you on Slack. Share the load – otherwise in the long run you might lose the joy of doing this.	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
1459	Alexey, you should train some moderators to help you on Slack. Share the load – otherwise in the long run you might lose the joy of doing this.	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1460	I am working on a Kaggle challenge with a topic that is interesting for me. Is it possible to use this work for upcoming projects?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1461	I am working on a Kaggle challenge with a topic that is interesting for me. Is it possible to use this work for upcoming projects?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1462	I am working on a Kaggle challenge with a topic that is interesting for me. Is it possible to use this work for upcoming projects?	"Actually, I don't remember exactly how I do this. The right approach would be to use only train data and not use validation data or test dataset. This is done on purpose for exactly this reason that you mentioned. We want to model a situation in which there is some unseen data – it's truly unseen, we do not see that all – and anything can happen to this unseen data, including the situation that you described, such as there is a value that is only present in the test data. 
Imagine that there is a new iPhone – when you train your model, this new iPhone didn't exist (you didn't have this category) but you apply this model, and then, all of a sudden, there is a new iPhone. How will your model react to this? You need to somehow model this situation. This is totally normal. This can and will happen in practice. We just want to understand what would be the effect of that. That's why we need to see this in training data. This is good and this is how it should be. I hope that is a good answer."
1463	I am working on a Kaggle challenge with a topic that is interesting for me. Is it possible to use this work for upcoming projects?	Okay. Yeah
1464	I am working on a Kaggle challenge with a topic that is interesting for me. Is it possible to use this work for upcoming projects?	Yes, it is definitely possible. By all means, please use it. You will have to add extra stuff, of course, like you will need to cover the deployment part. But it's completely up to you where you get data from. If it's a Kaggle competition, it's a Kaggle competition. But since this is your code, you’re not stealing it from anywhere, or rather “copying” not stealing. If you don't, I suppose, use this code for any other course, then it's totally fine.
1465	If using a local laptop, how much free storage will be required?	"Alexey
I don't know when this question was asked, but we did extend it."
1466	If using a local laptop, how much free storage will be required?	Yes. If Michael was here, he would probably confirm this because this is the background he has. He's an analyst. We also had other analysts who did pretty well in the course. So yeah, certainly you can do well. As a beginner in data engineering, I would say that we don't expect any data engineering knowledge for taking this course. If you're a beginner already – if you know some data engineering concepts – then it's more likely that you will do well in this course.
1467	If using a local laptop, how much free storage will be required?	We’re still in progress (as of January 26, 2023). I thought we would do this earlier. But it's still not finished.
1468	If using a local laptop, how much free storage will be required?	I don't know – Docker can be quite hungry for storage. 50-100 GB should be enough, I guess.
1469	If using a local laptop, how much free storage will be required?	Building projects. Maybe build another one, where instead of batch, you use streaming, maybe. If you're interested in a particular employer, try to build a project that is similar to what they’re working on. For example, if you want to apply for Spotify, try to find some music data and build a project for that.
1470	Can I use GitHub Codespaces to make the Prefect Docker deployments?	Yeah, we can pat you on your shoulder. [chuckles] There’s the position on the leaderboard. The leaderboard is anonymized so nobody will know that this hash belongs to you, but if you want, at the end we will have a forum where you can share your actual contact information – your actual information about you and you will be the first in this public leaderboard. This is how I can kind of give you a reward for that. But apart from that – eternal glory? Maybe that?
1471	Can I use GitHub Codespaces to make the Prefect Docker deployments?	We currently do not have any course partners yet. If you work at a company and you want to partner with us, please reach out and we will see how we can do it. In one of our courses, which was our first iteration of the Machine Learning Zoomcamp, we partnered with a company called Delphi. They got two interns from our course and they were quite satisfied with the outcome. You can actually read more about the work of these interns in our articles – Interview with Valerii Chetvertakov and then another one, Interview with Ken Wu. You can learn more about the interns and if you think that, at your company, you need good interns or juniors or you want to partner in any other way, please reach out and we can see how to make it work.
1472	Can I use GitHub Codespaces to make the Prefect Docker deployments?	This question is tricky, because I haven't yet published the homework for streaming. I think it's in progress right now. You can study streaming right now, but you will need to do your homework and submit the homework later, when it's released. But let's say you don't feel like studying data warehousing. Then just don't. I think you should, but let's say that you don't want to study DBT and you just want to do streaming. Feel free to skip DBT Batch and focus on streaming. That's fine. You don't have to do some models that you don't think are relevant for you.
1473	Can I use GitHub Codespaces to make the Prefect Docker deployments?	"Alexey
I don't know when this question was asked, but we did extend it."
1474	Can I use GitHub Codespaces to make the Prefect Docker deployments?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1475	I would like to create a portfolio of projects to get experience and showcase my skills. Can you recommend this strategy to make up/find projects?	"There was no registration deadline at all for this course. So you can still register. You can still sign up and you can follow the course. You will not be able to submit the homework assignments where the due date is already over because there are already solutions posted, so it doesn't make much sense to submit homework when the solutions are already there. You can follow the course but you will not be able to submit the homework. You can check our frequently asked questions and I do recommend doing this. Please remember the rule – if you have a question, first go check the frequently asked questions, and if you don't find the question there, go and ask in Slack. 
For this particular question, it’s already there. “Don't worry, you can take the course.” Maybe right now it's becoming a little bit more challenging to catch up with everything, but note that you can skip the midterm project. Right now, if you want to catch up with everything, if you skip the midterm project, it's fine. You will be able to do two capstone projects and still get the certificate. Also, you can just take the course in self-paced mode. There is nothing wrong with that either."
1476	I would like to create a portfolio of projects to get experience and showcase my skills. Can you recommend this strategy to make up/find projects?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1477	I would like to create a portfolio of projects to get experience and showcase my skills. Can you recommend this strategy to make up/find projects?	Yeah, it's probably not the best practice. I wouldn't do this in a real-life situation. In a real-life situation, we simply will not have unseen data. But the idea here was to get a dataset that is already familiar to you – the dataset we used in the previous homework – and just convert it to a binary problem. That was the goal we had in mind – how can we make it easier for you to learn these things without overloading you with a new dataset? That was the simplest approach we decided to take to actually do it here. Another alternative could have been, instead of asking you to compute the mean, just give you the number, but with the mean, it's a little bit more interesting, I think.
1478	I would like to create a portfolio of projects to get experience and showcase my skills. Can you recommend this strategy to make up/find projects?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1479	I would like to create a portfolio of projects to get experience and showcase my skills. Can you recommend this strategy to make up/find projects?	"Yeah, I can. First of all, you’ll have to do projects here for this course too. You can treat them as real projects – something that you will show to potential hiring managers. When you do this, keep this in mind – think of it as though you're not creating this project for this course. You're not creating this project for your peers to evaluate. Instead, think of this as if you're creating this for a hiring manager – one who will be looking at your project and trying to evaluate it. Keep this in mind when you work on this. Add as much documentation as possible, as much context as possible. 
The hiring manager might not be aware of the course and they might not know some things, so give as much context as they need to figure out what's going on. I don't think this is really the answer to the question you asked. I think it will definitely showcase your experience and skills if you write very good documentation for your projects. Concerning strategy – just find a dataset. Go to Kaggle, where you will see a news feed. For example, here’s a Jupiter Network Global AI challenge. You just open it up and look at what people do here. You will see some data.
[image for reference]
You can see what people work on and see what it’s about. In this example it’s “Predict hourly sales for cash registers across a retail chain.” This looks like a cool project. So how about you get this dataset and work on this? If you run out of ideas, just open Kaggle – you will see what people are working on and you can work on the same thing. It’s as easy as that. You can also spend a bit more time trying to figure out what you like and then find a dataset about that. As I said last time, the best thing is trying to think what is important for you, and then getting a dataset for that – collect the dataset yourself. 
If you have problems with that, just go to Kaggle and pick whatever people are working on. Here are some other projects [scrolls through Kaggle] Hourly energy consumption – looks pretty interesting. Data science job salaries – also pretty interesting. Any of these things can be used for a project. Again, Kaggle is not the only place where you can find data. Just today, we released a dump of our Slack into a special channel – I think it's called “slack dump”. There was an announcement in the general chat. You can also check it out, maybe do some analysis of that, and see what you can find there."
1480	In the credit scoring dataset, if I'm not wrong, when we were splitting with the decision tree, when seniority was high, it tended more to “default” status. Is this an anomaly?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1481	In the credit scoring dataset, if I'm not wrong, when we were splitting with the decision tree, when seniority was high, it tended more to “default” status. Is this an anomaly?	"Tim
I think a couple people asked this in ML Zoomcamp Slack. I was thinking about this a little bit. I think it's sort of a philosophical thing [chuckles] where we think about models and Bento is sort of separate a little bit from code. So your project directory is where all the stuff that you'll commit to Git. I don't think we think that it's the right model to commit your models to Git, for example. We think of ourselves a little bit more like Docker. You don't have Docker images in your project directory, right? You just have your Docker file and that's what you commit to Git. In the same way, we store your models and your Bento in that dedicated directory, and then sort of leave it up to you to push it to different places where you might build or where you might need it
Alexey
Do you know why you decided to save it locally on your local file system? For example, what MLflow does – you can save it to S3 directly, but in the case of Bento, you always first save it to local storage and then you decide “This model I want to export, but for the rest, I don't care.”
Tim
Right. First, I think it’s because that's the easiest way to do it, but also because I think with BentoML, the workflow is that a lot of times, somebody's building a model on their laptop, or somebody's building a model in a slightly less distributed system and they're assembling their Bento there. So I think that the fastest way to access that model is locally. We do give you the ability to push that model and pull that model from other places. But ultimately, when you assemble your Bento and then you assemble your container, all that stuff has to be there. I don't think we wanted separate abstractions for when a person is training and saving, versus when a person is building a Bento and pulling in the model and assembling the container. It's just the same abstraction, which is just a local repository which contains models.
Alexey 
Maybe you don't want to save every model every single time you change one single parameter to the cloud. [Tim agrees] Maybe at the end, after all the experiments, you want to save the final one. Meanwhile, for MLflow, the use case is different – you actually want to save every single experiment because you want to see the results of these experiments.
Tim
Right."
1482	In the credit scoring dataset, if I'm not wrong, when we were splitting with the decision tree, when seniority was high, it tended more to “default” status. Is this an anomaly?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1483	In the credit scoring dataset, if I'm not wrong, when we were splitting with the decision tree, when seniority was high, it tended more to “default” status. Is this an anomaly?	I guess not. That's the process of learning? Maybe more senior people don't… Yeah, I see where you're going with this. Senior people are not supposed to go bankrupt and default. They are probably supposed to pay back the debt. I guess these are the particularities of this dataset, I assume. Indeed, it's probably a data issue ,or not an issue, but a “characteristic” of this dataset.
1484	In the credit scoring dataset, if I'm not wrong, when we were splitting with the decision tree, when seniority was high, it tended more to “default” status. Is this an anomaly?	Yeah, it's probably not the best practice. I wouldn't do this in a real-life situation. In a real-life situation, we simply will not have unseen data. But the idea here was to get a dataset that is already familiar to you – the dataset we used in the previous homework – and just convert it to a binary problem. That was the goal we had in mind – how can we make it easier for you to learn these things without overloading you with a new dataset? That was the simplest approach we decided to take to actually do it here. Another alternative could have been, instead of asking you to compute the mean, just give you the number, but with the mean, it's a little bit more interesting, I think.
1485	How to fully utilize GCP credits? It's much more than this course will require. Also, guided end-to-end mini projects will be really helpful.	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
1486	How to fully utilize GCP credits? It's much more than this course will require. Also, guided end-to-end mini projects will be really helpful.	"Alexey
I'm not sure what this question means “how to fully utilize”. Well, come up with projects and utilize them. It's really up to you. I don't know how we can help you here. If you want to have some brainstorming sessions, just go to Slack and start a discussion there. I'm pretty sure we and the students will be very happy to take part in these discussions, contribute, and come up with some ideas. 
As for the end-to-end guided mini projects… yeah, sure. If you have some ideas, let us know. But then it also feels like another course. The projects we will have here are not super guided. They are more like, “This is the set of criteria you need to satisfy. Go and work on them.” It's kind of guided, but most of the time, you still do stuff on your own. But this is when you really learn. When you do a guided project, maybe you learn less than when you do a thing on your own. But again, in both these cases, let's start the discussion in Slack and see where it brings us because both these things are really interesting."
1487	How to fully utilize GCP credits? It's much more than this course will require. Also, guided end-to-end mini projects will be really helpful.	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1488	How to fully utilize GCP credits? It's much more than this course will require. Also, guided end-to-end mini projects will be really helpful.	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
1489	How to fully utilize GCP credits? It's much more than this course will require. Also, guided end-to-end mini projects will be really helpful.	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
1490	Do you recommend doing projects like those in #project-of-the-week (which seem to be more tutorial and discovery) to build a portfolio?	"Yes, this is a really good way to build your portfolio. By the way, if you don't know about #project-of-the-week, this is the repo. Well, we only had two iterations here. But here, it's more like a study group, where we focus on building and learning things. For example, we did this one about Streamlit – there is a task for day one, there's the task for day two. You are somehow on your own, but still at the same time with others, who are also going through this. This is, let's say, more independent. At the end of this thing, we will not have 30 people all with the same project – we’ll have 30 people with different projects. It's a different thing. It makes it a bit more interesting and unusual. 
It's definitely a good way to build a portfolio. If you have some ideas on what exactly you want to learn, you can suggest these ideas and we can do this as a #project-of-the-week. So yeah, I do recommend this approach. I think that’s one of the best ways to do projects is when you try to come up with a problem that you want to solve yourself. And you will have to do this for this course too. We will give you some guidelines, these guidelines will be this evaluation matrix that you’ve maybe seen, but you will actually have to find a dataset yourself, you will have to find a problem yourself, and then you will have to implement everything we did in the course yourself. At the end you will have a model that is deployed and could be used. And I think this is really good. This is what you want to have."
1491	Do you recommend doing projects like those in #project-of-the-week (which seem to be more tutorial and discovery) to build a portfolio?	"Alexey
It is quite important. I wouldn't say it's a must for all the projects. For example, if you use something like a tree-based model, then you shouldn't worry about scaling. I think I shared links to our Office Hours from the previous year. If you go to cohorts, and the previous year, and then you go to midterm project – in week ten, we show you how to use SciKit Learn pipelines. This is an amazing concept. So far, what we've been doing is Dictionary Vectorizer (dict_vectorizer) and then it was followed by a model. In the case of Bento, we needed to save this Dictionary Vectorizer separately, as an artifact (as a binary) and then we needed to load this and then we needed to apply the model. But with a pipeline, you can actually train a pipeline and just use BentoML to save the pipeline. It can be saved, right Tim? You can just take a pipeline and do BentoML.sklean.save_model and then the entire pipeline will be saved. Right? [Tim agrees] This is a cool thing. I don't have an example, but you can include this feature scaler here in your pipeline. There should be an example somewhere. I think we did that last year. There is an example without the pipeline. It should be here – classification, and then notebooks scaling this one. In this notebook, we show how to scale your features with standard scaling, or min/max scaler. If you check Office Hours #3 from last year I spoke more about that. It is an important topic. We intentionally did not include this because we cannot include everything, but this is an important topic. I recommend that you check it out. And using pipelines is cool because you will not need to save all this Scaler, Dictionary Vectorizer – all these things separately. Let's say with Bento, you can just take one pipeline, save it with Bento, and then you will have one binary on model. It makes things easier.
Tim 
And if you do have any kind of custom Tokenizer, or Scaler, or Vectorizer – you can always add it to that “custom objects” field.
Alexey
We talked about MLflow. It's very annoying, to be honest. With MLflow, I need to choose to save this in a file. Then I have a Pickle file and I tell MLflow, “Okay, there is an artifact that is already in this file that I want to upload together with the model.” So I upload it together with the model and then when I want to use it, I need to download this from the registry, then I need again to use Pickle to load this into the memory. There is too much overhead. With Bento, I really like how easy it is to save extra stuff.
Tim
Right. In our first version of BentoML, we were just saving models but then we had so many users saying “Well, how do I save my Tokenizer along with the model?” A lot of the time, the Tokenizer, the Vectorizer, is sort of one-to-one with the model. You may have a version of that as well. So it's nice to be able to version it with the model."
1492	Do you recommend doing projects like those in #project-of-the-week (which seem to be more tutorial and discovery) to build a portfolio?	I'm not sure I understand the question. The question is “Why test_train_split splits in two, but here, we split into three parts? For that there is a video in week one that explains why we need three datasets, not two. So you can go check it out. There is a problem called “multiple comparison problem” and that's why we use test/split to account for that. You will see later, in module three – there we can also use SciKit Learn train_test_split to split our original dataset into three parts and this is exactly what we will do.
1493	Do you recommend doing projects like those in #project-of-the-week (which seem to be more tutorial and discovery) to build a portfolio?	I think for binary features, just one and zero makes more sense than OneHotEncoding. OneHotEncoding will take care of that but you will have two columns that are the exact opposite of each other. So that is a bit redundant. I think it's better just to convert it to a Boolean column with ones and zeros.
1494	Do you recommend doing projects like those in #project-of-the-week (which seem to be more tutorial and discovery) to build a portfolio?	Maybe. I don't know what that means. But yeah. Maybe?
1495	Have you used logistic regression in your work?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1496	Have you used logistic regression in your work?	"Very, very often. This is probably the model that I use the most at my work. This is something I use pretty much for every project – for some projects. Not only is it a good first baseline, but also sometimes we just deploy this logistic regression, it works and then no further work is needed and we just leave it there. Sometimes these models are then improved with something like XGBoost or something, but logistic regression is like the workhorse of machine learning. It's used in many, many, many stations. 
Actually, in some situations, it's not really possible to use anything else except logistic regression. For example, at the previous company where I worked, which was an advertisement company, and there, it was very, very, very, very important to be able to make predictions very fast. Logistic regression is the best model for that. You cannot beat the speed of logistic regression with any other model. Maybe with a decision tree, but it will not be as good as logistic regression. Logistic regression works really well when you have a lot of features and this was the case in the company where I worked. It's really an important and useful model."
1497	Have you used logistic regression in your work?	Yeah, it's probably not the best practice. I wouldn't do this in a real-life situation. In a real-life situation, we simply will not have unseen data. But the idea here was to get a dataset that is already familiar to you – the dataset we used in the previous homework – and just convert it to a binary problem. That was the goal we had in mind – how can we make it easier for you to learn these things without overloading you with a new dataset? That was the simplest approach we decided to take to actually do it here. Another alternative could have been, instead of asking you to compute the mean, just give you the number, but with the mean, it's a little bit more interesting, I think.
1498	Have you used logistic regression in your work?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1499	Have you used logistic regression in your work?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1500	If they have a feature column with yes and no, should they convert it into 1 and 0 integer or OneHotEncoding takes care of that?	"Well, there are multiple ways you can deal with this. First – get more data, if possible. If not, try to reduce the number of features you have. Sometimes, when the data set is small, if you have a lot of features, it might be confusing for the model. Or try to have a model with a lot of regularization. This should help. Here, the main thing is that you need to come up with a reliable way of evaluating the performance of your model. 
You really need to think about how you can set up your validation strategy in such a way that it's reliable. Once you have that, you can start experimenting with all these regularization things, with dropping some columns, with techniques like upsampling and downsampling and things like this. The important thing here is setting up your validation strategy. Once you have that, everything else will come. You will just need to experiment and the experiments will show what the best way is."
1501	If they have a feature column with yes and no, should they convert it into 1 and 0 integer or OneHotEncoding takes care of that?	I think for binary features, just one and zero makes more sense than OneHotEncoding. OneHotEncoding will take care of that but you will have two columns that are the exact opposite of each other. So that is a bit redundant. I think it's better just to convert it to a Boolean column with ones and zeros.
1502	If they have a feature column with yes and no, should they convert it into 1 and 0 integer or OneHotEncoding takes care of that?	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
1503	If they have a feature column with yes and no, should they convert it into 1 and 0 integer or OneHotEncoding takes care of that?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1504	If they have a feature column with yes and no, should they convert it into 1 and 0 integer or OneHotEncoding takes care of that?	"Actually, I don't remember exactly how I do this. The right approach would be to use only train data and not use validation data or test dataset. This is done on purpose for exactly this reason that you mentioned. We want to model a situation in which there is some unseen data – it's truly unseen, we do not see that all – and anything can happen to this unseen data, including the situation that you described, such as there is a value that is only present in the test data. 
Imagine that there is a new iPhone – when you train your model, this new iPhone didn't exist (you didn't have this category) but you apply this model, and then, all of a sudden, there is a new iPhone. How will your model react to this? You need to somehow model this situation. This is totally normal. This can and will happen in practice. We just want to understand what would be the effect of that. That's why we need to see this in training data. This is good and this is how it should be. I hope that is a good answer."
1505	How do we choose numbers of folds (for k-fold cross validation)?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1506	How do we choose numbers of folds (for k-fold cross validation)?	"I think some of you asked that question in Slack already. Let me repeat what I answered to that question.  Imagine that you have a very big dataset. For this big dataset, training a model takes a lot of time. You don't want your k to be very large. Because if you have a very large K, it means you will spend a lot, a lot, a lot of time just training this model and then computing this course, and then taking averages. And then because your dataset is big, it will probably not be very different across different folds. Usually, this is the case when your dataset is large. Therefore, in large datasets, it makes sense to use k=2 for k-fold cross validation – so you just do it twice. 
Often, even sticking to the usual train validation split, like 80/20 or 75/25 makes even more sense, because you can just do it once without training it multiple times, because A) you save time and B) it's very likely that you will not see a very large difference across different folds when your dataset is large. You can experiment with this and see that it's actually true and that this course that you get on different folds are quite different. 
Then you can just do the usual train, validation set and use that because it will speed up your experiments. For medium-sized datasets, having k=three is fine. Here, you should think in terms of how much time you are willing to spend on doing your cross validation. You don't want to spend a lot of time here. So maybe k=3 should be sufficient. As your dataset size goes down, then maybe you can increase k. For example, for smaller datasets, k=five, or even k=10 in some cases, makes a lot of sense. That's how I typically choose. The smaller the dataset, the larger the k. And for big datasets, I usually don't use k-fold cross validation at all."
1507	How do we choose numbers of folds (for k-fold cross validation)?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1508	How do we choose numbers of folds (for k-fold cross validation)?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1509	How do we choose numbers of folds (for k-fold cross validation)?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
1510	What is the expected hours per week commitment required to complete this course satisfactorily for a beginner and practitioner data engineer?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
1511	What is the expected hours per week commitment required to complete this course satisfactorily for a beginner and practitioner data engineer?	No, you cannot.
1512	What is the expected hours per week commitment required to complete this course satisfactorily for a beginner and practitioner data engineer?	"Alexey
Yeah, that's not going to happen. I think ChatGPT will help us do our job, but it will not replace people, at least in my opinion. But I use it in my work and it's helpful. You should try using it too – if you want, of course."
1513	What is the expected hours per week commitment required to complete this course satisfactorily for a beginner and practitioner data engineer?	"Michael  
I think that would heavily depend on how much experience you have with Python, SQL, GCP. If I remember correctly, I think I probably did about six hours a week. Also, if you want to have the material really sink in, you'll go through it two or three times. So it really depends on the student and the week. Some will require more time. But I think if you manage your time well, the time commitment can be pretty small. 
Luis
Yeah. Like Michael said, I think it also depends a little bit on the issue of the team that you're talking with. For example, on SQL hours, I am already completely advanced, so it didn't take much time with that. It was really fast. But for Docker, I was completely a beginner, so it was harder. Besides that, there's the leaderboard, and if you want to be on the top of the leaderboard because it matters for you, then it will probably take more than that six hours a week. But I think five to six hours per week is expected.
Alexey  
I saw a similar question asked on Reddit and many people said that they’re putting in up to 10 hours, especially in the first week. The Docker week was the most challenging. Then the Airflow week, again, the problem there was mostly setting up. This is where people spent a lot of time. For example, BigQuery week was relatively easy because it's a managed service – you just go to Google Cloud Platform, and you don't need to set up anything locally. It's easier.
Do you guys remember what the most difficult weeks for you were? Was it the Docker and Airflow weeks or some others? 
Luis
Oh, definitely. The first week was the most difficult. Definitely. Actually, I must admit that I didn't do your week of Kafka. Sorry. I was completely busy with a lot of work and skipped the streaming week. [chuckles] Yeah, it was too hard for me.
Alexey
But it's actually a good point. Many people were asking me things like, “Hey, I don't want to study DBT. I'm interested in Spark.” Yeah, just go ahead and skip it in. We're not forcing you to study DBT. If all you're interested in is streaming, go ahead and watch the streaming lectures. They're already there. Then at the end, you just use the tech you want in your project and you don’t use the tech you don't want. You have this freedom. You have this flexibility. 
Luis
I don't know if someone asked already, but one thing that is important is that the projects you don't need are all the things of all the weeks. Don't think “Oh, I have to have streaming. I have to have Prefect. I have to have…” No. You don't have to do everything. Just some topics.
Ankush
Just remember, if you want to get on the leaderboard, you might need to do every week."
1514	What is the expected hours per week commitment required to complete this course satisfactorily for a beginner and practitioner data engineer?	Yes. As I’ve already said during the launch stream, you can start applying for jobs right now. You don't actually have to complete the course to find a job. Maybe you already know enough for you to convince an employer to hire you. By that, I mean you already know how to program, you know the command line, and things like that. It will be enough – it will be possible – and many people from the previous iteration of the course did that. They found a job. Some of them, not even as juniors.
1515	How does CRISP-DM relate with MLOps?	I wish Tim was here and would help me answer that because I don't know. I assume if you wanted to use Bento, you would probably use TensorFlow Lite – you don't want to use the usual TensorFlow for the same reasons that we talked about in the serverless module, because it's simply too large. Probably for Bento, you would use something like TensorFlow Lite and it will still work. Maybe another thing you can check – in our channel, we had a talk from one of the Bento folks, Building an ML Service Platform from the Ground Up. Here, Sean talks about the differences between TensorFlow Serving and Bento. With TensorFlow Serving, it's good – it's quite optimized for deep learning. There are some problems. First of all, it's written in C++ and expects your model in a certain format – this “saved model format”. Adding anything extra on top of that (like pre-processing, post-processing) becomes difficult, because you will need to have an extra step there. Bento makes it easier. But, again, I don't know. It's very difficult to give you a concrete recommendation without knowing what exactly you want to do. So it's case dependent. If you already use TensorFlow Serving in your company, then maybe it makes sense to stick to that. If we have Tim on Office Hours next week, maybe you can ask him that. He will be a better person to answer this question.
1516	How does CRISP-DM relate with MLOps?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1517	How does CRISP-DM relate with MLOps?	"There was no registration deadline at all for this course. So you can still register. You can still sign up and you can follow the course. You will not be able to submit the homework assignments where the due date is already over because there are already solutions posted, so it doesn't make much sense to submit homework when the solutions are already there. You can follow the course but you will not be able to submit the homework. You can check our frequently asked questions and I do recommend doing this. Please remember the rule – if you have a question, first go check the frequently asked questions, and if you don't find the question there, go and ask in Slack. 
For this particular question, it’s already there. “Don't worry, you can take the course.” Maybe right now it's becoming a little bit more challenging to catch up with everything, but note that you can skip the midterm project. Right now, if you want to catch up with everything, if you skip the midterm project, it's fine. You will be able to do two capstone projects and still get the certificate. Also, you can just take the course in self-paced mode. There is nothing wrong with that either."
1518	How does CRISP-DM relate with MLOps?	I don't think I understand the question. Is the question about what we do when we have duplicates in data and we split it and then there are duplicates? Some of the datasets in week one and then one split and then another one, or…? Oh, okay. Yeah, the way we do it is, we generate random numbers. I don't remember, to be honest, how exactly we do this. Let me quickly take a look. If we go to regression, notebook, and split – I think we shuffle data, not draw random numbers. Yeah, we take a range from zero to n exclusive. So n is not included and then we shuffle it. This way, we make sure that the same number does not end up in the different splits. I hope that answers your question.
1519	How does CRISP-DM relate with MLOps?	"MLOps is a set of processes and tools for productionizing machine learning, while CRISP-DM is a process. CRISP-DM was invented something like 12 years ago, when MLOps didn't really exist. But surprisingly, it covers it pretty well. 
https://commons.wikimedia.org/wiki/File:CRISP-DM_Process_Diagram.png
It also depends on how exactly you interpret each of the steps here. Here is the evaluation and deployment phase, which are pretty related to MLOps. Evaluation is how exactly you evaluate your model – how you're doing an A/B test or something like this. And deployment is how you productionize your model. Many of these things are quite related to MLOps."
1520	Let's say that after finishing courses, we launched a free web-app, how can we make it faster, and how to manage high traffic? Are there any free tools?	It is possible to do everything locally. You will need to do it by… I think you will need to do it in module five when we deploy something. Or actually, you will probably need to do it when we do your project. Yeah. It's fine to do everything locally. But if you have access to cloud and you want to practice with this, I do recommend practicing, because cloud is one of the skills that employers are looking for. They really need it. So if you want to be more employable, I do recommend learning about cloud.
1521	Let's say that after finishing courses, we launched a free web-app, how can we make it faster, and how to manage high traffic? Are there any free tools?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1522	Let's say that after finishing courses, we launched a free web-app, how can we make it faster, and how to manage high traffic? Are there any free tools?	No, it should not. Just keep working on this project. It should not affect anything. Of course, if you decide to copy the project that somebody did, then it will, of course, affect it. If people who evaluate your project will accidentally find this out, you will receive zero points and you will have to rely on the other two projects to pass the course. You will not be disqualified for the course, but for the project, you will. So don't do that. Apart from that, I don't see any problems. As long as it's not a Titanic or Iris dataset – it's not one of these popular datasets, that's fine.
1523	Let's say that after finishing courses, we launched a free web-app, how can we make it faster, and how to manage high traffic? Are there any free tools?	"Again, by “coursES” I’m not sure what you mean – probably Zoomcamps? If we are talking about the ML Zoomcamp – here, in addition to Flask, Docker, and or everything we've covered in week 5, we will also learn about BentoML in week seven. BentoML can actually help with making your web application faster. We will also talk about serverless – I think week 8 or 9 (I don't remember). Serverless can also scale up easily, so the more traffic you have, Amazon AWS will automatically scale it up and make sure that your lambda function can handle all the traffic. 
Other cloud providers have similar things to lambdas. In Google, I think it's Cloud Functions in Azure, it’s Cloud functions – I don't remember the exact names. But I think most of the big cloud providers have some sort of serverless option, so you can use that. Then there’s another option, which we will also cover, which is Kubernetes. In Kubernetes, there is an option to do autoscaling. That can work too. 
In this course, we do not cover autoscaling in the video materials – maybe we will have this as a homework assignment. We'll see. But the materials we will have in this course will give you enough foundation to actually go and read more about this topic and learn how autoscaling in Kubernetes works, and then implement this. But I think the simplest approach would be to first check out BentoML and then serverless could be also a good option."
1524	Let's say that after finishing courses, we launched a free web-app, how can we make it faster, and how to manage high traffic? Are there any free tools?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1525	Do we have to follow along with the lesson or is the homework completely independent from the lessons?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1526	Do we have to follow along with the lesson or is the homework completely independent from the lessons?	"You can check out a few projects in this playlist. In the playlist, you can see some of the videos here, for example, there's a project from Alvaro. And it's not just Alvaro, there’s one from Alvaro and Ninad. You can just check out these projects – there’s a project from Carolina and Hamad, a project from Lisa, and a project from Timur. The project from Timor may actually fit what you want. Timur has done a lot of work and it's very interesting. So check it out. Actually, Timur’s project is not a midterm project. It's more like a capstone project – there is some deep learning there, he uses Karis. So maybe focus on the other three. 
You can also go to our course web page, and then go to midterm project (you need to use the 2021 folder) and then in the readme there should be some assignments. There will be some form that says “To find the projects you need to review, use this table.” Here, you can see all the projects from previous students’ midterm projects. You can pick some of them and check if you like them or not."
1527	Do we have to follow along with the lesson or is the homework completely independent from the lessons?	For me, as a Russian citizen, it was actually surprising to learn that the World Cup started, because I simply didn't know that until last week, when people all of a sudden started watching football. I see that Ukraine isn't playing there – they have other problems right now, I suspect. I would root for them if they played. I guess Poland. I know that they won a match recently against Saudi Arabia – so, good job. I didn't watch it though. But if you asked me which team I would root for, then maybe Poland.
1528	Do we have to follow along with the lesson or is the homework completely independent from the lessons?	"The homework is not independent from the lessons. In the lessons, we show you how to do stuff and then the homework is to take the materials from the lessons and apply them independently. So you don't have to follow along with the lessons, but you can. It's up to you. If you feel that you already know this topic, just open the homework and try to do it. 
If you don't know how to solve it, go check the lessons. I'm also sure that if you already know, let's say, something about deep learning, then you can just find a solution online. Not the solution to the homework, of course, but the solution to the problem you have. Don't try to look for the solution to the homework problem online because that's cheating."
1529	Do we have to follow along with the lesson or is the homework completely independent from the lessons?	Maybe you can just go ahead to our repo and create a pull request. That will help. That is certainly useful and you will help immensely if you just go and create a pull request with that and we will just accept it. Great idea. Thank you.
1530	Data processing (transform) is after or before saving into Google BigQuery? Or do it directly to BigQuery?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1531	Data processing (transform) is after or before saving into Google BigQuery? Or do it directly to BigQuery?	"Alexey
The best way to contribute is to take notes and include them to the notes section. And tell your friends about the course. As a part of your Learning in Public, spread the word. It will help us. And don't forget to give us a star if you have."
1532	Data processing (transform) is after or before saving into Google BigQuery? Or do it directly to BigQuery?	Both. You can do it both ways and there are pros and cons. It really depends on the use case. From what I understood, it’s usually cheaper to first transform the data, put it into a data lake, and then load it to Google BigQuery. For example, in cases of DBT, this is what you do with DBT. You transform the data that is already in the data warehouse. Thus, both options work.
1533	Data processing (transform) is after or before saving into Google BigQuery? Or do it directly to BigQuery?	"Alexey
You can create a Python path variable in your .bashrc and it will be there permanently. Just in the same way as you do with PATH. I don't know why we do it like that. Maybe I just didn't mention that. But, of course, you can just put everything you want in .bashrc and this alliance (this code) will be executed every time you launch your virtual machine. This way you can execute these things automatically."
1534	Data processing (transform) is after or before saving into Google BigQuery? Or do it directly to BigQuery?	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
1535	Any thoughts on ChatGPT? There are a lot of rumors about it taking over the jobs of software professionals, a lot of layoffs, etc.	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
1536	Any thoughts on ChatGPT? There are a lot of rumors about it taking over the jobs of software professionals, a lot of layoffs, etc.	"Alexey
Yeah, that's not going to happen. I think ChatGPT will help us do our job, but it will not replace people, at least in my opinion. But I use it in my work and it's helpful. You should try using it too – if you want, of course."
1537	Any thoughts on ChatGPT? There are a lot of rumors about it taking over the jobs of software professionals, a lot of layoffs, etc.	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1538	Any thoughts on ChatGPT? There are a lot of rumors about it taking over the jobs of software professionals, a lot of layoffs, etc.	No, you cannot.
1539	Any thoughts on ChatGPT? There are a lot of rumors about it taking over the jobs of software professionals, a lot of layoffs, etc.	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
1540	Alexey says “Not surprised that the XGBoost model is the best for tabular data.” What is the alternative type of data if it's not tabular?	"Well, there are multiple ways you can deal with this. First – get more data, if possible. If not, try to reduce the number of features you have. Sometimes, when the data set is small, if you have a lot of features, it might be confusing for the model. Or try to have a model with a lot of regularization. This should help. Here, the main thing is that you need to come up with a reliable way of evaluating the performance of your model. 
You really need to think about how you can set up your validation strategy in such a way that it's reliable. Once you have that, you can start experimenting with all these regularization things, with dropping some columns, with techniques like upsampling and downsampling and things like this. The important thing here is setting up your validation strategy. Once you have that, everything else will come. You will just need to experiment and the experiments will show what the best way is."
1541	Alexey says “Not surprised that the XGBoost model is the best for tabular data.” What is the alternative type of data if it's not tabular?	"I don’t know… throw a coin and the coin will tell you what to do if you don't know. [chuckles] And then if the coin tells you that you should be a data engineer and you feel like “Umm… maybe not.” Then go with the other option. I don't have a better suggestion here. I can tell you to follow your passion and follow your heart… But yeah. [chuckles] I don't know. You’ll eventually need to make these decisions yourself. Just do a project in data engineering, do a project in data science, and then ask yourself what you enjoyed doing more. 
Also talk to people – talk to a few data engineers, people who already work as data engineers, people who already work as data scientists, and ask them what they don't like about their job. And then ask yourself, “Would you enjoy doing things that they dislike? Or would you be okay doing these things as well?” For example, what I don't like in the work of a data scientist is all this parameter tuning and parameter optimization, trying different features – all this stuff I find boring, but some people really love it. If some people tell you, “Eh, I don't like this part,” and you think, “Okay, it's actually not bad for me,” then maybe this is the right path for you. 
In the end, if you join as a data scientist, it doesn't mean that the door for a data engineer is closed for you. As a data scientist, you will get to do a lot of data engineering as well. I, as a data scientist, also do data engineering. I also do machine learning engineering. The same is not always true for data engineers. If you get hired as a data engineer, maybe you will get to do data science, maybe not. It depends on the team. So then, what you can explore is the so-called full-stack data scientist. But again, titles sometimes don't mean much. Don't try to force yourself into one of the predefined boxes. Sometimes you can work ‘officially’ as a software engineer and do both data science and data engineering and enjoy this work."
1542	Alexey says “Not surprised that the XGBoost model is the best for tabular data.” What is the alternative type of data if it's not tabular?	Image data, text data, time series (to some extent) although it is tabular. So images and text are mostly the ones that come to mind when I talk about non-tabular data. In module eight, we will see what to do with images and with NLP, you can check out a lot of resources on the internet on how to use neural networks, or how to use other things. Because if we use traditional methods, like the count vectorizer that I showed you in the Office Hours, it will still take non-tabular data and turn it into tabular data. But with neural networks, it does a bit more than just that. It's more complicated – more advanced. You can find a lot of examples on the internet.
1543	Alexey says “Not surprised that the XGBoost model is the best for tabular data.” What is the alternative type of data if it's not tabular?	I'm glad you asked. MLZoomcamp.com. All the materials are here and you will find them there.
1544	Alexey says “Not surprised that the XGBoost model is the best for tabular data.” What is the alternative type of data if it's not tabular?	"Scikit Learn is not very different. This is not exactly how Scikit Learn is implemented. It's a little bit different, but the idea is very similar. In Scikit Learn, they use smarter ways of doing this. For example, in week two, we have a case when we need to apply regularization. When we have correlated or very close to correlated columns, then our matrix becomes very unstable, so we cannot easily invert it. 
If what I’m saying now doesn't make sense, it will make sense once you start watching the videos. Anyways, what I'm trying to say is that in Scikit Learn, they have a smart way of working around this, such that your weights – your vector with weights, the W vector – does not contain insanely large values. They have smarter methods for doing this. But apart from that, it's very similar and you should not get two different results. You can experiment, and I do recommend experimenting with Scikit Learn. We will actually do this in week three. In week three, we will use Scikit Learn for some of these things. 
In week two, we will implement things ourselves. But starting from week three, we will only use Scikit Learn and other libraries, so we will stop implementing things ourselves. Here's just to give you a taste of what machine learning is inside. It’s just a bunch of formulas – algorithms – there is no magic. Internally, Scikit Learn implements them too."
1545	Why does R have a better stats reputation?	It is possible to do everything locally. You will need to do it by… I think you will need to do it in module five when we deploy something. Or actually, you will probably need to do it when we do your project. Yeah. It's fine to do everything locally. But if you have access to cloud and you want to practice with this, I do recommend practicing, because cloud is one of the skills that employers are looking for. They really need it. So if you want to be more employable, I do recommend learning about cloud.
1546	Why does R have a better stats reputation?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1547	Why does R have a better stats reputation?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1548	Why does R have a better stats reputation?	For historical reasons. They have been around in this area (in this domain) longer than Python. Python has only relatively recently broken into the data world. Before, it was not used for data. I think it became popular because of IPython and Jupyter Notebooks. You could very quickly experiment with things. For regular software engineers, Python is closer than R and that's why, I guess, they started using Python more and more often. Then good libraries appeared in Python, and slowly Python overtook R. But because it was probably more for engineers, and engineers are not very good at statistics, typically, that's why I guess all the stat packages are still in R. Of course, there is “stats methods”. I don’t remember the name of the package, but I remember that the abbreviation is SM. I think you will be able to find it on Google. Maybe somebody in the chat will write the name of this package.
1549	Why does R have a better stats reputation?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1550	What types of projects are expected for data analyst jobs? Which industries have the most opportunities for data analysts in terms of machine learning?	"If you take statistics for machine learning education, the first lecture will be this – you'll probably derive the formula for logistic regression and there will be a slide with all these assumptions about the linear regression model. Here is the Google result: 
https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/assumptions-of-linear-regression/

Go through this. They are usually useful in practice, I guess. But what I typically do is train a model, and then rely on validation to tell me if my model is doing well or not. So if there is something wrong with my validation, I will see that the metric validation is bad. It's just a lot of trial and error, rather than figuring out all these theoretical assumptions. 
I don't want to say that all these assumptions are not relevant. If you work as a data scientist, they still are. But I also found that the more practical way to understand if you can apply linear regression to the data or not, is to just apply it and see if it works or not. If it works – if the results on the validation are good – then it means that you can apply your model to this dataset. 
Maybe some of the assumptions are violated. For example, multicollinearity – this is actually the case in the lectures, when we needed to regularize (add regularization to the model) this assumption was violated. You will see in the lectures what exactly happened. So check it out. I don't think I will be able to give a good answer, because I am not prepared to talk about this, but any statistics book or theoretical machine learning book explains this."
1551	What types of projects are expected for data analyst jobs? Which industries have the most opportunities for data analysts in terms of machine learning?	Well, from my point of view (I might be mistaken because I'm not a data analyst) data analysts usually do not work with machine learning. They sometimes do, but it's not their main focus. Some analysts might use machine learning as an extra tool for doing what they need to do, like forecast sales, for example. But that's not their main domain expertise, let's say – not their main area. Machine learning is more a data scientist kind of thing. That being said, to answer your question, “What types of projects are expected for data analyst jobs?” They are usually about analytics. You get a dataset and then you need to, let's say, build a report from this dataset to get some business insights. Again, I don't work as a data analyst. That's pretty much using SQL and some data visualization tools to turn a dataset you have into something understandable and digestible. If you build a project like that – you can take any of the datasets we use and if you build some cool visualization for that –that's a good kind of project for a data analyst role.
1552	What types of projects are expected for data analyst jobs? Which industries have the most opportunities for data analysts in terms of machine learning?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1553	What types of projects are expected for data analyst jobs? Which industries have the most opportunities for data analysts in terms of machine learning?	I think for binary features, just one and zero makes more sense than OneHotEncoding. OneHotEncoding will take care of that but you will have two columns that are the exact opposite of each other. So that is a bit redundant. I think it's better just to convert it to a Boolean column with ones and zeros.
1554	What types of projects are expected for data analyst jobs? Which industries have the most opportunities for data analysts in terms of machine learning?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1555	How to explain in simple words to stakeholders, what machine learning and neural networks are?	"Tim 
I think, yes. We have a few examples on our website. I think with Colab, it becomes a little bit more difficult once you're hosting the Bento to access it, if it's running on a Google Colab server. But yeah, you can save it. What I would say is that you can save your model and then export your model to somewhere where you can then build your Bento there.
Alexey 
By default, Bento saves in a local file system (in your /home) so you need to have a way to save this model somewhere, let's say on the cloud, S3, whatever – somewhere externally. The workflow here is that you train your model in Colab, you save it, and then you deploy it somewhere. You don't deploy it on a Colab instance.
Tim
Right. When you save your model, I think it'll save it to the Colab instance. Then you have to use BentoML Export to push it to an S3 bucket or something where you can pick it up and deploy your Bento.
Alexey 
And there are options for deploying. We already discussed this in the course, when we looked at Elastic Beanstalk. I assume it can work with Bento. I haven't checked. [Tim agrees] But then what we saw this week, we saw how to deploy it with ECS, and then we can also deploy to Kubernetes EKS, we can also deploy to lambda – there are a ton of other options. Everywhere where you can deploy a Docker container (Docker image) you can deploy it there. Right? [Tim agrees]"
1556	How to explain in simple words to stakeholders, what machine learning and neural networks are?	Well, that's why we have module one. There, the very first unit (the very first video) the example with cars – just use this example. You can use that to explain what machine learning is. When it comes to neural networks, we have module eight, which is about neural networks. You can use that to explain what a neural network is. But if you want a one sentence explanation – it's a bunch of algorithms that extract patterns from data. I don't know – will it make sense to your stakeholders? If it will, good. If it will not, then you have these other resources that I mentioned.
1557	How to explain in simple words to stakeholders, what machine learning and neural networks are?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1558	How to explain in simple words to stakeholders, what machine learning and neural networks are?	"You will get out of tutorial hell when you start doing the project in this course, because it will be just a set of guidelines – this matrix – but you will have to do the rest yourself. You will have to find the problem yourself, you will have to do exploratory data analysis yourself, you will have to do data preparation yourself, you will have to do everything yourself – there will be no homework with the exact steps that you need to do to have a project. And this is how you get out of tutorial hell. 
Let's not think about this particular course, but in any setting – you want to find the problem that you want to solve. Once you have the problem, then try to think “What is the best way to solve this? What is the shortest way to solve it?” Or, at least, “What is the next step I need to take to solve it?” And then try to work your way through solving this problem. So focus on the problem. If you just do tutorials, you're not solving problems – you're just doing tutorials. So focus on the problem."
1559	How to explain in simple words to stakeholders, what machine learning and neural networks are?	"Tim 
Not really. We have users who use both Streamlit and Gradio. I think Streamlit and Gradio are a bit more about the presentation side. We do have a couple of users who have asked us about deeper Streamlit or Gradio support and I think that's something that's probably coming up in the next few months.
Alexey
For those who don't know what Streamlit or Gradio is – this is a way to create an interface for your services. Well, Gradio is focused on machine learning, meaning you create interfaces specifically for models. And then Streamlit is just a Python package for creating interfaces. We actually had a project at DataTalks.Club about Streamlit. I actually even have a tweet about this. This is what Streamlit looks like. In the video, you can see that you have this interface, and then based on the toggles that you move the core of your model changes. It gives you an easy way to build an interface like that. Meanwhile Bento focuses on quite a different use case, right? It’s not building interfaces, but serving models.
Tim 
Right, yeah. I think one of our users was asking for a really easy way, so that once you have your model and your Streamlit UI to be able to create a Bento deployment with one line in that way, anybody that you gave your Streamlit app to would be calling to that service and it could work anywhere. Which is a nice feature, I think.
Alexey 
Yeah, I think it is. I imagine if you have a Pydentic class there, then you can easily generate a Streamlit app for that.
Tim 
Right. Oh, that's a good idea. I hadn't thought of that. Yeah.
Alexey
And then in addition to that, the Swagger UI that you have, could be like having the actual interface. 
Tim
Oh, yeah, that would be cool. [chuckles]
Alexey
That could be a good Hackathon project, right?
Tim 
Yeah, for sure.
Alexey
From what I see – let's say you’ve built a model. Then you deployed this model and you have this nice API. You can query it with CURL. But what if you want to demo this project to your manager, or a manager of your manager, or somebody who does not necessarily have a technical background? If you give them the URL and say, “Okay, you need to execute the CURL command with a post request,” they would be like, “CURL what? What do you want from me? Just show me how it looks.” This is when tools like Streamlit come in handy. Instead of giving them a command line interface to query it, you just give them “Okay, this is the link. Play with this.” If Bento could do something like that – automatically generate things like that – many data scientists would thank you.
Tim
Yeah, I was just looking because I figured there's a library that turns Pydantic models into Streamlit apps. It looks like there is one that does that. I wonder if we could just plug into that app and then have that same thing. 
Alexey
That's cool. 
Tim
Yeah, that's awesome. That's kind of the cool part of open source – there are so many people out there with so many different projects and we get to collaborate with them and kind of build the best collaborations and combinations of these tools."
1560	For week 1 (ingesting data) I keep on getting error messages.	Yes, you will have to come up with your own individual projects. It's not easy, I know. But we'll help you. We will help you to come up with a project idea. That's actually on purpose because we want every project to be unique. Then it becomes a part of your portfolio when you look for a job.
1561	For week 1 (ingesting data) I keep on getting error messages.	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1562	For week 1 (ingesting data) I keep on getting error messages.	No, you cannot.
1563	For week 1 (ingesting data) I keep on getting error messages.	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
1564	For week 1 (ingesting data) I keep on getting error messages.	"Alexey
Please check the FAQ. If you don't find the answer there, ask in Slack. If you get an answer in Slack, please put it into the FAQ."
1565	What should my approach be if I'm juggling between both data engineering and machine learning Zoomcamps? Also will there be another round of DE peer-project review?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1566	What should my approach be if I'm juggling between both data engineering and machine learning Zoomcamps? Also will there be another round of DE peer-project review?	So for the first part “What should be my approach if I'm juggling between both?” Well, I don't know how much time you have. If you don't have a lot of time, I would suggest focusing on one and then come back to the other one. Actually, I need to make an announcement. As the course team from the Data Engineering Zoomcamp, we actually discussed it, and I'm happy to announce that there will be another iteration of Data Engineering Zoomcamp. Maybe what you can do now is just focus on ML Zoomcamp, and then come back to Data Engineering Zoomcamp when we launch it in January. There will be another announcement, but now you can know it. So, yes, we'll have another iteration in January.
1567	What should my approach be if I'm juggling between both data engineering and machine learning Zoomcamps? Also will there be another round of DE peer-project review?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
1568	What should my approach be if I'm juggling between both data engineering and machine learning Zoomcamps? Also will there be another round of DE peer-project review?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
1569	What should my approach be if I'm juggling between both data engineering and machine learning Zoomcamps? Also will there be another round of DE peer-project review?	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
1570	Can we move the homework deadline to January 30th instead of 26th? I had a hard time working on these over just one weekend and need another weekend to finish.	As of today, January 26th, 2023 – it's likely that we will extend it until the end of the week. But this doesn't mean you should put this off until the last day. If you do this, it's likely you will be late and we're not going to extend it a second time. Try to make it happen earlier. If you can't, we’ll see.
1571	Can we move the homework deadline to January 30th instead of 26th? I had a hard time working on these over just one weekend and need another weekend to finish.	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
1572	Can we move the homework deadline to January 30th instead of 26th? I had a hard time working on these over just one weekend and need another weekend to finish.	No, you cannot.
1573	Can we move the homework deadline to January 30th instead of 26th? I had a hard time working on these over just one weekend and need another weekend to finish.	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
1574	Can we move the homework deadline to January 30th instead of 26th? I had a hard time working on these over just one weekend and need another weekend to finish.	"Alexey
It's possible to apply to jobs right now. You don't have to take the course. Just start applying. Why do you need to take the course to start applying, right? It was possible to do it last year. Nobody is stopping you from doing this. But this course will give you some knowledge and add projects to your portfolio. That will be helpful in your job search. 
But I would encourage you to start applying to jobs right now to see what they actually want from you – what kind of topics come up, what kind of test assignments you get. You don't need to wait until the course is finished to start doing this. You can already learn a lot about the job market in your area or in the domain where you want to work.
Luis
I just want to add that when I did the 2022 cohort, I was in another company and the process of data engineering in that company was really similar to the Zoomcamp. What I started to notice was that the companies mostly use this architecture in engineering. So it's good for you to show yourself that you're starting to understand all this data engineering stuff. I think it's important.
Ankush
I just wanted to add. I don't know about applying for jobs, but once you have a job, if you want to get an increment, contact Michael. [chuckles]

Victoria
Michael moneymaker. [chuckles]"
1575	Is there a setup guide for all the software we need to use before starting the course?	"Alexey
No. I mean, we kind of told you. Yes, you maybe didn't see the intro video. But, sorry. No, it's late. We will actually recalculate the points for the second homework (HW 1B) because in the forum, it said that you can get up to one point, but we calculated that the cap there was seven. We are going to recalculate that. Thus, people who got a lot of points will get just one. Sorry about that. But don't focus on points. If you go to the leaderboard, we know that the person with this hash (722366d8b29ece9be3a7605363562c7c60d6918e) did seven links. Do you know who that is? Probably you don't. 
These points are virtual. It's nice to have a good score, but remember that these points mean whatever you want them to mean – not more than not less. Maybe it's okay if you get a few points less than others for the first homework, but the important thing here is consistency. If you just do your homework, even without doing any Learning in Public stuff, which I think you should do (you should post about the course). If you don't do this and you just do all the homework, by the end, you will be somewhere in the top 20. If you look at the leaderboard from the previous iteration, you will see that not everyone stuck around till the end. 
If you just do your homework, don't worry about these points, and you'll be fine. Sorry about that. I know that it can be discouraging that you didn't get as many points as others, but don't worry about that. Focus on learning. Maybe Michael has some other suggestions or some other things that I didn't mention, because he took part last year. But I think he was one of the people who did a few posts every week, right?
Michael
Yes. Actually, when I went through it, I missed the whole… you can get up to seven points, but I was just doing one every week and I just stuck with that. I was still near the top. I think the real value in that isn't necessarily the points but just being comfortable sharing – writing articles and making videos. That's the real value there. But everyone likes the clout value, too. As Alexey said, it will fluctuate, so if you keep up with it going forward, you'll probably be just fine."
1576	Is there a setup guide for all the software we need to use before starting the course?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
1577	Is there a setup guide for all the software we need to use before starting the course?	No, you cannot.
1578	Is there a setup guide for all the software we need to use before starting the course?	Yes, there is such a thing. If you go here and scroll down, you will see the Tools section. If you go to week 1, there is a video, Setting up the environment on cloud VM, which explains everything you need to do. Then you can also check out the notes from the students. For example, notes from Alvaro are quite comprehensive. I remember that some students also took a lot of notes about the environment preparation setup, so you can take a look at the notes too.
1579	Is there a setup guide for all the software we need to use before starting the course?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
1580	How and where to apply for remote DS jobs for freshers?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
1581	How and where to apply for remote DS jobs for freshers?	Yeah, I don't know. LinkedIn? There is also Angel.co, which I think is a good one too. Check that out. Maybe Glassdoor? The usual places where you look for a job.
1582	How and where to apply for remote DS jobs for freshers?	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
1583	How and where to apply for remote DS jobs for freshers?	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
1584	How and where to apply for remote DS jobs for freshers?	Yeah. If it works for you, use it. I don't mind.
1585	Why does the YouTube page cover a blue duck? Why do you like ducks so much?	If it's a question for me, specifically – in Windows, I use a tool called Windows Terminal. This is the tool I use. In the settings, they have Git Bash, the usual command prompt, then they have multiple subsystems for Linux terminals. There also used to be PowerShell. I don't use PowerShell, so I don't have it here. This is what it looks like. I really like this. It’s just called Windows terminal. Check it out.
1586	Why does the YouTube page cover a blue duck? Why do you like ducks so much?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
1587	Why does the YouTube page cover a blue duck? Why do you like ducks so much?	No, it's not in our plans
1588	Why does the YouTube page cover a blue duck? Why do you like ducks so much?	"Alexey
It's possible to apply to jobs right now. You don't have to take the course. Just start applying. Why do you need to take the course to start applying, right? It was possible to do it last year. Nobody is stopping you from doing this. But this course will give you some knowledge and add projects to your portfolio. That will be helpful in your job search. 
But I would encourage you to start applying to jobs right now to see what they actually want from you – what kind of topics come up, what kind of test assignments you get. You don't need to wait until the course is finished to start doing this. You can already learn a lot about the job market in your area or in the domain where you want to work.
Luis
I just want to add that when I did the 2022 cohort, I was in another company and the process of data engineering in that company was really similar to the Zoomcamp. What I started to notice was that the companies mostly use this architecture in engineering. So it's good for you to show yourself that you're starting to understand all this data engineering stuff. I think it's important.
Ankush
I just wanted to add. I don't know about applying for jobs, but once you have a job, if you want to get an increment, contact Michael. [chuckles]

Victoria
Michael moneymaker. [chuckles]"
1589	Why does the YouTube page cover a blue duck? Why do you like ducks so much?	"Kalise
Our duck is named Marvin. Jeff, do you want to say why he's blue or why we like ducks? [chuckles]
Jeff
Yeah. “Rubber ducking” is a super common thing when you get stuck. You should do this before you put the question on Slack – you should talk to the rubber duck. Oh! Michael’s got his duck. I got mine. I got Marvin here in the background, let’s bring Marvin over. So just instead of talking to anyone, you can just talk to Marvin – talk to your duck, explain your problem, and as you're explaining it, oftentimes you realize like, “Oh! It doesn't work, because I didn't do that thing that's really obvious!” or whatever. So rubber ducking is a common troubleshooting step for people as they are programming. That's where we got Marvin, our blue duck.
Alexey
And the reason why it's blue? It's just a nice color?
Jeff
It is a nice color. I feel like I should know the answer to that, but I don't know if there is a reason. [chuckles]
Alexey
Because why not blue, right?
Jeff
Why not blue? Yeah. [chuckles] Because then if you use DALL-E or some kind of AI generation to get blue docs, you can get all kinds of weird images.
Alexey
Yeah, I think that's what you do for your preview images, right? For your blog. It's all Stable Diffusion or DALL-E or something. 
Jeff
Yeah, mostly Stable Diffusion."
1590	Can we add you on LinkedIn?	From 5 to 20, depending on your experience. If you're already an experienced developer, software engineer, you know Python, you know cloud, it could be less than five. If you don't know any of these things and you never used a command line, then it's 20 hours. If some of these things apply to you, some don't, then probably around 10 hours.
1591	Can we add you on LinkedIn?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1592	Can we add you on LinkedIn?	Right now, I'm reaching the limit of connections. I used to accept everyone who sends me a request, but now I don't. Maybe add a note of why you want to connect with me. But you can just follow me, so you don't really need to connect with me on LinkedIn. If you want to ask me a question, the best way of doing that would be to go to Slack and ask a question on the public channel. When I see it, I will try to answer it.
1593	Can we add you on LinkedIn?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
1594	Can we add you on LinkedIn?	"Alexey
No. I mean, we kind of told you. Yes, you maybe didn't see the intro video. But, sorry. No, it's late. We will actually recalculate the points for the second homework (HW 1B) because in the forum, it said that you can get up to one point, but we calculated that the cap there was seven. We are going to recalculate that. Thus, people who got a lot of points will get just one. Sorry about that. But don't focus on points. If you go to the leaderboard, we know that the person with this hash (722366d8b29ece9be3a7605363562c7c60d6918e) did seven links. Do you know who that is? Probably you don't. 
These points are virtual. It's nice to have a good score, but remember that these points mean whatever you want them to mean – not more than not less. Maybe it's okay if you get a few points less than others for the first homework, but the important thing here is consistency. If you just do your homework, even without doing any Learning in Public stuff, which I think you should do (you should post about the course). If you don't do this and you just do all the homework, by the end, you will be somewhere in the top 20. If you look at the leaderboard from the previous iteration, you will see that not everyone stuck around till the end. 
If you just do your homework, don't worry about these points, and you'll be fine. Sorry about that. I know that it can be discouraging that you didn't get as many points as others, but don't worry about that. Focus on learning. Maybe Michael has some other suggestions or some other things that I didn't mention, because he took part last year. But I think he was one of the people who did a few posts every week, right?
Michael
Yes. Actually, when I went through it, I missed the whole… you can get up to seven points, but I was just doing one every week and I just stuck with that. I was still near the top. I think the real value in that isn't necessarily the points but just being comfortable sharing – writing articles and making videos. That's the real value there. But everyone likes the clout value, too. As Alexey said, it will fluctuate, so if you keep up with it going forward, you'll probably be just fine."
1595	Do I have to bring over some blocks from one place to another?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1596	Do I have to bring over some blocks from one place to another?	No. From what I understood, from what Ankush said, you don't need to know Java.
1597	Do I have to bring over some blocks from one place to another?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
1598	Do I have to bring over some blocks from one place to another?	Both. You can do it both ways and there are pros and cons. It really depends on the use case. From what I understood, it’s usually cheaper to first transform the data, put it into a data lake, and then load it to Google BigQuery. For example, in cases of DBT, this is what you do with DBT. You transform the data that is already in the data warehouse. Thus, both options work.
1599	Do I have to bring over some blocks from one place to another?	"Jeff
The answer is yes. It can be nice to define those blocks in code in Python. You can just run the script and boom, now they show up on your server – your new server, new workspace. We're also working on a way to make it a little easier to copy over all those blocks at once. That's something that, because of this course, I raised with some people and they are working on some things there. We're gonna try to make that as easy as possible. If you're using different workspaces or different servers, those blocks live on the server and you have to let them know about it."
1600	Because of the deduplication step, the numbers of the rows aren’t consistent.	"Alexey
Please check the FAQ. If you don't find the answer there, ask in Slack. If you get an answer in Slack, please put it into the FAQ."
1601	Because of the deduplication step, the numbers of the rows aren’t consistent.	No, you cannot.
1602	Because of the deduplication step, the numbers of the rows aren’t consistent.	Yeah, I think one project can host several things. I don't know exactly, to be honest. I think there is probably an official recommendation from GCP. Check that out.
1603	Because of the deduplication step, the numbers of the rows aren’t consistent.	"Alexey
For this case, if you go to our data engineering page, and then go to the homework, you will see this note: “If the answer does not match exactly, then select the closest option.” With that, if you have an answer, but it's somewhat not the same one, you just go with the closest one and then you'll get it right."
1604	Because of the deduplication step, the numbers of the rows aren’t consistent.	"Jeff
I’m not sure, exactly. It might be the case that you made the deployment and it's scheduled and it's just waiting, but you are not connected to that workspace with the space where your agent is running. Agents are looking for work maybe locally, and you're connected to Prefect cloud, for example. That's one possibility. Just make sure that your agent is pulling for work from the workspace where you actually have scheduled the work to run. You can check that when you start your agent – it'll tell you what it's connected to, up at the top of the message that comes right back when you start the agent, in the CLI."
1605	How do we test how good our validation framework is?	It is possible to do everything locally. You will need to do it by… I think you will need to do it in module five when we deploy something. Or actually, you will probably need to do it when we do your project. Yeah. It's fine to do everything locally. But if you have access to cloud and you want to practice with this, I do recommend practicing, because cloud is one of the skills that employers are looking for. They really need it. So if you want to be more employable, I do recommend learning about cloud.
1606	How do we test how good our validation framework is?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1607	How do we test how good our validation framework is?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1608	How do we test how good our validation framework is?	"A good validation framework will tell you…. that's a tricky one. You just need to try to make sure that it's representative of how the performance of your model will look in real life. So you just need to make it as real-life as possible. I'm afraid the answer here is that it's case-dependent. At the end, what is important is the number you get – the performance evaluation metric you get – from your validation framework. It should reflect how well your model will do on unseen data. Sometimes, just shuffling the data and taking a part of this is enough. 
Most of the time, you will also need to split your data by time. In most cases, you have a timestamp in your dataset, and in this case, it's better to use the timestamp  for splitting the data. Apart from that, maybe just use common sense. Try to develop a bit of domain knowledge about this dataset and ask yourself, “Okay, is this validation dataset representative enough of the problem we're trying to solve?” I don't actually know if there is a better answer to that. Maybe I'll think about this and tell you."
1609	How do we test how good our validation framework is?	Okay. Yeah
1610	I'm a database developer, but I want to migrate to data engineering. I already know basic Python. Do you think that it will be difficult?	No, but you can suggest some stuff in Slack. If we see that this is useful, we can also include this in the project repo
1611	I'm a database developer, but I want to migrate to data engineering. I already know basic Python. Do you think that it will be difficult?	"Alexey
From what I see (And Ankush, maybe you can talk more about this) this is a perfect profile for switching to data engineering. Am I right?
Ankush
Yeah, I think this is a good time to be an engineer. And if you have Python knowledge and some database knowledge, you are already ahead of the curve. It won't be too difficult. It will be challenging, of course. But I hope this course helps you out. And I hope that overall, it would be really worth the effort. Because I think data engineering is a rewarding career for that."
1612	I'm a database developer, but I want to migrate to data engineering. I already know basic Python. Do you think that it will be difficult?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
1613	I'm a database developer, but I want to migrate to data engineering. I already know basic Python. Do you think that it will be difficult?	"Ankush
I think if it's about finding a job, spend time learning AWS, because… come on – everybody's using AWS and you have a higher probability of finding a company which will have AWS. After that choose GCS or Azure or whatever you feel. But I think the main point is to learn the concepts. For example, let's talk about data warehouses for partitioning and clustering. In Prefect, it’s how to create your DAGs in particular structure or paralyzing your flow. I think if you learn this concept, it doesn't really matter which cloud platform you use at the end of the day, because all of their services will be pretty similar. I think once you grab the basic concepts and your base layer is good, it's very easy to adjust to both of the cloud environments. I have personally worked on AWS and Google Cloud. I've also worked a bit with Snowflake. Once you can get the ideas, or the concepts, it's really easy to make that switch.
Alexey
I'll add that my experience with Cloud is mostly AWS. Not mostly, I think, it’s only AWS. For this course, I used GCP for the first time and it wasn't that difficult. The UI is different. I actually think GCP has a better UI, in my opinion. It’s more intuitive. The only thing I needed to figure out was, “In AWS this thing is called this way. How was it called in GCP?” Then I would just Google it, find it, and then just use it. Most of the time, it was like that.
Ankush
I have a follow up question. What if ChatGPT is only integrated in Azure? Then what do we do? 
Alexey
Well… I'll have to use Azure. [chuckles]
Ankush
So we’ll need to migrate? [laughs]"
1614	I'm a database developer, but I want to migrate to data engineering. I already know basic Python. Do you think that it will be difficult?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1615	If we run into a problem, is this the correct sequence to follow? 1) Google doc frequently asked questions. 2) Ask in Slack. 3) Asked during Office Hours.	Yes. And the Office Hours step is probably more for the non-technical questions. If you have a large code snippet, you cannot really ask this during Office Hours.
1616	If we run into a problem, is this the correct sequence to follow? 1) Google doc frequently asked questions. 2) Ask in Slack. 3) Asked during Office Hours.	It's like 10 weeks or something like that. If you count the second attempt of the project, it will be like 13 weeks. Quite long, but not so long as, let's say our machine learning engineering course.
1617	If we run into a problem, is this the correct sequence to follow? 1) Google doc frequently asked questions. 2) Ask in Slack. 3) Asked during Office Hours.	Yes, you will have to come up with your own individual projects. It's not easy, I know. But we'll help you. We will help you to come up with a project idea. That's actually on purpose because we want every project to be unique. Then it becomes a part of your portfolio when you look for a job.
1618	If we run into a problem, is this the correct sequence to follow? 1) Google doc frequently asked questions. 2) Ask in Slack. 3) Asked during Office Hours.	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1619	If we run into a problem, is this the correct sequence to follow? 1) Google doc frequently asked questions. 2) Ask in Slack. 3) Asked during Office Hours.	"Victoria
I think it's also worth putting it as, it’s great, but why is it chosen as a cloud data platform? I saw that a lot of people were also talking about AWS and things like that. There’s even a channel of people that want to use Snowflake instead of GCP. The main reason why we chose GCP is because it has a very generous free tier that's $300. In the case of Snowflake, for example, it’s limited to one month. In the case of GCP it’s until you finish the money – you can use the backups and a lot of people use the virtual machines, and you can use BigQuery. It will cover all of that and you won’t spend anything from your money, which is the idea of the course. That's why we chose BigQuery. 
But the main goal that we have here is that you can apply the knowledge of a cloud data warehouse to any kind of data warehouse. The same with Prefect, you can also apply it to Airflow if you start working at a company that uses Airflow. Because at the end of the day, you should know how a cloud data warehouse and data warehousing works. In the case of specifics of BigQuery to something Snowflake – both our cloud data warehouses from my experience. BigQuery has a different way of connecting – it goes through an API – whereas in Snowflake you use the more well-known SQL. They both have their flavors or SQL. 
Big Query also has differences, for example, they call databases “projects,” they call schemas “datasets,” and things like that. So it changes terminology. They also have a different approach on cost. Something like, select, start, limit 10, for a BigQuery table could cost you a lot of money. But they tell you before that, how many bytes they'll scan and things like that. It’s just the smaller things, I would. Ankush, you're also very familiar with BigQuery. I've always used Snowflake or Redshift more, or Microsoft.
Ankush
I think the biggest reason for choosing BigQuery was that it just comes with Google Cloud Platform and it's free to use. That's one thing. I think, overall, Snowflake is generally more expensive than BigQuery, at least in my experience. But, of course, they’re super similar. If you look at Big Query optimizations, it's the same concept in Snowflake. 
The concepts, as Victoria said, are applicable to both the data warehouse solutions. It's just a flavor that we chose just because it's easy to have something like an inbuilt data warehouse where you actually start something on your own. This was one of the big reasons for choosing BigQuery.
Alexey
You're free to use AWS if you want, as well. I think that is also a channel where people want to use AWS for the course. We, unfortunately, will not be able to give you a lot of support there. Because with DBT, for example, I don't know how well it can connect to Redshift or Athena or whatever.
Victoria
Somebody is saying something in the chat. I think I was misunderstood. It says $300 is not a small amount. That's the amount of credits you get for free. You will not pay $300. The idea is that we chose GCP so you don't pay anything. You don't pay anything! [chuckles] 
Alexey
Yes, that's the main advantage. What you need is a Google account – you use it for registering at Google Cloud Platform. They will ask you for a credit card, but only to verify that you're a real human. This card is not used for a different Google Cloud Platform account. They will not charge you anything. To me, Google is trustworthy. I think you can trust it with your credit card. I mean, so far they didn’t violate the trust I have in them. It's a good deal, basically. You should take it. 
With AWS, most of the content we talk about – most of the things you will learn here about Google Cloud Platform – are easily transferable to AWS, like all these virtual machines, object storage, Spark. All these things work fine. In AWS, the buttons you need to click are different, or the Terraform script you need to write is different. But at the end, most of the concepts are still the same. You just need to map from Google Cloud Platform concept to AWS concept. But with AWS, you need to pay something."
1620	I am new in the data engineering space and I want to learn from A to Z here, but I’m still confused about where I should pick this up and start my learning effectively.	No, you cannot.
1621	I am new in the data engineering space and I want to learn from A to Z here, but I’m still confused about where I should pick this up and start my learning effectively.	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1622	I am new in the data engineering space and I want to learn from A to Z here, but I’m still confused about where I should pick this up and start my learning effectively.	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
1623	I am new in the data engineering space and I want to learn from A to Z here, but I’m still confused about where I should pick this up and start my learning effectively.	You are in the right place. That's why we have the course. Just follow week 1 to week 7 or to week 6, do the project, and you'll be fine.
1624	I am new in the data engineering space and I want to learn from A to Z here, but I’m still confused about where I should pick this up and start my learning effectively.	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
1625	Does an article substitute for a project?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1626	Does an article substitute for a project?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1627	Does an article substitute for a project?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
1628	Does an article substitute for a project?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1629	Does an article substitute for a project?	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
1630	I want to experiment with AWS outside of the free tier and have services turned on continuously. Any tips or guides to reach low cost (no more than $15/per month)?	"What you can do in AWS is – there is part with billing. You go to billing and you can set budget alerts. Then you can say, “Okay, my budget is $20 per month and if I reach 75% of that, then send me an alert.” You will get an alert and you will know “Okay, I actually spent too much money on this. Now I'm going to go to the AWS console and figure out where I actually spend my money.” And then you can turn off some services you don't want to use, for example Kinesis. We don't use Kinesis in this course, but we use it in the MLOps Zoomcamp course. There, we use Kinesis and it is relatively expensive. 
Another thing you can do, for example, we cover Kubernetes in this course, and you might accidentally forget to turn it off. Kubernetes is not covered by the free tier. If you want to run Kubernetes, you will have to pay. One thing that happened to me recently is that I killed the Kubernetes cluster, but somehow the notes from this cluster were still running. So they weren't killed – they weren't removed or terminated. Because I had this alert, I got an email saying, “Hey, it seems you spent $80, which is a lot higher than usual.” Then I went to the cost explorer and I saw, “Okay, I am spending money on this issue.” I went there, I noticed the instances that were running them and I killed them. Then I would propose to have multiple alerts like that. 
Maybe have one for $20, another for $50, another for $70, another for $100. When the first alert goes off, you go there and fix it – you want to make sure you stop spending the money. The second alert (the second threshold) will help you to make sure that you don’t keep spending the money. Also, I actually have some credits from AWS and I wanted to give them away for the participation in these projects. We wanted to do it for this one for recommenders. I didn't do this. 
For the next one – the next project of the week will be about Fast API and we should totally give these AWS credits to participants. So maybe keep an eye on that and remind me that I promised you that, in case I forget. Then you will get some credits. Not everyone – some of you will get them. If you just commit some project that is not really useful, you will not get credits. But we will probably decide which projects will get credits. I can give you like $50. Of course, it's not a lot, but you can experiment with different AWS stuff."
1631	I want to experiment with AWS outside of the free tier and have services turned on continuously. Any tips or guides to reach low cost (no more than $15/per month)?	"Tim
The best way to reach out to me is in the Bento ML Slack. Usually, you can join our Slack group and then you can just directly message me"
1632	I want to experiment with AWS outside of the free tier and have services turned on continuously. Any tips or guides to reach low cost (no more than $15/per month)?	It is possible to do everything locally. You will need to do it by… I think you will need to do it in module five when we deploy something. Or actually, you will probably need to do it when we do your project. Yeah. It's fine to do everything locally. But if you have access to cloud and you want to practice with this, I do recommend practicing, because cloud is one of the skills that employers are looking for. They really need it. So if you want to be more employable, I do recommend learning about cloud.
1633	I want to experiment with AWS outside of the free tier and have services turned on continuously. Any tips or guides to reach low cost (no more than $15/per month)?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1634	I want to experiment with AWS outside of the free tier and have services turned on continuously. Any tips or guides to reach low cost (no more than $15/per month)?	No. It was supposed to be today, but I changed it a little bit. If you go to the updated deadline calendar, you will see that the capstone project starts next week (December 5, 2022). For the evaluation, we give you a lot of time. During this time, depending on when you take your vacation, you will have some time to evaluate the project and then the capstone project starts next week. I think what happened is that when we added Bento ML here, it kind of messed up the whole schedule. The idea was to finish everything by Christmas but, unfortunately, we won't be able to finish everything by Christmas which probably means that if you do the midterm projects 1 and 2, you can get your certificate somewhere here or maybe at the end January. I don't know. Let's see. It will probably be easier to just give certificates at the end of January. Let's say if you finish everything by New Year’s, then you're free to stop taking the course. I know it was very difficult and time consuming. You might as well just finish everything and start the data engineering course or enjoy your life, or do many other things. We'll also have an article, which starts pretty soon. During the next Office Hours, I should probably talk more about that.
1635	How should I evaluate an RMSE to be considered low or high?	"Tim
Yeah, we have a lot of users who use MLflow and BentoML together. Once you have a model that’s sort of your “finalized” model – you save it to your MLflow registry and then (it depends on what your CI/CD pipeline looks like) but BentoML has a command to import from an MLflow registry. You can tell which MLflow model is the one that you want to deploy. It's very similar to save_model, except instead of pulling from your local environment, you're pulling from an MLflow registry. So you import the model, it automatically gets pulled into your Bento and then you can deploy it. On the BentoML documentation site, there's a big framework document on how to integrate with MLflow. 
Alexey
When searching, one of the suggestions was BentoML vs MLflow. I guess there are also some similarities because with MLflow, you can serve models. 
Tim
The thing about serving models is that it's a part of the pipeline that you just have to do. Every single framework out there has some way to serve a model – you train the model and then it just puts it up there to be able to serve. Typically, frameworks don't specialize in serving those. BentoML specializes in making the serving part really, really fast and really, really easy. What you find a lot of the time is, when you have serving logic in a really large end-to-end pipeline, it just puts the model there and then the only thing that you could submit to the model is the data. But we know in practice that there's usually business logic around this – pre-transformation logic, post-transformation logic, there's the version of the model that was saved. There are a lot of these other components that you want around the model, not just the model for inference. And of course there's the performance layer underneath. The BentoML architecture kind of brings that all together into one deployable, rather than just giving you one point to call inference for your model. 
Alexey
That is a comment in the live chat that says “There is PyCaret, too.” This is how you use PyCaret with Bento. 
Tim
Right. BentoML has lots and lots of integrations and we're only adding to them every day. The idea is to get the model from wherever you built it, bring it into your Bento, and then you can deploy it anywhere. Once it's in the Bento – in this standard packaging – it's nice because then we've got lots of different tools to deploy to Lambda, SageMaker, and lots of different places. And the places to deploy, we're adding to every week and month as well.
Alexey  
Since you can have a Docker container at the end – you do “bento_containerize” right? That's the command? And then you have a Docker container (Docker image) that you can deploy everywhere where you can deploy a Docker container, which is pretty much every place on the internet, right? 
Tim
Right. Some places, though, require specialized… 
Alexey
Like Lambda, right? 
Tim
Yeah, like Lambda won't accept just a normal Docker container. You have to actually create special endpoints. When we deploy to Lambda, we create those endpoints for you and make sure that it's behind an API gateway and things like that. For certain services, there are a lot of nuances that we kind of take care of for you – as long as you're in the Bento standard format.
Alexey 
For those who have no idea what MLflow is, and why we are talking about this – you don't need to worry about this right now. But at some point, this tool and other similar tools will be quite handy. You can check out our MLOps Zoomcamp course, after you finish this one. Don't try to do multiple courses at the same time. That could be too much. But of course, if you're curious, go check it out."
1636	How should I evaluate an RMSE to be considered low or high?	Why do you think it will be irrelevant? Do you think we'll just identify all churned users, bribe them with discounts, and live happily ever after? At some point, other users will consider leaving too, so we actually need to run this model regularly. We will need to update this model regularly – retrain – because the model will probably make mistakes. So it's probably a never-ending process here. We will always need to keep an eye on this model – probably in an automated way. Maybe we have a process that automatically retrains it every half a year or something. But I don't think just deploying this model and using it will solve the churn problem. Maybe I misunderstood your question. I don't know.
1637	How should I evaluate an RMSE to be considered low or high?	"This is very problem-specific. You cannot. If you're evaluating a project and you see that they have this RMSE, there's nothing you can do with this information. There is no criteria to even evaluate this. If you notice, there is no criterion that says, “Okay, the model quality is good or not.” So we don't evaluate this, because all of this is problem-specific. If this is for your project, what you can do is train a baseline. The baseline could be just predicting the average value. 
Let's say you have all the cars – you take the average price for these cars and then you compute the root mean squared error of this prediction. You just simply predict the average for all the cars. You will get some root mean squared error and then you train a simple model. Then this simple model, hopefully, should be an improvement over the baseline and then you can see what the improvement is. It might be, I don't know – 200, 300%. Then, because you already know the baseline value, you can compare the improvement over the baseline. This is how you can understand if it's low or high. If it's worse than the baseline, it's low, of course. But if it's better than the baseline, then it's better. You can see that, for example, XGBoost gives you even better performance than linear regression, and you can have a relative ranking of the models."
1638	How should I evaluate an RMSE to be considered low or high?	Yeah, if it doesn't look like we're using it – we are not using it. Maybe we just want to set it aside and forget about it.
1639	How should I evaluate an RMSE to be considered low or high?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1640	Is it possible to apply to jobs after the course?	"Jeff
I'm not sure exactly what's being referenced here. But I'm guessing maybe the GitHub question from the homework – the GitHub block question turned out to be a little trickier than anticipated for folks. This is a great question. I got a couple of things on it here. One is that there is some stuff in the documentation, but maybe not a complete handhold of things. The storage doc does talk about code storage and flow storage in general here, and a little bit about how to do it. 
If you would like more detailed information, it’s always a great idea to look and think about me opening an issue. If you go to the GitHub repository for Prefect, you can click on “new issue”. If you want, you can propose a feature enhancement, if you want to call it that, or report a bug if you think you should have more information, and then put in what you would like to see. 
If you want to go one step further at something I really recommend, even like the next extension of Learning in Public is contributing to projects. This is an open source project here. You're welcome to contribute to it. You could go ahead and fork the repository, clone it down, and make a change to the documentation once you've figured things out there. Lots of lots of folks I know did get there with some support, through the FAQ and through Slack. It's partly what the videos are also for, in the course, to help you out as you're going. That's what I would suggest there. 
But I will also suggest to our docs folks, that we perhaps enhance the GitHub section in particular."
1641	Is it possible to apply to jobs after the course?	I would say to focus on projects, not on tools. In this course, you will get a chance to do this. If you open a job description for data engineers, you will see a lot, a lot, a lot of different technologies. Maybe think of a project that you can do for this particular company and do this project using, for example, the tools you learn here or the tools you want to learn, and build a portfolio. I guess this is more of a tip for getting a data engineering job, rather than for somebody who's starting their data engineering career. But if you already have a job, then maybe think of the business impact of what you need. I think I will not spend too much time talking about that because this requires a one hour long discussion. Perhaps we should have a podcast interview about that. If you can think of any potential guests for this podcast interview, you can let me know and we can try to invite these people and talk about this in more detail. One thing I want to share with you is that in our YouTube channel, we had a few interesting talks. We had this Getting a Data Engineering Job with Jeff talk. It’s a very nice presentation. Then we also had an interview with Jeff. So these two podcasts are quite interesting. You can also check out other ones. I think you'll find a lot of interesting content here.
1642	Is it possible to apply to jobs after the course?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
1643	Is it possible to apply to jobs after the course?	Building projects. Maybe build another one, where instead of batch, you use streaming, maybe. If you're interested in a particular employer, try to build a project that is similar to what they’re working on. For example, if you want to apply for Spotify, try to find some music data and build a project for that.
1644	Is it possible to apply to jobs after the course?	"Alexey
It's possible to apply to jobs right now. You don't have to take the course. Just start applying. Why do you need to take the course to start applying, right? It was possible to do it last year. Nobody is stopping you from doing this. But this course will give you some knowledge and add projects to your portfolio. That will be helpful in your job search. 
But I would encourage you to start applying to jobs right now to see what they actually want from you – what kind of topics come up, what kind of test assignments you get. You don't need to wait until the course is finished to start doing this. You can already learn a lot about the job market in your area or in the domain where you want to work.
Luis
I just want to add that when I did the 2022 cohort, I was in another company and the process of data engineering in that company was really similar to the Zoomcamp. What I started to notice was that the companies mostly use this architecture in engineering. So it's good for you to show yourself that you're starting to understand all this data engineering stuff. I think it's important.
Ankush
I just wanted to add. I don't know about applying for jobs, but once you have a job, if you want to get an increment, contact Michael. [chuckles]

Victoria
Michael moneymaker. [chuckles]"
1645	Is it possible to use the same data for the next project but investigate it with a deep learning framework?	Yeah. If it works for you, use it. I don't mind.
1646	Is it possible to use the same data for the next project but investigate it with a deep learning framework?	I would prefer that you don't do this and that you take another dataset instead. If you take the same EDA (exploratory data analysis) and you repeat it in a new project, it's kind of plagiarism. If you don't do this – if you do everything completely from scratch – it's kind of okay. But maybe you want to have a variety of things for your portfolio? So maybe try to get a different dataset, just to get exposure to different problems. It's not a super strict requirement – as long as you're not plagiarizing or self-plagiarizing, you're good.
1647	Is it possible to use the same data for the next project but investigate it with a deep learning framework?	That's a very similar question to when we should use the lambda and when we should use Kubernetes. If you want to use Kubernetes, then go with TensorFlow Serving. Otherwise, TensorFlow Lite. There is one thing where you have to use TensorFlow Lite – where you cannot use TensorFlow Serving – and this is when you want to deploy your model on a mobile device. I have a phone here. If you want to create an app with a model, then you will have to use TensorFlow Lite because you will not be able to use TensorFlow Serving. TensorFlow Serving also adds a bit of complexity. You will need to have this gateway service, you will need to have GRPC service, so that becomes more complex. If you have a lot of load, then you need to do this otherwise, TensorFlow Lite.
1648	Is it possible to use the same data for the next project but investigate it with a deep learning framework?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
1649	Is it possible to use the same data for the next project but investigate it with a deep learning framework?	I think for binary features, just one and zero makes more sense than OneHotEncoding. OneHotEncoding will take care of that but you will have two columns that are the exact opposite of each other. So that is a bit redundant. I think it's better just to convert it to a Boolean column with ones and zeros.
1650	Do I need to complete both projects to get the certificate or just one?	"Alexey
Yeah, that's not going to happen. I think ChatGPT will help us do our job, but it will not replace people, at least in my opinion. But I use it in my work and it's helpful. You should try using it too – if you want, of course."
1651	Do I need to complete both projects to get the certificate or just one?	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
1652	Do I need to complete both projects to get the certificate or just one?	Just one. It's either the first attempt or the second. You should not try both.
1653	Do I need to complete both projects to get the certificate or just one?	Yes, you will have to come up with your own individual projects. It's not easy, I know. But we'll help you. We will help you to come up with a project idea. That's actually on purpose because we want every project to be unique. Then it becomes a part of your portfolio when you look for a job.
1654	Do I need to complete both projects to get the certificate or just one?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1655	Maybe you have some links to run a GPU with models in local?	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
1656	Maybe you have some links to run a GPU with models in local?	"[chuckles] No, I don't. Just go to Google and you'll find a lot of them. I don't suggest spending time on that. If you want – not because I asked you to do it – because it's likely that you will spend a lot of time figuring out how it works and why the stuff in this article doesn't work on your machine. I've been there. I don't recommend it. 
I would suggest you try to go with Saturn cloud or something similar and avoid spending a week on figuring out how to set up cuDNN and all these things on your machine, especially if it's a Windows machine. It’s terrible. I did set up something like this on Windows as well and it wasn't easy. I don't recommend doing this. It’s better to spend this time on something else. [chuckles] But if you want – if you insist on doing this – the internet is your friend. 
Just go to Google and you will find a lot of resources. The catch there is that not all of them actually work and you'll have to figure out which tutorial works and which doesn't. I think on Ubuntu it’s actually simpler, at least according to some of the students who configured this. Maybe just go to Slack and ask, “Hey, I need help with configuring my local GPU.” Because I know at least two students did this. So if you're brave – do this."
1657	Maybe you have some links to run a GPU with models in local?	"It all depends on what you mean by “full stack ML engineer”. As you’ve noticed, data engineer Zoomcamp is not about machine learning – it's about data engineering. If in your opinion a “full stack ML engineer” needs to know data engineering, then you should do data engineering. If not, then no. From my point of view, there is a “full stack of data scientist”. What I put in the definition of a full stack data scientist is – a person who can do all the steps in the CRISP-DM process. They can help product managers with business understanding, they can work with analysts in data understanding, they can help data engineers in the data preparation step, they can do the modeling, and they can deploy the models. If you want to be that kind of person, then, of course, you also need to learn a bit of product management and analytics, which we don't have courses for. But data engineering – preparing data before it goes to a model – will certainly be helpful. 
Also, machine learning ops Zoomcamp (MLOps Zoomcamp) will be helpful as well. I would suggest, if you really want to focus on machine learning engineering, then probably doing MLOps Zoomcamp will make more sense for you. But data engineering Zoomcamp will be quite useful in the future, because data scientists and ML engineers tend to work on data pipelines as well. 
Maybe not all the content will be useful for you. For example, the content about data warehousing – as a data scientist, I don't find it super useful in my work to me personally. Also, the module about analytics engineering – it's nice, but it's not something I do day to day. But the modules about batch, the modules about Spark, the modules about streaming – this is something that can be useful, because this is something we could use in building machine learning pipelines as well. 
To put data in a machine learning model, we need to prepare it – we need to create data pipelines – and this is pretty relevant. But you can just check it out. All the content is already there. You can just go and check it out. If you like it, do it – if you don't, don’t. I would suggest maybe going with this one (ML Zoomcamp)."
1658	Maybe you have some links to run a GPU with models in local?	Yes. We will actually cover cross validation this week. Sometimes, if your dataset is small, (this was one of the questions today) then using cross validation is a very good idea.
1659	Maybe you have some links to run a GPU with models in local?	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
1660	Where can I find resources/materials for this program?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1661	Where can I find resources/materials for this program?	I'm glad you asked. MLZoomcamp.com. All the materials are here and you will find them there.
1662	Where can I find resources/materials for this program?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1663	Where can I find resources/materials for this program?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1664	Where can I find resources/materials for this program?	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
1665	Do you know why your CI is failing?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
1666	Do you know why your CI is failing?	"Tim
I think a couple people asked this in ML Zoomcamp Slack. I was thinking about this a little bit. I think it's sort of a philosophical thing [chuckles] where we think about models and Bento is sort of separate a little bit from code. So your project directory is where all the stuff that you'll commit to Git. I don't think we think that it's the right model to commit your models to Git, for example. We think of ourselves a little bit more like Docker. You don't have Docker images in your project directory, right? You just have your Docker file and that's what you commit to Git. In the same way, we store your models and your Bento in that dedicated directory, and then sort of leave it up to you to push it to different places where you might build or where you might need it
Alexey
Do you know why you decided to save it locally on your local file system? For example, what MLflow does – you can save it to S3 directly, but in the case of Bento, you always first save it to local storage and then you decide “This model I want to export, but for the rest, I don't care.”
Tim
Right. First, I think it’s because that's the easiest way to do it, but also because I think with BentoML, the workflow is that a lot of times, somebody's building a model on their laptop, or somebody's building a model in a slightly less distributed system and they're assembling their Bento there. So I think that the fastest way to access that model is locally. We do give you the ability to push that model and pull that model from other places. But ultimately, when you assemble your Bento and then you assemble your container, all that stuff has to be there. I don't think we wanted separate abstractions for when a person is training and saving, versus when a person is building a Bento and pulling in the model and assembling the container. It's just the same abstraction, which is just a local repository which contains models.
Alexey 
Maybe you don't want to save every model every single time you change one single parameter to the cloud. [Tim agrees] Maybe at the end, after all the experiments, you want to save the final one. Meanwhile, for MLflow, the use case is different – you actually want to save every single experiment because you want to see the results of these experiments.
Tim
Right."
1667	Do you know why your CI is failing?	I don't think I understand the question. What do you mean by “general points”? Do you mean “What are the similarities between them?” For NLP, you can use some of the ideas from computer vision. For example, convolutional neural networks work for NLP as well. But that's pretty much the extent of what I know about that. [chuckles] I don't know much about NLP, to be honest – only the basics, like embeddings. I don't know how to answer that question.
1668	Do you know why your CI is failing?	"Tim
I actually don't. [chuckles] I think there’s some really obscure version of Ubuntu, actually, that is not passing. We tried to regress it on multiple OSs and I think there's an obscure version of Ubuntu in there.
Alexey
[chuckles] I will not go there. You mentioned that the best way to contribute is to just run it and see if it works. Knowing how many different environments students of this course have is already a good contribution because all of us have different versions of Windows, different versions of shells in this OS. With Windows life is always dangerous and full of surprises. [laughs]
Tim
Well, Mac M1 is kind of similar. But yes, Windows can show problems sometimes. I think the best way that I've found to have a consistent environment is just to create a Ubuntu instance in AWS and then just go from there. And then if my environment completely blows up, I can just stop the machine and then start a new one. [chuckles]"
1669	Do you know why your CI is failing?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1670	From an employer’s perspective, along with this course, what else would you want to see from job applicants in their portfolio? What else should we work on?	Building projects. Maybe build another one, where instead of batch, you use streaming, maybe. If you're interested in a particular employer, try to build a project that is similar to what they’re working on. For example, if you want to apply for Spotify, try to find some music data and build a project for that.
1671	From an employer’s perspective, along with this course, what else would you want to see from job applicants in their portfolio? What else should we work on?	"Alexey
The first thing about the dataset – what kind of dataset do you want to use? Or what kind of problem do you want to solve? Once you figure this out, then you're basically ready to start working on a project. Then in the project, you need to decide if you want to do streaming or batch. For batch, it's using things like Prefect, Spark, or DBT. For streaming, it’s using the materials from the last lecture (week 2). Once you decide that, you will just implement this and you will find all the information you need here in the week 7 project repo. Just go through this and if you have any questions left, let us know. Keep in mind that these are the criteria that other people (your peers) will use when evaluating your project. Perhaps you can already think about that and how you want to implement your project in such a way that you maximize the score you get from these criteria."
1672	From an employer’s perspective, along with this course, what else would you want to see from job applicants in their portfolio? What else should we work on?	No, you cannot.
1673	From an employer’s perspective, along with this course, what else would you want to see from job applicants in their portfolio? What else should we work on?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
1674	From an employer’s perspective, along with this course, what else would you want to see from job applicants in their portfolio? What else should we work on?	We currently do not have any course partners yet. If you work at a company and you want to partner with us, please reach out and we will see how we can do it. In one of our courses, which was our first iteration of the Machine Learning Zoomcamp, we partnered with a company called Delphi. They got two interns from our course and they were quite satisfied with the outcome. You can actually read more about the work of these interns in our articles – Interview with Valerii Chetvertakov and then another one, Interview with Ken Wu. You can learn more about the interns and if you think that, at your company, you need good interns or juniors or you want to partner in any other way, please reach out and we can see how to make it work.
1675	What to use: MLflow, Kubeflow, SeldonCore? No preliminary requirements defined.	"Actually, I don't remember exactly how I do this. The right approach would be to use only train data and not use validation data or test dataset. This is done on purpose for exactly this reason that you mentioned. We want to model a situation in which there is some unseen data – it's truly unseen, we do not see that all – and anything can happen to this unseen data, including the situation that you described, such as there is a value that is only present in the test data. 
Imagine that there is a new iPhone – when you train your model, this new iPhone didn't exist (you didn't have this category) but you apply this model, and then, all of a sudden, there is a new iPhone. How will your model react to this? You need to somehow model this situation. This is totally normal. This can and will happen in practice. We just want to understand what would be the effect of that. That's why we need to see this in training data. This is good and this is how it should be. I hope that is a good answer."
1676	What to use: MLflow, Kubeflow, SeldonCore? No preliminary requirements defined.	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1677	What to use: MLflow, Kubeflow, SeldonCore? No preliminary requirements defined.	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1678	What to use: MLflow, Kubeflow, SeldonCore? No preliminary requirements defined.	"You will probably also need to learn how to pass interviews. This course does not teach that, but you will need to acquire the skill somehow. The best way to acquire these skills is by interviewing. But apart from that, I think this course covers let's say 80% of the skills that you will need for a job and the remaining 20% depends on the company. 
So you will need to do some research on the company where you want to work, see what kind of tools they use, what kind of problems they solve, how much emphasis they make in the interviews on the theoretical part. You will need to somehow figure this out and, based on that, see what you need to learn. I think with just this content, you should be able to get a machine learning-related job."
1679	What to use: MLflow, Kubeflow, SeldonCore? No preliminary requirements defined.	To me, these tools focus on completely different areas. You can use all of them, actually. MLflow is about experiment tracking. Kubeflow is about creating model pipelines (if you’re talking about Kubeflow, because there is also a thing called KServe, which used to be a part of Kubeflow but now this is a framework for serving machine learning models. It's very similar to some SeldonCore). That you used for serving models. All three of these tools can actually play well together. What I would suggest you take, if you want to learn more about this and if you're interested in this – there’s another course that we have that you can take after this one, which is the MLOps Zoomcamp, where we talk about MLflow (in module two). Then we talk about allocation and machine learning pipelines. We don't use Kubeflow pipelines here, we use Prefect, but the concepts are pretty similar. We also talk about model deployment, which is kind of similar to SeldonCore and KServe. Also remember that we have a module about KServe in this course. It’s actually an optional module. You don't have to take it and it's not graded if you take it, because it's pretty advanced. I think it's slightly outdated, too. But recently, I was going through this module and all the things that didn't work – I fixed. So it's actually up-to-date, but it might be a bit outdated. You can check this one out too. SeldonCore is pretty similar. The main difference between KServe and BentoML is that for KServe and for SeldonCore, you have to have Kubernetes. Bento works without Kubernetes, but for this, you have to have Kubernetes. So you need to know Kubernetes at least a little bit. That's why we cover Kubernetes and then we cover KServe in addition to that. You can also deploy models with MLflow, but it's not the main focus. I wouldn't use MLflow for model deployment. To me, it's more of an experiment tracking and model registry tool. If you want to learn more about that, please check our MLOps course.
1680	Store raw data in GCS-> Pull & Transform Data-> Store in GCS-> Pull into BigQuery. Is this okay or the data after transformation should be only stored directly in BigQuery?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1681	Store raw data in GCS-> Pull & Transform Data-> Store in GCS-> Pull into BigQuery. Is this okay or the data after transformation should be only stored directly in BigQuery?	"Alexey
To me, it looks fine. I mean, it really depends on your use case, but for me, this is perfectly fine. There could be reasons for skipping data in GCS and putting it directly to BigQuery. I mean first putting it into BigQuery and then transforming it there. So I think this is how you typically use DBT. I don't think there is any right or wrong way."
1682	Store raw data in GCS-> Pull & Transform Data-> Store in GCS-> Pull into BigQuery. Is this okay or the data after transformation should be only stored directly in BigQuery?	Contributions are welcome. If you know how to do this, then please create a guide and then share it with us. You can also create a pull request and include the link in the course repo.
1683	Store raw data in GCS-> Pull & Transform Data-> Store in GCS-> Pull into BigQuery. Is this okay or the data after transformation should be only stored directly in BigQuery?	Both. You can do it both ways and there are pros and cons. It really depends on the use case. From what I understood, it’s usually cheaper to first transform the data, put it into a data lake, and then load it to Google BigQuery. For example, in cases of DBT, this is what you do with DBT. You transform the data that is already in the data warehouse. Thus, both options work.
1684	Store raw data in GCS-> Pull & Transform Data-> Store in GCS-> Pull into BigQuery. Is this okay or the data after transformation should be only stored directly in BigQuery?	I don't understand the question, to be honest. We don't have the generation part. The generation part is what was done already for us by the New York Taxi Limousine Company. Storage – yeah, they also store it. They host the data. And yeah, we cover ingestion, transform, and serving with the dashboard.
1685	Will it be possible to find a job as a junior in data engineering after completing the course?	I don't have enough context to answer this question. I don't know. Ask in Slack, please.
1686	Will it be possible to find a job as a junior in data engineering after completing the course?	No, you cannot.
1687	Will it be possible to find a job as a junior in data engineering after completing the course?	"Alexey
I think 16 gigs of RAM should be sufficient. But again, this year, we are not doing Airflow. Airflow was the most problematic week when it came to local machines. When we tried to run Airflow in Docker, my computer was basically heating up and was going to fly away on the fence. It was too much for it. In the end, I did most of the stuff in a remote virtual machine. This is what you should do as well – use a remote virtual machine because you have this $300 credit. But 16 gigs should be enough, 8 gigs will not be enough. 
Michael
Just in case they didn't catch that. If you don't have enough RAM or CPU, you can just start from the beginning in a virtual machine in GCP – you've got more than enough credits to make it through the course and be running that machine from the start.
Alexey
That's the best thing. Then if everyone has the same platform, same operating system (which is Ubuntu) it just makes it a lot easier to help you. You just say, “Okay, I'm using this virtual machine. It's Ubuntu. This is the problem I have. And then anyone can reproduce this problem, hopefully. Meanwhile, if you’re running this on Windows or MacOS, then everyone has a slightly different version of Windows and sometimes it's just impossible to produce."
1688	Will it be possible to find a job as a junior in data engineering after completing the course?	Yes. As I’ve already said during the launch stream, you can start applying for jobs right now. You don't actually have to complete the course to find a job. Maybe you already know enough for you to convince an employer to hire you. By that, I mean you already know how to program, you know the command line, and things like that. It will be enough – it will be possible – and many people from the previous iteration of the course did that. They found a job. Some of them, not even as juniors.
1689	Will it be possible to find a job as a junior in data engineering after completing the course?	We’re still in progress (as of January 26, 2023). I thought we would do this earlier. But it's still not finished.
1690	Any tips for understanding from scratch, hypothesis testing and p-values?	"There is a good book about that called OpenIntro Statistics. You can check that one out. There is also a book called Think Stats. That's also a good one. Check these two books out, especially Think Stats. I think it's free. Both of these books are actually free. As you can see, it's available as a PDF. Just go through this book. 
Think Stats is good because it shows code – there is a lot of code. It just shows you all these things like hypothesis testing, p-value, and everything. To some extent, it's kind of similar to this Zoomcamp. They focus first on a project and then they implement some of the things. Check that one out."
1691	Any tips for understanding from scratch, hypothesis testing and p-values?	I guess, use validation.
1692	Any tips for understanding from scratch, hypothesis testing and p-values?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1693	Any tips for understanding from scratch, hypothesis testing and p-values?	"You probably mean for logistic regression, right? For logistic regression, it's doing something very similar to what we saw in module 2. It is trying to minimize the… (I'm just trying to figure out how I can explain it without going into too many details and not to get lost myself and not to confuse you). It's probably a good idea to refer to some other course. I think I partly talked about this. Let me check. Here I talk a little bit about this, in ML Zoomcamp Office Hours week #4, where I explain a little bit how exactly it works. But it's a very superficial, let's say, overview of exactly what's happening there. 
If you want to learn in more detail how exactly the process works there, maybe you can just go to Google and search for “gradient descent, logistic regression,” or something like this. The first result will probably have a good explanation of what's happening under the hood. SciKit Learn does not use this exact method – it doesn't use gradient descent – it uses something more complex (more advanced) than that. But this is roughly what's happening under the hood. I hope you're satisfied by this answer. I don't think I can give you a better one with the time we have. It will be like a separate lecture. If you don't want to go into details, you can think of this as something that’s similar to logistic regression happening there but we add sigmoid on top of that."
1694	Any tips for understanding from scratch, hypothesis testing and p-values?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1695	Do we leave the GCP instances running until the course completion?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1696	Do we leave the GCP instances running until the course completion?	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
1697	Do we leave the GCP instances running until the course completion?	I will not answer this right now. We did not plan anything like that. I think Anna prepared some materials about how to deploy Prefect, not with Terraform, but with some Google Cloud Platform stuff. I might be mistaken, but in practice, I think we use something like Kubernetes. Kubernetes is not managed through Terraform. I will leave this question answered and in the next Office Hours when we'll talk more about Prefect. Somebody from the Prefect team will answer this question. Probably Jeff will be in our next Office Hours.
1698	Do we leave the GCP instances running until the course completion?	Yes, you will have to come up with your own individual projects. It's not easy, I know. But we'll help you. We will help you to come up with a project idea. That's actually on purpose because we want every project to be unique. Then it becomes a part of your portfolio when you look for a job.
1699	Do we leave the GCP instances running until the course completion?	I wouldn't. You can just stop the instance without terminating it and then you can resume it. It's effectively doing the same thing as powering off your machine and then turning it on when you're back and want to work.
1700	Can you please explain what is the best way to save data when it is collected with parsing – in BigQuery or collecting it in Postgres first?	No, you cannot.
1701	Can you please explain what is the best way to save data when it is collected with parsing – in BigQuery or collecting it in Postgres first?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
1702	Can you please explain what is the best way to save data when it is collected with parsing – in BigQuery or collecting it in Postgres first?	Yes. If Michael was here, he would probably confirm this because this is the background he has. He's an analyst. We also had other analysts who did pretty well in the course. So yeah, certainly you can do well. As a beginner in data engineering, I would say that we don't expect any data engineering knowledge for taking this course. If you're a beginner already – if you know some data engineering concepts – then it's more likely that you will do well in this course.
1703	Can you please explain what is the best way to save data when it is collected with parsing – in BigQuery or collecting it in Postgres first?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
1704	Can you please explain what is the best way to save data when it is collected with parsing – in BigQuery or collecting it in Postgres first?	"Alexey
I would actually suggest neither. I would actually suggest something like a data lake first. And then from this data lake, you can parse it – you can transform it and you can save it in BigQuery, Postgres, whatever data storage you want."
1705	Will you help us with coming up with interesting project ideas?	"Alexey
Well, I can suggest a project for this course. At the end, after you complete this course, after you complete all the modules, as a part of this course, you will have to work on the project. This is how you will get a certificate at the end – if you pass the project. This is a good project to include in your portfolio. 
Michael
I don't know if I have much more to add other than being active in online communities. This course is a great place to start. There are a lot of large cities that have meetup groups, which are always great. One of the good side effects of the whole COVID pandemic is that a lot of those are remote now. If you Google “San Francisco Big Data meetup,” meet people, talk with them, get their insights. I think that'd be a good way to approach that.
Jeff
Plus one just for getting involved in communities, whether it's conferences, online or in-person is great. Volunteer, if you can, at those. That's often a great way to help out and work with people. Go to meetups. I run a meetup in DC on data science. If anyone's in the DC area, there are data engineering meetups in town too. If there isn’t one somewhere, and if there's a few people around, I encourage you to start one. That's something you can do, regardless of your level of experience.
30:06  
Alexey
Then I'm going to do another shameless plug. We have a podcast and the topic of career changing comes up pretty often there. One interesting one is from Juan Pablo. Here he's actually not talking about… well, it's kind of related to analytics engineering. He was a math teacher, which doesn't really qualify as an IT person. He was also working as an Uber driver. In the podcast, he tells his story of how he actually did this. He talks exactly about that. He talks about going to meetups. Instead of not talking about this, just go and check it out. That's not the only relevant podcast episode. You can just go through everything we have and see if anything catches your interest. I'm sure you'll find a lot of interesting stuff."
1706	Will you help us with coming up with interesting project ideas?	Yes, we will share some ideas with you. In this project folder, we have datasets. You can just go through these datasets and find something is interesting – you can just take it and do it. If you really struggle with ideas, of course, ask in Slack and we will be happy to help you.
1707	Will you help us with coming up with interesting project ideas?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
1708	Will you help us with coming up with interesting project ideas?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1709	Will you help us with coming up with interesting project ideas?	No, you cannot.
1710	How do you know if you should choose a data engineer or a data scientist as your career path? What if you're confused between the two?	I do not. I was actually thinking about doing this, but it will probably be quite expensive. If any of you are interested in this, maybe write to me in Slack and then I can think about how to organize that. But so far, there is no “official” program for that.
1711	How do you know if you should choose a data engineer or a data scientist as your career path? What if you're confused between the two?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1712	How do you know if you should choose a data engineer or a data scientist as your career path? What if you're confused between the two?	[chuckles] I'm not prepared to answer that question. I don't have a top three for guidelines. Maybe this is something we can discuss next time. I don't know how to answer that. Maybe what you can do is go to our YouTube channel (I know I keep doing this) where we just had this Responsible and Explainable AI Interview about that. So maybe check it out. I will mark this as “answered” and then maybe this is something we can go through next time, like we did last time – some of the questions I didn't answer and we moved it to this week. So probably we’re going to keep doing it like this.
1713	How do you know if you should choose a data engineer or a data scientist as your career path? What if you're confused between the two?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1714	How do you know if you should choose a data engineer or a data scientist as your career path? What if you're confused between the two?	"I don’t know… throw a coin and the coin will tell you what to do if you don't know. [chuckles] And then if the coin tells you that you should be a data engineer and you feel like “Umm… maybe not.” Then go with the other option. I don't have a better suggestion here. I can tell you to follow your passion and follow your heart… But yeah. [chuckles] I don't know. You’ll eventually need to make these decisions yourself. Just do a project in data engineering, do a project in data science, and then ask yourself what you enjoyed doing more. 
Also talk to people – talk to a few data engineers, people who already work as data engineers, people who already work as data scientists, and ask them what they don't like about their job. And then ask yourself, “Would you enjoy doing things that they dislike? Or would you be okay doing these things as well?” For example, what I don't like in the work of a data scientist is all this parameter tuning and parameter optimization, trying different features – all this stuff I find boring, but some people really love it. If some people tell you, “Eh, I don't like this part,” and you think, “Okay, it's actually not bad for me,” then maybe this is the right path for you. 
In the end, if you join as a data scientist, it doesn't mean that the door for a data engineer is closed for you. As a data scientist, you will get to do a lot of data engineering as well. I, as a data scientist, also do data engineering. I also do machine learning engineering. The same is not always true for data engineers. If you get hired as a data engineer, maybe you will get to do data science, maybe not. It depends on the team. So then, what you can explore is the so-called full-stack data scientist. But again, titles sometimes don't mean much. Don't try to force yourself into one of the predefined boxes. Sometimes you can work ‘officially’ as a software engineer and do both data science and data engineering and enjoy this work."
1715	In my project deployment, I want to post requests not in the JSON format, but text used to collect my data (scrape). How can I do this? Write a function on service.py?	Yeah. For example, do you use Flask? In Flask, you can see what kind of data... But wait, why don't you just put the data – the text – in your JSON? Your JSON file will look like this curly bracket and text is a parameter and then the text that you have, you just put it as a value and you send that. That will make your life simpler, I think, because you might also want to add some extra data (some metadata) about the text. So I would actually suggest you stick to JSON and put your text and JSON. But Google is your friend. You can just check for the framework you use – if it's Flask, then check for “Flask text input content type” – something like that. Or the same with Bento. With Bento, it's actually more explicit. If you remember, we defined the type – it could be JSON, it could be a numpy array. They probably also support text, but you will need to check the docs.
1716	In my project deployment, I want to post requests not in the JSON format, but text used to collect my data (scrape). How can I do this? Write a function on service.py?	It is possible to do everything locally. You will need to do it by… I think you will need to do it in module five when we deploy something. Or actually, you will probably need to do it when we do your project. Yeah. It's fine to do everything locally. But if you have access to cloud and you want to practice with this, I do recommend practicing, because cloud is one of the skills that employers are looking for. They really need it. So if you want to be more employable, I do recommend learning about cloud.
1717	In my project deployment, I want to post requests not in the JSON format, but text used to collect my data (scrape). How can I do this? Write a function on service.py?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
1718	In my project deployment, I want to post requests not in the JSON format, but text used to collect my data (scrape). How can I do this? Write a function on service.py?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1719	In my project deployment, I want to post requests not in the JSON format, but text used to collect my data (scrape). How can I do this? Write a function on service.py?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1720	Is there any relation between BentoML and Streamlit or Gradio?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
1721	Is there any relation between BentoML and Streamlit or Gradio?	"I don't know how to answer that, because I work as a principal data scientist. I work on pretty much everything that the company needs me to work with. This means that all the models are related, to some extent, to online classifieds – to online marketplaces. I need to deal more with domain expertise, maybe, rather than ML knowledge. I guess that's the answer. 
We have a lot of different use cases – we have recommender systems, we have search, we have moderation. We actually have an article about this. For example, Learning-To-Rank: Sorting Search Results. My colleague wrote this one. I think we have an article about data science, Data Science at OLX which happens to be written by me. You can see what kind of use cases we solve. Maybe it could be interesting for you."
1722	Is there any relation between BentoML and Streamlit or Gradio?	That's a very similar question to when we should use the lambda and when we should use Kubernetes. If you want to use Kubernetes, then go with TensorFlow Serving. Otherwise, TensorFlow Lite. There is one thing where you have to use TensorFlow Lite – where you cannot use TensorFlow Serving – and this is when you want to deploy your model on a mobile device. I have a phone here. If you want to create an app with a model, then you will have to use TensorFlow Lite because you will not be able to use TensorFlow Serving. TensorFlow Serving also adds a bit of complexity. You will need to have this gateway service, you will need to have GRPC service, so that becomes more complex. If you have a lot of load, then you need to do this otherwise, TensorFlow Lite.
1723	Is there any relation between BentoML and Streamlit or Gradio?	"Tim 
Not really. We have users who use both Streamlit and Gradio. I think Streamlit and Gradio are a bit more about the presentation side. We do have a couple of users who have asked us about deeper Streamlit or Gradio support and I think that's something that's probably coming up in the next few months.
Alexey
For those who don't know what Streamlit or Gradio is – this is a way to create an interface for your services. Well, Gradio is focused on machine learning, meaning you create interfaces specifically for models. And then Streamlit is just a Python package for creating interfaces. We actually had a project at DataTalks.Club about Streamlit. I actually even have a tweet about this. This is what Streamlit looks like. In the video, you can see that you have this interface, and then based on the toggles that you move the core of your model changes. It gives you an easy way to build an interface like that. Meanwhile Bento focuses on quite a different use case, right? It’s not building interfaces, but serving models.
Tim 
Right, yeah. I think one of our users was asking for a really easy way, so that once you have your model and your Streamlit UI to be able to create a Bento deployment with one line in that way, anybody that you gave your Streamlit app to would be calling to that service and it could work anywhere. Which is a nice feature, I think.
Alexey 
Yeah, I think it is. I imagine if you have a Pydentic class there, then you can easily generate a Streamlit app for that.
Tim 
Right. Oh, that's a good idea. I hadn't thought of that. Yeah.
Alexey
And then in addition to that, the Swagger UI that you have, could be like having the actual interface. 
Tim
Oh, yeah, that would be cool. [chuckles]
Alexey
That could be a good Hackathon project, right?
Tim 
Yeah, for sure.
Alexey
From what I see – let's say you’ve built a model. Then you deployed this model and you have this nice API. You can query it with CURL. But what if you want to demo this project to your manager, or a manager of your manager, or somebody who does not necessarily have a technical background? If you give them the URL and say, “Okay, you need to execute the CURL command with a post request,” they would be like, “CURL what? What do you want from me? Just show me how it looks.” This is when tools like Streamlit come in handy. Instead of giving them a command line interface to query it, you just give them “Okay, this is the link. Play with this.” If Bento could do something like that – automatically generate things like that – many data scientists would thank you.
Tim
Yeah, I was just looking because I figured there's a library that turns Pydantic models into Streamlit apps. It looks like there is one that does that. I wonder if we could just plug into that app and then have that same thing. 
Alexey
That's cool. 
Tim
Yeah, that's awesome. That's kind of the cool part of open source – there are so many people out there with so many different projects and we get to collaborate with them and kind of build the best collaborations and combinations of these tools."
1724	Is there any relation between BentoML and Streamlit or Gradio?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
1725	Can we mix the type of transformations (different transformations for different features)?	"I am glad you asked. Project of the Week has nothing to do with the course to the extent that it's just different initiatives that are run in our community. We have a course and we also have Project of the Week. We also have webinars, we also have podcasts. They are all different activities. For the Project of the Week, this week, we'll have a project about recommender systems. The idea there is that every day, you get a set of tasks. On Wednesday, you get this set of tasks. You need to come up with an idea. You need to find a dataset for this idea, and you need to share your progress. Then on day two, you will need to go through the suggested materials and also find some materials on your own and do this thing – learn about the basics, do exploratory data analysis, understand this data that you found and share your progress. As you can see on the project page, every day, you get a set of tasks and you need to do them. So it's not like we're telling you exactly what to do like in the Zoomcamp (in the videos we tell you that “This is the exact sequence of actions you need to execute and this is the result you get.”) With the Project of the Week, it's more independent. We just give you four bullet points and then it's up to you to actually do these things. And then, of course, you share your progress. The idea behind this project is that, at the end, you have a project that you can add to your portfolio. This is not a project that you just took from the tutorial and followed along, but this is a project that you did with some guidance. These are two very different things. That's the idea behind Project of the Week. For the project that we'll have as a part of the course, the idea is somewhat similar, except you will get less guidance – you will have more independence, let's say. The guidance we have for the project in the course is the form of a bunch of criteria that you need to satisfy. Then it's up to you to come up with a plan. But here, for Project the Week, we have a suggested plan that you can follow and hopefully, at the end, come up with a result. Keep in mind that for Project of the Week, some things might not go according to plan and that’s fine. This is just a suggested plan. We don't know if it will actually work exactly like we outlined day by day. It's more just to give you some guidance, but it’s up to you how exactly to approach that. 
There is a comment that says “I did not know about this.” Yes, we have a channel in our slack. It's called #project-of-the-week and it starts this week Wednesday. You'll notice that there's the digit one in the name “2022-10-19-recommenders-1.md”. We'll also have a follow-up project about recommender systems. If you want to take part in this (by “take part” I mean, propose your own topic) for example, some of you were asking about time series, some of you were asking about NLP, some of you were asking about other things in the comments in live chat. Daniel mentioned audio – If you want to learn any of these libraries, we can do a Project of the Week to learn these libraries or methods or approaches or whatever."
1726	Can we mix the type of transformations (different transformations for different features)?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1727	Can we mix the type of transformations (different transformations for different features)?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1728	Can we mix the type of transformations (different transformations for different features)?	I think so? I need a bit more context to answer this one. But yeah, you can mix transformations in any way you want.
1729	Can we mix the type of transformations (different transformations for different features)?	It is possible to do everything locally. You will need to do it by… I think you will need to do it in module five when we deploy something. Or actually, you will probably need to do it when we do your project. Yeah. It's fine to do everything locally. But if you have access to cloud and you want to practice with this, I do recommend practicing, because cloud is one of the skills that employers are looking for. They really need it. So if you want to be more employable, I do recommend learning about cloud.
1730	What do recruiters look for when hiring for remote DS jobs?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1731	What do recruiters look for when hiring for remote DS jobs?	"Tim
Yeah, we have a lot of users who use MLflow and BentoML together. Once you have a model that’s sort of your “finalized” model – you save it to your MLflow registry and then (it depends on what your CI/CD pipeline looks like) but BentoML has a command to import from an MLflow registry. You can tell which MLflow model is the one that you want to deploy. It's very similar to save_model, except instead of pulling from your local environment, you're pulling from an MLflow registry. So you import the model, it automatically gets pulled into your Bento and then you can deploy it. On the BentoML documentation site, there's a big framework document on how to integrate with MLflow. 
Alexey
When searching, one of the suggestions was BentoML vs MLflow. I guess there are also some similarities because with MLflow, you can serve models. 
Tim
The thing about serving models is that it's a part of the pipeline that you just have to do. Every single framework out there has some way to serve a model – you train the model and then it just puts it up there to be able to serve. Typically, frameworks don't specialize in serving those. BentoML specializes in making the serving part really, really fast and really, really easy. What you find a lot of the time is, when you have serving logic in a really large end-to-end pipeline, it just puts the model there and then the only thing that you could submit to the model is the data. But we know in practice that there's usually business logic around this – pre-transformation logic, post-transformation logic, there's the version of the model that was saved. There are a lot of these other components that you want around the model, not just the model for inference. And of course there's the performance layer underneath. The BentoML architecture kind of brings that all together into one deployable, rather than just giving you one point to call inference for your model. 
Alexey
That is a comment in the live chat that says “There is PyCaret, too.” This is how you use PyCaret with Bento. 
Tim
Right. BentoML has lots and lots of integrations and we're only adding to them every day. The idea is to get the model from wherever you built it, bring it into your Bento, and then you can deploy it anywhere. Once it's in the Bento – in this standard packaging – it's nice because then we've got lots of different tools to deploy to Lambda, SageMaker, and lots of different places. And the places to deploy, we're adding to every week and month as well.
Alexey  
Since you can have a Docker container at the end – you do “bento_containerize” right? That's the command? And then you have a Docker container (Docker image) that you can deploy everywhere where you can deploy a Docker container, which is pretty much every place on the internet, right? 
Tim
Right. Some places, though, require specialized… 
Alexey
Like Lambda, right? 
Tim
Yeah, like Lambda won't accept just a normal Docker container. You have to actually create special endpoints. When we deploy to Lambda, we create those endpoints for you and make sure that it's behind an API gateway and things like that. For certain services, there are a lot of nuances that we kind of take care of for you – as long as you're in the Bento standard format.
Alexey 
For those who have no idea what MLflow is, and why we are talking about this – you don't need to worry about this right now. But at some point, this tool and other similar tools will be quite handy. You can check out our MLOps Zoomcamp course, after you finish this one. Don't try to do multiple courses at the same time. That could be too much. But of course, if you're curious, go check it out."
1732	What do recruiters look for when hiring for remote DS jobs?	"That's like a pretty generic question, so the answer will also be generic. They will look at the skills you have, they will look at the match – how the skills you have match what is needed. I guess that's it. This is what recruiters usually look for in candidates. If you're interested, another thing you can check out is our podcasts. 
We have an interview, Recruiting Data Professionals with Alicja Notowska. Alicja is a recruiter. She was working as a recruiter at Zalando, which is a very large company in Germany and she shared some tips from your point of view of a recruiter and what she looks for when hiring people. So check it out. It’s quite insightful. You will probably find useful things there."
1733	What do recruiters look for when hiring for remote DS jobs?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1734	What do recruiters look for when hiring for remote DS jobs?	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
1735	Does AWS ML services the best over Azure? I think the question is “Are the services in AWS better than in Azure?”	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
1736	Does AWS ML services the best over Azure? I think the question is “Are the services in AWS better than in Azure?”	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1737	Does AWS ML services the best over Azure? I think the question is “Are the services in AWS better than in Azure?”	I do not know. I have not used services from Azure to say that. I don't think they are better or worse. They're probably similar in terms of what they can do. They're just different in terms of how you use them and the interface. Just pick whatever you like more. I don't think there is any significant difference.
1738	Does AWS ML services the best over Azure? I think the question is “Are the services in AWS better than in Azure?”	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1739	Does AWS ML services the best over Azure? I think the question is “Are the services in AWS better than in Azure?”	"If you take statistics for machine learning education, the first lecture will be this – you'll probably derive the formula for logistic regression and there will be a slide with all these assumptions about the linear regression model. Here is the Google result: 
https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/assumptions-of-linear-regression/

Go through this. They are usually useful in practice, I guess. But what I typically do is train a model, and then rely on validation to tell me if my model is doing well or not. So if there is something wrong with my validation, I will see that the metric validation is bad. It's just a lot of trial and error, rather than figuring out all these theoretical assumptions. 
I don't want to say that all these assumptions are not relevant. If you work as a data scientist, they still are. But I also found that the more practical way to understand if you can apply linear regression to the data or not, is to just apply it and see if it works or not. If it works – if the results on the validation are good – then it means that you can apply your model to this dataset. 
Maybe some of the assumptions are violated. For example, multicollinearity – this is actually the case in the lectures, when we needed to regularize (add regularization to the model) this assumption was violated. You will see in the lectures what exactly happened. So check it out. I don't think I will be able to give a good answer, because I am not prepared to talk about this, but any statistics book or theoretical machine learning book explains this."
1740	During what time does the GCP billing update?	"Alexey
Yeah, that's not going to happen. I think ChatGPT will help us do our job, but it will not replace people, at least in my opinion. But I use it in my work and it's helpful. You should try using it too – if you want, of course."
1741	During what time does the GCP billing update?	No, you cannot.
1742	During what time does the GCP billing update?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
1743	During what time does the GCP billing update?	"Jeff
I’m not sure, exactly. It might be the case that you made the deployment and it's scheduled and it's just waiting, but you are not connected to that workspace with the space where your agent is running. Agents are looking for work maybe locally, and you're connected to Prefect cloud, for example. That's one possibility. Just make sure that your agent is pulling for work from the workspace where you actually have scheduled the work to run. You can check that when you start your agent – it'll tell you what it's connected to, up at the top of the message that comes right back when you start the agent, in the CLI."
1744	During what time does the GCP billing update?	"Alexey
I don't know. Maybe I'll put this question to Ankush as well. But check the website. I think this is one of the good examples when you can just use Google to find the answer."
1745	Is ETA sometimes called epochs?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1746	Is ETA sometimes called epochs?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
1747	Is ETA sometimes called epochs?	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
1748	Is ETA sometimes called epochs?	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
1749	Is ETA sometimes called epochs?	No, ETA is not called epochs. You will see epochs in the deep learning module. An epoch is typically one iteration over your entire dataset. I will not spend time now explaining what an epoch is. Maybe when it's time, you can check the deep learning video and if still not clear what an epoch is, then we can discuss it. Okay, I hope this explanation makes sense. If it doesn't – again, in the deep learning module, we will talk about a similar concept called “learning rate”. It's actually not similar. It's the same concept. There is actually an entire unit about this called learning rate. You can check that out. Maybe just check this unit separately from the rest and maybe it will help you understand ETA a little bit more.
1750	From your perspective, how is the market for data science and data engineering jobs in Germany?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
1751	From your perspective, how is the market for data science and data engineering jobs in Germany?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1752	From your perspective, how is the market for data science and data engineering jobs in Germany?	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
1753	From your perspective, how is the market for data science and data engineering jobs in Germany?	"Tim
Yeah, we have a lot of users who use MLflow and BentoML together. Once you have a model that’s sort of your “finalized” model – you save it to your MLflow registry and then (it depends on what your CI/CD pipeline looks like) but BentoML has a command to import from an MLflow registry. You can tell which MLflow model is the one that you want to deploy. It's very similar to save_model, except instead of pulling from your local environment, you're pulling from an MLflow registry. So you import the model, it automatically gets pulled into your Bento and then you can deploy it. On the BentoML documentation site, there's a big framework document on how to integrate with MLflow. 
Alexey
When searching, one of the suggestions was BentoML vs MLflow. I guess there are also some similarities because with MLflow, you can serve models. 
Tim
The thing about serving models is that it's a part of the pipeline that you just have to do. Every single framework out there has some way to serve a model – you train the model and then it just puts it up there to be able to serve. Typically, frameworks don't specialize in serving those. BentoML specializes in making the serving part really, really fast and really, really easy. What you find a lot of the time is, when you have serving logic in a really large end-to-end pipeline, it just puts the model there and then the only thing that you could submit to the model is the data. But we know in practice that there's usually business logic around this – pre-transformation logic, post-transformation logic, there's the version of the model that was saved. There are a lot of these other components that you want around the model, not just the model for inference. And of course there's the performance layer underneath. The BentoML architecture kind of brings that all together into one deployable, rather than just giving you one point to call inference for your model. 
Alexey
That is a comment in the live chat that says “There is PyCaret, too.” This is how you use PyCaret with Bento. 
Tim
Right. BentoML has lots and lots of integrations and we're only adding to them every day. The idea is to get the model from wherever you built it, bring it into your Bento, and then you can deploy it anywhere. Once it's in the Bento – in this standard packaging – it's nice because then we've got lots of different tools to deploy to Lambda, SageMaker, and lots of different places. And the places to deploy, we're adding to every week and month as well.
Alexey  
Since you can have a Docker container at the end – you do “bento_containerize” right? That's the command? And then you have a Docker container (Docker image) that you can deploy everywhere where you can deploy a Docker container, which is pretty much every place on the internet, right? 
Tim
Right. Some places, though, require specialized… 
Alexey
Like Lambda, right? 
Tim
Yeah, like Lambda won't accept just a normal Docker container. You have to actually create special endpoints. When we deploy to Lambda, we create those endpoints for you and make sure that it's behind an API gateway and things like that. For certain services, there are a lot of nuances that we kind of take care of for you – as long as you're in the Bento standard format.
Alexey 
For those who have no idea what MLflow is, and why we are talking about this – you don't need to worry about this right now. But at some point, this tool and other similar tools will be quite handy. You can check out our MLOps Zoomcamp course, after you finish this one. Don't try to do multiple courses at the same time. That could be too much. But of course, if you're curious, go check it out."
1754	From your perspective, how is the market for data science and data engineering jobs in Germany?	Pretty good. Now there is a bit of recession – maybe not a bit, but the job market is also declining a little bit. So maybe there are not as many jobs as a year ago, but it's still possible to find a job. There are still job postings. At OLX (where I work) we’re hiring. Maybe we actually hire less in Germany than before, but still – companies are hiring.
1755	Can we take a break, please?	"Yes, of course, by all means – take a break. This course is not meant to squeeze all the life juice out of you. You can take the course at your own pace, too – remember that. Please take things easy and don't worry about missing homework. The world will not stop spinning if you miss a homework assignment. Please be mindful of your own capacity and don't overwork yourself. Also, I should mention again, if you are a little bit behind and you cannot make it to the midterm project, it's not a big deal. It's fine. 
You can catch up later. December and January, (December mostly) will be pretty slow. So you can catch up with everything – all the content – by the end of December and then do a project in parallel to that. Then you will have all of January for another capstone project. Remember, you only need two projects. If you do these two projects as Capstone 1 and Capstone 2, then you will be fine."
1756	Can we take a break, please?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1757	Can we take a break, please?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1758	Can we take a break, please?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
1759	Can we take a break, please?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1760	How does the linear regression algorithm know how to estimate the right weight for a particular feature, given that some features are more important than others?	I wish Tim was here and would help me answer that because I don't know. I assume if you wanted to use Bento, you would probably use TensorFlow Lite – you don't want to use the usual TensorFlow for the same reasons that we talked about in the serverless module, because it's simply too large. Probably for Bento, you would use something like TensorFlow Lite and it will still work. Maybe another thing you can check – in our channel, we had a talk from one of the Bento folks, Building an ML Service Platform from the Ground Up. Here, Sean talks about the differences between TensorFlow Serving and Bento. With TensorFlow Serving, it's good – it's quite optimized for deep learning. There are some problems. First of all, it's written in C++ and expects your model in a certain format – this “saved model format”. Adding anything extra on top of that (like pre-processing, post-processing) becomes difficult, because you will need to have an extra step there. Bento makes it easier. But, again, I don't know. It's very difficult to give you a concrete recommendation without knowing what exactly you want to do. So it's case dependent. If you already use TensorFlow Serving in your company, then maybe it makes sense to stick to that. If we have Tim on Office Hours next week, maybe you can ask him that. He will be a better person to answer this question.
1761	How does the linear regression algorithm know how to estimate the right weight for a particular feature, given that some features are more important than others?	"Actually, I don't remember exactly how I do this. The right approach would be to use only train data and not use validation data or test dataset. This is done on purpose for exactly this reason that you mentioned. We want to model a situation in which there is some unseen data – it's truly unseen, we do not see that all – and anything can happen to this unseen data, including the situation that you described, such as there is a value that is only present in the test data. 
Imagine that there is a new iPhone – when you train your model, this new iPhone didn't exist (you didn't have this category) but you apply this model, and then, all of a sudden, there is a new iPhone. How will your model react to this? You need to somehow model this situation. This is totally normal. This can and will happen in practice. We just want to understand what would be the effect of that. That's why we need to see this in training data. This is good and this is how it should be. I hope that is a good answer."
1762	How does the linear regression algorithm know how to estimate the right weight for a particular feature, given that some features are more important than others?	Yes, that's the main reason we have two capstone projects. This is for those who are catching up to be able to get the certificate. Now there are holidays, like Christmas holidays and then January holidays in some countries – like in Russia, for example, people usually take holidays not during the end of December, but at the beginning of January. No matter in which part of the world you are in, you still have one month to catch up (November) and then start working on the capstone. Actually, I’ve now realized that the capstone starts quite soon. But for capstone 1, you only need the first six weeks or so, and then you can finish this project. Then during the holidays, you can watch Kubernetes, neural networks and so on, if you want. Then you can do capstone 2. That's probably better. But yeah, it's fine. You should do 2 capstones.
1763	How does the linear regression algorithm know how to estimate the right weight for a particular feature, given that some features are more important than others?	[chuckles] I'm not prepared to answer that question. I don't have a top three for guidelines. Maybe this is something we can discuss next time. I don't know how to answer that. Maybe what you can do is go to our YouTube channel (I know I keep doing this) where we just had this Responsible and Explainable AI Interview about that. So maybe check it out. I will mark this as “answered” and then maybe this is something we can go through next time, like we did last time – some of the questions I didn't answer and we moved it to this week. So probably we’re going to keep doing it like this.
1764	How does the linear regression algorithm know how to estimate the right weight for a particular feature, given that some features are more important than others?	"Yeah, that's an interesting question. This is something we do not cover in detail in this course. Again, maybe (if you want) you can refer to this Andrew Ng course for more theory. Here we don't cover a lot of theory – we focus more on practice. But from the theoretical point of view, linear regression is trying to optimize the so-called “loss function” and it's doing it by using some mathematics. A normal equation that we saw is one of the ways of finding the minimum for this loss function. Basically, the weights we get at the end are the best in terms of minimizing this loss function. The weights that we get, they make sure that this loss function is minimal. I don't know if it makes sense to you or not, but there is some mathematical foundation behind this. There is an explanation of why it happens this way. 
If you want to find out more about this, maybe you can check about a thing called “gradient descent,” which explains the loss function – we move down by tweaking our weights in such a way that, when we arrive at the end, the weights we have in our model, they are the best ones with respect to this particular loss function. I think we slightly cover loss functions minimization – I think maybe in neural networks. I don't remember. Here, we focus more on practical aspects. You kind of put some trust into the idea that the model gives you a good enough estimate, and then use it from a practical point of view. But if you want to learn more theory, then I think there is no better course than this one – Machine Learning by Andrew Ng. From my point of view, that was one of the best courses I took when it comes to theoretical machine learning."
1765	Will you teach NLP? I want to learn and implement NLP for market news analysis. Another use case I want to try is understanding data (tabular, image, audio).	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1766	Will you teach NLP? I want to learn and implement NLP for market news analysis. Another use case I want to try is understanding data (tabular, image, audio).	"It all depends on what you mean by “full stack ML engineer”. As you’ve noticed, data engineer Zoomcamp is not about machine learning – it's about data engineering. If in your opinion a “full stack ML engineer” needs to know data engineering, then you should do data engineering. If not, then no. From my point of view, there is a “full stack of data scientist”. What I put in the definition of a full stack data scientist is – a person who can do all the steps in the CRISP-DM process. They can help product managers with business understanding, they can work with analysts in data understanding, they can help data engineers in the data preparation step, they can do the modeling, and they can deploy the models. If you want to be that kind of person, then, of course, you also need to learn a bit of product management and analytics, which we don't have courses for. But data engineering – preparing data before it goes to a model – will certainly be helpful. 
Also, machine learning ops Zoomcamp (MLOps Zoomcamp) will be helpful as well. I would suggest, if you really want to focus on machine learning engineering, then probably doing MLOps Zoomcamp will make more sense for you. But data engineering Zoomcamp will be quite useful in the future, because data scientists and ML engineers tend to work on data pipelines as well. 
Maybe not all the content will be useful for you. For example, the content about data warehousing – as a data scientist, I don't find it super useful in my work to me personally. Also, the module about analytics engineering – it's nice, but it's not something I do day to day. But the modules about batch, the modules about Spark, the modules about streaming – this is something that can be useful, because this is something we could use in building machine learning pipelines as well. 
To put data in a machine learning model, we need to prepare it – we need to create data pipelines – and this is pretty relevant. But you can just check it out. All the content is already there. You can just go and check it out. If you like it, do it – if you don't, don’t. I would suggest maybe going with this one (ML Zoomcamp)."
1767	Will you teach NLP? I want to learn and implement NLP for market news analysis. Another use case I want to try is understanding data (tabular, image, audio).	Not as a part of this course. We will not cover NLP here. Maybe if there is a lot of interest in NLP, yes. I am actually thinking about what kind of course I should make, so if you have any suggestions, please reach out. I want to start thinking about the next course, so maybe by the end of this year we'll have some sort of outline. If there is a lot of interest in NLP, it could be NLP. But I don't know NLP that well myself. I know the basics, so how deep do we want to go there? So please tell me what you're interested in regarding NLP, and then we can see if this will happen at some point.
1768	Will you teach NLP? I want to learn and implement NLP for market news analysis. Another use case I want to try is understanding data (tabular, image, audio).	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1769	Will you teach NLP? I want to learn and implement NLP for market news analysis. Another use case I want to try is understanding data (tabular, image, audio).	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
1770	The metrics that we have seen could be used for multi-class?	"Yes or no. Accuracy can be used for multi-class classification – precision, recall and ROC curve, they're all for binary classification. But it's still possible to extend them for multi-class. First of all, there is a thing called one versus all classification. In this case, let's say we have three classes. [Image for reference] We have the green, blue and red classes. 
First, we actually see this problem as three binary classification problems and we train a separate model for each. For example, we can train a model for distinguishing green from the rest – maybe that will be our first logistic regression model. Then we can train a model for distinguishing the blue squares from the rest. And then we can have a third model for distinguishing the red crosses from the rest. This is called the “one vs all” approach. When we do this, we will end up with three binary models, and then we can compute precision, recall, F1 score, ROC, for each of these models separately and then we can merge them. We can take an average and see what the precision and recall are for this average. So that's one of the approaches. 
There are other approaches you can take. Let's say, ROC curve SciKit Learn, there is a way to extend it. [Image for reference] You see, for this roc_auc_score, they can actually do this for multi-class cases. There are different ways of how you can take average between these three classifiers. These micro/macro weighted samples – you can just read this description, this recommendation and see what the different ways of doing this are. I think this is a common approach. This is the one I just described – you compute them independently and then you kind of take the average."
1771	The metrics that we have seen could be used for multi-class?	"You probably mean for logistic regression, right? For logistic regression, it's doing something very similar to what we saw in module 2. It is trying to minimize the… (I'm just trying to figure out how I can explain it without going into too many details and not to get lost myself and not to confuse you). It's probably a good idea to refer to some other course. I think I partly talked about this. Let me check. Here I talk a little bit about this, in ML Zoomcamp Office Hours week #4, where I explain a little bit how exactly it works. But it's a very superficial, let's say, overview of exactly what's happening there. 
If you want to learn in more detail how exactly the process works there, maybe you can just go to Google and search for “gradient descent, logistic regression,” or something like this. The first result will probably have a good explanation of what's happening under the hood. SciKit Learn does not use this exact method – it doesn't use gradient descent – it uses something more complex (more advanced) than that. But this is roughly what's happening under the hood. I hope you're satisfied by this answer. I don't think I can give you a better one with the time we have. It will be like a separate lecture. If you don't want to go into details, you can think of this as something that’s similar to logistic regression happening there but we add sigmoid on top of that."
1772	The metrics that we have seen could be used for multi-class?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1773	The metrics that we have seen could be used for multi-class?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1774	The metrics that we have seen could be used for multi-class?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1775	Will we see deep networks with tabular data instead of image classification or is it not very recommended?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1776	Will we see deep networks with tabular data instead of image classification or is it not very recommended?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1777	Will we see deep networks with tabular data instead of image classification or is it not very recommended?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1778	Will we see deep networks with tabular data instead of image classification or is it not very recommended?	"Tim 
Not really. We have users who use both Streamlit and Gradio. I think Streamlit and Gradio are a bit more about the presentation side. We do have a couple of users who have asked us about deeper Streamlit or Gradio support and I think that's something that's probably coming up in the next few months.
Alexey
For those who don't know what Streamlit or Gradio is – this is a way to create an interface for your services. Well, Gradio is focused on machine learning, meaning you create interfaces specifically for models. And then Streamlit is just a Python package for creating interfaces. We actually had a project at DataTalks.Club about Streamlit. I actually even have a tweet about this. This is what Streamlit looks like. In the video, you can see that you have this interface, and then based on the toggles that you move the core of your model changes. It gives you an easy way to build an interface like that. Meanwhile Bento focuses on quite a different use case, right? It’s not building interfaces, but serving models.
Tim 
Right, yeah. I think one of our users was asking for a really easy way, so that once you have your model and your Streamlit UI to be able to create a Bento deployment with one line in that way, anybody that you gave your Streamlit app to would be calling to that service and it could work anywhere. Which is a nice feature, I think.
Alexey 
Yeah, I think it is. I imagine if you have a Pydentic class there, then you can easily generate a Streamlit app for that.
Tim 
Right. Oh, that's a good idea. I hadn't thought of that. Yeah.
Alexey
And then in addition to that, the Swagger UI that you have, could be like having the actual interface. 
Tim
Oh, yeah, that would be cool. [chuckles]
Alexey
That could be a good Hackathon project, right?
Tim 
Yeah, for sure.
Alexey
From what I see – let's say you’ve built a model. Then you deployed this model and you have this nice API. You can query it with CURL. But what if you want to demo this project to your manager, or a manager of your manager, or somebody who does not necessarily have a technical background? If you give them the URL and say, “Okay, you need to execute the CURL command with a post request,” they would be like, “CURL what? What do you want from me? Just show me how it looks.” This is when tools like Streamlit come in handy. Instead of giving them a command line interface to query it, you just give them “Okay, this is the link. Play with this.” If Bento could do something like that – automatically generate things like that – many data scientists would thank you.
Tim
Yeah, I was just looking because I figured there's a library that turns Pydantic models into Streamlit apps. It looks like there is one that does that. I wonder if we could just plug into that app and then have that same thing. 
Alexey
That's cool. 
Tim
Yeah, that's awesome. That's kind of the cool part of open source – there are so many people out there with so many different projects and we get to collaborate with them and kind of build the best collaborations and combinations of these tools."
1779	Will we see deep networks with tabular data instead of image classification or is it not very recommended?	Yeah, you can do this. The sky's the limit. Again, use a validation dataset to find out and then remember about the trade-offs. The trade-offs are speed, I guess, and the complexity of your model. You need to take that into account and then use the validation framework that you set up to make the best decision. So use data to make the decisions.
1780	What is the difference between CMD and ENTRYPOINT?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1781	What is the difference between CMD and ENTRYPOINT?	Okay, what I will do is I will copy this question and paste it into Google and see what it answers. I don't know, is this good enough? Or do you want to know more about that? I usually go with ENTRYPOINT. In lambda, we use CMD because ENTRYPOINT is already specified in the base Docker image. Then using CMD, you can kind of overwrite, but only partly. CMD is like a part of ENTRYPOINT. As far as I remember, that's the main difference between them. Please look it up. I don't remember too much about it.
1782	What is the difference between CMD and ENTRYPOINT?	"You can check out a few projects in this playlist. In the playlist, you can see some of the videos here, for example, there's a project from Alvaro. And it's not just Alvaro, there’s one from Alvaro and Ninad. You can just check out these projects – there’s a project from Carolina and Hamad, a project from Lisa, and a project from Timur. The project from Timor may actually fit what you want. Timur has done a lot of work and it's very interesting. So check it out. Actually, Timur’s project is not a midterm project. It's more like a capstone project – there is some deep learning there, he uses Karis. So maybe focus on the other three. 
You can also go to our course web page, and then go to midterm project (you need to use the 2021 folder) and then in the readme there should be some assignments. There will be some form that says “To find the projects you need to review, use this table.” Here, you can see all the projects from previous students’ midterm projects. You can pick some of them and check if you like them or not."
1783	What is the difference between CMD and ENTRYPOINT?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
1784	What is the difference between CMD and ENTRYPOINT?	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
1785	What would your suggestions be for projects that have impact from the interview and business perspective? 1) Choice 2) Data selection and gathering 3) Technicality	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1786	What would your suggestions be for projects that have impact from the interview and business perspective? 1) Choice 2) Data selection and gathering 3) Technicality	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1787	What would your suggestions be for projects that have impact from the interview and business perspective? 1) Choice 2) Data selection and gathering 3) Technicality	"Any project will work as long as it's not Titanic, or Iris or MNIST – or similar popular datasets. It would be best if you collect the data for this project yourself. That's a really, really, really big plus. Most people just find a CSV file on Kaggle and there is nothing wrong with this. Most candidates that have portfolios find a CSV on Kaggle and then they create a Jupyter notebook where they train a logistic regression model and they call it a day. But if you really want to set yourself apart from these people – again, there is nothing wrong with this, that's a good project already – but if you want to go the extra mile, you can, first of all, deploy your model. Then, show how you deployed this – and this is what we will cover in this course. In this course, we will train a model and then deploy it as well. 
If you want to go one more extra mile, then consider collecting the dataset yourself. You can just scrape data from somewhere, or you can use crowdsourcing platforms or you can just take pictures yourself. For example, for the book Machine Learning Bookcamp, there’s a chapter about deep learning and image classification. For this chapter I actually collected the data myself – I curated the data myself, but there are pictures that I took myself as well. I actually put clothes on the floor, and then took my phone and took pictures of the clothes. Then I also asked other people to do the same. This is how we collected the dataset for the book. Now we also use this for the course. 
If you do something like this, it will be insanely amazing. Very few people actually do this – very few people collect the dataset themselves for the portfolio projects. Then, you don’t only do your project end-to-end, because you start with collecting the data, and then you finish with deploying it, but it also shows that you care about this thing. Usually, you will only do this for things that you find interesting or important or something that’s close to you. These types of projects are especially interesting. I know some people who were looking for a flat in Berlin – in Berlin, it's very difficult to rent a flat – so they wrote a scraper that scrapes Berlin flats and then helps to find a flat. I think they had a model for predicting the price and they see how different the price is from prediction and use that to somehow find the flat. 
Here, you have a reason – a goal, a problem you want to solve – and then you collect the data yourself, you build the model, and then you deploy it. And you actually use it to solve your problem. This is amazing – this is as close to a business perspective as possible."
1788	What would your suggestions be for projects that have impact from the interview and business perspective? 1) Choice 2) Data selection and gathering 3) Technicality	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
1789	What would your suggestions be for projects that have impact from the interview and business perspective? 1) Choice 2) Data selection and gathering 3) Technicality	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
1790	How are you going to validate the Kaggle competitions? Should we fill out a form so they know we are participating?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1791	How are you going to validate the Kaggle competitions? Should we fill out a form so they know we are participating?	I think the easiest way to understand what it means is to just try different ETAs, and then see exactly how your learning curves – the plots where you show the performance – how exactly they look like. If it's too big, you will see that your model overfits very quickly – on the training set, it reaches 100% performance very quickly, but on the validation dataset, it becomes worse and worse after just a few trees (after a few iterations). So it happens very fast. If you set a learning rate that’s too small, then you will need a huge load of trees to actually have anything meaningful. It will simply take too much time to learn anything, which would be the case of underfitting. So selecting the right learning rate also takes some trial and error – you need to try different values to see what works best. What I usually do is try 0.3, I try 0.1, and then I try 0.01 0.05 – then I just look at these plots and based on that, I make my decision. Also, keep in mind that when you have a lot of trees, your model becomes slower. You don't want to have a slow model. Sometimes it's better to set the learning rate a bit higher so that you have fewer trees. So it's better that way.
1792	How are you going to validate the Kaggle competitions? Should we fill out a form so they know we are participating?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1793	How are you going to validate the Kaggle competitions? Should we fill out a form so they know we are participating?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1794	How are you going to validate the Kaggle competitions? Should we fill out a form so they know we are participating?	Yes, I did create a form recently. I should have created it earlier. You will submit your Learning in Public links here. But then you also will submit the name of the leaderboard and the score for Learning in Public. This is how you do this. This is how we will know, otherwise, it's tricky.
1795	Conda vs Pipenv? We started with Conda (W1). Is it recommended to install Pipenv inside Conda? Or use Conda for “dev” and Pipenv separately when deploying?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1796	Conda vs Pipenv? We started with Conda (W1). Is it recommended to install Pipenv inside Conda? Or use Conda for “dev” and Pipenv separately when deploying?	Yeah, it's probably not the best practice. I wouldn't do this in a real-life situation. In a real-life situation, we simply will not have unseen data. But the idea here was to get a dataset that is already familiar to you – the dataset we used in the previous homework – and just convert it to a binary problem. That was the goal we had in mind – how can we make it easier for you to learn these things without overloading you with a new dataset? That was the simplest approach we decided to take to actually do it here. Another alternative could have been, instead of asking you to compute the mean, just give you the number, but with the mean, it's a little bit more interesting, I think.
1797	Conda vs Pipenv? We started with Conda (W1). Is it recommended to install Pipenv inside Conda? Or use Conda for “dev” and Pipenv separately when deploying?	"I use Pipenv inside Conda. If I do $ which pipenv – it will be Pipenv inside Anaconda. [Terminal image as reference] There is nothing wrong with this. I use Anaconda as a general purpose Python distribution with the packages I need and then I install everything I need personally on top of that, like XGBoost, Pipenv, TensorFlow – everything that Anaconda does not have by default. What is good about Anaconda is that it's separate from the system Python I have. 
There is a system Python on this computer, even though it's Windows – it's not really a system Python. But on Linux, there would be the system Python, which you don't want to touch. That's why I like Anaconda, because you put it inside your home directory and then in this home directory, you can do whatever you want. And if something happens, you can just remove your Anaconda and then your system Python will stay untouched. That's the good thing about Anaconda."
1798	Conda vs Pipenv? We started with Conda (W1). Is it recommended to install Pipenv inside Conda? Or use Conda for “dev” and Pipenv separately when deploying?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1799	Conda vs Pipenv? We started with Conda (W1). Is it recommended to install Pipenv inside Conda? Or use Conda for “dev” and Pipenv separately when deploying?	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
1800	How many hours per week should I anticipate dedicating to the course?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
1801	How many hours per week should I anticipate dedicating to the course?	From 5 to 20, depending on your experience. If you're already an experienced developer, software engineer, you know Python, you know cloud, it could be less than five. If you don't know any of these things and you never used a command line, then it's 20 hours. If some of these things apply to you, some don't, then probably around 10 hours.
1802	How many hours per week should I anticipate dedicating to the course?	"Alexey
No. I mean, we kind of told you. Yes, you maybe didn't see the intro video. But, sorry. No, it's late. We will actually recalculate the points for the second homework (HW 1B) because in the forum, it said that you can get up to one point, but we calculated that the cap there was seven. We are going to recalculate that. Thus, people who got a lot of points will get just one. Sorry about that. But don't focus on points. If you go to the leaderboard, we know that the person with this hash (722366d8b29ece9be3a7605363562c7c60d6918e) did seven links. Do you know who that is? Probably you don't. 
These points are virtual. It's nice to have a good score, but remember that these points mean whatever you want them to mean – not more than not less. Maybe it's okay if you get a few points less than others for the first homework, but the important thing here is consistency. If you just do your homework, even without doing any Learning in Public stuff, which I think you should do (you should post about the course). If you don't do this and you just do all the homework, by the end, you will be somewhere in the top 20. If you look at the leaderboard from the previous iteration, you will see that not everyone stuck around till the end. 
If you just do your homework, don't worry about these points, and you'll be fine. Sorry about that. I know that it can be discouraging that you didn't get as many points as others, but don't worry about that. Focus on learning. Maybe Michael has some other suggestions or some other things that I didn't mention, because he took part last year. But I think he was one of the people who did a few posts every week, right?
Michael
Yes. Actually, when I went through it, I missed the whole… you can get up to seven points, but I was just doing one every week and I just stuck with that. I was still near the top. I think the real value in that isn't necessarily the points but just being comfortable sharing – writing articles and making videos. That's the real value there. But everyone likes the clout value, too. As Alexey said, it will fluctuate, so if you keep up with it going forward, you'll probably be just fine."
1803	How many hours per week should I anticipate dedicating to the course?	"Alexey
For this case, if you go to our data engineering page, and then go to the homework, you will see this note: “If the answer does not match exactly, then select the closest option.” With that, if you have an answer, but it's somewhat not the same one, you just go with the closest one and then you'll get it right."
1804	How many hours per week should I anticipate dedicating to the course?	No, you cannot.
1805	In Terraform, we have to use some credentials. What is the best way to store credentials safely? Can you create some videos about basic information security approaches?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1806	In Terraform, we have to use some credentials. What is the best way to store credentials safely? Can you create some videos about basic information security approaches?	Well, what you can always do is create a new account. Don't forget to unlink your credit card if you want to use the same credit card. Unlink it from the billing from the old account, and then link it with a new one. Or if you have a different credit card, then you can just use that. For example, I use Revolut as a bank and in Revolut, you can create one-time virtual cards. That could be an option, too.
1807	In Terraform, we have to use some credentials. What is the best way to store credentials safely? Can you create some videos about basic information security approaches?	No, you cannot.
1808	In Terraform, we have to use some credentials. What is the best way to store credentials safely? Can you create some videos about basic information security approaches?	Well, I'm not really a professional in this area. What typically happens for example, in the Google Cloud Platform, everyone keeps the file in a home directory. This is not version controlled. You basically don't commit this file. This is one approach. I think this is a pretty common one. I mostly use AWS. In AWS, there are ways to authenticate, which does not require putting any credentials in your Git. Once you authenticate, you can just apply Terraform stuff. That's how it's usually done. I think it's not the best answer, honestly. Again, I'm not a professional in this. If any one of you knows what the best approach is, maybe write it in Slack and we can possibly create a video about that. Maybe you can write a blog post, if you want, about that. But yeah, I don't know.
1809	In Terraform, we have to use some credentials. What is the best way to store credentials safely? Can you create some videos about basic information security approaches?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
1810	Sometimes companies ask for a skill that I don't have, even though I'm good with most common data engineer technologies.	No, you cannot.
1811	Sometimes companies ask for a skill that I don't have, even though I'm good with most common data engineer technologies.	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
1812	Sometimes companies ask for a skill that I don't have, even though I'm good with most common data engineer technologies.	Everything you do here is individual. You don't form teams.
1813	Sometimes companies ask for a skill that I don't have, even though I'm good with most common data engineer technologies.	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
1814	Sometimes companies ask for a skill that I don't have, even though I'm good with most common data engineer technologies.	They may ask for a skill, but it doesn't mean that they really have to have it. You don't have to match the job description 100%. Companies usually realize that, let's say, if you know Prefect but don't know Airflow – if you know GCP but don't know AWS – it's still fine. These skills are transferable. Usually, employers will consider you anyway if you have experience with related technologies.
1815	Do we have reading materials for every week? It’s nice to read on topics more in-depth.	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
1816	Do we have reading materials for every week? It’s nice to read on topics more in-depth.	"Ankush
I think if it's about finding a job, spend time learning AWS, because… come on – everybody's using AWS and you have a higher probability of finding a company which will have AWS. After that choose GCS or Azure or whatever you feel. But I think the main point is to learn the concepts. For example, let's talk about data warehouses for partitioning and clustering. In Prefect, it’s how to create your DAGs in particular structure or paralyzing your flow. I think if you learn this concept, it doesn't really matter which cloud platform you use at the end of the day, because all of their services will be pretty similar. I think once you grab the basic concepts and your base layer is good, it's very easy to adjust to both of the cloud environments. I have personally worked on AWS and Google Cloud. I've also worked a bit with Snowflake. Once you can get the ideas, or the concepts, it's really easy to make that switch.
Alexey
I'll add that my experience with Cloud is mostly AWS. Not mostly, I think, it’s only AWS. For this course, I used GCP for the first time and it wasn't that difficult. The UI is different. I actually think GCP has a better UI, in my opinion. It’s more intuitive. The only thing I needed to figure out was, “In AWS this thing is called this way. How was it called in GCP?” Then I would just Google it, find it, and then just use it. Most of the time, it was like that.
Ankush
I have a follow up question. What if ChatGPT is only integrated in Azure? Then what do we do? 
Alexey
Well… I'll have to use Azure. [chuckles]
Ankush
So we’ll need to migrate? [laughs]"
1817	Do we have reading materials for every week? It’s nice to read on topics more in-depth.	No. But if some of you find anything interesting, please share the links in Slack
1818	Do we have reading materials for every week? It’s nice to read on topics more in-depth.	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
1819	Do we have reading materials for every week? It’s nice to read on topics more in-depth.	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1820	Can an Oracle DBA with 10 years of experience transform into a data engineering career?	Well, what you can always do is create a new account. Don't forget to unlink your credit card if you want to use the same credit card. Unlink it from the billing from the old account, and then link it with a new one. Or if you have a different credit card, then you can just use that. For example, I use Revolut as a bank and in Revolut, you can create one-time virtual cards. That could be an option, too.
1821	Can an Oracle DBA with 10 years of experience transform into a data engineering career?	Yes
1822	Can an Oracle DBA with 10 years of experience transform into a data engineering career?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1823	Can an Oracle DBA with 10 years of experience transform into a data engineering career?	"Michael
It's not necessary. We do have recipes for using Terraform with Prefect with different things. We talked about the Prefect recipes repo, but I'll put a link to it here. 
Jeff
This is in a docs and we have a GitHub repo where we have lots of different recipes. Some of them here explain things like, say you want to run an agent – how can you do that? There are so many different cloud environments. You can put things on Kubernetes, you can put things on something like a more serverless platform like Fargate. For example, on the right there, if you were wanting to use AWS with Prefect and ECS (elastic container service) then with Terraform – we have some recipes for that. So there's just so many different permutations of possible ways to do things. A lot of these are helpful ways to go and look at options, but it depends on what you're using in the real world (in a use case). I don't think we have anything specific for this course that requires us doing this, so we don't have anything further for it.
Alexey
I guess, the usual setup that I saw (not necessarily Prefect-related) you typically use Terraform for setting up infrastructure or things like a Kubernetes cluster and configure it there. But then there is a separate repo with configuration for Kubernetes that is not maintained/managed through Terraform but through something else. For that, you don't really need to Terraform – and then Prefect could live in this Kubernetes cluster.
Jeff 
Yeah, we do have some guys working with Kubernetes in here, for sure. I see some of them here. We do have a Helm chart that we can use. If you're into the Kubernetes world, there are lots of things there. It's a whole rabbit hole.
Alexey
Here are the Prefect Helm charts.
Kalise
Also, there are a lot of YouTube videos we have that can help if you're trying to use some of these resources – how to go about deploying the Helm chart with Kubernetes and stuff, like step-by-step tutorials as well. Generally, if you just go to the Prefect YouTube channel, you'll see a lot of those tutorials. There should be some playlists there that have various things.
Alexey
Is this a duck in the cover or what?
Jeff
That is. It's the top of a duck. Blue ducks!"
1824	Can an Oracle DBA with 10 years of experience transform into a data engineering career?	No, you cannot.
1825	The tasks that we have done in this homework – can this be considered as a real project? I guess a real project might be more complete, end-to-end.	"We had a talk about this on our YouTube channel. As far as I remember, it was called Machine Learning Design Patterns. These patterns were mostly about engineering – engineering patterns – but the first pattern, as far as I remember, was about rebalancing. So check it out. Sara explains when to use these techniques. Here, the important part is that you apply these techniques only to your training dataset. You keep your validation dataset intact – you don't change it – because you want to have a reliable way of evaluating the performance of your model. You apply different techniques to your training dataset and then you see how exactly it changes the performance on your validation dataset. 
So you apply these techniques to train, experiment, try different ones, and then go with the one that has the best uplift for your score on validation. That's usually how you do this. This will tell you if you actually need any of these techniques, or whether just throwing all the data that you have into the model is good enough, so you don't need to make the process more complicated than it should be."
1826	The tasks that we have done in this homework – can this be considered as a real project? I guess a real project might be more complete, end-to-end.	Rescale is already preprocessing. Actually, in exception, this is what they do. It's actually the scaling there. There are two operations, I think. I don't remember exactly, but it's rescaling plus something else. So rescaling is preprocessing.
1827	The tasks that we have done in this homework – can this be considered as a real project? I guess a real project might be more complete, end-to-end.	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1828	The tasks that we have done in this homework – can this be considered as a real project? I guess a real project might be more complete, end-to-end.	"The homework is not meant to give you end-to-end project experience – it's meant to make sure you understand, or reiterate, all the topics we cover in the videos and in the course materials. We try to focus on the most important parts there and also maybe push you a little bit out of what we covered. So you need to do a bit of exploration yourself. Maybe there haven’t been as of yet, but there will be homework assignments where you will actually need to move a little bit outside of what was presented in the course, and then do a bit of Googling yourself. 
Homework assignments are not “real projects” in the sense that you probably shouldn't add this to your portfolio and call it “Hey, I predicted a car price dataset. I used this dataset and predicted a car price. Here's my portfolio project.” I wouldn't do this. But for the projects we will have as part of this course – yes, you can definitely do that and they will be more complete and end-to-end, definitely. Especially compared to what we do in the homework."
1829	The tasks that we have done in this homework – can this be considered as a real project? I guess a real project might be more complete, end-to-end.	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1830	Is the Data Engineering Zoomcamp playlist on YouTube complete or can we expect some new videos to be added?	"Alexey
Yeah, we talked about that in Slack, so please check it. I think this happens because some of the columns have missing values and when you use Pandas for converting CSV to Parquet, it reads columns with missing values as double. That's why this happens."
1831	Is the Data Engineering Zoomcamp playlist on YouTube complete or can we expect some new videos to be added?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1832	Is the Data Engineering Zoomcamp playlist on YouTube complete or can we expect some new videos to be added?	"Michael
It's not necessary. We do have recipes for using Terraform with Prefect with different things. We talked about the Prefect recipes repo, but I'll put a link to it here. 
Jeff
This is in a docs and we have a GitHub repo where we have lots of different recipes. Some of them here explain things like, say you want to run an agent – how can you do that? There are so many different cloud environments. You can put things on Kubernetes, you can put things on something like a more serverless platform like Fargate. For example, on the right there, if you were wanting to use AWS with Prefect and ECS (elastic container service) then with Terraform – we have some recipes for that. So there's just so many different permutations of possible ways to do things. A lot of these are helpful ways to go and look at options, but it depends on what you're using in the real world (in a use case). I don't think we have anything specific for this course that requires us doing this, so we don't have anything further for it.
Alexey
I guess, the usual setup that I saw (not necessarily Prefect-related) you typically use Terraform for setting up infrastructure or things like a Kubernetes cluster and configure it there. But then there is a separate repo with configuration for Kubernetes that is not maintained/managed through Terraform but through something else. For that, you don't really need to Terraform – and then Prefect could live in this Kubernetes cluster.
Jeff 
Yeah, we do have some guys working with Kubernetes in here, for sure. I see some of them here. We do have a Helm chart that we can use. If you're into the Kubernetes world, there are lots of things there. It's a whole rabbit hole.
Alexey
Here are the Prefect Helm charts.
Kalise
Also, there are a lot of YouTube videos we have that can help if you're trying to use some of these resources – how to go about deploying the Helm chart with Kubernetes and stuff, like step-by-step tutorials as well. Generally, if you just go to the Prefect YouTube channel, you'll see a lot of those tutorials. There should be some playlists there that have various things.
Alexey
Is this a duck in the cover or what?
Jeff
That is. It's the top of a duck. Blue ducks!"
1833	Is the Data Engineering Zoomcamp playlist on YouTube complete or can we expect some new videos to be added?	"Alexey
It's more or less complete. There is one thing we wanted to update, which are the TerraForm videos. Sejal, unfortunately, got sick. In Europe, there was a big outbreak of the flu. I was also one of them. Sejal caught it a bit later than others. So she's unfortunately now sick. She's recovering. We'll have some videos from her there. But fundamentally, not much will change. There are videos from her from last year, but in those videos, the code is already written – not everyone liked this approach, so we wanted to re-do this. And you will see these videos. Right now we have the old material, but that's the only change we still want to do. Depending on the problems you have right now, we will see. This is what we did last time. We saw the problems many students had and we recorded some videos to answer those problems.
Ankush
I just wanted to add that I'm doing some more videos on Kafka. The current playlist is not complete on Kafka and I will be adding at least three, four more videos on that.
Alexey
Anyone else is working on more videos right now?
Victoria
I was planning to do something like “What changed in the last year for DBT?” For example, now you can do Python there as well. But no update to the project."
1834	Is the Data Engineering Zoomcamp playlist on YouTube complete or can we expect some new videos to be added?	I will not answer this right now. We did not plan anything like that. I think Anna prepared some materials about how to deploy Prefect, not with Terraform, but with some Google Cloud Platform stuff. I might be mistaken, but in practice, I think we use something like Kubernetes. Kubernetes is not managed through Terraform. I will leave this question answered and in the next Office Hours when we'll talk more about Prefect. Somebody from the Prefect team will answer this question. Probably Jeff will be in our next Office Hours.
1835	Apart from this course content, what are the other skill sets that I need to learn to get into a ML-related job?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1836	Apart from this course content, what are the other skill sets that I need to learn to get into a ML-related job?	"Tim
I think a couple people asked this in ML Zoomcamp Slack. I was thinking about this a little bit. I think it's sort of a philosophical thing [chuckles] where we think about models and Bento is sort of separate a little bit from code. So your project directory is where all the stuff that you'll commit to Git. I don't think we think that it's the right model to commit your models to Git, for example. We think of ourselves a little bit more like Docker. You don't have Docker images in your project directory, right? You just have your Docker file and that's what you commit to Git. In the same way, we store your models and your Bento in that dedicated directory, and then sort of leave it up to you to push it to different places where you might build or where you might need it
Alexey
Do you know why you decided to save it locally on your local file system? For example, what MLflow does – you can save it to S3 directly, but in the case of Bento, you always first save it to local storage and then you decide “This model I want to export, but for the rest, I don't care.”
Tim
Right. First, I think it’s because that's the easiest way to do it, but also because I think with BentoML, the workflow is that a lot of times, somebody's building a model on their laptop, or somebody's building a model in a slightly less distributed system and they're assembling their Bento there. So I think that the fastest way to access that model is locally. We do give you the ability to push that model and pull that model from other places. But ultimately, when you assemble your Bento and then you assemble your container, all that stuff has to be there. I don't think we wanted separate abstractions for when a person is training and saving, versus when a person is building a Bento and pulling in the model and assembling the container. It's just the same abstraction, which is just a local repository which contains models.
Alexey 
Maybe you don't want to save every model every single time you change one single parameter to the cloud. [Tim agrees] Maybe at the end, after all the experiments, you want to save the final one. Meanwhile, for MLflow, the use case is different – you actually want to save every single experiment because you want to see the results of these experiments.
Tim
Right."
1837	Apart from this course content, what are the other skill sets that I need to learn to get into a ML-related job?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1838	Apart from this course content, what are the other skill sets that I need to learn to get into a ML-related job?	"You will probably also need to learn how to pass interviews. This course does not teach that, but you will need to acquire the skill somehow. The best way to acquire these skills is by interviewing. But apart from that, I think this course covers let's say 80% of the skills that you will need for a job and the remaining 20% depends on the company. 
So you will need to do some research on the company where you want to work, see what kind of tools they use, what kind of problems they solve, how much emphasis they make in the interviews on the theoretical part. You will need to somehow figure this out and, based on that, see what you need to learn. I think with just this content, you should be able to get a machine learning-related job."
1839	Apart from this course content, what are the other skill sets that I need to learn to get into a ML-related job?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1840	When training images, is it better to use them in color or in black and white?	This comes up pretty often. First of all, I will recommend going to our YouTube channel where you will see this talk from Kishan. Actually, in this talk, I already saw a book I was about to recommend. This book is called Forecasting Principles and Practice. This is the book you want to read if you want to learn about time series. It's in R, but that's fine. If you're doing time series, you probably want to stick to R anyway. But many things work in Python as well – for example, this exponential smoothing. You can implement it in Python yourself. This is like the easiest, in my opinion, the most simple method for doing time series forecasting. You can implement this and it's actually quite fun to implement and tweak it to see how it works. So if you have some time and want to learn more about time series, try to implement this exponential smoothing.
1841	When training images, is it better to use them in color or in black and white?	"If you take statistics for machine learning education, the first lecture will be this – you'll probably derive the formula for logistic regression and there will be a slide with all these assumptions about the linear regression model. Here is the Google result: 
https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/assumptions-of-linear-regression/

Go through this. They are usually useful in practice, I guess. But what I typically do is train a model, and then rely on validation to tell me if my model is doing well or not. So if there is something wrong with my validation, I will see that the metric validation is bad. It's just a lot of trial and error, rather than figuring out all these theoretical assumptions. 
I don't want to say that all these assumptions are not relevant. If you work as a data scientist, they still are. But I also found that the more practical way to understand if you can apply linear regression to the data or not, is to just apply it and see if it works or not. If it works – if the results on the validation are good – then it means that you can apply your model to this dataset. 
Maybe some of the assumptions are violated. For example, multicollinearity – this is actually the case in the lectures, when we needed to regularize (add regularization to the model) this assumption was violated. You will see in the lectures what exactly happened. So check it out. I don't think I will be able to give a good answer, because I am not prepared to talk about this, but any statistics book or theoretical machine learning book explains this."
1842	When training images, is it better to use them in color or in black and white?	You will probably not like this answer, but use validation to find out. Train two models – one model uses color, another uses black and white – you compare them and you will see which one performs better. You can also look at speed, perhaps, because this could be another important factor. I assume that the black and white one will train faster. But that's it. Just experiment and see you for your dataset, which one works better, and stick to it. In general, for any question about machine learning, Like, “Is it better to use X rather than Y?” The answer is always validation. Just try both and see whatever option performs better on the validation.
1843	When training images, is it better to use them in color or in black and white?	I think the easiest way to understand what it means is to just try different ETAs, and then see exactly how your learning curves – the plots where you show the performance – how exactly they look like. If it's too big, you will see that your model overfits very quickly – on the training set, it reaches 100% performance very quickly, but on the validation dataset, it becomes worse and worse after just a few trees (after a few iterations). So it happens very fast. If you set a learning rate that’s too small, then you will need a huge load of trees to actually have anything meaningful. It will simply take too much time to learn anything, which would be the case of underfitting. So selecting the right learning rate also takes some trial and error – you need to try different values to see what works best. What I usually do is try 0.3, I try 0.1, and then I try 0.01 0.05 – then I just look at these plots and based on that, I make my decision. Also, keep in mind that when you have a lot of trees, your model becomes slower. You don't want to have a slow model. Sometimes it's better to set the learning rate a bit higher so that you have fewer trees. So it's better that way.
1844	When training images, is it better to use them in color or in black and white?	"Well, there are multiple ways you can deal with this. First – get more data, if possible. If not, try to reduce the number of features you have. Sometimes, when the data set is small, if you have a lot of features, it might be confusing for the model. Or try to have a model with a lot of regularization. This should help. Here, the main thing is that you need to come up with a reliable way of evaluating the performance of your model. 
You really need to think about how you can set up your validation strategy in such a way that it's reliable. Once you have that, you can start experimenting with all these regularization things, with dropping some columns, with techniques like upsampling and downsampling and things like this. The important thing here is setting up your validation strategy. Once you have that, everything else will come. You will just need to experiment and the experiments will show what the best way is."
1845	I missed the last two weeks. What would you recommend I do in order to get up to speed with the course?	No, you cannot.
1846	I missed the last two weeks. What would you recommend I do in order to get up to speed with the course?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
1847	I missed the last two weeks. What would you recommend I do in order to get up to speed with the course?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1848	I missed the last two weeks. What would you recommend I do in order to get up to speed with the course?	"Jeff
I would watch the videos and do the work in the course and do the homework"
1849	I missed the last two weeks. What would you recommend I do in order to get up to speed with the course?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
1850	How can I save and load a BentoML model with features?	I'm glad you asked. MLZoomcamp.com. All the materials are here and you will find them there.
1851	How can I save and load a BentoML model with features?	I think the easiest way to understand what it means is to just try different ETAs, and then see exactly how your learning curves – the plots where you show the performance – how exactly they look like. If it's too big, you will see that your model overfits very quickly – on the training set, it reaches 100% performance very quickly, but on the validation dataset, it becomes worse and worse after just a few trees (after a few iterations). So it happens very fast. If you set a learning rate that’s too small, then you will need a huge load of trees to actually have anything meaningful. It will simply take too much time to learn anything, which would be the case of underfitting. So selecting the right learning rate also takes some trial and error – you need to try different values to see what works best. What I usually do is try 0.3, I try 0.1, and then I try 0.01 0.05 – then I just look at these plots and based on that, I make my decision. Also, keep in mind that when you have a lot of trees, your model becomes slower. You don't want to have a slow model. Sometimes it's better to set the learning rate a bit higher so that you have fewer trees. So it's better that way.
1852	How can I save and load a BentoML model with features?	"You will probably also need to learn how to pass interviews. This course does not teach that, but you will need to acquire the skill somehow. The best way to acquire these skills is by interviewing. But apart from that, I think this course covers let's say 80% of the skills that you will need for a job and the remaining 20% depends on the company. 
So you will need to do some research on the company where you want to work, see what kind of tools they use, what kind of problems they solve, how much emphasis they make in the interviews on the theoretical part. You will need to somehow figure this out and, based on that, see what you need to learn. I think with just this content, you should be able to get a machine learning-related job."
1853	How can I save and load a BentoML model with features?	"I think you're referring to the problem we had, which is in the frequently asked questions. The problem that we had there was this one: “ValueError: training data did not have the following fields: age, amount”. I don't know how to solve this problem. I talked to Tim. Tim will talk to the BentoML team and they will figure this out. 
Maybe this is the backend – there will be an issue on GitHub. You can follow this issue and see when it will be solved. Or maybe there is an easy fix, such that you just need to specify the feature name somewhere. I don't know yet, but I asked Tim to help me find the answer. Once we have the answer, we'll share the answer with you."
1854	How can I save and load a BentoML model with features?	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
1855	In which situation would we choose to use Spark or DBT to perform data transformations?	I would say to focus on projects, not on tools. In this course, you will get a chance to do this. If you open a job description for data engineers, you will see a lot, a lot, a lot of different technologies. Maybe think of a project that you can do for this particular company and do this project using, for example, the tools you learn here or the tools you want to learn, and build a portfolio. I guess this is more of a tip for getting a data engineering job, rather than for somebody who's starting their data engineering career. But if you already have a job, then maybe think of the business impact of what you need. I think I will not spend too much time talking about that because this requires a one hour long discussion. Perhaps we should have a podcast interview about that. If you can think of any potential guests for this podcast interview, you can let me know and we can try to invite these people and talk about this in more detail. One thing I want to share with you is that in our YouTube channel, we had a few interesting talks. We had this Getting a Data Engineering Job with Jeff talk. It’s a very nice presentation. Then we also had an interview with Jeff. So these two podcasts are quite interesting. You can also check out other ones. I think you'll find a lot of interesting content here.
1856	In which situation would we choose to use Spark or DBT to perform data transformations?	"Alexey
I think I tried to answer this question in Slack already. Let me try to recall what I said there. Spark requires an infrastructure to run it. This is more like a downside of Spark. Because with DBT, you delegate all the compute – all the transformation happens in a data warehouse. DBT just orchestrates this transformation. With Spark, it takes the data out of the data warehouse, transforms it in the cluster, and then puts it back. So it's a different thing. In the case of DBT, the data never leaves the warehouse. In the case of Spark, it does leave and it processes it. 
There are pros and cons to that. For some cases, Spark is just cheaper, especially if it's not a data warehouse, but a data lake that you’re using. With Spark it’s just a lot easier to access the data lake and do all the transformations there on the data lake, and then save the data back to the data lake. On a large scale, when you have a lot of data, it becomes cheaper. With Spark, you also kind of have more control, because it's Python or Java – whatever you use. This is code – you can test this code, you can… it's just easier to manage this code, because it's code. Meanwhile, in DBT – of course, you have tests in DBT, but they are different. They're not like unit tests. So you have more control, more flexibility – for more complex things, for more complex transformations, perhaps Spark is a better option. 
I don't remember what else I wrote there. I think we should put this in the FAQ somewhere."
1857	In which situation would we choose to use Spark or DBT to perform data transformations?	"Jeff
There are lots of different things there. This is something I used to teach folks doing data science-type work and it applies for data engineering, too. I don't have a good checklist at the tip of my fingers, but you do want to look at things like “Are your data in the right type? Do you have missing values, libraries, rate expectations (which we have integration for)?” That can help with trying to make sure your data looks how you think it'll look, in terms of the statistical properties of it. Hypothesis can also generate some ideas there, or some other Python library. Pandera for everything being how you expect, values matching what you expect. So there are a lot of different tools in Python that you can use there. It's a good question. It can be a lot of different things."
1858	In which situation would we choose to use Spark or DBT to perform data transformations?	"Alexey
Please check the FAQ. If you don't find the answer there, ask in Slack. If you get an answer in Slack, please put it into the FAQ."
1859	In which situation would we choose to use Spark or DBT to perform data transformations?	"Alexey
I don't know when this question was asked, but we did extend it."
1860	What are the advantages of starting a Spark cluster manually (starting master and worker in CLI) versus the one we started in the code? Is there any difference?	"Alexey
No. It's just in one way, you start it manually by clicking a button, and in another (in CLI or in the code)  instead of clicking a button, you're using the CLI or you do it from code. I don't think there is any difference. Or maybe you have some concrete example in mind? If so please, ask in Slack, if you want to clarify."
1861	What are the advantages of starting a Spark cluster manually (starting master and worker in CLI) versus the one we started in the code? Is there any difference?	I would say to focus on projects, not on tools. In this course, you will get a chance to do this. If you open a job description for data engineers, you will see a lot, a lot, a lot of different technologies. Maybe think of a project that you can do for this particular company and do this project using, for example, the tools you learn here or the tools you want to learn, and build a portfolio. I guess this is more of a tip for getting a data engineering job, rather than for somebody who's starting their data engineering career. But if you already have a job, then maybe think of the business impact of what you need. I think I will not spend too much time talking about that because this requires a one hour long discussion. Perhaps we should have a podcast interview about that. If you can think of any potential guests for this podcast interview, you can let me know and we can try to invite these people and talk about this in more detail. One thing I want to share with you is that in our YouTube channel, we had a few interesting talks. We had this Getting a Data Engineering Job with Jeff talk. It’s a very nice presentation. Then we also had an interview with Jeff. So these two podcasts are quite interesting. You can also check out other ones. I think you'll find a lot of interesting content here.
1862	What are the advantages of starting a Spark cluster manually (starting master and worker in CLI) versus the one we started in the code? Is there any difference?	We currently do not have any course partners yet. If you work at a company and you want to partner with us, please reach out and we will see how we can do it. In one of our courses, which was our first iteration of the Machine Learning Zoomcamp, we partnered with a company called Delphi. They got two interns from our course and they were quite satisfied with the outcome. You can actually read more about the work of these interns in our articles – Interview with Valerii Chetvertakov and then another one, Interview with Ken Wu. You can learn more about the interns and if you think that, at your company, you need good interns or juniors or you want to partner in any other way, please reach out and we can see how to make it work.
1863	What are the advantages of starting a Spark cluster manually (starting master and worker in CLI) versus the one we started in the code? Is there any difference?	No, you cannot.
1864	What are the advantages of starting a Spark cluster manually (starting master and worker in CLI) versus the one we started in the code? Is there any difference?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
1865	Can you explain a little bit what an “end-to-end project” really means? All the steps in the CRISP DM process?	"Actually, I don't remember exactly how I do this. The right approach would be to use only train data and not use validation data or test dataset. This is done on purpose for exactly this reason that you mentioned. We want to model a situation in which there is some unseen data – it's truly unseen, we do not see that all – and anything can happen to this unseen data, including the situation that you described, such as there is a value that is only present in the test data. 
Imagine that there is a new iPhone – when you train your model, this new iPhone didn't exist (you didn't have this category) but you apply this model, and then, all of a sudden, there is a new iPhone. How will your model react to this? You need to somehow model this situation. This is totally normal. This can and will happen in practice. We just want to understand what would be the effect of that. That's why we need to see this in training data. This is good and this is how it should be. I hope that is a good answer."
1866	Can you explain a little bit what an “end-to-end project” really means? All the steps in the CRISP DM process?	Well, I'm glad you asked, because we have a data engineering course. If you take it, you will understand a bit more – we do not go into details about how exactly it connects with this course. If you remember, when we talked about CRISP DM, one of the steps there is data preparation and this is what data engineering covers. Once the data is prepared, once the data is in a data lake or data warehouse, then we, data scientists, machine learning engineers, get the data, train the model, deploy this model, and take it to the other steps of the process.
1867	Can you explain a little bit what an “end-to-end project” really means? All the steps in the CRISP DM process?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
1868	Can you explain a little bit what an “end-to-end project” really means? All the steps in the CRISP DM process?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1869	Can you explain a little bit what an “end-to-end project” really means? All the steps in the CRISP DM process?	"Exactly. This would be a really end-to-end project in any sense. So when you start with nothing and you end with a deployed and functioning system with monitoring, that is as end-to-end as possible. But then you can maybe try to make it a bit less end-to-end by getting some dataset that you already found or maybe collecting some dataset yourself? I don't think it will be end-to-end in this sense, but still. 
When you get some data, and then you deploy it, it's already better if you just train a model in Jupyter Notebook and call it a day. So just a Jupyter Notebook is not an end-to-end project, definitely, but going through all the steps in the CRISP DM process is definitely end-to-end."
1870	In deep learning, why is it that in some cases it's not necessary to do preprocessing for images and only rescale?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1871	In deep learning, why is it that in some cases it's not necessary to do preprocessing for images and only rescale?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
1872	In deep learning, why is it that in some cases it's not necessary to do preprocessing for images and only rescale?	Rescale is already preprocessing. Actually, in exception, this is what they do. It's actually the scaling there. There are two operations, I think. I don't remember exactly, but it's rescaling plus something else. So rescaling is preprocessing.
1873	In deep learning, why is it that in some cases it's not necessary to do preprocessing for images and only rescale?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1874	In deep learning, why is it that in some cases it's not necessary to do preprocessing for images and only rescale?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1875	What are the general points between computer vision and NLP?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1876	What are the general points between computer vision and NLP?	I don't think I understand the question. What do you mean by “general points”? Do you mean “What are the similarities between them?” For NLP, you can use some of the ideas from computer vision. For example, convolutional neural networks work for NLP as well. But that's pretty much the extent of what I know about that. [chuckles] I don't know much about NLP, to be honest – only the basics, like embeddings. I don't know how to answer that question.
1877	What are the general points between computer vision and NLP?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
1878	What are the general points between computer vision and NLP?	Yeah. If it works for you, use it. I don't mind.
1879	What are the general points between computer vision and NLP?	"If you take statistics for machine learning education, the first lecture will be this – you'll probably derive the formula for logistic regression and there will be a slide with all these assumptions about the linear regression model. Here is the Google result: 
https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/assumptions-of-linear-regression/

Go through this. They are usually useful in practice, I guess. But what I typically do is train a model, and then rely on validation to tell me if my model is doing well or not. So if there is something wrong with my validation, I will see that the metric validation is bad. It's just a lot of trial and error, rather than figuring out all these theoretical assumptions. 
I don't want to say that all these assumptions are not relevant. If you work as a data scientist, they still are. But I also found that the more practical way to understand if you can apply linear regression to the data or not, is to just apply it and see if it works or not. If it works – if the results on the validation are good – then it means that you can apply your model to this dataset. 
Maybe some of the assumptions are violated. For example, multicollinearity – this is actually the case in the lectures, when we needed to regularize (add regularization to the model) this assumption was violated. You will see in the lectures what exactly happened. So check it out. I don't think I will be able to give a good answer, because I am not prepared to talk about this, but any statistics book or theoretical machine learning book explains this."
1880	When you split the dataset into two-week content, how do we avoid having repeated data?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1881	When you split the dataset into two-week content, how do we avoid having repeated data?	I don't think I understand the question. Is the question about what we do when we have duplicates in data and we split it and then there are duplicates? Some of the datasets in week one and then one split and then another one, or…? Oh, okay. Yeah, the way we do it is, we generate random numbers. I don't remember, to be honest, how exactly we do this. Let me quickly take a look. If we go to regression, notebook, and split – I think we shuffle data, not draw random numbers. Yeah, we take a range from zero to n exclusive. So n is not included and then we shuffle it. This way, we make sure that the same number does not end up in the different splits. I hope that answers your question.
1882	When you split the dataset into two-week content, how do we avoid having repeated data?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1883	When you split the dataset into two-week content, how do we avoid having repeated data?	"There was no registration deadline at all for this course. So you can still register. You can still sign up and you can follow the course. You will not be able to submit the homework assignments where the due date is already over because there are already solutions posted, so it doesn't make much sense to submit homework when the solutions are already there. You can follow the course but you will not be able to submit the homework. You can check our frequently asked questions and I do recommend doing this. Please remember the rule – if you have a question, first go check the frequently asked questions, and if you don't find the question there, go and ask in Slack. 
For this particular question, it’s already there. “Don't worry, you can take the course.” Maybe right now it's becoming a little bit more challenging to catch up with everything, but note that you can skip the midterm project. Right now, if you want to catch up with everything, if you skip the midterm project, it's fine. You will be able to do two capstone projects and still get the certificate. Also, you can just take the course in self-paced mode. There is nothing wrong with that either."
1884	When you split the dataset into two-week content, how do we avoid having repeated data?	I don't find it particularly useful to be honest. For binary, yes, it is definitely used. For multi-class… it becomes difficult to interpret, to be honest. So for multiclass, in my experience, accuracy works the best. The rest become more and more difficult to understand as the number of classes rises.
1885	What do you think should be an ideal tech stack for a data scientist/ML engineer?	"There is no silver bullet. Ideally, you need to have enough context to say “This is a good solution. This is not a good solution.” There is no one-size-fits-all tool. There is an ‘okay’ stack that works for many cases. For me, I would use Scikit Learn and XGBoost, for example. Some models from Scikit Learn, like linear or logistic regression, decision tree, random forest – and then I would use gradient boosted tree from XGBoost. Some people would go with LightGBM or CatBoost. Then I would deploy them with something like Flask and Kubernetes or with Lambda. That is usually sufficient for most cases. 
If we’re talking about web surfing, then even more often, I will deploy things with AWS Batch. AWS Batch is a thing where you can just take a Docker container and deploy it – then it will run periodically. Let's say if you want to run it every day, you can do this with AWS batch. The things we cover here, I think are quite applicable to the majority of problems. So you’ll probably cover most of the problems if you follow this course. I don't dare to call it ideal, but I think it's good enough to cover most of your needs."
1886	What do you think should be an ideal tech stack for a data scientist/ML engineer?	"You probably mean for logistic regression, right? For logistic regression, it's doing something very similar to what we saw in module 2. It is trying to minimize the… (I'm just trying to figure out how I can explain it without going into too many details and not to get lost myself and not to confuse you). It's probably a good idea to refer to some other course. I think I partly talked about this. Let me check. Here I talk a little bit about this, in ML Zoomcamp Office Hours week #4, where I explain a little bit how exactly it works. But it's a very superficial, let's say, overview of exactly what's happening there. 
If you want to learn in more detail how exactly the process works there, maybe you can just go to Google and search for “gradient descent, logistic regression,” or something like this. The first result will probably have a good explanation of what's happening under the hood. SciKit Learn does not use this exact method – it doesn't use gradient descent – it uses something more complex (more advanced) than that. But this is roughly what's happening under the hood. I hope you're satisfied by this answer. I don't think I can give you a better one with the time we have. It will be like a separate lecture. If you don't want to go into details, you can think of this as something that’s similar to logistic regression happening there but we add sigmoid on top of that."
1887	What do you think should be an ideal tech stack for a data scientist/ML engineer?	"To be honest, I don't know what the typical requirements for computer vision jobs are. They can vary from one company to another. I will give you a general piece of advice that I would give in any other case. You need to do some research about what the company is doing and understand what their problems are. What kinds of problems are they solving? Then try to solve a similar problem. For example, if it's a company like where I work – online classifieds – you can go to Kaggle and look at what kind of computer vision problems companies in the online classifieds domain are putting out there. 
Or just go through the tech blog and see what kind of articles that this company is publishing. Then try to build the project around that. For example, let's take OLX, the company where I work. You can find an article here, Fighting fraud with Triplet Loss. In this article, we show how we use computer vision to find duplicates – how to find image duplicates – and then this article describes that. So if you do something similar in your project, you can just add it to your CV and then talk about this during your interview. That's much better than just a random project. It will certainly help to pique attention. I guess that's the most important factor, try to do some research, find what kind of problems the company is interested in solving and then solve these problems. Then use it to build your portfolio."
1888	What do you think should be an ideal tech stack for a data scientist/ML engineer?	Well, that's why we have module one. There, the very first unit (the very first video) the example with cars – just use this example. You can use that to explain what machine learning is. When it comes to neural networks, we have module eight, which is about neural networks. You can use that to explain what a neural network is. But if you want a one sentence explanation – it's a bunch of algorithms that extract patterns from data. I don't know – will it make sense to your stakeholders? If it will, good. If it will not, then you have these other resources that I mentioned.
1889	What do you think should be an ideal tech stack for a data scientist/ML engineer?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1890	During which weeks will we do the project?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
1891	During which weeks will we do the project?	"There are lots of different varieties of approaches here. One thing that you might do is use some kind of data ingestion tool, as I mentioned in the live FAQ discussion – something like Airbyte or Fivetran – if you have lots of data that you're ingesting, that can be a common way to stay on top of that. Or you could do it through a script, kind of like we do here. 
But what you might do is run your agent on a virtual environment in Google Cloud VM, for example, or in AWS, or somewhere else. You could run it in Kubernetes. There are a number of places where maybe you would have that running and have your infrastructure there, so that you were just pulling down the code not to your local machine, but it would just be in your VM. 
If you're actually going and using Pandas, you're putting the data into memory, so you do need to have some scripts that will do that. And you can go directly between Google Cloud Storage and BigQuery with the Prefect GCP library. So that Prefect GCP library that you installed, that you've been using, does have some options in there. I think it's in the BigQuery module to actually get data directly into BigQuery from GCS. So there are a lot of options there. Check out Prefect GCP to see how to do it."
1892	During which weeks will we do the project?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1893	During which weeks will we do the project?	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
1894	During which weeks will we do the project?	"Alexey
I will show you. You go to our 2023 cohort, and you look at the deadline calendar. It will tell you during which weeks you do the project."
1895	How to work with F1 and when to use it?	[chuckles] I'm not prepared to answer that question. I don't have a top three for guidelines. Maybe this is something we can discuss next time. I don't know how to answer that. Maybe what you can do is go to our YouTube channel (I know I keep doing this) where we just had this Responsible and Explainable AI Interview about that. So maybe check it out. I will mark this as “answered” and then maybe this is something we can go through next time, like we did last time – some of the questions I didn't answer and we moved it to this week. So probably we’re going to keep doing it like this.
1896	How to work with F1 and when to use it?	I wish Tim was here and would help me answer that because I don't know. I assume if you wanted to use Bento, you would probably use TensorFlow Lite – you don't want to use the usual TensorFlow for the same reasons that we talked about in the serverless module, because it's simply too large. Probably for Bento, you would use something like TensorFlow Lite and it will still work. Maybe another thing you can check – in our channel, we had a talk from one of the Bento folks, Building an ML Service Platform from the Ground Up. Here, Sean talks about the differences between TensorFlow Serving and Bento. With TensorFlow Serving, it's good – it's quite optimized for deep learning. There are some problems. First of all, it's written in C++ and expects your model in a certain format – this “saved model format”. Adding anything extra on top of that (like pre-processing, post-processing) becomes difficult, because you will need to have an extra step there. Bento makes it easier. But, again, I don't know. It's very difficult to give you a concrete recommendation without knowing what exactly you want to do. So it's case dependent. If you already use TensorFlow Serving in your company, then maybe it makes sense to stick to that. If we have Tim on Office Hours next week, maybe you can ask him that. He will be a better person to answer this question.
1897	How to work with F1 and when to use it?	Not always. For logistic regression, it does. For decision trees and random forest and XGBoost – not really. Then there is a thing called “calibration”. SciKit Learn calibration. You will need to check this out. I think there are also examples of how to do this with random forest. If you can't find examples here, there are examples on Kaggle. But practically speaking, I don't remember the last time I needed to use something like this. It's helpful when you want to train multiple models. Let's say you've been using logistic regression in production for some time and then you make some decisions based on this threshold. But then you want to deploy a new model (XGBoost) and you want to make sure that the ranges you have in the previous model are the same in the new model so the distribution looks kind of similar. This is where calibration helps. It's a useful thing, but not every application needs it.
1898	How to work with F1 and when to use it?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1899	How to work with F1 and when to use it?	How F1 works, you hopefully learned from the homework. When to use it? I think always – when you want to use precision and recall (and you value them equally). When you want to have good precision and you also want to have good recall, then in this case, you need to use F1. In cases when you value precision more than recall, then you need to use a different approach – maybe just plotting things. There's actually a special case of F score called Fβ score (F beta score). Here, depending on beta, you give more weight to precision or recall. But usually the F1 score is good enough, because F1 gives equal weights to both.
1900	Is linear regression applicable in real life or is it more of a simple baseline model?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1901	Is linear regression applicable in real life or is it more of a simple baseline model?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
1902	Is linear regression applicable in real life or is it more of a simple baseline model?	"It's both. It is very, very applicable in real life. Usually, most projects start with a simple baseline – without any machine learning – and then you can use the next baseline, which would be linear regression. Then your model can work using linear regression until you think “Okay, now it's time I need to improve it,” and then maybe you can use something like XGBoost or something like this. So it's very applicable and it’s applicable as a simple baseline model. 
Sometimes you just don't need a more complex model. Let's say if you have XGBoost, then it becomes more difficult to serve because it's more computationally demanding, while linear regression is just matrix multiplication. It's very simple. It's very performant. It is a good first model that you always should use and you should always start with."
1903	Is linear regression applicable in real life or is it more of a simple baseline model?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
1904	Is linear regression applicable in real life or is it more of a simple baseline model?	I don't think I understand your question, to be honest. Maybe I'll try to rephrase it… Or ask in Slack, maybe?
1905	In homework three, it does not look like we are using the test dataset at all to answer the given question. Please advise if I missed something.	Yeah, it's probably not the best practice. I wouldn't do this in a real-life situation. In a real-life situation, we simply will not have unseen data. But the idea here was to get a dataset that is already familiar to you – the dataset we used in the previous homework – and just convert it to a binary problem. That was the goal we had in mind – how can we make it easier for you to learn these things without overloading you with a new dataset? That was the simplest approach we decided to take to actually do it here. Another alternative could have been, instead of asking you to compute the mean, just give you the number, but with the mean, it's a little bit more interesting, I think.
1906	In homework three, it does not look like we are using the test dataset at all to answer the given question. Please advise if I missed something.	"Tim
Yeah, we have a lot of users who use MLflow and BentoML together. Once you have a model that’s sort of your “finalized” model – you save it to your MLflow registry and then (it depends on what your CI/CD pipeline looks like) but BentoML has a command to import from an MLflow registry. You can tell which MLflow model is the one that you want to deploy. It's very similar to save_model, except instead of pulling from your local environment, you're pulling from an MLflow registry. So you import the model, it automatically gets pulled into your Bento and then you can deploy it. On the BentoML documentation site, there's a big framework document on how to integrate with MLflow. 
Alexey
When searching, one of the suggestions was BentoML vs MLflow. I guess there are also some similarities because with MLflow, you can serve models. 
Tim
The thing about serving models is that it's a part of the pipeline that you just have to do. Every single framework out there has some way to serve a model – you train the model and then it just puts it up there to be able to serve. Typically, frameworks don't specialize in serving those. BentoML specializes in making the serving part really, really fast and really, really easy. What you find a lot of the time is, when you have serving logic in a really large end-to-end pipeline, it just puts the model there and then the only thing that you could submit to the model is the data. But we know in practice that there's usually business logic around this – pre-transformation logic, post-transformation logic, there's the version of the model that was saved. There are a lot of these other components that you want around the model, not just the model for inference. And of course there's the performance layer underneath. The BentoML architecture kind of brings that all together into one deployable, rather than just giving you one point to call inference for your model. 
Alexey
That is a comment in the live chat that says “There is PyCaret, too.” This is how you use PyCaret with Bento. 
Tim
Right. BentoML has lots and lots of integrations and we're only adding to them every day. The idea is to get the model from wherever you built it, bring it into your Bento, and then you can deploy it anywhere. Once it's in the Bento – in this standard packaging – it's nice because then we've got lots of different tools to deploy to Lambda, SageMaker, and lots of different places. And the places to deploy, we're adding to every week and month as well.
Alexey  
Since you can have a Docker container at the end – you do “bento_containerize” right? That's the command? And then you have a Docker container (Docker image) that you can deploy everywhere where you can deploy a Docker container, which is pretty much every place on the internet, right? 
Tim
Right. Some places, though, require specialized… 
Alexey
Like Lambda, right? 
Tim
Yeah, like Lambda won't accept just a normal Docker container. You have to actually create special endpoints. When we deploy to Lambda, we create those endpoints for you and make sure that it's behind an API gateway and things like that. For certain services, there are a lot of nuances that we kind of take care of for you – as long as you're in the Bento standard format.
Alexey 
For those who have no idea what MLflow is, and why we are talking about this – you don't need to worry about this right now. But at some point, this tool and other similar tools will be quite handy. You can check out our MLOps Zoomcamp course, after you finish this one. Don't try to do multiple courses at the same time. That could be too much. But of course, if you're curious, go check it out."
1907	In homework three, it does not look like we are using the test dataset at all to answer the given question. Please advise if I missed something.	Yeah, if it doesn't look like we're using it – we are not using it. Maybe we just want to set it aside and forget about it.
1908	In homework three, it does not look like we are using the test dataset at all to answer the given question. Please advise if I missed something.	"I am glad you asked. Project of the Week has nothing to do with the course to the extent that it's just different initiatives that are run in our community. We have a course and we also have Project of the Week. We also have webinars, we also have podcasts. They are all different activities. For the Project of the Week, this week, we'll have a project about recommender systems. The idea there is that every day, you get a set of tasks. On Wednesday, you get this set of tasks. You need to come up with an idea. You need to find a dataset for this idea, and you need to share your progress. Then on day two, you will need to go through the suggested materials and also find some materials on your own and do this thing – learn about the basics, do exploratory data analysis, understand this data that you found and share your progress. As you can see on the project page, every day, you get a set of tasks and you need to do them. So it's not like we're telling you exactly what to do like in the Zoomcamp (in the videos we tell you that “This is the exact sequence of actions you need to execute and this is the result you get.”) With the Project of the Week, it's more independent. We just give you four bullet points and then it's up to you to actually do these things. And then, of course, you share your progress. The idea behind this project is that, at the end, you have a project that you can add to your portfolio. This is not a project that you just took from the tutorial and followed along, but this is a project that you did with some guidance. These are two very different things. That's the idea behind Project of the Week. For the project that we'll have as a part of the course, the idea is somewhat similar, except you will get less guidance – you will have more independence, let's say. The guidance we have for the project in the course is the form of a bunch of criteria that you need to satisfy. Then it's up to you to come up with a plan. But here, for Project the Week, we have a suggested plan that you can follow and hopefully, at the end, come up with a result. Keep in mind that for Project of the Week, some things might not go according to plan and that’s fine. This is just a suggested plan. We don't know if it will actually work exactly like we outlined day by day. It's more just to give you some guidance, but it’s up to you how exactly to approach that. 
There is a comment that says “I did not know about this.” Yes, we have a channel in our slack. It's called #project-of-the-week and it starts this week Wednesday. You'll notice that there's the digit one in the name “2022-10-19-recommenders-1.md”. We'll also have a follow-up project about recommender systems. If you want to take part in this (by “take part” I mean, propose your own topic) for example, some of you were asking about time series, some of you were asking about NLP, some of you were asking about other things in the comments in live chat. Daniel mentioned audio – If you want to learn any of these libraries, we can do a Project of the Week to learn these libraries or methods or approaches or whatever."
1909	In homework three, it does not look like we are using the test dataset at all to answer the given question. Please advise if I missed something.	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1910	If I select the Kaggle dataset, can I download it locally for the project?	"Alexey
You can. Why not?"
1911	If I select the Kaggle dataset, can I download it locally for the project?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1912	If I select the Kaggle dataset, can I download it locally for the project?	"Alexey
The best way to contribute is to take notes and include them to the notes section. And tell your friends about the course. As a part of your Learning in Public, spread the word. It will help us. And don't forget to give us a star if you have."
1913	If I select the Kaggle dataset, can I download it locally for the project?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
1914	If I select the Kaggle dataset, can I download it locally for the project?	Our thoughts are that this is indeed an exciting area. This is an in-demand skill and that's why we have a section about analytics engineering in this course.
1915	I need advice on how to build credibility as a data engineer. I’m thinking of freelancing, open source and Git projects.	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
1916	I need advice on how to build credibility as a data engineer. I’m thinking of freelancing, open source and Git projects.	No, you cannot.
1917	I need advice on how to build credibility as a data engineer. I’m thinking of freelancing, open source and Git projects.	Yep, projects are the best way. Just start working. That's already good enough. You can also think about giving talks and so on.
1918	I need advice on how to build credibility as a data engineer. I’m thinking of freelancing, open source and Git projects.	Just one. It's either the first attempt or the second. You should not try both.
1919	I need advice on how to build credibility as a data engineer. I’m thinking of freelancing, open source and Git projects.	Building projects. Maybe build another one, where instead of batch, you use streaming, maybe. If you're interested in a particular employer, try to build a project that is similar to what they’re working on. For example, if you want to apply for Spotify, try to find some music data and build a project for that.
1920	I was able to run the Terraform part of homework 1 from my laptop. Can I keep doing this?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
1921	I was able to run the Terraform part of homework 1 from my laptop. Can I keep doing this?	"Alexey
The first thing about the dataset – what kind of dataset do you want to use? Or what kind of problem do you want to solve? Once you figure this out, then you're basically ready to start working on a project. Then in the project, you need to decide if you want to do streaming or batch. For batch, it's using things like Prefect, Spark, or DBT. For streaming, it’s using the materials from the last lecture (week 2). Once you decide that, you will just implement this and you will find all the information you need here in the week 7 project repo. Just go through this and if you have any questions left, let us know. Keep in mind that these are the criteria that other people (your peers) will use when evaluating your project. Perhaps you can already think about that and how you want to implement your project in such a way that you maximize the score you get from these criteria."
1922	I was able to run the Terraform part of homework 1 from my laptop. Can I keep doing this?	Everything you do here is individual. You don't form teams.
1923	I was able to run the Terraform part of homework 1 from my laptop. Can I keep doing this?	No, you cannot.
1924	I was able to run the Terraform part of homework 1 from my laptop. Can I keep doing this?	Yes, you can keep doing this. You don't really need a cloud VM. A cloud VM just makes things simpler for us in the sense that everyone has the same environment. Then, if you have some problems, we can help. But if everything works locally, just keep using your local environment.
1925	The batch processing is to ingest data (in week 5 this is done with PySpark) but isn't it the same if we use Prefect? Can we ingest data with orchestration?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
1926	The batch processing is to ingest data (in week 5 this is done with PySpark) but isn't it the same if we use Prefect? Can we ingest data with orchestration?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1927	The batch processing is to ingest data (in week 5 this is done with PySpark) but isn't it the same if we use Prefect? Can we ingest data with orchestration?	No, you cannot.
1928	The batch processing is to ingest data (in week 5 this is done with PySpark) but isn't it the same if we use Prefect? Can we ingest data with orchestration?	Batch processing is not only used for ingesting data. That's one of the things. In general, any data transformation is done with batch processing. You can ingest data to your data warehouse, you can transform data that you have in your data warehouse, you can put then data into your data lake, you can get your data from a data lake, transform it to a data warehouse – there are thousands of different options and alternatives. For some cases Prefect is fine. It's lightweight and you can do simple stuff there. But sometimes you just need to process a lot of data and then this is where you would use PySpark. You would not try to do this in Prefect because it will put too much load on your Prefect agents. Usually, in practice, you want to keep them lightweight and delegate all the execution of work to some external computing environments such as PySpark, or Spark in general. Yeah, you can ingest data with orchestration. But when you look at the content of week 5, you'll see that it's much more than that. Prefect can be used to execute PySpark backdrops.
1929	The batch processing is to ingest data (in week 5 this is done with PySpark) but isn't it the same if we use Prefect? Can we ingest data with orchestration?	"There are lots of different varieties of approaches here. One thing that you might do is use some kind of data ingestion tool, as I mentioned in the live FAQ discussion – something like Airbyte or Fivetran – if you have lots of data that you're ingesting, that can be a common way to stay on top of that. Or you could do it through a script, kind of like we do here. 
But what you might do is run your agent on a virtual environment in Google Cloud VM, for example, or in AWS, or somewhere else. You could run it in Kubernetes. There are a number of places where maybe you would have that running and have your infrastructure there, so that you were just pulling down the code not to your local machine, but it would just be in your VM. 
If you're actually going and using Pandas, you're putting the data into memory, so you do need to have some scripts that will do that. And you can go directly between Google Cloud Storage and BigQuery with the Prefect GCP library. So that Prefect GCP library that you installed, that you've been using, does have some options in there. I think it's in the BigQuery module to actually get data directly into BigQuery from GCS. So there are a lot of options there. Check out Prefect GCP to see how to do it."
1930	When I deploy from a Python code, why don't I see the .yaml file being created in the current repo?	No, you cannot.
1931	When I deploy from a Python code, why don't I see the .yaml file being created in the current repo?	We currently do not have any course partners yet. If you work at a company and you want to partner with us, please reach out and we will see how we can do it. In one of our courses, which was our first iteration of the Machine Learning Zoomcamp, we partnered with a company called Delphi. They got two interns from our course and they were quite satisfied with the outcome. You can actually read more about the work of these interns in our articles – Interview with Valerii Chetvertakov and then another one, Interview with Ken Wu. You can learn more about the interns and if you think that, at your company, you need good interns or juniors or you want to partner in any other way, please reach out and we can see how to make it work.
1932	When I deploy from a Python code, why don't I see the .yaml file being created in the current repo?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
1933	When I deploy from a Python code, why don't I see the .yaml file being created in the current repo?	I don't have enough context to answer this question. I don't know. Ask in Slack, please.
1934	When I deploy from a Python code, why don't I see the .yaml file being created in the current repo?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
1935	For the capstone projects, can we use new models? For example, for object detection.	"Tim
I think a couple people asked this in ML Zoomcamp Slack. I was thinking about this a little bit. I think it's sort of a philosophical thing [chuckles] where we think about models and Bento is sort of separate a little bit from code. So your project directory is where all the stuff that you'll commit to Git. I don't think we think that it's the right model to commit your models to Git, for example. We think of ourselves a little bit more like Docker. You don't have Docker images in your project directory, right? You just have your Docker file and that's what you commit to Git. In the same way, we store your models and your Bento in that dedicated directory, and then sort of leave it up to you to push it to different places where you might build or where you might need it
Alexey
Do you know why you decided to save it locally on your local file system? For example, what MLflow does – you can save it to S3 directly, but in the case of Bento, you always first save it to local storage and then you decide “This model I want to export, but for the rest, I don't care.”
Tim
Right. First, I think it’s because that's the easiest way to do it, but also because I think with BentoML, the workflow is that a lot of times, somebody's building a model on their laptop, or somebody's building a model in a slightly less distributed system and they're assembling their Bento there. So I think that the fastest way to access that model is locally. We do give you the ability to push that model and pull that model from other places. But ultimately, when you assemble your Bento and then you assemble your container, all that stuff has to be there. I don't think we wanted separate abstractions for when a person is training and saving, versus when a person is building a Bento and pulling in the model and assembling the container. It's just the same abstraction, which is just a local repository which contains models.
Alexey 
Maybe you don't want to save every model every single time you change one single parameter to the cloud. [Tim agrees] Maybe at the end, after all the experiments, you want to save the final one. Meanwhile, for MLflow, the use case is different – you actually want to save every single experiment because you want to see the results of these experiments.
Tim
Right."
1936	For the capstone projects, can we use new models? For example, for object detection.	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
1937	For the capstone projects, can we use new models? For example, for object detection.	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1938	For the capstone projects, can we use new models? For example, for object detection.	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1939	For the capstone projects, can we use new models? For example, for object detection.	Yes, by all means, you can. But please make sure you document everything in the readme file. You need to assume that the people who will review your projects do not know anything about object detection, which will probably be the case because we didn't cover it. You will need to describe all of that in the readme. Like: What is object detection? What exactly are you doing? What is the function? What is the metric you use for evaluating? How does it work? Then people will look at your project and understand what's happening there. So you need to put more effort in documentation, but this is actually a good thing because your reviewers will also learn something new. If you feel like you can do that, then by all means, do that.
1940	What is a good target for accuracy in data modeling?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1941	What is a good target for accuracy in data modeling?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1942	What is a good target for accuracy in data modeling?	"Tim
Yeah, we have a lot of users who use MLflow and BentoML together. Once you have a model that’s sort of your “finalized” model – you save it to your MLflow registry and then (it depends on what your CI/CD pipeline looks like) but BentoML has a command to import from an MLflow registry. You can tell which MLflow model is the one that you want to deploy. It's very similar to save_model, except instead of pulling from your local environment, you're pulling from an MLflow registry. So you import the model, it automatically gets pulled into your Bento and then you can deploy it. On the BentoML documentation site, there's a big framework document on how to integrate with MLflow. 
Alexey
When searching, one of the suggestions was BentoML vs MLflow. I guess there are also some similarities because with MLflow, you can serve models. 
Tim
The thing about serving models is that it's a part of the pipeline that you just have to do. Every single framework out there has some way to serve a model – you train the model and then it just puts it up there to be able to serve. Typically, frameworks don't specialize in serving those. BentoML specializes in making the serving part really, really fast and really, really easy. What you find a lot of the time is, when you have serving logic in a really large end-to-end pipeline, it just puts the model there and then the only thing that you could submit to the model is the data. But we know in practice that there's usually business logic around this – pre-transformation logic, post-transformation logic, there's the version of the model that was saved. There are a lot of these other components that you want around the model, not just the model for inference. And of course there's the performance layer underneath. The BentoML architecture kind of brings that all together into one deployable, rather than just giving you one point to call inference for your model. 
Alexey
That is a comment in the live chat that says “There is PyCaret, too.” This is how you use PyCaret with Bento. 
Tim
Right. BentoML has lots and lots of integrations and we're only adding to them every day. The idea is to get the model from wherever you built it, bring it into your Bento, and then you can deploy it anywhere. Once it's in the Bento – in this standard packaging – it's nice because then we've got lots of different tools to deploy to Lambda, SageMaker, and lots of different places. And the places to deploy, we're adding to every week and month as well.
Alexey  
Since you can have a Docker container at the end – you do “bento_containerize” right? That's the command? And then you have a Docker container (Docker image) that you can deploy everywhere where you can deploy a Docker container, which is pretty much every place on the internet, right? 
Tim
Right. Some places, though, require specialized… 
Alexey
Like Lambda, right? 
Tim
Yeah, like Lambda won't accept just a normal Docker container. You have to actually create special endpoints. When we deploy to Lambda, we create those endpoints for you and make sure that it's behind an API gateway and things like that. For certain services, there are a lot of nuances that we kind of take care of for you – as long as you're in the Bento standard format.
Alexey 
For those who have no idea what MLflow is, and why we are talking about this – you don't need to worry about this right now. But at some point, this tool and other similar tools will be quite handy. You can check out our MLOps Zoomcamp course, after you finish this one. Don't try to do multiple courses at the same time. That could be too much. But of course, if you're curious, go check it out."
1943	What is a good target for accuracy in data modeling?	There is no good target. It's all case dependent. In some cases, if you have 90% accuracy, this is very suspicious – you probably should be worried and check your data for issues. Sometimes, you can have even 50% accuracy – in some cases it’s good. It also depends on what the skewness of the data is, when you have a lot of examples of one class, but very few for another class – like a class imbalance. It also depends on that. As you remember, “accuracy” is actually a misleading metric. In some cases, you should look at things like precision and recall – and you shouldn't consider looking at accuracy at all.
1944	What is a good target for accuracy in data modeling?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
1945	Can we use BentoML with Colab?	"Tim 
I think, yes. We have a few examples on our website. I think with Colab, it becomes a little bit more difficult once you're hosting the Bento to access it, if it's running on a Google Colab server. But yeah, you can save it. What I would say is that you can save your model and then export your model to somewhere where you can then build your Bento there.
Alexey 
By default, Bento saves in a local file system (in your /home) so you need to have a way to save this model somewhere, let's say on the cloud, S3, whatever – somewhere externally. The workflow here is that you train your model in Colab, you save it, and then you deploy it somewhere. You don't deploy it on a Colab instance.
Tim
Right. When you save your model, I think it'll save it to the Colab instance. Then you have to use BentoML Export to push it to an S3 bucket or something where you can pick it up and deploy your Bento.
Alexey 
And there are options for deploying. We already discussed this in the course, when we looked at Elastic Beanstalk. I assume it can work with Bento. I haven't checked. [Tim agrees] But then what we saw this week, we saw how to deploy it with ECS, and then we can also deploy to Kubernetes EKS, we can also deploy to lambda – there are a ton of other options. Everywhere where you can deploy a Docker container (Docker image) you can deploy it there. Right? [Tim agrees]"
1946	Can we use BentoML with Colab?	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
1947	Can we use BentoML with Colab?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1948	Can we use BentoML with Colab?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1949	Can we use BentoML with Colab?	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
1950	I can't find the ny_rides.json file to complete the environment setup.	I will not answer this right now. We did not plan anything like that. I think Anna prepared some materials about how to deploy Prefect, not with Terraform, but with some Google Cloud Platform stuff. I might be mistaken, but in practice, I think we use something like Kubernetes. Kubernetes is not managed through Terraform. I will leave this question answered and in the next Office Hours when we'll talk more about Prefect. Somebody from the Prefect team will answer this question. Probably Jeff will be in our next Office Hours.
1951	I can't find the ny_rides.json file to complete the environment setup.	When you create a project and then you create this role – the JSON file – in the Terraform BIOS, you can see it. And this is the JSON file with the credentials. If you go here, to the Terraform set up and take a look at this one. This is application credentials. You basically follow these instructions. This is the service account. You download the JSON file, and then this is the file.
1952	I can't find the ny_rides.json file to complete the environment setup.	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
1953	I can't find the ny_rides.json file to complete the environment setup.	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1954	I can't find the ny_rides.json file to complete the environment setup.	If you have time, then why not? But as I said, this course requires some commitment from many people lately. Doing two courses in parallel might be ambitious, but if you have time, then why not?
1955	Can you teach us more about using roc_auc_score for feature importance?	"Tim 
Not really. We have users who use both Streamlit and Gradio. I think Streamlit and Gradio are a bit more about the presentation side. We do have a couple of users who have asked us about deeper Streamlit or Gradio support and I think that's something that's probably coming up in the next few months.
Alexey
For those who don't know what Streamlit or Gradio is – this is a way to create an interface for your services. Well, Gradio is focused on machine learning, meaning you create interfaces specifically for models. And then Streamlit is just a Python package for creating interfaces. We actually had a project at DataTalks.Club about Streamlit. I actually even have a tweet about this. This is what Streamlit looks like. In the video, you can see that you have this interface, and then based on the toggles that you move the core of your model changes. It gives you an easy way to build an interface like that. Meanwhile Bento focuses on quite a different use case, right? It’s not building interfaces, but serving models.
Tim 
Right, yeah. I think one of our users was asking for a really easy way, so that once you have your model and your Streamlit UI to be able to create a Bento deployment with one line in that way, anybody that you gave your Streamlit app to would be calling to that service and it could work anywhere. Which is a nice feature, I think.
Alexey 
Yeah, I think it is. I imagine if you have a Pydentic class there, then you can easily generate a Streamlit app for that.
Tim 
Right. Oh, that's a good idea. I hadn't thought of that. Yeah.
Alexey
And then in addition to that, the Swagger UI that you have, could be like having the actual interface. 
Tim
Oh, yeah, that would be cool. [chuckles]
Alexey
That could be a good Hackathon project, right?
Tim 
Yeah, for sure.
Alexey
From what I see – let's say you’ve built a model. Then you deployed this model and you have this nice API. You can query it with CURL. But what if you want to demo this project to your manager, or a manager of your manager, or somebody who does not necessarily have a technical background? If you give them the URL and say, “Okay, you need to execute the CURL command with a post request,” they would be like, “CURL what? What do you want from me? Just show me how it looks.” This is when tools like Streamlit come in handy. Instead of giving them a command line interface to query it, you just give them “Okay, this is the link. Play with this.” If Bento could do something like that – automatically generate things like that – many data scientists would thank you.
Tim
Yeah, I was just looking because I figured there's a library that turns Pydantic models into Streamlit apps. It looks like there is one that does that. I wonder if we could just plug into that app and then have that same thing. 
Alexey
That's cool. 
Tim
Yeah, that's awesome. That's kind of the cool part of open source – there are so many people out there with so many different projects and we get to collaborate with them and kind of build the best collaborations and combinations of these tools."
1956	Can you teach us more about using roc_auc_score for feature importance?	"We had a talk about this on our YouTube channel. As far as I remember, it was called Machine Learning Design Patterns. These patterns were mostly about engineering – engineering patterns – but the first pattern, as far as I remember, was about rebalancing. So check it out. Sara explains when to use these techniques. Here, the important part is that you apply these techniques only to your training dataset. You keep your validation dataset intact – you don't change it – because you want to have a reliable way of evaluating the performance of your model. You apply different techniques to your training dataset and then you see how exactly it changes the performance on your validation dataset. 
So you apply these techniques to train, experiment, try different ones, and then go with the one that has the best uplift for your score on validation. That's usually how you do this. This will tell you if you actually need any of these techniques, or whether just throwing all the data that you have into the model is good enough, so you don't need to make the process more complicated than it should be."
1957	Can you teach us more about using roc_auc_score for feature importance?	"You probably mean for logistic regression, right? For logistic regression, it's doing something very similar to what we saw in module 2. It is trying to minimize the… (I'm just trying to figure out how I can explain it without going into too many details and not to get lost myself and not to confuse you). It's probably a good idea to refer to some other course. I think I partly talked about this. Let me check. Here I talk a little bit about this, in ML Zoomcamp Office Hours week #4, where I explain a little bit how exactly it works. But it's a very superficial, let's say, overview of exactly what's happening there. 
If you want to learn in more detail how exactly the process works there, maybe you can just go to Google and search for “gradient descent, logistic regression,” or something like this. The first result will probably have a good explanation of what's happening under the hood. SciKit Learn does not use this exact method – it doesn't use gradient descent – it uses something more complex (more advanced) than that. But this is roughly what's happening under the hood. I hope you're satisfied by this answer. I don't think I can give you a better one with the time we have. It will be like a separate lecture. If you don't want to go into details, you can think of this as something that’s similar to logistic regression happening there but we add sigmoid on top of that."
1958	Can you teach us more about using roc_auc_score for feature importance?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1959	Can you teach us more about using roc_auc_score for feature importance?	"Yes. The idea here is – for numerical features, you can think of a numerical feature as a score. The output of your model is a score and a feature is also a score. It's just a number between some minimum value and some maximum value. Then you can follow through the same procedure as we did for the classifier (for the output of the classifier) but for the score. In our case, the output for logistic regression is always a number between zero and one. But for the ROC core, it actually doesn't matter what the scale of your variable is. It can be something between zero and one, it can be something between -100 and 100, it can be something between -1000 and 1000, or between 0 and 100 – it does not matter. 
What you can do there is just take this feature and go through the same procedure as we did in the lectures. Try to plot it with different thresholds and then see what happens. You can also have an ROC plot for the feature, not for the output of the model, and then based on that, you can see what the area under this ROC curve is. That will give you an idea of how well this feature separates positive examples from negative examples. This is what this ROC curve actually does. That's roughly the idea behind using the ROC curve for feature importance. You kind of pretend that this is the output of your model – this is a score – and then you do everything we did in the lectures to understand how well this feature separates positive and negative examples."
1960	How can I see the feedback on my midterm project?	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
1961	How can I see the feedback on my midterm project?	Check out the beginning of this video, where I showed how to do this
1962	How can I see the feedback on my midterm project?	Rescale is already preprocessing. Actually, in exception, this is what they do. It's actually the scaling there. There are two operations, I think. I don't remember exactly, but it's rescaling plus something else. So rescaling is preprocessing.
1963	How can I see the feedback on my midterm project?	"If you look at the criteria, you will see that model performance is not one of the criteria. If your model does not give you great performance, it will not affect the score that you will get at the end. This is intentional. The reason it's like that is because it's very hard to control this. The performance on the dataset of a model is very dataset-specific. For some datasets, this is how it is. In some cases, for example, for click prediction – if you want to predict if somebody will click on an ad or not, the performance of the model is usually bad. You cannot predict with very high probability if a person will click on an ad or not. But it doesn't mean that these models aren't useful. But the performance that you will observe on the validation dataset will be bad. And that's okay. 
This is just a characteristic (a feature) of this particular dataset – of this particular problem. That's why it's not a criterion, but you do need to try to train multiple models, as you see here (in the criteria for model training), and then try to tune their parameters. In the end, maybe you will not have a great model – you will not have great performance for this model – but at least you tried and this is what we evaluate here. We don't evaluate the performance, but whether you tune the parameters."
1964	How can I see the feedback on my midterm project?	[chuckles] I'm not prepared to answer that question. I don't have a top three for guidelines. Maybe this is something we can discuss next time. I don't know how to answer that. Maybe what you can do is go to our YouTube channel (I know I keep doing this) where we just had this Responsible and Explainable AI Interview about that. So maybe check it out. I will mark this as “answered” and then maybe this is something we can go through next time, like we did last time – some of the questions I didn't answer and we moved it to this week. So probably we’re going to keep doing it like this.
1965	Will there eventually be instructions on how to deploy Prefect with Terraform (or will this be necessary)?	"Alexey
It's possible to apply to jobs right now. You don't have to take the course. Just start applying. Why do you need to take the course to start applying, right? It was possible to do it last year. Nobody is stopping you from doing this. But this course will give you some knowledge and add projects to your portfolio. That will be helpful in your job search. 
But I would encourage you to start applying to jobs right now to see what they actually want from you – what kind of topics come up, what kind of test assignments you get. You don't need to wait until the course is finished to start doing this. You can already learn a lot about the job market in your area or in the domain where you want to work.
Luis
I just want to add that when I did the 2022 cohort, I was in another company and the process of data engineering in that company was really similar to the Zoomcamp. What I started to notice was that the companies mostly use this architecture in engineering. So it's good for you to show yourself that you're starting to understand all this data engineering stuff. I think it's important.
Ankush
I just wanted to add. I don't know about applying for jobs, but once you have a job, if you want to get an increment, contact Michael. [chuckles]

Victoria
Michael moneymaker. [chuckles]"
1966	Will there eventually be instructions on how to deploy Prefect with Terraform (or will this be necessary)?	"Alexey
No. I mean, we kind of told you. Yes, you maybe didn't see the intro video. But, sorry. No, it's late. We will actually recalculate the points for the second homework (HW 1B) because in the forum, it said that you can get up to one point, but we calculated that the cap there was seven. We are going to recalculate that. Thus, people who got a lot of points will get just one. Sorry about that. But don't focus on points. If you go to the leaderboard, we know that the person with this hash (722366d8b29ece9be3a7605363562c7c60d6918e) did seven links. Do you know who that is? Probably you don't. 
These points are virtual. It's nice to have a good score, but remember that these points mean whatever you want them to mean – not more than not less. Maybe it's okay if you get a few points less than others for the first homework, but the important thing here is consistency. If you just do your homework, even without doing any Learning in Public stuff, which I think you should do (you should post about the course). If you don't do this and you just do all the homework, by the end, you will be somewhere in the top 20. If you look at the leaderboard from the previous iteration, you will see that not everyone stuck around till the end. 
If you just do your homework, don't worry about these points, and you'll be fine. Sorry about that. I know that it can be discouraging that you didn't get as many points as others, but don't worry about that. Focus on learning. Maybe Michael has some other suggestions or some other things that I didn't mention, because he took part last year. But I think he was one of the people who did a few posts every week, right?
Michael
Yes. Actually, when I went through it, I missed the whole… you can get up to seven points, but I was just doing one every week and I just stuck with that. I was still near the top. I think the real value in that isn't necessarily the points but just being comfortable sharing – writing articles and making videos. That's the real value there. But everyone likes the clout value, too. As Alexey said, it will fluctuate, so if you keep up with it going forward, you'll probably be just fine."
1967	Will there eventually be instructions on how to deploy Prefect with Terraform (or will this be necessary)?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
1968	Will there eventually be instructions on how to deploy Prefect with Terraform (or will this be necessary)?	"Michael
It's not necessary. We do have recipes for using Terraform with Prefect with different things. We talked about the Prefect recipes repo, but I'll put a link to it here. 
Jeff
This is in a docs and we have a GitHub repo where we have lots of different recipes. Some of them here explain things like, say you want to run an agent – how can you do that? There are so many different cloud environments. You can put things on Kubernetes, you can put things on something like a more serverless platform like Fargate. For example, on the right there, if you were wanting to use AWS with Prefect and ECS (elastic container service) then with Terraform – we have some recipes for that. So there's just so many different permutations of possible ways to do things. A lot of these are helpful ways to go and look at options, but it depends on what you're using in the real world (in a use case). I don't think we have anything specific for this course that requires us doing this, so we don't have anything further for it.
Alexey
I guess, the usual setup that I saw (not necessarily Prefect-related) you typically use Terraform for setting up infrastructure or things like a Kubernetes cluster and configure it there. But then there is a separate repo with configuration for Kubernetes that is not maintained/managed through Terraform but through something else. For that, you don't really need to Terraform – and then Prefect could live in this Kubernetes cluster.
Jeff 
Yeah, we do have some guys working with Kubernetes in here, for sure. I see some of them here. We do have a Helm chart that we can use. If you're into the Kubernetes world, there are lots of things there. It's a whole rabbit hole.
Alexey
Here are the Prefect Helm charts.
Kalise
Also, there are a lot of YouTube videos we have that can help if you're trying to use some of these resources – how to go about deploying the Helm chart with Kubernetes and stuff, like step-by-step tutorials as well. Generally, if you just go to the Prefect YouTube channel, you'll see a lot of those tutorials. There should be some playlists there that have various things.
Alexey
Is this a duck in the cover or what?
Jeff
That is. It's the top of a duck. Blue ducks!"
1969	Will there eventually be instructions on how to deploy Prefect with Terraform (or will this be necessary)?	"Alexey
Yeah. They are kind of like folders and there are files in these folders. They can be stored in S3 or HDFS or Google Cloud Storage. Indeed, we save the data before transformation and if we want, we can also save the data after transformation in a data lake. As to why we save transformed data in a data lake rather than just a data warehouse – sometimes it's just cheaper. In a data warehouse like BigQuery or Snowflake, you need to pay a lot for storing the data there and for accessing the data. If you just save the data in a data lake, your indexing – the way how you can speed up the queries – is limited, so you will probably need to read all the files in a folder, but usually it's cheaper, especially if you are doing something like a full scan. When you need to access all the data for a day or inside a partition, then data lakes are usually much cheaper. Usually, that's why we save transformed data in the data lake – mostly because of that."
1970	Is one-hot encoding recommended instead of dummy-encoding?	I wish Tim was here and would help me answer that because I don't know. I assume if you wanted to use Bento, you would probably use TensorFlow Lite – you don't want to use the usual TensorFlow for the same reasons that we talked about in the serverless module, because it's simply too large. Probably for Bento, you would use something like TensorFlow Lite and it will still work. Maybe another thing you can check – in our channel, we had a talk from one of the Bento folks, Building an ML Service Platform from the Ground Up. Here, Sean talks about the differences between TensorFlow Serving and Bento. With TensorFlow Serving, it's good – it's quite optimized for deep learning. There are some problems. First of all, it's written in C++ and expects your model in a certain format – this “saved model format”. Adding anything extra on top of that (like pre-processing, post-processing) becomes difficult, because you will need to have an extra step there. Bento makes it easier. But, again, I don't know. It's very difficult to give you a concrete recommendation without knowing what exactly you want to do. So it's case dependent. If you already use TensorFlow Serving in your company, then maybe it makes sense to stick to that. If we have Tim on Office Hours next week, maybe you can ask him that. He will be a better person to answer this question.
1971	Is one-hot encoding recommended instead of dummy-encoding?	I think for binary features, just one and zero makes more sense than OneHotEncoding. OneHotEncoding will take care of that but you will have two columns that are the exact opposite of each other. So that is a bit redundant. I think it's better just to convert it to a Boolean column with ones and zeros.
1972	Is one-hot encoding recommended instead of dummy-encoding?	"Yes. Well, dummy-encoding is… what's the difference – I don't remember. We don't include one of the columns, right? Initially, when I said that hot-code encoding is recommended instead of diamond coding, I thought you were asking if we should use one-hot encoding from SciKit Learn versus one-hot encoding from Pandas. In Pandas, we have this get_dummies method, which works like one-hot encoding, but the problem with this method is that it will create different… Let's say if you're on your training data you have this variable “make” with only 10 different values, but in validation you have 11 – you will have a different size of your matrix. You will see this covered this week, when we cover one-hot encoding. 
Actually, using one-hot encoding from SciKit Learn is recommended. The way we implement this is through dictionary vectorizer (DictVectorizer) and this is something that makes it easier to deploy models later. This week, you will see how we implemented one-hot encoding and in week five, you will see how we can use this dictionary vectorizer to turn a request that is coming to our web service, and then turn it into a matrix, and then predict whatever we're predicting."
1973	Is one-hot encoding recommended instead of dummy-encoding?	"Tim 
Not really. We have users who use both Streamlit and Gradio. I think Streamlit and Gradio are a bit more about the presentation side. We do have a couple of users who have asked us about deeper Streamlit or Gradio support and I think that's something that's probably coming up in the next few months.
Alexey
For those who don't know what Streamlit or Gradio is – this is a way to create an interface for your services. Well, Gradio is focused on machine learning, meaning you create interfaces specifically for models. And then Streamlit is just a Python package for creating interfaces. We actually had a project at DataTalks.Club about Streamlit. I actually even have a tweet about this. This is what Streamlit looks like. In the video, you can see that you have this interface, and then based on the toggles that you move the core of your model changes. It gives you an easy way to build an interface like that. Meanwhile Bento focuses on quite a different use case, right? It’s not building interfaces, but serving models.
Tim 
Right, yeah. I think one of our users was asking for a really easy way, so that once you have your model and your Streamlit UI to be able to create a Bento deployment with one line in that way, anybody that you gave your Streamlit app to would be calling to that service and it could work anywhere. Which is a nice feature, I think.
Alexey 
Yeah, I think it is. I imagine if you have a Pydentic class there, then you can easily generate a Streamlit app for that.
Tim 
Right. Oh, that's a good idea. I hadn't thought of that. Yeah.
Alexey
And then in addition to that, the Swagger UI that you have, could be like having the actual interface. 
Tim
Oh, yeah, that would be cool. [chuckles]
Alexey
That could be a good Hackathon project, right?
Tim 
Yeah, for sure.
Alexey
From what I see – let's say you’ve built a model. Then you deployed this model and you have this nice API. You can query it with CURL. But what if you want to demo this project to your manager, or a manager of your manager, or somebody who does not necessarily have a technical background? If you give them the URL and say, “Okay, you need to execute the CURL command with a post request,” they would be like, “CURL what? What do you want from me? Just show me how it looks.” This is when tools like Streamlit come in handy. Instead of giving them a command line interface to query it, you just give them “Okay, this is the link. Play with this.” If Bento could do something like that – automatically generate things like that – many data scientists would thank you.
Tim
Yeah, I was just looking because I figured there's a library that turns Pydantic models into Streamlit apps. It looks like there is one that does that. I wonder if we could just plug into that app and then have that same thing. 
Alexey
That's cool. 
Tim
Yeah, that's awesome. That's kind of the cool part of open source – there are so many people out there with so many different projects and we get to collaborate with them and kind of build the best collaborations and combinations of these tools."
1974	Is one-hot encoding recommended instead of dummy-encoding?	"Alexey
CI stands for continuous integration. This is a way to – let's say you wrote some code, and then you push it to GitHub. Then on GitHub, there's GitHub actions, which is a way to automatically run some checks on your code – run tests, deploy things somewhere. Actually, again, I will do a shameless plug – in our MLOps Zoomcamp, we talked about GitHub actions in the best practices module. Right now, don't worry about this. Focus on your projects. But after you do your projects, it's really worth spending some time learning about best practices. These best practices include things like writing tests, using make files and also tools like CI/CD, infrastructure as code – we cover all that in the module. But if you are interested in these things, maybe it's actually worth taking that course too. We'll have another iteration in May. Not super soon – I’m just telling you that to get you excited."
1975	What function or library can I use to separate a set of images and training validation and test?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1976	What function or library can I use to separate a set of images and training validation and test?	I guess not. That's the process of learning? Maybe more senior people don't… Yeah, I see where you're going with this. Senior people are not supposed to go bankrupt and default. They are probably supposed to pay back the debt. I guess these are the particularities of this dataset, I assume. Indeed, it's probably a data issue ,or not an issue, but a “characteristic” of this dataset.
1977	What function or library can I use to separate a set of images and training validation and test?	"I don't know how to answer that, because I work as a principal data scientist. I work on pretty much everything that the company needs me to work with. This means that all the models are related, to some extent, to online classifieds – to online marketplaces. I need to deal more with domain expertise, maybe, rather than ML knowledge. I guess that's the answer. 
We have a lot of different use cases – we have recommender systems, we have search, we have moderation. We actually have an article about this. For example, Learning-To-Rank: Sorting Search Results. My colleague wrote this one. I think we have an article about data science, Data Science at OLX which happens to be written by me. You can see what kind of use cases we solve. Maybe it could be interesting for you."
1978	What function or library can I use to separate a set of images and training validation and test?	You're talking about images, so the way I would do this – maybe there is a library for that. I would use a thing called glob. I would use this glob function from Python to get all the files in the directory with images. Then you have a list of files and then you can use something like train/test/split from SciKit Learn to split it into three groups. Each of these groups would be train, validation, and test sets. This is how I would do this. It's not a library, so you will still need to use like four lines of code or something like that. Use glob, and then train/test/split from SciKit Learn and then you'll be able to do this. Of course, that might not be enough. You might also need to write a bunch of extra code for putting this into directories. For example, in this clothing dataset, I already put this dataset into three folders – we have train, and then we have classes here. So you might also need to write some code for distributing all your files into different subfolders. But this is also not a lot of code, like 10 lines maybe 12 maybe 20, give or take. I don't know – but it's not too awful. Maybe some of you will figure this out – you can post it in Slack, so then others can also use it.
1979	What function or library can I use to separate a set of images and training validation and test?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
1980	How do you do data cleaning of a survey if all the columns are categorical and most of them have values like ones and zeros (i.e. True and False)?	"You can check out a few projects in this playlist. In the playlist, you can see some of the videos here, for example, there's a project from Alvaro. And it's not just Alvaro, there’s one from Alvaro and Ninad. You can just check out these projects – there’s a project from Carolina and Hamad, a project from Lisa, and a project from Timur. The project from Timor may actually fit what you want. Timur has done a lot of work and it's very interesting. So check it out. Actually, Timur’s project is not a midterm project. It's more like a capstone project – there is some deep learning there, he uses Karis. So maybe focus on the other three. 
You can also go to our course web page, and then go to midterm project (you need to use the 2021 folder) and then in the readme there should be some assignments. There will be some form that says “To find the projects you need to review, use this table.” Here, you can see all the projects from previous students’ midterm projects. You can pick some of them and check if you like them or not."
1981	How do you do data cleaning of a survey if all the columns are categorical and most of them have values like ones and zeros (i.e. True and False)?	"It's basically ready. You don't need to do any data cleaning if it's true or false. You just take it as is and put them inside a model. I think. Of course, it depends on things like if there is missing data. Then maybe you need to do something with this missing data. 
But from what I see here – if it's just ones and zeros – it’s good to go. You can just take this and use it as is. It's actually quite a simple situation from what it seems. Maybe I don't see some complexities there. There could be some, but on the surface it looks rather straightforward."
1982	How do you do data cleaning of a survey if all the columns are categorical and most of them have values like ones and zeros (i.e. True and False)?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1983	How do you do data cleaning of a survey if all the columns are categorical and most of them have values like ones and zeros (i.e. True and False)?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
1984	How do you do data cleaning of a survey if all the columns are categorical and most of them have values like ones and zeros (i.e. True and False)?	I guess not. That's the process of learning? Maybe more senior people don't… Yeah, I see where you're going with this. Senior people are not supposed to go bankrupt and default. They are probably supposed to pay back the debt. I guess these are the particularities of this dataset, I assume. Indeed, it's probably a data issue ,or not an issue, but a “characteristic” of this dataset.
1985	In the real world, would we create a container for our Terraform setup?	No, you cannot.
1986	In the real world, would we create a container for our Terraform setup?	I would say to focus on projects, not on tools. In this course, you will get a chance to do this. If you open a job description for data engineers, you will see a lot, a lot, a lot of different technologies. Maybe think of a project that you can do for this particular company and do this project using, for example, the tools you learn here or the tools you want to learn, and build a portfolio. I guess this is more of a tip for getting a data engineering job, rather than for somebody who's starting their data engineering career. But if you already have a job, then maybe think of the business impact of what you need. I think I will not spend too much time talking about that because this requires a one hour long discussion. Perhaps we should have a podcast interview about that. If you can think of any potential guests for this podcast interview, you can let me know and we can try to invite these people and talk about this in more detail. One thing I want to share with you is that in our YouTube channel, we had a few interesting talks. We had this Getting a Data Engineering Job with Jeff talk. It’s a very nice presentation. Then we also had an interview with Jeff. So these two podcasts are quite interesting. You can also check out other ones. I think you'll find a lot of interesting content here.
1987	In the real world, would we create a container for our Terraform setup?	Terraform is not for creating containers. It's for creating infrastructure – different services in the cloud. One of these things could be creating a Kubernetes cluster, for example, creating… I don't remember how these services are called in Google Cloud Platform, but there are some other container orchestrators. Then you deploy your Docker containers there.
1988	In the real world, would we create a container for our Terraform setup?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
1989	In the real world, would we create a container for our Terraform setup?	"Alexey
Yeah, we talked about that in Slack, so please check it. I think this happens because some of the columns have missing values and when you use Pandas for converting CSV to Parquet, it reads columns with missing values as double. That's why this happens."
1990	How to improve the accuracy of the prediction? How to address the issue if model accuracy is drastically different between test set and validation set?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
1991	How to improve the accuracy of the prediction? How to address the issue if model accuracy is drastically different between test set and validation set?	Yeah, it's probably not the best practice. I wouldn't do this in a real-life situation. In a real-life situation, we simply will not have unseen data. But the idea here was to get a dataset that is already familiar to you – the dataset we used in the previous homework – and just convert it to a binary problem. That was the goal we had in mind – how can we make it easier for you to learn these things without overloading you with a new dataset? That was the simplest approach we decided to take to actually do it here. Another alternative could have been, instead of asking you to compute the mean, just give you the number, but with the mean, it's a little bit more interesting, I think.
1992	How to improve the accuracy of the prediction? How to address the issue if model accuracy is drastically different between test set and validation set?	"If you have very good accuracy on the validation set, but you have very bad accuracy on the test set, you probably overfit. Maybe you should have a model that is less powerful. Then you should also check how exactly you split the data. Is there a difference in distribution between test and validation sets? Maybe this is the even more important thing to check. Does your data look different in the validation and test sets? If your data looks different in the test set from the validation set, then maybe you need to try to design your validation framework in such a way that it's similar to your test dataset. 
You should also ask yourself, “Is my test dataset really representative of unseen data or not?” It's probably that somewhere the model overfit, it may be using some data that is not present in the test dataset or maybe the model just got lucky. So then, based on what you think the issue is, you try to fix that. Sometimes it may even mean that you do your split differently – in such a way that validation, train, and test datasets may have the same distribution (depending on your problem)."
1993	How to improve the accuracy of the prediction? How to address the issue if model accuracy is drastically different between test set and validation set?	"There was no registration deadline at all for this course. So you can still register. You can still sign up and you can follow the course. You will not be able to submit the homework assignments where the due date is already over because there are already solutions posted, so it doesn't make much sense to submit homework when the solutions are already there. You can follow the course but you will not be able to submit the homework. You can check our frequently asked questions and I do recommend doing this. Please remember the rule – if you have a question, first go check the frequently asked questions, and if you don't find the question there, go and ask in Slack. 
For this particular question, it’s already there. “Don't worry, you can take the course.” Maybe right now it's becoming a little bit more challenging to catch up with everything, but note that you can skip the midterm project. Right now, if you want to catch up with everything, if you skip the midterm project, it's fine. You will be able to do two capstone projects and still get the certificate. Also, you can just take the course in self-paced mode. There is nothing wrong with that either."
1994	How to improve the accuracy of the prediction? How to address the issue if model accuracy is drastically different between test set and validation set?	It is possible to do everything locally. You will need to do it by… I think you will need to do it in module five when we deploy something. Or actually, you will probably need to do it when we do your project. Yeah. It's fine to do everything locally. But if you have access to cloud and you want to practice with this, I do recommend practicing, because cloud is one of the skills that employers are looking for. They really need it. So if you want to be more employable, I do recommend learning about cloud.
1995	How can you help people that can't use GCP because of their location?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
1996	How can you help people that can't use GCP because of their location?	For everything we do in this course, it's possible to run things locally. In some cases, when you need to create a Google Cloud Storage bucket or when you need to create a BigQuery table, you cannot do this locally. But for many things, you can. You can set up your Postgres instance, and we show you how to do this. We showed this in the first week’s video. In the Prefect videos, we also show how to do things locally. Then, you will basically need to skip week 3 because you will not be able to use BigQuery. And then week 4, we show how to use DBT with Postgres. Then with Spark, we show how to run things locally. And for week 6, with Confluent, I think you can still get access to Confluent Cloud, but you can also run things locally – run Kafka locally.
1997	How can you help people that can't use GCP because of their location?	"Michael
Actually, I do have a resource – Real Python. They do have some paid content, but they also have a lot of free content that I look at frequently. Also, if you go to sites like LeetCode, or do the Advent of Code – not necessarily to solve the problems, but looking at others’ solutions – you can see some very good examples of clean code. Type Hinting is something that I’ve picked up a few years back from Advent of Code. I still don't do it all the time, but when you see more experienced people write code, you kind of know what you're aiming for. And I think that gives you a lot of direction.
Alexey
I think in this video, Getting a Data Engineering Job, Jeff also talks about learning good coding practices. I remember that the project he recommended to look at, if you want to learn more about good coding, was Prefect. He said, “Yeah, go check out Prefect. It's a great way to learn how to write good Python code.”I guess it's a good plug, right? [chuckles] 
Then there is also a book called Clean Code. It's about Java. It's a very nice book. It's actually not about Java, it's about clean code, but the examples there are in Java. For me, it was very useful when I was starting coding. But I was a Java developer. What I know is there are examples in this book in Python. Even though the book is about Java, there are GitHub repos where the same concepts are illustrated with Python. You can check that out, too.
Jeff
Yeah, I haven't read this book. But there's also a Clean Code in Python book that is out there. It has good reviews on Amazon. It could be worth checking out."
1998	How can you help people that can't use GCP because of their location?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
1999	How can you help people that can't use GCP because of their location?	Our thoughts are that this is indeed an exciting area. This is an in-demand skill and that's why we have a section about analytics engineering in this course.
2000	What topic would you consider fundamental and relevant for time series analysis and forecasting in real time? Would you teach this or can you give suggestions?	"We will not cover it in this course. There is a good book about this called Time Series Forecasting Principles and Practice. It's a bit oriented on R, but I think you can find some code for Python as well in this book. If you want to learn more about time series and forecasting, this is the book you should check out. Usually things like exponential smoothing are efficient for most cases. We also recently had a webinar about this called Building the Modern Geospatial Data Stack with Ramiro Aznar. 
We actually had multiple – we had this one, called Probabilistic Demand Forecasting at Scale, which is quite a nice talk. And this one, called Feature Engineering for Time Series Forecasting. These two webinars are quite comprehensive, so check them out. But I would start with the book that I showed you. I think in this question “fundamental” means things like exponential smoothing. I think this is quite an easy approach and it works quite well."
2001	What topic would you consider fundamental and relevant for time series analysis and forecasting in real time? Would you teach this or can you give suggestions?	"I am glad you asked. Project of the Week has nothing to do with the course to the extent that it's just different initiatives that are run in our community. We have a course and we also have Project of the Week. We also have webinars, we also have podcasts. They are all different activities. For the Project of the Week, this week, we'll have a project about recommender systems. The idea there is that every day, you get a set of tasks. On Wednesday, you get this set of tasks. You need to come up with an idea. You need to find a dataset for this idea, and you need to share your progress. Then on day two, you will need to go through the suggested materials and also find some materials on your own and do this thing – learn about the basics, do exploratory data analysis, understand this data that you found and share your progress. As you can see on the project page, every day, you get a set of tasks and you need to do them. So it's not like we're telling you exactly what to do like in the Zoomcamp (in the videos we tell you that “This is the exact sequence of actions you need to execute and this is the result you get.”) With the Project of the Week, it's more independent. We just give you four bullet points and then it's up to you to actually do these things. And then, of course, you share your progress. The idea behind this project is that, at the end, you have a project that you can add to your portfolio. This is not a project that you just took from the tutorial and followed along, but this is a project that you did with some guidance. These are two very different things. That's the idea behind Project of the Week. For the project that we'll have as a part of the course, the idea is somewhat similar, except you will get less guidance – you will have more independence, let's say. The guidance we have for the project in the course is the form of a bunch of criteria that you need to satisfy. Then it's up to you to come up with a plan. But here, for Project the Week, we have a suggested plan that you can follow and hopefully, at the end, come up with a result. Keep in mind that for Project of the Week, some things might not go according to plan and that’s fine. This is just a suggested plan. We don't know if it will actually work exactly like we outlined day by day. It's more just to give you some guidance, but it’s up to you how exactly to approach that. 
There is a comment that says “I did not know about this.” Yes, we have a channel in our slack. It's called #project-of-the-week and it starts this week Wednesday. You'll notice that there's the digit one in the name “2022-10-19-recommenders-1.md”. We'll also have a follow-up project about recommender systems. If you want to take part in this (by “take part” I mean, propose your own topic) for example, some of you were asking about time series, some of you were asking about NLP, some of you were asking about other things in the comments in live chat. Daniel mentioned audio – If you want to learn any of these libraries, we can do a Project of the Week to learn these libraries or methods or approaches or whatever."
2002	What topic would you consider fundamental and relevant for time series analysis and forecasting in real time? Would you teach this or can you give suggestions?	I don't know what inclined here means. But I think I already gave a recommendation – I think these two machine learning by Andrew Ng and this other one are quite orthogonal. They focus on different things.
2003	What topic would you consider fundamental and relevant for time series analysis and forecasting in real time? Would you teach this or can you give suggestions?	"To be honest, I don't know what the typical requirements for computer vision jobs are. They can vary from one company to another. I will give you a general piece of advice that I would give in any other case. You need to do some research about what the company is doing and understand what their problems are. What kinds of problems are they solving? Then try to solve a similar problem. For example, if it's a company like where I work – online classifieds – you can go to Kaggle and look at what kind of computer vision problems companies in the online classifieds domain are putting out there. 
Or just go through the tech blog and see what kind of articles that this company is publishing. Then try to build the project around that. For example, let's take OLX, the company where I work. You can find an article here, Fighting fraud with Triplet Loss. In this article, we show how we use computer vision to find duplicates – how to find image duplicates – and then this article describes that. So if you do something similar in your project, you can just add it to your CV and then talk about this during your interview. That's much better than just a random project. It will certainly help to pique attention. I guess that's the most important factor, try to do some research, find what kind of problems the company is interested in solving and then solve these problems. Then use it to build your portfolio."
2004	What topic would you consider fundamental and relevant for time series analysis and forecasting in real time? Would you teach this or can you give suggestions?	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
2005	Are you going to update videos for week 3? Currently, they are using Airflow.	"Alexey
Please check the FAQ. If you don't find the answer there, ask in Slack. If you get an answer in Slack, please put it into the FAQ."
2006	Are you going to update videos for week 3? Currently, they are using Airflow.	I already removed one of these videos from the playlist and I removed it from the GitHub repo. Right now, all these materials – everything you need orchestration – is in week 2, including the part when you need to do some BigQuery stuff. This was the part in week 3 that was a part about using Airflow and BigQuery. Now it’s in week 2. Everything is already there and you should not see any Airflow stuff. We might have accidentally missed some of this, so if you see some Airflow stuff somewhere, please let us know.
2007	Are you going to update videos for week 3? Currently, they are using Airflow.	No, you cannot.
2008	Are you going to update videos for week 3? Currently, they are using Airflow.	"Alexey
No. I mean, we kind of told you. Yes, you maybe didn't see the intro video. But, sorry. No, it's late. We will actually recalculate the points for the second homework (HW 1B) because in the forum, it said that you can get up to one point, but we calculated that the cap there was seven. We are going to recalculate that. Thus, people who got a lot of points will get just one. Sorry about that. But don't focus on points. If you go to the leaderboard, we know that the person with this hash (722366d8b29ece9be3a7605363562c7c60d6918e) did seven links. Do you know who that is? Probably you don't. 
These points are virtual. It's nice to have a good score, but remember that these points mean whatever you want them to mean – not more than not less. Maybe it's okay if you get a few points less than others for the first homework, but the important thing here is consistency. If you just do your homework, even without doing any Learning in Public stuff, which I think you should do (you should post about the course). If you don't do this and you just do all the homework, by the end, you will be somewhere in the top 20. If you look at the leaderboard from the previous iteration, you will see that not everyone stuck around till the end. 
If you just do your homework, don't worry about these points, and you'll be fine. Sorry about that. I know that it can be discouraging that you didn't get as many points as others, but don't worry about that. Focus on learning. Maybe Michael has some other suggestions or some other things that I didn't mention, because he took part last year. But I think he was one of the people who did a few posts every week, right?
Michael
Yes. Actually, when I went through it, I missed the whole… you can get up to seven points, but I was just doing one every week and I just stuck with that. I was still near the top. I think the real value in that isn't necessarily the points but just being comfortable sharing – writing articles and making videos. That's the real value there. But everyone likes the clout value, too. As Alexey said, it will fluctuate, so if you keep up with it going forward, you'll probably be just fine."
2009	Are you going to update videos for week 3? Currently, they are using Airflow.	"Alexey
Well, I can suggest a project for this course. At the end, after you complete this course, after you complete all the modules, as a part of this course, you will have to work on the project. This is how you will get a certificate at the end – if you pass the project. This is a good project to include in your portfolio. 
Michael
I don't know if I have much more to add other than being active in online communities. This course is a great place to start. There are a lot of large cities that have meetup groups, which are always great. One of the good side effects of the whole COVID pandemic is that a lot of those are remote now. If you Google “San Francisco Big Data meetup,” meet people, talk with them, get their insights. I think that'd be a good way to approach that.
Jeff
Plus one just for getting involved in communities, whether it's conferences, online or in-person is great. Volunteer, if you can, at those. That's often a great way to help out and work with people. Go to meetups. I run a meetup in DC on data science. If anyone's in the DC area, there are data engineering meetups in town too. If there isn’t one somewhere, and if there's a few people around, I encourage you to start one. That's something you can do, regardless of your level of experience.
30:06  
Alexey
Then I'm going to do another shameless plug. We have a podcast and the topic of career changing comes up pretty often there. One interesting one is from Juan Pablo. Here he's actually not talking about… well, it's kind of related to analytics engineering. He was a math teacher, which doesn't really qualify as an IT person. He was also working as an Uber driver. In the podcast, he tells his story of how he actually did this. He talks exactly about that. He talks about going to meetups. Instead of not talking about this, just go and check it out. That's not the only relevant podcast episode. You can just go through everything we have and see if anything catches your interest. I'm sure you'll find a lot of interesting stuff."
2010	Is there any penalty for joining late?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2011	Is there any penalty for joining late?	Yeah, it's probably not the best practice. I wouldn't do this in a real-life situation. In a real-life situation, we simply will not have unseen data. But the idea here was to get a dataset that is already familiar to you – the dataset we used in the previous homework – and just convert it to a binary problem. That was the goal we had in mind – how can we make it easier for you to learn these things without overloading you with a new dataset? That was the simplest approach we decided to take to actually do it here. Another alternative could have been, instead of asking you to compute the mean, just give you the number, but with the mean, it's a little bit more interesting, I think.
2012	Is there any penalty for joining late?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
2013	Is there any penalty for joining late?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
2014	Is there any penalty for joining late?	There is no penalty for joining late. You can join at any point of time. You can actually take the course at your own pace. Nobody will penalize you for that. So take your time. Again, I will refer to this list of frequently asked questions. Please check it out.
2015	Can I get the certificate if I didn't send the midterm project?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2016	Can I get the certificate if I didn't send the midterm project?	"Actually, I don't remember exactly how I do this. The right approach would be to use only train data and not use validation data or test dataset. This is done on purpose for exactly this reason that you mentioned. We want to model a situation in which there is some unseen data – it's truly unseen, we do not see that all – and anything can happen to this unseen data, including the situation that you described, such as there is a value that is only present in the test data. 
Imagine that there is a new iPhone – when you train your model, this new iPhone didn't exist (you didn't have this category) but you apply this model, and then, all of a sudden, there is a new iPhone. How will your model react to this? You need to somehow model this situation. This is totally normal. This can and will happen in practice. We just want to understand what would be the effect of that. That's why we need to see this in training data. This is good and this is how it should be. I hope that is a good answer."
2017	Can I get the certificate if I didn't send the midterm project?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
2018	Can I get the certificate if I didn't send the midterm project?	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
2019	Can I get the certificate if I didn't send the midterm project?	"You can. Remember that we have three projects and you need to complete two of them to get a certificate. This means that if you miss this one – if you didn't finish this one – you have to do Capstone 1 and Capstone 2 projects. If you pass them, you will get a certificate. Also, let's say you just joined the course now – you can still catch up.
 You can watch the videos and by the time we have the Capstone 1 project, you can already start doing it. Then we will have a bit of a break (for Christmas, New Year, and so on) and if you don't have any plans for that, you can keep on watching lectures, and then also work on your second project. Then you'll be fine."
2020	How to drive imbalanced classes, maybe some pattern or process with these classes?	"I am glad you asked. Project of the Week has nothing to do with the course to the extent that it's just different initiatives that are run in our community. We have a course and we also have Project of the Week. We also have webinars, we also have podcasts. They are all different activities. For the Project of the Week, this week, we'll have a project about recommender systems. The idea there is that every day, you get a set of tasks. On Wednesday, you get this set of tasks. You need to come up with an idea. You need to find a dataset for this idea, and you need to share your progress. Then on day two, you will need to go through the suggested materials and also find some materials on your own and do this thing – learn about the basics, do exploratory data analysis, understand this data that you found and share your progress. As you can see on the project page, every day, you get a set of tasks and you need to do them. So it's not like we're telling you exactly what to do like in the Zoomcamp (in the videos we tell you that “This is the exact sequence of actions you need to execute and this is the result you get.”) With the Project of the Week, it's more independent. We just give you four bullet points and then it's up to you to actually do these things. And then, of course, you share your progress. The idea behind this project is that, at the end, you have a project that you can add to your portfolio. This is not a project that you just took from the tutorial and followed along, but this is a project that you did with some guidance. These are two very different things. That's the idea behind Project of the Week. For the project that we'll have as a part of the course, the idea is somewhat similar, except you will get less guidance – you will have more independence, let's say. The guidance we have for the project in the course is the form of a bunch of criteria that you need to satisfy. Then it's up to you to come up with a plan. But here, for Project the Week, we have a suggested plan that you can follow and hopefully, at the end, come up with a result. Keep in mind that for Project of the Week, some things might not go according to plan and that’s fine. This is just a suggested plan. We don't know if it will actually work exactly like we outlined day by day. It's more just to give you some guidance, but it’s up to you how exactly to approach that. 
There is a comment that says “I did not know about this.” Yes, we have a channel in our slack. It's called #project-of-the-week and it starts this week Wednesday. You'll notice that there's the digit one in the name “2022-10-19-recommenders-1.md”. We'll also have a follow-up project about recommender systems. If you want to take part in this (by “take part” I mean, propose your own topic) for example, some of you were asking about time series, some of you were asking about NLP, some of you were asking about other things in the comments in live chat. Daniel mentioned audio – If you want to learn any of these libraries, we can do a Project of the Week to learn these libraries or methods or approaches or whatever."
2021	How to drive imbalanced classes, maybe some pattern or process with these classes?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
2022	How to drive imbalanced classes, maybe some pattern or process with these classes?	"Tim
Yeah, we have a lot of users who use MLflow and BentoML together. Once you have a model that’s sort of your “finalized” model – you save it to your MLflow registry and then (it depends on what your CI/CD pipeline looks like) but BentoML has a command to import from an MLflow registry. You can tell which MLflow model is the one that you want to deploy. It's very similar to save_model, except instead of pulling from your local environment, you're pulling from an MLflow registry. So you import the model, it automatically gets pulled into your Bento and then you can deploy it. On the BentoML documentation site, there's a big framework document on how to integrate with MLflow. 
Alexey
When searching, one of the suggestions was BentoML vs MLflow. I guess there are also some similarities because with MLflow, you can serve models. 
Tim
The thing about serving models is that it's a part of the pipeline that you just have to do. Every single framework out there has some way to serve a model – you train the model and then it just puts it up there to be able to serve. Typically, frameworks don't specialize in serving those. BentoML specializes in making the serving part really, really fast and really, really easy. What you find a lot of the time is, when you have serving logic in a really large end-to-end pipeline, it just puts the model there and then the only thing that you could submit to the model is the data. But we know in practice that there's usually business logic around this – pre-transformation logic, post-transformation logic, there's the version of the model that was saved. There are a lot of these other components that you want around the model, not just the model for inference. And of course there's the performance layer underneath. The BentoML architecture kind of brings that all together into one deployable, rather than just giving you one point to call inference for your model. 
Alexey
That is a comment in the live chat that says “There is PyCaret, too.” This is how you use PyCaret with Bento. 
Tim
Right. BentoML has lots and lots of integrations and we're only adding to them every day. The idea is to get the model from wherever you built it, bring it into your Bento, and then you can deploy it anywhere. Once it's in the Bento – in this standard packaging – it's nice because then we've got lots of different tools to deploy to Lambda, SageMaker, and lots of different places. And the places to deploy, we're adding to every week and month as well.
Alexey  
Since you can have a Docker container at the end – you do “bento_containerize” right? That's the command? And then you have a Docker container (Docker image) that you can deploy everywhere where you can deploy a Docker container, which is pretty much every place on the internet, right? 
Tim
Right. Some places, though, require specialized… 
Alexey
Like Lambda, right? 
Tim
Yeah, like Lambda won't accept just a normal Docker container. You have to actually create special endpoints. When we deploy to Lambda, we create those endpoints for you and make sure that it's behind an API gateway and things like that. For certain services, there are a lot of nuances that we kind of take care of for you – as long as you're in the Bento standard format.
Alexey 
For those who have no idea what MLflow is, and why we are talking about this – you don't need to worry about this right now. But at some point, this tool and other similar tools will be quite handy. You can check out our MLOps Zoomcamp course, after you finish this one. Don't try to do multiple courses at the same time. That could be too much. But of course, if you're curious, go check it out."
2023	How to drive imbalanced classes, maybe some pattern or process with these classes?	"I'm trying to understand the question… So the question is “What to do if you have class imbalance, but instead of two classes, you have three classes?” For that, when you define your train_test_split in SciKit Learn they have a thing called stratify. You can put your target variable here and then what this function will do is use these data points (this target) to make sure that the distribution between train, test, and validation of this variable is the same. I'm not sure if what I say makes any sense to you, but maybe I should give more context. 
Your test, train, and validation should have the same amount of the same distribution for all – for example, if we are talking about binary classification, the amount of zeros and ones should be the same in both train, validation and test. And when we have class imbalance, then sometimes it's difficult to make sure that this happens. Because in a usual case, when we don't use stratification, what can happen is that we have more, let's say, positive examples in the train dataset than in validation – just by chance, because the positive examples are quite rare and stratification makes sure that it doesn't happen. Apart from that, you don't need to do anything else, to my knowledge. Maybe I misunderstood your question, so maybe ask a follow-up question in Slack."
2024	How to drive imbalanced classes, maybe some pattern or process with these classes?	"Actually, I don't remember exactly how I do this. The right approach would be to use only train data and not use validation data or test dataset. This is done on purpose for exactly this reason that you mentioned. We want to model a situation in which there is some unseen data – it's truly unseen, we do not see that all – and anything can happen to this unseen data, including the situation that you described, such as there is a value that is only present in the test data. 
Imagine that there is a new iPhone – when you train your model, this new iPhone didn't exist (you didn't have this category) but you apply this model, and then, all of a sudden, there is a new iPhone. How will your model react to this? You need to somehow model this situation. This is totally normal. This can and will happen in practice. We just want to understand what would be the effect of that. That's why we need to see this in training data. This is good and this is how it should be. I hope that is a good answer."
2025	If you were interviewing a candidate who has done this course, what would be some of the important questions that you want him/her to give a good understanding of?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
2026	If you were interviewing a candidate who has done this course, what would be some of the important questions that you want him/her to give a good understanding of?	"You will get out of tutorial hell when you start doing the project in this course, because it will be just a set of guidelines – this matrix – but you will have to do the rest yourself. You will have to find the problem yourself, you will have to do exploratory data analysis yourself, you will have to do data preparation yourself, you will have to do everything yourself – there will be no homework with the exact steps that you need to do to have a project. And this is how you get out of tutorial hell. 
Let's not think about this particular course, but in any setting – you want to find the problem that you want to solve. Once you have the problem, then try to think “What is the best way to solve this? What is the shortest way to solve it?” Or, at least, “What is the next step I need to take to solve it?” And then try to work your way through solving this problem. So focus on the problem. If you just do tutorials, you're not solving problems – you're just doing tutorials. So focus on the problem."
2027	If you were interviewing a candidate who has done this course, what would be some of the important questions that you want him/her to give a good understanding of?	"I structure the interviews that I conduct around projects. Typically, a technical interview that I conduct is usually one hour long. The first five minutes is just a bit of introduction, where I say a few words about myself and then you also say a few words about yourself. 
Then, in the next 25 minutes, we talk about your past project – your past experience. Here, I would ask you to tell me a few words about yourself, select a project, and then ask you to talk me through this project. How did it start? Who was working on this project? What is the main problem that this project was meant to solve? What is the main goal? 
Then we would go into technical details – which kind of model you selected, why, and so on. Here, I wouldn't ask specifically theoretical questions just out of nowhere. All these questions would be based on your experience – on everything you say. Basically, if you said that, “For this project, I used XGBoost,” then I would ask “Why XGBoost? Why not something else? Was it better? What kind of other models did you try? Why this particular choice? How exactly did you evaluate your model? This is when all the topics from this course would start appearing? “What is the difference between XGBoost and Random Forest? Why did you decide to settle on this particular model? Then, based on the materials of this course, you can answer these questions. 
Also, I spend a bit of time asking about your deployment experience, which is also something we cover in this course. This would be the first 25 minutes. The next 20 minutes is about live coding. I ask a problem and then we use Python to solve this problem. And then the last 10 minutes is the time for the candidate to ask questions. 
Maybe I’ll just add a bit of a shameless plug. If you go to my GitHub profile, and then look at the pinned repositories – the first one is data science interviews – you will find a lot of interview questions. For many of these questions, you will find answers in this course, but also in this resource. For example, here’s “What is gradient boosting trees?” and so on. This is quite a useful thing. Also, I was talking about coding for the second half of the interviews I conduct – I might ask some things from here. I typically don't ask SQL. I know many companies do ask this. But I would ask something from this part for the coding part of the interview.
In the course, we focus a lot on projects. There is a lot of emphasis on doing projects in this course. Perhaps this is what you should pay attention to. Then, when you have interviews after finishing this course, you will probably also talk about the projects you did. This is when all these things might start coming up. Sometimes the questions are trivia questions. By “trivia” I mean that somebody just comes across this list of questions (on GitHub), and they go “Let me flip a coin and ask this question.” Then they just ask this question, “How do we check if a variable follows a normal distribution?” And then you need to answer that. 
Maybe for this particular question, by the way, you will not find an answer in the course. Or at least partially – maybe we talk a bit about bell-shaped curves. Then again, it's not the end of the world if you don't know the answer to some of the trivia questions. I think in the end what matters is your projects, your portfolio, and not if you know some encyclopedic material or not. So focus on projects."
2028	If you were interviewing a candidate who has done this course, what would be some of the important questions that you want him/her to give a good understanding of?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2029	If you were interviewing a candidate who has done this course, what would be some of the important questions that you want him/her to give a good understanding of?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
2030	I want to work in ML engineering – ideally, freelance. Do you think picking recommender systems as a topic to work on in my portfolio is a good strategy?	Yeah, I think it is. I don't see a reason why it's not.
2031	I want to work in ML engineering – ideally, freelance. Do you think picking recommender systems as a topic to work on in my portfolio is a good strategy?	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
2032	I want to work in ML engineering – ideally, freelance. Do you think picking recommender systems as a topic to work on in my portfolio is a good strategy?	"I wouldn't call it an issue, actually. It's more like a feature than a bug of RMSE. Let's say you have a model that predicts price and you want to know, on average, how wrong your model is in dollars. That's the purpose of RMSE. Or if you're predicting the age of somebody, then you want to know, on average, how wrong your model is in years. It gives us this understanding of how wrong the model is, on average, on the same scale as our target variable, while R-Squared and MAPE do not give us that. They serve different purposes. 
I never actually used R-Squared in practice. For me, it's always misleading. I cannot really interpret it quite well. But MAPE is a good one – it has its own problems and I think we talked about that in the last Office Hours. It's good if you look at multiple metrics at the same time – you look at RMSE, you look at MAPE, you perhaps look at mean absolute error (MAE) and then, based on that, you make some decisions whether to use this model or not."
2033	I want to work in ML engineering – ideally, freelance. Do you think picking recommender systems as a topic to work on in my portfolio is a good strategy?	"It all depends on what you mean by “full stack ML engineer”. As you’ve noticed, data engineer Zoomcamp is not about machine learning – it's about data engineering. If in your opinion a “full stack ML engineer” needs to know data engineering, then you should do data engineering. If not, then no. From my point of view, there is a “full stack of data scientist”. What I put in the definition of a full stack data scientist is – a person who can do all the steps in the CRISP-DM process. They can help product managers with business understanding, they can work with analysts in data understanding, they can help data engineers in the data preparation step, they can do the modeling, and they can deploy the models. If you want to be that kind of person, then, of course, you also need to learn a bit of product management and analytics, which we don't have courses for. But data engineering – preparing data before it goes to a model – will certainly be helpful. 
Also, machine learning ops Zoomcamp (MLOps Zoomcamp) will be helpful as well. I would suggest, if you really want to focus on machine learning engineering, then probably doing MLOps Zoomcamp will make more sense for you. But data engineering Zoomcamp will be quite useful in the future, because data scientists and ML engineers tend to work on data pipelines as well. 
Maybe not all the content will be useful for you. For example, the content about data warehousing – as a data scientist, I don't find it super useful in my work to me personally. Also, the module about analytics engineering – it's nice, but it's not something I do day to day. But the modules about batch, the modules about Spark, the modules about streaming – this is something that can be useful, because this is something we could use in building machine learning pipelines as well. 
To put data in a machine learning model, we need to prepare it – we need to create data pipelines – and this is pretty relevant. But you can just check it out. All the content is already there. You can just go and check it out. If you like it, do it – if you don't, don’t. I would suggest maybe going with this one (ML Zoomcamp)."
2034	I want to work in ML engineering – ideally, freelance. Do you think picking recommender systems as a topic to work on in my portfolio is a good strategy?	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
2035	Is it possible to use Tableau for week 4 instead of Google Data Studio?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
2036	Is it possible to use Tableau for week 4 instead of Google Data Studio?	Yes, you will have to come up with your own individual projects. It's not easy, I know. But we'll help you. We will help you to come up with a project idea. That's actually on purpose because we want every project to be unique. Then it becomes a part of your portfolio when you look for a job.
2037	Is it possible to use Tableau for week 4 instead of Google Data Studio?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
2038	Is it possible to use Tableau for week 4 instead of Google Data Studio?	Yes, you can do this, especially for your project. For your project, you can choose any technology you want. It could be Tableau instead of what we've covered.
2039	Is it possible to use Tableau for week 4 instead of Google Data Studio?	Our thoughts are that this is indeed an exciting area. This is an in-demand skill and that's why we have a section about analytics engineering in this course.
2040	What would be the best techniques to use to improve a linear regression model?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2041	What would be the best techniques to use to improve a linear regression model?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
2042	What would be the best techniques to use to improve a linear regression model?	"Tim
I think a couple people asked this in ML Zoomcamp Slack. I was thinking about this a little bit. I think it's sort of a philosophical thing [chuckles] where we think about models and Bento is sort of separate a little bit from code. So your project directory is where all the stuff that you'll commit to Git. I don't think we think that it's the right model to commit your models to Git, for example. We think of ourselves a little bit more like Docker. You don't have Docker images in your project directory, right? You just have your Docker file and that's what you commit to Git. In the same way, we store your models and your Bento in that dedicated directory, and then sort of leave it up to you to push it to different places where you might build or where you might need it
Alexey
Do you know why you decided to save it locally on your local file system? For example, what MLflow does – you can save it to S3 directly, but in the case of Bento, you always first save it to local storage and then you decide “This model I want to export, but for the rest, I don't care.”
Tim
Right. First, I think it’s because that's the easiest way to do it, but also because I think with BentoML, the workflow is that a lot of times, somebody's building a model on their laptop, or somebody's building a model in a slightly less distributed system and they're assembling their Bento there. So I think that the fastest way to access that model is locally. We do give you the ability to push that model and pull that model from other places. But ultimately, when you assemble your Bento and then you assemble your container, all that stuff has to be there. I don't think we wanted separate abstractions for when a person is training and saving, versus when a person is building a Bento and pulling in the model and assembling the container. It's just the same abstraction, which is just a local repository which contains models.
Alexey 
Maybe you don't want to save every model every single time you change one single parameter to the cloud. [Tim agrees] Maybe at the end, after all the experiments, you want to save the final one. Meanwhile, for MLflow, the use case is different – you actually want to save every single experiment because you want to see the results of these experiments.
Tim
Right."
2043	What would be the best techniques to use to improve a linear regression model?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
2044	What would be the best techniques to use to improve a linear regression model?	"Usually, it's not about the model in practice. It’s not about the model you use, or the algorithm you use, but it's more about features. You need to work on creating new features if you want to improve your model. Linear regression, for example, is a powerful model – but sometimes, because it's a linear model, there are some limitations. In many cases, for example, XGBoost will have better predictions, so you can use that. But again, it's usually a lot of manual analysis – exploratory data analysis, understanding which features make sense, and then maybe adding more features, experimenting with different features. 
Here, it's very important to have a good validation framework using which you can test your ideas. You think, “Okay, this feature will be useful.” You add this feature, you implement this feature, then you train a new model, and then you see how well it improves your score on validation. This is the best indicator that you're actually improving something. 
A short, one-line answer to this would be “The best technique is cross-validation or just validation."
2045	Are there any specifications for the size of the project dataset? The one I'm interested in has only 2000 records.	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
2046	Are there any specifications for the size of the project dataset? The one I'm interested in has only 2000 records.	You can follow the same setup here. Apart from creating a virtual machine and doing port forwarding, everything else that you do in this video also applies to a local Linux environment. Maybe you will also need to install Google Cloud SDK, because on a virtual machine, you already have it – you don't need to install it. Locally, you will need to install it. Apart from that, you can just follow the same stuff for a Linux computer. For Windows or Mac, I don't think we have a guideline.
2047	Are there any specifications for the size of the project dataset? The one I'm interested in has only 2000 records.	No, you cannot.
2048	Are there any specifications for the size of the project dataset? The one I'm interested in has only 2000 records.	"Alexey
We don't explicitly have any limits on the size of the dataset, but 2000 is very low. Maybe you can find something bigger? Or maybe you can find a way to make it bigger – to make it more interesting? Typically, the engineers deal with more data than 2000 records. But I don't think we have any strict limitations on the size. I think we already had this discussion last year. I don't remember if we explicitly put any limits. Just use your own judgment. Imagine that a hiring manager is looking at your project. You probably want to make a good impression, right? And I don't think 2000 records will make a good impression, or a 100-kilobyte dataset will make a good impression. Megabytes? Eh. But if it's in gigabytes, it's already good."
2049	Are there any specifications for the size of the project dataset? The one I'm interested in has only 2000 records.	Everything you do here is individual. You don't form teams.
2050	Jeff, can you please record the live Prefect workshops?	"Jeff
I don't know. When hosting meetups, I'll just draw on [a board]. It's great to have hybrid options for folks. People who can be remote, people can be in-person – it definitely adds complexity in how things are done. There is a lot more with logistics and things. The short answer is no. But if people are interested in some kind of online Prefect-specific course, send me a message in Slack. I'm there. Let me know about your interest level. It sounds like there's some interest.
Alexey
At least six people are interested. 
Jeff
[chuckles] At least six. 
Alexey
Maybe more, but they just didn’t know that they could vote."
2051	Jeff, can you please record the live Prefect workshops?	No, you cannot.
2052	Jeff, can you please record the live Prefect workshops?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
2053	Jeff, can you please record the live Prefect workshops?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
2054	Jeff, can you please record the live Prefect workshops?	"Jeff
There are lots of different things there. This is something I used to teach folks doing data science-type work and it applies for data engineering, too. I don't have a good checklist at the tip of my fingers, but you do want to look at things like “Are your data in the right type? Do you have missing values, libraries, rate expectations (which we have integration for)?” That can help with trying to make sure your data looks how you think it'll look, in terms of the statistical properties of it. Hypothesis can also generate some ideas there, or some other Python library. Pandera for everything being how you expect, values matching what you expect. So there are a lot of different tools in Python that you can use there. It's a good question. It can be a lot of different things."
2055	How to get a data engineering role for non-IT people with a two year gap? Please suggest any project.	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
2056	How to get a data engineering role for non-IT people with a two year gap? Please suggest any project.	No, you cannot.
2057	How to get a data engineering role for non-IT people with a two year gap? Please suggest any project.	"Alexey
Well, I can suggest a project for this course. At the end, after you complete this course, after you complete all the modules, as a part of this course, you will have to work on the project. This is how you will get a certificate at the end – if you pass the project. This is a good project to include in your portfolio. 
Michael
I don't know if I have much more to add other than being active in online communities. This course is a great place to start. There are a lot of large cities that have meetup groups, which are always great. One of the good side effects of the whole COVID pandemic is that a lot of those are remote now. If you Google “San Francisco Big Data meetup,” meet people, talk with them, get their insights. I think that'd be a good way to approach that.
Jeff
Plus one just for getting involved in communities, whether it's conferences, online or in-person is great. Volunteer, if you can, at those. That's often a great way to help out and work with people. Go to meetups. I run a meetup in DC on data science. If anyone's in the DC area, there are data engineering meetups in town too. If there isn’t one somewhere, and if there's a few people around, I encourage you to start one. That's something you can do, regardless of your level of experience.
30:06  
Alexey
Then I'm going to do another shameless plug. We have a podcast and the topic of career changing comes up pretty often there. One interesting one is from Juan Pablo. Here he's actually not talking about… well, it's kind of related to analytics engineering. He was a math teacher, which doesn't really qualify as an IT person. He was also working as an Uber driver. In the podcast, he tells his story of how he actually did this. He talks exactly about that. He talks about going to meetups. Instead of not talking about this, just go and check it out. That's not the only relevant podcast episode. You can just go through everything we have and see if anything catches your interest. I'm sure you'll find a lot of interesting stuff."
2058	How to get a data engineering role for non-IT people with a two year gap? Please suggest any project.	Yes, you will have to come up with your own individual projects. It's not easy, I know. But we'll help you. We will help you to come up with a project idea. That's actually on purpose because we want every project to be unique. Then it becomes a part of your portfolio when you look for a job.
2059	How to get a data engineering role for non-IT people with a two year gap? Please suggest any project.	"Alexey
Yeah, that's not going to happen. I think ChatGPT will help us do our job, but it will not replace people, at least in my opinion. But I use it in my work and it's helpful. You should try using it too – if you want, of course."
2060	I'm a beginner in data engineering but have experience as an analyst working with Python, MySQL, and Tableau. Do you think I can do well here?	We currently do not have any course partners yet. If you work at a company and you want to partner with us, please reach out and we will see how we can do it. In one of our courses, which was our first iteration of the Machine Learning Zoomcamp, we partnered with a company called Delphi. They got two interns from our course and they were quite satisfied with the outcome. You can actually read more about the work of these interns in our articles – Interview with Valerii Chetvertakov and then another one, Interview with Ken Wu. You can learn more about the interns and if you think that, at your company, you need good interns or juniors or you want to partner in any other way, please reach out and we can see how to make it work.
2061	I'm a beginner in data engineering but have experience as an analyst working with Python, MySQL, and Tableau. Do you think I can do well here?	Yes. If Michael was here, he would probably confirm this because this is the background he has. He's an analyst. We also had other analysts who did pretty well in the course. So yeah, certainly you can do well. As a beginner in data engineering, I would say that we don't expect any data engineering knowledge for taking this course. If you're a beginner already – if you know some data engineering concepts – then it's more likely that you will do well in this course.
2062	I'm a beginner in data engineering but have experience as an analyst working with Python, MySQL, and Tableau. Do you think I can do well here?	No, you cannot.
2063	I'm a beginner in data engineering but have experience as an analyst working with Python, MySQL, and Tableau. Do you think I can do well here?	For Spark, we use Python. So it's PySpark. So what Java aspects will be covered? Ankush already uploaded the videos for week 6, just go and check that out.
2064	I'm a beginner in data engineering but have experience as an analyst working with Python, MySQL, and Tableau. Do you think I can do well here?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
2065	How should we select the best model? How should I choose the parameters for hyperparameter tuning?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
2066	How should we select the best model? How should I choose the parameters for hyperparameter tuning?	"Actually, I don't remember exactly how I do this. The right approach would be to use only train data and not use validation data or test dataset. This is done on purpose for exactly this reason that you mentioned. We want to model a situation in which there is some unseen data – it's truly unseen, we do not see that all – and anything can happen to this unseen data, including the situation that you described, such as there is a value that is only present in the test data. 
Imagine that there is a new iPhone – when you train your model, this new iPhone didn't exist (you didn't have this category) but you apply this model, and then, all of a sudden, there is a new iPhone. How will your model react to this? You need to somehow model this situation. This is totally normal. This can and will happen in practice. We just want to understand what would be the effect of that. That's why we need to see this in training data. This is good and this is how it should be. I hope that is a good answer."
2067	How should we select the best model? How should I choose the parameters for hyperparameter tuning?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
2068	How should we select the best model? How should I choose the parameters for hyperparameter tuning?	You use validation for that and then you see what works best on validation. Then you use that test dataset to make sure you do not overfit. That's how we do it.
2069	How should we select the best model? How should I choose the parameters for hyperparameter tuning?	"You will probably also need to learn how to pass interviews. This course does not teach that, but you will need to acquire the skill somehow. The best way to acquire these skills is by interviewing. But apart from that, I think this course covers let's say 80% of the skills that you will need for a job and the remaining 20% depends on the company. 
So you will need to do some research on the company where you want to work, see what kind of tools they use, what kind of problems they solve, how much emphasis they make in the interviews on the theoretical part. You will need to somehow figure this out and, based on that, see what you need to learn. I think with just this content, you should be able to get a machine learning-related job."
2070	Can data science jobs be done remotely for freshers?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
2071	Can data science jobs be done remotely for freshers?	I do sometimes. Not very often these days, but we have quite a few models that require deep learning. Usually, these models are related to images. We have some images on the platform, so when we need to understand what's happening on these images, we use deep learning.
2072	Can data science jobs be done remotely for freshers?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2073	Can data science jobs be done remotely for freshers?	Yeah, they're similar. Not all of them will ask you to deploy stuff, but for data science, it's often necessary to do simple exploratory data analysis then train the model and write some conclusions.
2074	Can data science jobs be done remotely for freshers?	"Yes, of course, it can. The sky's the limit. But I think it will be quite challenging for somebody without experience to get a data science job remotely. Then again, it means remotely within the same country – then it's possible. For example, at OLX (the company where I work) we had both interns and juniors who were hired to work remotely – if they are in the same city or in the same country because we're hiring remotely. No problems with that. But let's say if you're based in one country and you want to work remotely for an American company, or a European company, then it might be tricky. 
Not every company is ready to do that. There are companies who do that. You probably need to find these “remote-first” companies that hire everywhere and then you will need to convince them that you are good enough, even though you don't have experience so that you can work for them. It’s tricky. This is one of the reasons we actively encourage you to do this “learning in public” part. If you do this often, then you will be able to get a job as a fresher, even though you might not have “formal” experience. But because you did all these projects, you shared your learning and maybe some people started following you, then companies might notice you. Then it will be easier for them to hire you. 
I think this is actually what happened last year. I don't promise it will happen this year, too – but last year, there was a company, Delphi, who hired two interns. The company is Berlin-based and one of the interns was in Milan, Italy, and the other intern was in Ukraine. So it wasn't the same country, yet they managed to hire them. So I think it's possible. It was actually possible with this course. I don't know if we will have this kind of setup this year. 
I haven't heard from any company who wants to hire interns this year. If you're working at such a company and you need interns, then please get in touch with me. People who graduate from this course will be really good interns, I am sure. But you can maybe help hire people like this person who asked the question."
2075	Are there any rewards for the best students?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
2076	Are there any rewards for the best students?	Yeah, we can pat you on your shoulder. [chuckles] There’s the position on the leaderboard. The leaderboard is anonymized so nobody will know that this hash belongs to you, but if you want, at the end we will have a forum where you can share your actual contact information – your actual information about you and you will be the first in this public leaderboard. This is how I can kind of give you a reward for that. But apart from that – eternal glory? Maybe that?
2077	Are there any rewards for the best students?	No, you cannot.
2078	Are there any rewards for the best students?	"Alexey
Google Cloud Platform is not mandatory. You can do most of the course without a cloud. You will not be able to use BigQuery, obviously, because it's a cloud offering. But you can use local postgres instead of that and I think you can run almost everything locally. For example, for streaming, instead of using Confluent Cloud, you can use Kafka. You can set it up locally.
Ankush
We also provide Docker images for everything, but the videos will be more on Confluent Cloud. But you can always change that.
Alexey  
I know that there are people from Nigeria, Iran, who cannot register in Google Cloud Platform. You can do things locally."
2079	Are there any rewards for the best students?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
2080	If I want to train my dataset for a capstone project, where should I start now? I am looking for a “Hello World” to create my first dataset.	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
2081	If I want to train my dataset for a capstone project, where should I start now? I am looking for a “Hello World” to create my first dataset.	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
2082	If I want to train my dataset for a capstone project, where should I start now? I am looking for a “Hello World” to create my first dataset.	Yeah. If it works for you, use it. I don't mind.
2083	If I want to train my dataset for a capstone project, where should I start now? I am looking for a “Hello World” to create my first dataset.	"By “train,” I assume you mean you want to collect your dataset. What you should start with is asking yourself what kind of problem you want to solve. Once you identify the problem, you then need to think about “Where can I get this data? Do I need to collect this data yourself? Or can I scrape it? Or can I maybe buy it or find it somewhere?” So that's what you need to ask yourself. Then with the goal in mind, you go to the internet, you try to find the dataset. Maybe you don't find anything so then you can see if there is a website that has this data. For example, if you want to collect a dataset for house prices, then you go to a website that lists houses and then you can scrape this information from the website, or scrape information from Amazon, or scrape information from some other resource. So then you can scrape this data and in this way, you collect the dataset. 
If the information you are looking for does not exist at all, then you will probably need to collect a dataset yourself. It really depends on the nature of the dataset. You can ask some crowdsourcing platforms, such as Amazon Mechanical Turk or Toloka. There are actually multiple companies like this (that do crowdsourcing). With Toloka, we actually had a video on our channel. It was a workshop with Toloka. I think with Toloka, we probably have another workshop soon, as well. So keep an eye on this. But yeah, so you can use crowdsourcing, or you can just go and collect it yourself. It's a very generic recommendation, because I don't know what exactly you want to do. Maybe you can share more information and we can talk about this in Slack."
2084	If I want to train my dataset for a capstone project, where should I start now? I am looking for a “Hello World” to create my first dataset.	I guess, use validation.
2085	Could the course be a little flexible by adapting other clouds besides GCP?	For the course material, we cannot re-record it for other clouds. If you know what you are doing, by all means – use other clouds. For your projects, you can do whatever you want.
2086	Could the course be a little flexible by adapting other clouds besides GCP?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
2087	Could the course be a little flexible by adapting other clouds besides GCP?	No. But if some of you find anything interesting, please share the links in Slack
2088	Could the course be a little flexible by adapting other clouds besides GCP?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
2089	Could the course be a little flexible by adapting other clouds besides GCP?	Just one. It's either the first attempt or the second. You should not try both.
2090	In the course, it’s mentioned that it's better to destroy infra using Terraform (the storage buckets and BG_dataset). If I apply them again, will all the data be gone?	"Victoria
I think it's also worth putting it as, it’s great, but why is it chosen as a cloud data platform? I saw that a lot of people were also talking about AWS and things like that. There’s even a channel of people that want to use Snowflake instead of GCP. The main reason why we chose GCP is because it has a very generous free tier that's $300. In the case of Snowflake, for example, it’s limited to one month. In the case of GCP it’s until you finish the money – you can use the backups and a lot of people use the virtual machines, and you can use BigQuery. It will cover all of that and you won’t spend anything from your money, which is the idea of the course. That's why we chose BigQuery. 
But the main goal that we have here is that you can apply the knowledge of a cloud data warehouse to any kind of data warehouse. The same with Prefect, you can also apply it to Airflow if you start working at a company that uses Airflow. Because at the end of the day, you should know how a cloud data warehouse and data warehousing works. In the case of specifics of BigQuery to something Snowflake – both our cloud data warehouses from my experience. BigQuery has a different way of connecting – it goes through an API – whereas in Snowflake you use the more well-known SQL. They both have their flavors or SQL. 
Big Query also has differences, for example, they call databases “projects,” they call schemas “datasets,” and things like that. So it changes terminology. They also have a different approach on cost. Something like, select, start, limit 10, for a BigQuery table could cost you a lot of money. But they tell you before that, how many bytes they'll scan and things like that. It’s just the smaller things, I would. Ankush, you're also very familiar with BigQuery. I've always used Snowflake or Redshift more, or Microsoft.
Ankush
I think the biggest reason for choosing BigQuery was that it just comes with Google Cloud Platform and it's free to use. That's one thing. I think, overall, Snowflake is generally more expensive than BigQuery, at least in my experience. But, of course, they’re super similar. If you look at Big Query optimizations, it's the same concept in Snowflake. 
The concepts, as Victoria said, are applicable to both the data warehouse solutions. It's just a flavor that we chose just because it's easy to have something like an inbuilt data warehouse where you actually start something on your own. This was one of the big reasons for choosing BigQuery.
Alexey
You're free to use AWS if you want, as well. I think that is also a channel where people want to use AWS for the course. We, unfortunately, will not be able to give you a lot of support there. Because with DBT, for example, I don't know how well it can connect to Redshift or Athena or whatever.
Victoria
Somebody is saying something in the chat. I think I was misunderstood. It says $300 is not a small amount. That's the amount of credits you get for free. You will not pay $300. The idea is that we chose GCP so you don't pay anything. You don't pay anything! [chuckles] 
Alexey
Yes, that's the main advantage. What you need is a Google account – you use it for registering at Google Cloud Platform. They will ask you for a credit card, but only to verify that you're a real human. This card is not used for a different Google Cloud Platform account. They will not charge you anything. To me, Google is trustworthy. I think you can trust it with your credit card. I mean, so far they didn’t violate the trust I have in them. It's a good deal, basically. You should take it. 
With AWS, most of the content we talk about – most of the things you will learn here about Google Cloud Platform – are easily transferable to AWS, like all these virtual machines, object storage, Spark. All these things work fine. In AWS, the buttons you need to click are different, or the Terraform script you need to write is different. But at the end, most of the concepts are still the same. You just need to map from Google Cloud Platform concept to AWS concept. But with AWS, you need to pay something."
2091	In the course, it’s mentioned that it's better to destroy infra using Terraform (the storage buckets and BG_dataset). If I apply them again, will all the data be gone?	"There are lots of different varieties of approaches here. One thing that you might do is use some kind of data ingestion tool, as I mentioned in the live FAQ discussion – something like Airbyte or Fivetran – if you have lots of data that you're ingesting, that can be a common way to stay on top of that. Or you could do it through a script, kind of like we do here. 
But what you might do is run your agent on a virtual environment in Google Cloud VM, for example, or in AWS, or somewhere else. You could run it in Kubernetes. There are a number of places where maybe you would have that running and have your infrastructure there, so that you were just pulling down the code not to your local machine, but it would just be in your VM. 
If you're actually going and using Pandas, you're putting the data into memory, so you do need to have some scripts that will do that. And you can go directly between Google Cloud Storage and BigQuery with the Prefect GCP library. So that Prefect GCP library that you installed, that you've been using, does have some options in there. I think it's in the BigQuery module to actually get data directly into BigQuery from GCS. So there are a lot of options there. Check out Prefect GCP to see how to do it."
2092	In the course, it’s mentioned that it's better to destroy infra using Terraform (the storage buckets and BG_dataset). If I apply them again, will all the data be gone?	I think the idea here is that you can do this, but you don't necessarily have to do it right now. You can do this at the end. Because if you do destroy it now, of course, the buckets and BigQuery datasets will be gone. When you apply it again in the future, it will be recreated. Indeed, the data will be gone. Don't destroy.
2093	In the course, it’s mentioned that it's better to destroy infra using Terraform (the storage buckets and BG_dataset). If I apply them again, will all the data be gone?	Yeah, we can pat you on your shoulder. [chuckles] There’s the position on the leaderboard. The leaderboard is anonymized so nobody will know that this hash belongs to you, but if you want, at the end we will have a forum where you can share your actual contact information – your actual information about you and you will be the first in this public leaderboard. This is how I can kind of give you a reward for that. But apart from that – eternal glory? Maybe that?
2094	In the course, it’s mentioned that it's better to destroy infra using Terraform (the storage buckets and BG_dataset). If I apply them again, will all the data be gone?	Just one. It's either the first attempt or the second. You should not try both.
2095	On a scale from 1 to 10, how weird is it to have a Zoom call where you're the only one with a camera?	It's super weird. I got used to this, but the first time I did it, it was super weird. It's really strange. On a scale from 1 to 10? I don't know. 8, maybe. It's much more interactive (or less weird) when there is somebody on the other end of this call. But I know that you're actually listening there. I just don't see you. Which makes it maybe a little bit less weird.
2096	On a scale from 1 to 10, how weird is it to have a Zoom call where you're the only one with a camera?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2097	On a scale from 1 to 10, how weird is it to have a Zoom call where you're the only one with a camera?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
2098	On a scale from 1 to 10, how weird is it to have a Zoom call where you're the only one with a camera?	"Actually, I don't remember exactly how I do this. The right approach would be to use only train data and not use validation data or test dataset. This is done on purpose for exactly this reason that you mentioned. We want to model a situation in which there is some unseen data – it's truly unseen, we do not see that all – and anything can happen to this unseen data, including the situation that you described, such as there is a value that is only present in the test data. 
Imagine that there is a new iPhone – when you train your model, this new iPhone didn't exist (you didn't have this category) but you apply this model, and then, all of a sudden, there is a new iPhone. How will your model react to this? You need to somehow model this situation. This is totally normal. This can and will happen in practice. We just want to understand what would be the effect of that. That's why we need to see this in training data. This is good and this is how it should be. I hope that is a good answer."
2099	On a scale from 1 to 10, how weird is it to have a Zoom call where you're the only one with a camera?	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
2100	How to approach tabular regression sales data when data, for example, for January to August, 150-200 units, suddenly jumps to 400 units in September.	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2101	How to approach tabular regression sales data when data, for example, for January to August, 150-200 units, suddenly jumps to 400 units in September.	"Tim
I think a couple people asked this in ML Zoomcamp Slack. I was thinking about this a little bit. I think it's sort of a philosophical thing [chuckles] where we think about models and Bento is sort of separate a little bit from code. So your project directory is where all the stuff that you'll commit to Git. I don't think we think that it's the right model to commit your models to Git, for example. We think of ourselves a little bit more like Docker. You don't have Docker images in your project directory, right? You just have your Docker file and that's what you commit to Git. In the same way, we store your models and your Bento in that dedicated directory, and then sort of leave it up to you to push it to different places where you might build or where you might need it
Alexey
Do you know why you decided to save it locally on your local file system? For example, what MLflow does – you can save it to S3 directly, but in the case of Bento, you always first save it to local storage and then you decide “This model I want to export, but for the rest, I don't care.”
Tim
Right. First, I think it’s because that's the easiest way to do it, but also because I think with BentoML, the workflow is that a lot of times, somebody's building a model on their laptop, or somebody's building a model in a slightly less distributed system and they're assembling their Bento there. So I think that the fastest way to access that model is locally. We do give you the ability to push that model and pull that model from other places. But ultimately, when you assemble your Bento and then you assemble your container, all that stuff has to be there. I don't think we wanted separate abstractions for when a person is training and saving, versus when a person is building a Bento and pulling in the model and assembling the container. It's just the same abstraction, which is just a local repository which contains models.
Alexey 
Maybe you don't want to save every model every single time you change one single parameter to the cloud. [Tim agrees] Maybe at the end, after all the experiments, you want to save the final one. Meanwhile, for MLflow, the use case is different – you actually want to save every single experiment because you want to see the results of these experiments.
Tim
Right."
2102	How to approach tabular regression sales data when data, for example, for January to August, 150-200 units, suddenly jumps to 400 units in September.	It is possible to do everything locally. You will need to do it by… I think you will need to do it in module five when we deploy something. Or actually, you will probably need to do it when we do your project. Yeah. It's fine to do everything locally. But if you have access to cloud and you want to practice with this, I do recommend practicing, because cloud is one of the skills that employers are looking for. They really need it. So if you want to be more employable, I do recommend learning about cloud.
2103	How to approach tabular regression sales data when data, for example, for January to August, 150-200 units, suddenly jumps to 400 units in September.	I don't think I understand your question, to be honest. Maybe I'll try to rephrase it… Or ask in Slack, maybe?
2104	How to approach tabular regression sales data when data, for example, for January to August, 150-200 units, suddenly jumps to 400 units in September.	"We're not covering time series data here. I'm not really an expert in that. Maybe I can give you a quick answer from what I would do in this case. Here, you probably want to have the same number of samples (units) for each month. I think in Pandas, you can do this with the resample method. This resample would make sure that you have the same amount of records for each of the dates, for example – for each of the months, days and so on. I think I would go in this direction. 
When it comes to actually predicting, it's a totally different story. The usual approach is like what we cover here for linear regression – they don't necessarily work here. They work with some adjustments. For that, I will, again, invite you to check our channel. In our channel, we covered these topics recently. There was the podcast called Feature Engineering for Time Series Forecasting from Kishan and then Probabilistic Demand Forecasting at Scale. These two talks could be interesting for you if you're into Time Series."
2105	Is the attempt more important than the accuracy/recall of the model? The dataset I'm using is proving difficult to get a good recall value.	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
2106	Is the attempt more important than the accuracy/recall of the model? The dataset I'm using is proving difficult to get a good recall value.	Yeah, I think this is what I just said. Yes, it is more important. Some datasets are just inherently difficult to deal with, like click prediction, fraud prediction – with this kind of data, you usually cannot have a very good score. If you have a very good score there, then something is probably wrong. Maybe you are overfitting or you have some sort of leakage or whatever. If I see a model for click prediction that is super accurate, it will be very suspicious.
2107	Is the attempt more important than the accuracy/recall of the model? The dataset I'm using is proving difficult to get a good recall value.	"Tim
I think a couple people asked this in ML Zoomcamp Slack. I was thinking about this a little bit. I think it's sort of a philosophical thing [chuckles] where we think about models and Bento is sort of separate a little bit from code. So your project directory is where all the stuff that you'll commit to Git. I don't think we think that it's the right model to commit your models to Git, for example. We think of ourselves a little bit more like Docker. You don't have Docker images in your project directory, right? You just have your Docker file and that's what you commit to Git. In the same way, we store your models and your Bento in that dedicated directory, and then sort of leave it up to you to push it to different places where you might build or where you might need it
Alexey
Do you know why you decided to save it locally on your local file system? For example, what MLflow does – you can save it to S3 directly, but in the case of Bento, you always first save it to local storage and then you decide “This model I want to export, but for the rest, I don't care.”
Tim
Right. First, I think it’s because that's the easiest way to do it, but also because I think with BentoML, the workflow is that a lot of times, somebody's building a model on their laptop, or somebody's building a model in a slightly less distributed system and they're assembling their Bento there. So I think that the fastest way to access that model is locally. We do give you the ability to push that model and pull that model from other places. But ultimately, when you assemble your Bento and then you assemble your container, all that stuff has to be there. I don't think we wanted separate abstractions for when a person is training and saving, versus when a person is building a Bento and pulling in the model and assembling the container. It's just the same abstraction, which is just a local repository which contains models.
Alexey 
Maybe you don't want to save every model every single time you change one single parameter to the cloud. [Tim agrees] Maybe at the end, after all the experiments, you want to save the final one. Meanwhile, for MLflow, the use case is different – you actually want to save every single experiment because you want to see the results of these experiments.
Tim
Right."
2108	Is the attempt more important than the accuracy/recall of the model? The dataset I'm using is proving difficult to get a good recall value.	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
2109	Is the attempt more important than the accuracy/recall of the model? The dataset I'm using is proving difficult to get a good recall value.	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2110	Does anyone know any good resources for clean coding in Python?	For Spark, we use Python. So it's PySpark. So what Java aspects will be covered? Ankush already uploaded the videos for week 6, just go and check that out.
2111	Does anyone know any good resources for clean coding in Python?	I will not answer this right now. We did not plan anything like that. I think Anna prepared some materials about how to deploy Prefect, not with Terraform, but with some Google Cloud Platform stuff. I might be mistaken, but in practice, I think we use something like Kubernetes. Kubernetes is not managed through Terraform. I will leave this question answered and in the next Office Hours when we'll talk more about Prefect. Somebody from the Prefect team will answer this question. Probably Jeff will be in our next Office Hours.
2112	Does anyone know any good resources for clean coding in Python?	"There are lots of different varieties of approaches here. One thing that you might do is use some kind of data ingestion tool, as I mentioned in the live FAQ discussion – something like Airbyte or Fivetran – if you have lots of data that you're ingesting, that can be a common way to stay on top of that. Or you could do it through a script, kind of like we do here. 
But what you might do is run your agent on a virtual environment in Google Cloud VM, for example, or in AWS, or somewhere else. You could run it in Kubernetes. There are a number of places where maybe you would have that running and have your infrastructure there, so that you were just pulling down the code not to your local machine, but it would just be in your VM. 
If you're actually going and using Pandas, you're putting the data into memory, so you do need to have some scripts that will do that. And you can go directly between Google Cloud Storage and BigQuery with the Prefect GCP library. So that Prefect GCP library that you installed, that you've been using, does have some options in there. I think it's in the BigQuery module to actually get data directly into BigQuery from GCS. So there are a lot of options there. Check out Prefect GCP to see how to do it."
2113	Does anyone know any good resources for clean coding in Python?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
2114	Does anyone know any good resources for clean coding in Python?	"Michael
Actually, I do have a resource – Real Python. They do have some paid content, but they also have a lot of free content that I look at frequently. Also, if you go to sites like LeetCode, or do the Advent of Code – not necessarily to solve the problems, but looking at others’ solutions – you can see some very good examples of clean code. Type Hinting is something that I’ve picked up a few years back from Advent of Code. I still don't do it all the time, but when you see more experienced people write code, you kind of know what you're aiming for. And I think that gives you a lot of direction.
Alexey
I think in this video, Getting a Data Engineering Job, Jeff also talks about learning good coding practices. I remember that the project he recommended to look at, if you want to learn more about good coding, was Prefect. He said, “Yeah, go check out Prefect. It's a great way to learn how to write good Python code.”I guess it's a good plug, right? [chuckles] 
Then there is also a book called Clean Code. It's about Java. It's a very nice book. It's actually not about Java, it's about clean code, but the examples there are in Java. For me, it was very useful when I was starting coding. But I was a Java developer. What I know is there are examples in this book in Python. Even though the book is about Java, there are GitHub repos where the same concepts are illustrated with Python. You can check that out, too.
Jeff
Yeah, I haven't read this book. But there's also a Clean Code in Python book that is out there. It has good reviews on Amazon. It could be worth checking out."
2115	Is ROC useful for multi-class?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
2116	Is ROC useful for multi-class?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
2117	Is ROC useful for multi-class?	I don't find it particularly useful to be honest. For binary, yes, it is definitely used. For multi-class… it becomes difficult to interpret, to be honest. So for multiclass, in my experience, accuracy works the best. The rest become more and more difficult to understand as the number of classes rises.
2118	Is ROC useful for multi-class?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2119	Is ROC useful for multi-class?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
2120	What will the job market be like in 2023?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
2121	What will the job market be like in 2023?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
2122	What will the job market be like in 2023?	Since we had more signups this year than last year, I hope it will be more than 100. But let's see.
2123	What will the job market be like in 2023?	"Alexey
That’s a very difficult question. I don't know if I should attempt doing any forecasting. Anyone else want to answer that question?
Luis
I can answer because… Well, I don't know if I can, but if I may. I left my previous company in the last week of the year (2022) and I already am in lots of processes, including in the final process to be a data engineer. Today, it's the 16th of January, 2023. The market is completely imbalanced, in the good sense – for us. [chuckles] 
Alexey
So there’s hope that, even though there is a recession and layoffs, it's not difficult to find a job. That's what you're saying?
Luis
Exactly, exactly. Completely. Actually, the pandemic was a good thing for us in IT, because a lot of companies are recruiting remotely, so you don't even need to go to the United States to work for that company, or other similar situations."
2124	What will the job market be like in 2023?	No, you cannot.
2125	What is the difference between Streamlit and Gradio?	I guess not. That's the process of learning? Maybe more senior people don't… Yeah, I see where you're going with this. Senior people are not supposed to go bankrupt and default. They are probably supposed to pay back the debt. I guess these are the particularities of this dataset, I assume. Indeed, it's probably a data issue ,or not an issue, but a “characteristic” of this dataset.
2126	What is the difference between Streamlit and Gradio?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
2127	What is the difference between Streamlit and Gradio?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
2128	What is the difference between Streamlit and Gradio?	For those who don't know, Streamlit and Gradio are frameworks for creating user interfaces for machine learning. The main difference between them is that Gradio is designed specifically for machine learning, while Streamlit is a general purpose user interface library in Python. You can think of them as Flask and Bento ML. With Flask, you can deploy a machine learning model, but you can also deploy much, much more. While in Bento ML you can only deploy machine learning. Here, it's the same – with Streamlit, you can build a machine learning interface for a machine learning model, but you can also build pretty much anything. With Gradio, it's only for machine learning. At least this is my understanding. I have used Streamlit. I have not extensively used Gradio.
2129	What is the difference between Streamlit and Gradio?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
2130	Is ELT only for large companies or can it be applied to small companies?	"Alexey
Yeah, it can be applied to every company – to any company."
2131	Is ELT only for large companies or can it be applied to small companies?	Just one. It's either the first attempt or the second. You should not try both.
2132	Is ELT only for large companies or can it be applied to small companies?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
2133	Is ELT only for large companies or can it be applied to small companies?	"There are lots of different varieties of approaches here. One thing that you might do is use some kind of data ingestion tool, as I mentioned in the live FAQ discussion – something like Airbyte or Fivetran – if you have lots of data that you're ingesting, that can be a common way to stay on top of that. Or you could do it through a script, kind of like we do here. 
But what you might do is run your agent on a virtual environment in Google Cloud VM, for example, or in AWS, or somewhere else. You could run it in Kubernetes. There are a number of places where maybe you would have that running and have your infrastructure there, so that you were just pulling down the code not to your local machine, but it would just be in your VM. 
If you're actually going and using Pandas, you're putting the data into memory, so you do need to have some scripts that will do that. And you can go directly between Google Cloud Storage and BigQuery with the Prefect GCP library. So that Prefect GCP library that you installed, that you've been using, does have some options in there. I think it's in the BigQuery module to actually get data directly into BigQuery from GCS. So there are a lot of options there. Check out Prefect GCP to see how to do it."
2134	Is ELT only for large companies or can it be applied to small companies?	"Jeff
There are lots of different things there. This is something I used to teach folks doing data science-type work and it applies for data engineering, too. I don't have a good checklist at the tip of my fingers, but you do want to look at things like “Are your data in the right type? Do you have missing values, libraries, rate expectations (which we have integration for)?” That can help with trying to make sure your data looks how you think it'll look, in terms of the statistical properties of it. Hypothesis can also generate some ideas there, or some other Python library. Pandera for everything being how you expect, values matching what you expect. So there are a lot of different tools in Python that you can use there. It's a good question. It can be a lot of different things."
2135	Can you elaborate on when to use SciKit Learn’s OneHotEncoder versus Dictionary Vectorizer?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
2136	Can you elaborate on when to use SciKit Learn’s OneHotEncoder versus Dictionary Vectorizer?	That's a very similar question to when we should use the lambda and when we should use Kubernetes. If you want to use Kubernetes, then go with TensorFlow Serving. Otherwise, TensorFlow Lite. There is one thing where you have to use TensorFlow Lite – where you cannot use TensorFlow Serving – and this is when you want to deploy your model on a mobile device. I have a phone here. If you want to create an app with a model, then you will have to use TensorFlow Lite because you will not be able to use TensorFlow Serving. TensorFlow Serving also adds a bit of complexity. You will need to have this gateway service, you will need to have GRPC service, so that becomes more complex. If you have a lot of load, then you need to do this otherwise, TensorFlow Lite.
2137	Can you elaborate on when to use SciKit Learn’s OneHotEncoder versus Dictionary Vectorizer?	I personally prefer Dictionary Vectorizer because, later in module five, we will put it inside a web service and the web service receives a JSON object – usually it's a dictionary. So we can just take whatever we get as a request, put this inside the Dictionary Vectorizer and then get a matrix. But with OneHotEncoder, I find it more difficult to use – you need to do more. Usually, it's just a matter of taste, I would say. I think if I go back to last year’s Office Hours, in one of the videos I talk about OneHotEncoding. I basically show how to use it. I don’t remember which one, so I think I should go through this playlist and see. I think I described it somewhere. But my personal preference is to use Dictionary Vectorizer because it is simpler to use and you can see how much more things you need to do for OneHotEncoder. But again, this is just personal preference. With both of them, at the end, you actually get the same result, so it doesn't matter.
2138	Can you elaborate on when to use SciKit Learn’s OneHotEncoder versus Dictionary Vectorizer?	I guess not. That's the process of learning? Maybe more senior people don't… Yeah, I see where you're going with this. Senior people are not supposed to go bankrupt and default. They are probably supposed to pay back the debt. I guess these are the particularities of this dataset, I assume. Indeed, it's probably a data issue ,or not an issue, but a “characteristic” of this dataset.
2139	Can you elaborate on when to use SciKit Learn’s OneHotEncoder versus Dictionary Vectorizer?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2140	Why do we use Kafka and not pop/sub?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
2141	Why do we use Kafka and not pop/sub?	Yes
2142	Why do we use Kafka and not pop/sub?	No, you cannot.
2143	Why do we use Kafka and not pop/sub?	I will need to ask Ankush. Maybe ask that in Slack and then he will answer.
2144	Why do we use Kafka and not pop/sub?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
2145	Can I still be part of the deep learning class? I will appreciate it if you address this on the Telegram channel.	Which deep learning class are you talking about? Maybe I don't understand. If you're talking about the competition – you can still take part in the competition. There is still more than one month to go. It ends on February 1, 2023. One week before the end, you will not be able to submit. The last time to enter the competition will be one week before February 1, 2023. If you're talking about something else, perhaps this module, then it's always open. You can come back to this material when you have time and just take it.
2146	Can I still be part of the deep learning class? I will appreciate it if you address this on the Telegram channel.	"If you take statistics for machine learning education, the first lecture will be this – you'll probably derive the formula for logistic regression and there will be a slide with all these assumptions about the linear regression model. Here is the Google result: 
https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/assumptions-of-linear-regression/

Go through this. They are usually useful in practice, I guess. But what I typically do is train a model, and then rely on validation to tell me if my model is doing well or not. So if there is something wrong with my validation, I will see that the metric validation is bad. It's just a lot of trial and error, rather than figuring out all these theoretical assumptions. 
I don't want to say that all these assumptions are not relevant. If you work as a data scientist, they still are. But I also found that the more practical way to understand if you can apply linear regression to the data or not, is to just apply it and see if it works or not. If it works – if the results on the validation are good – then it means that you can apply your model to this dataset. 
Maybe some of the assumptions are violated. For example, multicollinearity – this is actually the case in the lectures, when we needed to regularize (add regularization to the model) this assumption was violated. You will see in the lectures what exactly happened. So check it out. I don't think I will be able to give a good answer, because I am not prepared to talk about this, but any statistics book or theoretical machine learning book explains this."
2147	Can I still be part of the deep learning class? I will appreciate it if you address this on the Telegram channel.	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
2148	Can I still be part of the deep learning class? I will appreciate it if you address this on the Telegram channel.	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
2149	Can I still be part of the deep learning class? I will appreciate it if you address this on the Telegram channel.	I guess not. That's the process of learning? Maybe more senior people don't… Yeah, I see where you're going with this. Senior people are not supposed to go bankrupt and default. They are probably supposed to pay back the debt. I guess these are the particularities of this dataset, I assume. Indeed, it's probably a data issue ,or not an issue, but a “characteristic” of this dataset.
2150	I want to apply to a computer vision job together with web pages/web services. Please give advice for achieving this.	"You will get out of tutorial hell when you start doing the project in this course, because it will be just a set of guidelines – this matrix – but you will have to do the rest yourself. You will have to find the problem yourself, you will have to do exploratory data analysis yourself, you will have to do data preparation yourself, you will have to do everything yourself – there will be no homework with the exact steps that you need to do to have a project. And this is how you get out of tutorial hell. 
Let's not think about this particular course, but in any setting – you want to find the problem that you want to solve. Once you have the problem, then try to think “What is the best way to solve this? What is the shortest way to solve it?” Or, at least, “What is the next step I need to take to solve it?” And then try to work your way through solving this problem. So focus on the problem. If you just do tutorials, you're not solving problems – you're just doing tutorials. So focus on the problem."
2151	I want to apply to a computer vision job together with web pages/web services. Please give advice for achieving this.	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
2152	I want to apply to a computer vision job together with web pages/web services. Please give advice for achieving this.	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2153	I want to apply to a computer vision job together with web pages/web services. Please give advice for achieving this.	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
2154	I want to apply to a computer vision job together with web pages/web services. Please give advice for achieving this.	I guess not. That's the process of learning? Maybe more senior people don't… Yeah, I see where you're going with this. Senior people are not supposed to go bankrupt and default. They are probably supposed to pay back the debt. I guess these are the particularities of this dataset, I assume. Indeed, it's probably a data issue ,or not an issue, but a “characteristic” of this dataset.
2155	How to prepare for data engineering interviews?	We currently do not have any course partners yet. If you work at a company and you want to partner with us, please reach out and we will see how we can do it. In one of our courses, which was our first iteration of the Machine Learning Zoomcamp, we partnered with a company called Delphi. They got two interns from our course and they were quite satisfied with the outcome. You can actually read more about the work of these interns in our articles – Interview with Valerii Chetvertakov and then another one, Interview with Ken Wu. You can learn more about the interns and if you think that, at your company, you need good interns or juniors or you want to partner in any other way, please reach out and we can see how to make it work.
2156	How to prepare for data engineering interviews?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
2157	How to prepare for data engineering interviews?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
2158	How to prepare for data engineering interviews?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
2159	How to prepare for data engineering interviews?	"Jeff
There is a link here, as I shared in Slack. If you click on the Blocks page, anywhere in the UI here, there’s Prefect docs right here on the bottom left, if you're logged into Prefect Cloud."
2160	How to choose how/where to deploy a model?	"Scikit Learn is not very different. This is not exactly how Scikit Learn is implemented. It's a little bit different, but the idea is very similar. In Scikit Learn, they use smarter ways of doing this. For example, in week two, we have a case when we need to apply regularization. When we have correlated or very close to correlated columns, then our matrix becomes very unstable, so we cannot easily invert it. 
If what I’m saying now doesn't make sense, it will make sense once you start watching the videos. Anyways, what I'm trying to say is that in Scikit Learn, they have a smart way of working around this, such that your weights – your vector with weights, the W vector – does not contain insanely large values. They have smarter methods for doing this. But apart from that, it's very similar and you should not get two different results. You can experiment, and I do recommend experimenting with Scikit Learn. We will actually do this in week three. In week three, we will use Scikit Learn for some of these things. 
In week two, we will implement things ourselves. But starting from week three, we will only use Scikit Learn and other libraries, so we will stop implementing things ourselves. Here's just to give you a taste of what machine learning is inside. It’s just a bunch of formulas – algorithms – there is no magic. Internally, Scikit Learn implements them too."
2161	How to choose how/where to deploy a model?	I wish Tim was here and would help me answer that because I don't know. I assume if you wanted to use Bento, you would probably use TensorFlow Lite – you don't want to use the usual TensorFlow for the same reasons that we talked about in the serverless module, because it's simply too large. Probably for Bento, you would use something like TensorFlow Lite and it will still work. Maybe another thing you can check – in our channel, we had a talk from one of the Bento folks, Building an ML Service Platform from the Ground Up. Here, Sean talks about the differences between TensorFlow Serving and Bento. With TensorFlow Serving, it's good – it's quite optimized for deep learning. There are some problems. First of all, it's written in C++ and expects your model in a certain format – this “saved model format”. Adding anything extra on top of that (like pre-processing, post-processing) becomes difficult, because you will need to have an extra step there. Bento makes it easier. But, again, I don't know. It's very difficult to give you a concrete recommendation without knowing what exactly you want to do. So it's case dependent. If you already use TensorFlow Serving in your company, then maybe it makes sense to stick to that. If we have Tim on Office Hours next week, maybe you can ask him that. He will be a better person to answer this question.
2162	How to choose how/where to deploy a model?	Rescale is already preprocessing. Actually, in exception, this is what they do. It's actually the scaling there. There are two operations, I think. I don't remember exactly, but it's rescaling plus something else. So rescaling is preprocessing.
2163	How to choose how/where to deploy a model?	Yes, there is a thing called the validation dataset that you can use to guide your decision. That's the best thing you can do. Another thing you should take into consideration is time. Let's say that we have a lot of categorical variables, then fitting a decision will take more time than a logistic regression. Then applying it will also take more time. So this is something you should also consider in your experiments. But in the end, you probably want to use the validation set to guide you.
2164	How to choose how/where to deploy a model?	This is maybe a long discussion. For simple projects, lambda is probably the best choice, because you don't need to worry about paying for the time when your servers are up. Kubernetes is usually needed when you have a lot of traffic. For example, when you have 1 million requests per day, then it's time to consider Kubernetes because it could be cheaper than lambda. I guess this is like a rule of thumb – if you have a lot of requests go with Kubernetes, if you don't have a lot of requests, go with lambda. Also, for Kubernetes, you need people who will know Kubernetes well. If you're just a data scientist and in your company, nobody else uses Kubernetes, I don't think it's a good idea to use Kubernetes. Only use Kubernetes if you already have Kubernetes in your company and if there are people who know what they’re doing and know how to operate a Kubernetes cluster. Because as data scientists, as machine learning engineers, that might not be your core responsibility to maintain Kubernetes clusters. You will probably have a lot of other work, so you don't want to give yourself any more work by maintaining the cluster.
2165	By the end of the project, is it possible to do a project simulating a real workplace (teams deciding the approach, closely simulating a work environment)?	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
2166	By the end of the project, is it possible to do a project simulating a real workplace (teams deciding the approach, closely simulating a work environment)?	I think for binary features, just one and zero makes more sense than OneHotEncoding. OneHotEncoding will take care of that but you will have two columns that are the exact opposite of each other. So that is a bit redundant. I think it's better just to convert it to a Boolean column with ones and zeros.
2167	By the end of the project, is it possible to do a project simulating a real workplace (teams deciding the approach, closely simulating a work environment)?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
2168	By the end of the project, is it possible to do a project simulating a real workplace (teams deciding the approach, closely simulating a work environment)?	Yeah. If it works for you, use it. I don't mind.
2169	By the end of the project, is it possible to do a project simulating a real workplace (teams deciding the approach, closely simulating a work environment)?	"I'm happy to support you if you want to organize that. In DataTalks.Club, we have so many initiatives that it will be very difficult for us to actively organize this. But if you want to do this, I can give you all the support you need. I can promote this, I can send it in the mailing list, I can announce this on LinkedIn. So I will be very happy to organize that, but you will need to think of what kind of project it could be. We can actually think together. If you want, you can start putting some of these ideas in a Google document and then we can discuss this and run it. But somebody needs to start this initiative. 
I, unfortunately, cannot do this – but I will be very happy if we do this together. We can think of some cases, actually. One of the examples somebody shared in these Office Hours was to train a model that could select the best answer from the FAQ. We have a lot of answers here and many people go to Slack to ask a question. Many of these questions are already in this FAQ document. So a good “close to real-life” project could be to build a system that can actually select the best answer from the FAQ for a question in Slack. It could be a nice idea – if you’re interested, let me know and we can see how to organize that. Or maybe you have some other ideas."
2170	What's your say on the future of Google Vertex AI and Dataform?	Okay, what is Dataform? Manage data pipelines in BigQuery. The page looks good. [chuckles] But apart from that, I cannot tell you more. Future of Google Vertex AI? Well, Google Vertex AI seems quite cool. It simplifies many things. It has a future, but also, I haven't used it a lot. But I can compare it with SageMaker. It's a similar thing on AWS. With SageMaker, you can start using machine learning quite quickly. But then at some point, it becomes restrictive. There is only a certain way of doing things. I use SageMaker a lot and then, over time, I realized that I like the approach with Flask and Kubernetes a lot more, for example. But yeah, they look quite good and I think it's worth learning them if you want to do things fast. Why not?
2171	What's your say on the future of Google Vertex AI and Dataform?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
2172	What's your say on the future of Google Vertex AI and Dataform?	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
2173	What's your say on the future of Google Vertex AI and Dataform?	"Very, very often. This is probably the model that I use the most at my work. This is something I use pretty much for every project – for some projects. Not only is it a good first baseline, but also sometimes we just deploy this logistic regression, it works and then no further work is needed and we just leave it there. Sometimes these models are then improved with something like XGBoost or something, but logistic regression is like the workhorse of machine learning. It's used in many, many, many stations. 
Actually, in some situations, it's not really possible to use anything else except logistic regression. For example, at the previous company where I worked, which was an advertisement company, and there, it was very, very, very, very important to be able to make predictions very fast. Logistic regression is the best model for that. You cannot beat the speed of logistic regression with any other model. Maybe with a decision tree, but it will not be as good as logistic regression. Logistic regression works really well when you have a lot of features and this was the case in the company where I worked. It's really an important and useful model."
2174	What's your say on the future of Google Vertex AI and Dataform?	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
2175	Is Google Cloud Platform mandatory?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
2176	Is Google Cloud Platform mandatory?	No, you cannot.
2177	Is Google Cloud Platform mandatory?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
2178	Is Google Cloud Platform mandatory?	"Alexey
Google Cloud Platform is not mandatory. You can do most of the course without a cloud. You will not be able to use BigQuery, obviously, because it's a cloud offering. But you can use local postgres instead of that and I think you can run almost everything locally. For example, for streaming, instead of using Confluent Cloud, you can use Kafka. You can set it up locally.
Ankush
We also provide Docker images for everything, but the videos will be more on Confluent Cloud. But you can always change that.
Alexey  
I know that there are people from Nigeria, Iran, who cannot register in Google Cloud Platform. You can do things locally."
2179	Is Google Cloud Platform mandatory?	"Alexey
You can create a Python path variable in your .bashrc and it will be there permanently. Just in the same way as you do with PATH. I don't know why we do it like that. Maybe I just didn't mention that. But, of course, you can just put everything you want in .bashrc and this alliance (this code) will be executed every time you launch your virtual machine. This way you can execute these things automatically."
2180	We have AWS, GCP, Azure, which we can work with in data engineering. Is the logic the same in all of them?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
2181	We have AWS, GCP, Azure, which we can work with in data engineering. Is the logic the same in all of them?	No, you cannot.
2182	We have AWS, GCP, Azure, which we can work with in data engineering. Is the logic the same in all of them?	"There are lots of different varieties of approaches here. One thing that you might do is use some kind of data ingestion tool, as I mentioned in the live FAQ discussion – something like Airbyte or Fivetran – if you have lots of data that you're ingesting, that can be a common way to stay on top of that. Or you could do it through a script, kind of like we do here. 
But what you might do is run your agent on a virtual environment in Google Cloud VM, for example, or in AWS, or somewhere else. You could run it in Kubernetes. There are a number of places where maybe you would have that running and have your infrastructure there, so that you were just pulling down the code not to your local machine, but it would just be in your VM. 
If you're actually going and using Pandas, you're putting the data into memory, so you do need to have some scripts that will do that. And you can go directly between Google Cloud Storage and BigQuery with the Prefect GCP library. So that Prefect GCP library that you installed, that you've been using, does have some options in there. I think it's in the BigQuery module to actually get data directly into BigQuery from GCS. So there are a lot of options there. Check out Prefect GCP to see how to do it."
2183	We have AWS, GCP, Azure, which we can work with in data engineering. Is the logic the same in all of them?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
2184	We have AWS, GCP, Azure, which we can work with in data engineering. Is the logic the same in all of them?	"Alexey
Yeah, they're very similar. The services you have on AWS, you'll have exactly the same service, but with a different name in GCP and Azure."
2185	What do you think about plotline (ggplot in Python) for data visualization?	Working, I guess. So yeah – work after courses. That's where I see you.
2186	What do you think about plotline (ggplot in Python) for data visualization?	I have not used it, so I don't know.
2187	What do you think about plotline (ggplot in Python) for data visualization?	"Tim
The best way to reach out to me is in the Bento ML Slack. Usually, you can join our Slack group and then you can just directly message me"
2188	What do you think about plotline (ggplot in Python) for data visualization?	I'm not sure I understand the question. The question is “Why test_train_split splits in two, but here, we split into three parts? For that there is a video in week one that explains why we need three datasets, not two. So you can go check it out. There is a problem called “multiple comparison problem” and that's why we use test/split to account for that. You will see later, in module three – there we can also use SciKit Learn train_test_split to split our original dataset into three parts and this is exactly what we will do.
2189	What do you think about plotline (ggplot in Python) for data visualization?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
2190	Is it acceptable to do homework 2 using Scikit Learn?	"That's like a pretty generic question, so the answer will also be generic. They will look at the skills you have, they will look at the match – how the skills you have match what is needed. I guess that's it. This is what recruiters usually look for in candidates. If you're interested, another thing you can check out is our podcasts. 
We have an interview, Recruiting Data Professionals with Alicja Notowska. Alicja is a recruiter. She was working as a recruiter at Zalando, which is a very large company in Germany and she shared some tips from your point of view of a recruiter and what she looks for when hiring people. So check it out. It’s quite insightful. You will probably find useful things there."
2191	Is it acceptable to do homework 2 using Scikit Learn?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
2192	Is it acceptable to do homework 2 using Scikit Learn?	Yes, but I cannot guarantee that you will arrive at the same answer as without it. To be on the safe side, it’s better to use the materials from the course. Nothing is stopping you from trying it with Scikit Learn as well. If you think it's more useful for you to do it this way, then by all means do it. Again, homework is just for checking your understanding of the course content. You decide what you do with your time. I would recommend first understanding how to implement linear regression with NumPy. Then in week three, when we start covering SciKit Learn, you can just use it from that point on.
2193	Is it acceptable to do homework 2 using Scikit Learn?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
2194	Is it acceptable to do homework 2 using Scikit Learn?	"There was no registration deadline at all for this course. So you can still register. You can still sign up and you can follow the course. You will not be able to submit the homework assignments where the due date is already over because there are already solutions posted, so it doesn't make much sense to submit homework when the solutions are already there. You can follow the course but you will not be able to submit the homework. You can check our frequently asked questions and I do recommend doing this. Please remember the rule – if you have a question, first go check the frequently asked questions, and if you don't find the question there, go and ask in Slack. 
For this particular question, it’s already there. “Don't worry, you can take the course.” Maybe right now it's becoming a little bit more challenging to catch up with everything, but note that you can skip the midterm project. Right now, if you want to catch up with everything, if you skip the midterm project, it's fine. You will be able to do two capstone projects and still get the certificate. Also, you can just take the course in self-paced mode. There is nothing wrong with that either."
2195	Do you need an orchestrator on streaming pipelines?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
2196	Do you need an orchestrator on streaming pipelines?	Yes, you will have to come up with your own individual projects. It's not easy, I know. But we'll help you. We will help you to come up with a project idea. That's actually on purpose because we want every project to be unique. Then it becomes a part of your portfolio when you look for a job.
2197	Do you need an orchestrator on streaming pipelines?	Well, what you can always do is create a new account. Don't forget to unlink your credit card if you want to use the same credit card. Unlink it from the billing from the old account, and then link it with a new one. Or if you have a different credit card, then you can just use that. For example, I use Revolut as a bank and in Revolut, you can create one-time virtual cards. That could be an option, too.
2198	Do you need an orchestrator on streaming pipelines?	No, you cannot.
2199	Do you need an orchestrator on streaming pipelines?	"Alexey
Typically you do not, because in the streaming pipeline, the consumers that you have in streams are reactive. Once there is a message in a queue or in a topic, the consumers consume this message and do something. Then they often put the result to another stream and then there is another consumer that reads from this stream and produces something. The execution is triggered automatically because the consumers are reactive. That's why you don't really need an orchestrator. They just wait for their piece of work to work on (to consume), they consume and then they put the results somewhere. Meanwhile, in batch processing, there should be a way to execute a thing, like a job after a job, or a task after a task, in a particular sequence. That's why we need an orchestrator there. There, it's not reactive. Something needs to execute things in order."
2200	What's the difference between MLOps and ML engineering?	"Alexey
Yeah, we talked about that in Slack, so please check it. I think this happens because some of the columns have missing values and when you use Pandas for converting CSV to Parquet, it reads columns with missing values as double. That's why this happens."
2201	What's the difference between MLOps and ML engineering?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
2202	What's the difference between MLOps and ML engineering?	No, you cannot.
2203	What's the difference between MLOps and ML engineering?	"Alexey
Ankush will probably be a better person to answer that. Maybe ask that in Slack and see what the answer is. From what I know (the way we organize it where my work is) each department or team has its own S3 bucket. Inside this S3 bucket, we have different folders, or prefixes, for different data sources – for different projects, let's say. And then inside each project, there could be different specific data sources. And in these data sources, data is usually partitioned by day, but there could be other partitions, too. So there are partitions inside these folders and then, eventually, there are parquet files. I hope that answers your question. There are probably articles about this. What I described is something that our data engineers did. I did not take part in that. I just use it. Maybe if anyone knows any good articles about that, please share them in Slack."
2204	What's the difference between MLOps and ML engineering?	We have an article about that at DataTalks.Club. If you go to the Articles section, there is an article called Roles in a Data Team. I discuss different roles, including machine learning engineering and MLOps engineering. MLOps is not really a role – yet, at least. Usually, MLOps is a set of tools and best practices for making sure it's easier to productionize machine learning models. Usually machine learning engineers apply these MLOps principles. But sometimes there is a separate role – it can be DevOps, site reliability engineers, or sometimes it's MLOps engineers – whose job is to make sure that the infrastructure for running machine learning projects is there and it's stable, reliable, and so on. For data engineering, the counterpart is DataOps. I think the concepts are pretty similar.
2205	Is it possible to do the course in Scala instead of Python?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
2206	Is it possible to do the course in Scala instead of Python?	Yes, you will have to come up with your own individual projects. It's not easy, I know. But we'll help you. We will help you to come up with a project idea. That's actually on purpose because we want every project to be unique. Then it becomes a part of your portfolio when you look for a job.
2207	Is it possible to do the course in Scala instead of Python?	Our thoughts are that this is indeed an exciting area. This is an in-demand skill and that's why we have a section about analytics engineering in this course.
2208	Is it possible to do the course in Scala instead of Python?	For the homework assignments, we expect that you do them in Python. Prefect, to my knowledge, does not work in Scala. For example, for Spark, you can use Scala. For Kafka, you can use Scala. If you want to use Scala, use it. For your final project, you can use Scala as well, just add as much documentation as possible for people who will review your projects, because they most likely don't know Scala.
2209	Is it possible to do the course in Scala instead of Python?	"Victoria
I think it's also worth putting it as, it’s great, but why is it chosen as a cloud data platform? I saw that a lot of people were also talking about AWS and things like that. There’s even a channel of people that want to use Snowflake instead of GCP. The main reason why we chose GCP is because it has a very generous free tier that's $300. In the case of Snowflake, for example, it’s limited to one month. In the case of GCP it’s until you finish the money – you can use the backups and a lot of people use the virtual machines, and you can use BigQuery. It will cover all of that and you won’t spend anything from your money, which is the idea of the course. That's why we chose BigQuery. 
But the main goal that we have here is that you can apply the knowledge of a cloud data warehouse to any kind of data warehouse. The same with Prefect, you can also apply it to Airflow if you start working at a company that uses Airflow. Because at the end of the day, you should know how a cloud data warehouse and data warehousing works. In the case of specifics of BigQuery to something Snowflake – both our cloud data warehouses from my experience. BigQuery has a different way of connecting – it goes through an API – whereas in Snowflake you use the more well-known SQL. They both have their flavors or SQL. 
Big Query also has differences, for example, they call databases “projects,” they call schemas “datasets,” and things like that. So it changes terminology. They also have a different approach on cost. Something like, select, start, limit 10, for a BigQuery table could cost you a lot of money. But they tell you before that, how many bytes they'll scan and things like that. It’s just the smaller things, I would. Ankush, you're also very familiar with BigQuery. I've always used Snowflake or Redshift more, or Microsoft.
Ankush
I think the biggest reason for choosing BigQuery was that it just comes with Google Cloud Platform and it's free to use. That's one thing. I think, overall, Snowflake is generally more expensive than BigQuery, at least in my experience. But, of course, they’re super similar. If you look at Big Query optimizations, it's the same concept in Snowflake. 
The concepts, as Victoria said, are applicable to both the data warehouse solutions. It's just a flavor that we chose just because it's easy to have something like an inbuilt data warehouse where you actually start something on your own. This was one of the big reasons for choosing BigQuery.
Alexey
You're free to use AWS if you want, as well. I think that is also a channel where people want to use AWS for the course. We, unfortunately, will not be able to give you a lot of support there. Because with DBT, for example, I don't know how well it can connect to Redshift or Athena or whatever.
Victoria
Somebody is saying something in the chat. I think I was misunderstood. It says $300 is not a small amount. That's the amount of credits you get for free. You will not pay $300. The idea is that we chose GCP so you don't pay anything. You don't pay anything! [chuckles] 
Alexey
Yes, that's the main advantage. What you need is a Google account – you use it for registering at Google Cloud Platform. They will ask you for a credit card, but only to verify that you're a real human. This card is not used for a different Google Cloud Platform account. They will not charge you anything. To me, Google is trustworthy. I think you can trust it with your credit card. I mean, so far they didn’t violate the trust I have in them. It's a good deal, basically. You should take it. 
With AWS, most of the content we talk about – most of the things you will learn here about Google Cloud Platform – are easily transferable to AWS, like all these virtual machines, object storage, Spark. All these things work fine. In AWS, the buttons you need to click are different, or the Terraform script you need to write is different. But at the end, most of the concepts are still the same. You just need to map from Google Cloud Platform concept to AWS concept. But with AWS, you need to pay something."
2210	Is it possible to use Prefectural Spark or Google Dataproc?	"Jeff
We do have a Spark integration through a couple things. Let's look here in the collections again. We have an integration for you that you could use with Databricks. Spark often runs with Databricks. That's a good option there. Fugue is another tool that could be used. It unifies a number of different ways you could run code, but includes Spark and Dask and Ray also. So that could be a good tool. And if you wanted to write Spark code directly and use it with Prefect, you could integrate with something – we don't have something built out right there, but I think folks do kick off Spark jobs with Prefect. If you want to do things in a couple different ways, there are options there. We don't have a direct Dataproc module in Prefect GCP, or a direct block. But surely, there's an API there, and you can probably kick things back and forth."
2211	Is it possible to use Prefectural Spark or Google Dataproc?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
2212	Is it possible to use Prefectural Spark or Google Dataproc?	Yes. As I’ve already said during the launch stream, you can start applying for jobs right now. You don't actually have to complete the course to find a job. Maybe you already know enough for you to convince an employer to hire you. By that, I mean you already know how to program, you know the command line, and things like that. It will be enough – it will be possible – and many people from the previous iteration of the course did that. They found a job. Some of them, not even as juniors.
2213	Is it possible to use Prefectural Spark or Google Dataproc?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
2214	Is it possible to use Prefectural Spark or Google Dataproc?	No, you cannot.
2215	What is the difference between calling a flow from a flow versus a function defined with @subflow?	"Alexey
Yeah, that's not going to happen. I think ChatGPT will help us do our job, but it will not replace people, at least in my opinion. But I use it in my work and it's helpful. You should try using it too – if you want, of course."
2216	What is the difference between calling a flow from a flow versus a function defined with @subflow?	"Ankush
I think if it's about finding a job, spend time learning AWS, because… come on – everybody's using AWS and you have a higher probability of finding a company which will have AWS. After that choose GCS or Azure or whatever you feel. But I think the main point is to learn the concepts. For example, let's talk about data warehouses for partitioning and clustering. In Prefect, it’s how to create your DAGs in particular structure or paralyzing your flow. I think if you learn this concept, it doesn't really matter which cloud platform you use at the end of the day, because all of their services will be pretty similar. I think once you grab the basic concepts and your base layer is good, it's very easy to adjust to both of the cloud environments. I have personally worked on AWS and Google Cloud. I've also worked a bit with Snowflake. Once you can get the ideas, or the concepts, it's really easy to make that switch.
Alexey
I'll add that my experience with Cloud is mostly AWS. Not mostly, I think, it’s only AWS. For this course, I used GCP for the first time and it wasn't that difficult. The UI is different. I actually think GCP has a better UI, in my opinion. It’s more intuitive. The only thing I needed to figure out was, “In AWS this thing is called this way. How was it called in GCP?” Then I would just Google it, find it, and then just use it. Most of the time, it was like that.
Ankush
I have a follow up question. What if ChatGPT is only integrated in Azure? Then what do we do? 
Alexey
Well… I'll have to use Azure. [chuckles]
Ankush
So we’ll need to migrate? [laughs]"
2217	What is the difference between calling a flow from a flow versus a function defined with @subflow?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
2218	What is the difference between calling a flow from a flow versus a function defined with @subflow?	"Jeff
I'm not sure exactly what's being referenced here. But I'm guessing maybe the GitHub question from the homework – the GitHub block question turned out to be a little trickier than anticipated for folks. This is a great question. I got a couple of things on it here. One is that there is some stuff in the documentation, but maybe not a complete handhold of things. The storage doc does talk about code storage and flow storage in general here, and a little bit about how to do it. 
If you would like more detailed information, it’s always a great idea to look and think about me opening an issue. If you go to the GitHub repository for Prefect, you can click on “new issue”. If you want, you can propose a feature enhancement, if you want to call it that, or report a bug if you think you should have more information, and then put in what you would like to see. 
If you want to go one step further at something I really recommend, even like the next extension of Learning in Public is contributing to projects. This is an open source project here. You're welcome to contribute to it. You could go ahead and fork the repository, clone it down, and make a change to the documentation once you've figured things out there. Lots of lots of folks I know did get there with some support, through the FAQ and through Slack. It's partly what the videos are also for, in the course, to help you out as you're going. That's what I would suggest there. 
But I will also suggest to our docs folks, that we perhaps enhance the GitHub section in particular."
2219	What is the difference between calling a flow from a flow versus a function defined with @subflow?	"Jeff
We don't have an @subflow decorator, so you wouldn't use that. But if you use some flow-decorated function and call another function with that flow also decorated, that's just kind of the way we use subflows. There is also another pattern where you can run deployments. You can call a function to run deployments called run_deployment from within a flow. That lets you just, “Alright, I want to run a bunch of things here. Maybe I have them in a loop or something.” That's a pretty cool way to go."
2220	Which companies could you recommend to apply if we successfully finish the courses?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2221	Which companies could you recommend to apply if we successfully finish the courses?	Just take your time. Go through the materials at your own pace. If we talk about the syllabus, you will only need the second, third, fourth – up to the midterm project. This should be sufficient – up to the seventh module. This should be sufficient to finish the midterm project and the capstone project. You can treat the modules after 7 as extra. They are useful – they are very useful. But if you're really short on time, you can just skip them and use the materials from the first seven modules to complete your capstone project. Once you complete your capstone project, once you must submit everything you did, then you can take the modules after 8 at your own pace. That could be one approach. Another approach could be just taking them without caring about the certificate. Just focus on learning and do things at your own pace. Don't rush.
2222	Which companies could you recommend to apply if we successfully finish the courses?	"You probably mean for logistic regression, right? For logistic regression, it's doing something very similar to what we saw in module 2. It is trying to minimize the… (I'm just trying to figure out how I can explain it without going into too many details and not to get lost myself and not to confuse you). It's probably a good idea to refer to some other course. I think I partly talked about this. Let me check. Here I talk a little bit about this, in ML Zoomcamp Office Hours week #4, where I explain a little bit how exactly it works. But it's a very superficial, let's say, overview of exactly what's happening there. 
If you want to learn in more detail how exactly the process works there, maybe you can just go to Google and search for “gradient descent, logistic regression,” or something like this. The first result will probably have a good explanation of what's happening under the hood. SciKit Learn does not use this exact method – it doesn't use gradient descent – it uses something more complex (more advanced) than that. But this is roughly what's happening under the hood. I hope you're satisfied by this answer. I don't think I can give you a better one with the time we have. It will be like a separate lecture. If you don't want to go into details, you can think of this as something that’s similar to logistic regression happening there but we add sigmoid on top of that."
2223	Which companies could you recommend to apply if we successfully finish the courses?	"“CoursES,” plural? I don't know which courses you mean. Probably, I assume you mean the ML Zoomcamp? Maybe other Zoomcamps too? Actually, the answer is the same – if you like the company, just apply. Apply to companies that you like and that's it. I don't feel that there should be any specific answer to this question. It’s up to you. 
Look at the companies that are hiring around you in the country where you live or companies that are hiring remotely – look if they have positions that you like and apply for these positions. It’s as simple as that. Maybe you wanted to hear specific company names. But, I don't know."
2224	Which companies could you recommend to apply if we successfully finish the courses?	"We don't touch databases here. It's “semi-end-to-end,” if you will. [chuckles] If we think about the process for data projects – first, we have “business understanding” and “data understanding”. This course focuses more on this part:

Partly data preparation, and partly evaluation – maybe more deployment. There’s a focus on modeling and deployment. I would say that the focus is more on Data Preparation, Modeling and Deployment.

For example, Data Understanding and Data Preparation would be data engineering.

And then Evaluation and Deployment would be MLOps.

And then business understanding is more related to product management. There is also a thing called AI product management or ML product management, where they talk more about how exactly the process should look like and so on."
2225	Google considers MLOps as a part of data engineering. What do you think about this?	"Victoria
I think that goes a little bit to the fact that almost none of us except Ankush are data engineers. We're all capable, or we think we're capable, of teaching something that is relevant to data engineers. It's the case where every company has their own world, or roles, and they set it differently. Ankush, you could even share that, at one company you work with DBT, and then at another company that was being done by someone else, right?
Ankush
Yeah, exactly. Completely, completely different there. Data engineering in its own self has become a very gray area, honestly. There is also a lot of DataOps there, there is a lot of MLOps there, ML engineering is also considered somehow data engineering. The only thing left is data science and I'm pretty sure that will also become data engineering sooner or later. [chuckles] But this is very specific to Google or very specific to a company you're working for. 
I’ve worked in a company where data engineers do mostly SQL and DBT. There are companies where data engineers are supposed to do MLOps, DataOps, and data engineering. And obviously, others which just focus on data engineering and classify them as streaming data engineers and batch data engineers. It's really very specific to the company you're talking about."
2226	Google considers MLOps as a part of data engineering. What do you think about this?	"Alexey
No. I mean, we kind of told you. Yes, you maybe didn't see the intro video. But, sorry. No, it's late. We will actually recalculate the points for the second homework (HW 1B) because in the forum, it said that you can get up to one point, but we calculated that the cap there was seven. We are going to recalculate that. Thus, people who got a lot of points will get just one. Sorry about that. But don't focus on points. If you go to the leaderboard, we know that the person with this hash (722366d8b29ece9be3a7605363562c7c60d6918e) did seven links. Do you know who that is? Probably you don't. 
These points are virtual. It's nice to have a good score, but remember that these points mean whatever you want them to mean – not more than not less. Maybe it's okay if you get a few points less than others for the first homework, but the important thing here is consistency. If you just do your homework, even without doing any Learning in Public stuff, which I think you should do (you should post about the course). If you don't do this and you just do all the homework, by the end, you will be somewhere in the top 20. If you look at the leaderboard from the previous iteration, you will see that not everyone stuck around till the end. 
If you just do your homework, don't worry about these points, and you'll be fine. Sorry about that. I know that it can be discouraging that you didn't get as many points as others, but don't worry about that. Focus on learning. Maybe Michael has some other suggestions or some other things that I didn't mention, because he took part last year. But I think he was one of the people who did a few posts every week, right?
Michael
Yes. Actually, when I went through it, I missed the whole… you can get up to seven points, but I was just doing one every week and I just stuck with that. I was still near the top. I think the real value in that isn't necessarily the points but just being comfortable sharing – writing articles and making videos. That's the real value there. But everyone likes the clout value, too. As Alexey said, it will fluctuate, so if you keep up with it going forward, you'll probably be just fine."
2227	Google considers MLOps as a part of data engineering. What do you think about this?	BigQuery is a data warehouse and it's optimized. It's usually faster. AWS Athena is a data lake. You will see the difference in week 2 – there is a video that explains what a data lake is. Then in week 3, you will see what the data warehouse is. But AWS Athena is more like a data lake. You can still run all these queries, but maybe they will be slower and I also think they will be cheaper. For analytical queries that – for queries where you want to get results quickly, I usually use BigQuery. Again, like these are two different clouds, two different technologies. The counterpart of BigQuery would be Redshift. But I think Redshift is slower than BigQuery as well.
2228	Google considers MLOps as a part of data engineering. What do you think about this?	No, you cannot.
2229	Google considers MLOps as a part of data engineering. What do you think about this?	I would say to focus on projects, not on tools. In this course, you will get a chance to do this. If you open a job description for data engineers, you will see a lot, a lot, a lot of different technologies. Maybe think of a project that you can do for this particular company and do this project using, for example, the tools you learn here or the tools you want to learn, and build a portfolio. I guess this is more of a tip for getting a data engineering job, rather than for somebody who's starting their data engineering career. But if you already have a job, then maybe think of the business impact of what you need. I think I will not spend too much time talking about that because this requires a one hour long discussion. Perhaps we should have a podcast interview about that. If you can think of any potential guests for this podcast interview, you can let me know and we can try to invite these people and talk about this in more detail. One thing I want to share with you is that in our YouTube channel, we had a few interesting talks. We had this Getting a Data Engineering Job with Jeff talk. It’s a very nice presentation. Then we also had an interview with Jeff. So these two podcasts are quite interesting. You can also check out other ones. I think you'll find a lot of interesting content here.
2230	Adding to the DS and DE jobs in Germany – do you need to know German?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
2231	Adding to the DS and DE jobs in Germany – do you need to know German?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
2232	Adding to the DS and DE jobs in Germany – do you need to know German?	"Alexey
It is quite important. I wouldn't say it's a must for all the projects. For example, if you use something like a tree-based model, then you shouldn't worry about scaling. I think I shared links to our Office Hours from the previous year. If you go to cohorts, and the previous year, and then you go to midterm project – in week ten, we show you how to use SciKit Learn pipelines. This is an amazing concept. So far, what we've been doing is Dictionary Vectorizer (dict_vectorizer) and then it was followed by a model. In the case of Bento, we needed to save this Dictionary Vectorizer separately, as an artifact (as a binary) and then we needed to load this and then we needed to apply the model. But with a pipeline, you can actually train a pipeline and just use BentoML to save the pipeline. It can be saved, right Tim? You can just take a pipeline and do BentoML.sklean.save_model and then the entire pipeline will be saved. Right? [Tim agrees] This is a cool thing. I don't have an example, but you can include this feature scaler here in your pipeline. There should be an example somewhere. I think we did that last year. There is an example without the pipeline. It should be here – classification, and then notebooks scaling this one. In this notebook, we show how to scale your features with standard scaling, or min/max scaler. If you check Office Hours #3 from last year I spoke more about that. It is an important topic. We intentionally did not include this because we cannot include everything, but this is an important topic. I recommend that you check it out. And using pipelines is cool because you will not need to save all this Scaler, Dictionary Vectorizer – all these things separately. Let's say with Bento, you can just take one pipeline, save it with Bento, and then you will have one binary on model. It makes things easier.
Tim 
And if you do have any kind of custom Tokenizer, or Scaler, or Vectorizer – you can always add it to that “custom objects” field.
Alexey
We talked about MLflow. It's very annoying, to be honest. With MLflow, I need to choose to save this in a file. Then I have a Pickle file and I tell MLflow, “Okay, there is an artifact that is already in this file that I want to upload together with the model.” So I upload it together with the model and then when I want to use it, I need to download this from the registry, then I need again to use Pickle to load this into the memory. There is too much overhead. With Bento, I really like how easy it is to save extra stuff.
Tim
Right. In our first version of BentoML, we were just saving models but then we had so many users saying “Well, how do I save my Tokenizer along with the model?” A lot of the time, the Tokenizer, the Vectorizer, is sort of one-to-one with the model. You may have a version of that as well. So it's nice to be able to version it with the model."
2233	Adding to the DS and DE jobs in Germany – do you need to know German?	I am ashamed to admit that I don't speak German that well. I wouldn't be able to use German for work. Plus I've been in Germany for quite a long time, too. That's why I'm kind of ashamed to admit that. [chuckles] But you can live quite well in Berlin. Here, I’m talking about Berlin – the rest of Germany is different – but in Berlin you can get around without speaking a word of German. I do speak some German but I'm not fluent enough to actually use German at work. Maybe if I had to use German at work I would become more fluent, but so far, most jobs don't require German.
2234	Adding to the DS and DE jobs in Germany – do you need to know German?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
2235	Following week 3 videos, but whenever I try creating a table, I get an error related to data types (for example, “passenger count does not match double type”). Any tips?	"Alexey
No. I mean, we kind of told you. Yes, you maybe didn't see the intro video. But, sorry. No, it's late. We will actually recalculate the points for the second homework (HW 1B) because in the forum, it said that you can get up to one point, but we calculated that the cap there was seven. We are going to recalculate that. Thus, people who got a lot of points will get just one. Sorry about that. But don't focus on points. If you go to the leaderboard, we know that the person with this hash (722366d8b29ece9be3a7605363562c7c60d6918e) did seven links. Do you know who that is? Probably you don't. 
These points are virtual. It's nice to have a good score, but remember that these points mean whatever you want them to mean – not more than not less. Maybe it's okay if you get a few points less than others for the first homework, but the important thing here is consistency. If you just do your homework, even without doing any Learning in Public stuff, which I think you should do (you should post about the course). If you don't do this and you just do all the homework, by the end, you will be somewhere in the top 20. If you look at the leaderboard from the previous iteration, you will see that not everyone stuck around till the end. 
If you just do your homework, don't worry about these points, and you'll be fine. Sorry about that. I know that it can be discouraging that you didn't get as many points as others, but don't worry about that. Focus on learning. Maybe Michael has some other suggestions or some other things that I didn't mention, because he took part last year. But I think he was one of the people who did a few posts every week, right?
Michael
Yes. Actually, when I went through it, I missed the whole… you can get up to seven points, but I was just doing one every week and I just stuck with that. I was still near the top. I think the real value in that isn't necessarily the points but just being comfortable sharing – writing articles and making videos. That's the real value there. But everyone likes the clout value, too. As Alexey said, it will fluctuate, so if you keep up with it going forward, you'll probably be just fine."
2236	Following week 3 videos, but whenever I try creating a table, I get an error related to data types (for example, “passenger count does not match double type”). Any tips?	If it's a question for me, specifically – in Windows, I use a tool called Windows Terminal. This is the tool I use. In the settings, they have Git Bash, the usual command prompt, then they have multiple subsystems for Linux terminals. There also used to be PowerShell. I don't use PowerShell, so I don't have it here. This is what it looks like. I really like this. It’s just called Windows terminal. Check it out.
2237	Following week 3 videos, but whenever I try creating a table, I get an error related to data types (for example, “passenger count does not match double type”). Any tips?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
2238	Following week 3 videos, but whenever I try creating a table, I get an error related to data types (for example, “passenger count does not match double type”). Any tips?	"Alexey
Google Cloud Platform is not mandatory. You can do most of the course without a cloud. You will not be able to use BigQuery, obviously, because it's a cloud offering. But you can use local postgres instead of that and I think you can run almost everything locally. For example, for streaming, instead of using Confluent Cloud, you can use Kafka. You can set it up locally.
Ankush
We also provide Docker images for everything, but the videos will be more on Confluent Cloud. But you can always change that.
Alexey  
I know that there are people from Nigeria, Iran, who cannot register in Google Cloud Platform. You can do things locally."
2239	Following week 3 videos, but whenever I try creating a table, I get an error related to data types (for example, “passenger count does not match double type”). Any tips?	"Alexey
Yeah, we talked about that in Slack, so please check it. I think this happens because some of the columns have missing values and when you use Pandas for converting CSV to Parquet, it reads columns with missing values as double. That's why this happens."
2240	Is it possible to make workflow orchestration tools serverless so that I do not need to host it, I just need to pay when flows run?	No, you cannot.
2241	Is it possible to make workflow orchestration tools serverless so that I do not need to host it, I just need to pay when flows run?	"Alexey
I know that in AWS, they have step functions, which is a serverless workflow orchestrator. It’s fine if it works for you.
Jeff
Yeah. Prefect has some serverless options too. Depending on your cloud provider, we support almost all of them. Here's a link to a post that Anna wrote. There are other patterns, too. We are working on something where – right now, you have to put your agent somewhere, so you could put your agent even on ECS, potentially. But we're looking at a managed solution where if someone just doesn't want to hassle with it, we can just deploy things for them using Prefect Cloud. But that's a few months off, probably. 
Alexey
Here, what does this project do? It says streaming. That's something? 
Jeff
Yeah, this one does a number of different things. If I recall, it's using Cloud Functions and it's using GitHub Actions, to do a CI/CD kind of deployment. There are a couple things going on there. But Anna has a whole repository and a walkthrough in that blog post. That's just one example of doing it.
Kalise
Essentially, what it's gonna do is – the same way in the videos, how you can run a Prefect flow just by calling a Python function, you can run that wherever. And if you're connected to cloud, you're going to be able to visualize that. You're not going to have to host the UI, the Orion server. So if you had some sort of event happening and you just wanted to execute your Python function as a flow to get visibility, you can just do it right there. 
Alexey  
I think in GCP, there is also something similar to step functions."
2242	Is it possible to make workflow orchestration tools serverless so that I do not need to host it, I just need to pay when flows run?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
2243	Is it possible to make workflow orchestration tools serverless so that I do not need to host it, I just need to pay when flows run?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
2244	Is it possible to make workflow orchestration tools serverless so that I do not need to host it, I just need to pay when flows run?	Yes, you will have to come up with your own individual projects. It's not easy, I know. But we'll help you. We will help you to come up with a project idea. That's actually on purpose because we want every project to be unique. Then it becomes a part of your portfolio when you look for a job.
2245	Any plan for Prefect to enhance their documentation? The docs didn't show examples of how to create it. I need to read the code of the class instead.	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
2246	Any plan for Prefect to enhance their documentation? The docs didn't show examples of how to create it. I need to read the code of the class instead.	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
2247	Any plan for Prefect to enhance their documentation? The docs didn't show examples of how to create it. I need to read the code of the class instead.	No, you cannot.
2248	Any plan for Prefect to enhance their documentation? The docs didn't show examples of how to create it. I need to read the code of the class instead.	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
2249	Any plan for Prefect to enhance their documentation? The docs didn't show examples of how to create it. I need to read the code of the class instead.	"Jeff
I'm not sure exactly what's being referenced here. But I'm guessing maybe the GitHub question from the homework – the GitHub block question turned out to be a little trickier than anticipated for folks. This is a great question. I got a couple of things on it here. One is that there is some stuff in the documentation, but maybe not a complete handhold of things. The storage doc does talk about code storage and flow storage in general here, and a little bit about how to do it. 
If you would like more detailed information, it’s always a great idea to look and think about me opening an issue. If you go to the GitHub repository for Prefect, you can click on “new issue”. If you want, you can propose a feature enhancement, if you want to call it that, or report a bug if you think you should have more information, and then put in what you would like to see. 
If you want to go one step further at something I really recommend, even like the next extension of Learning in Public is contributing to projects. This is an open source project here. You're welcome to contribute to it. You could go ahead and fork the repository, clone it down, and make a change to the documentation once you've figured things out there. Lots of lots of folks I know did get there with some support, through the FAQ and through Slack. It's partly what the videos are also for, in the course, to help you out as you're going. That's what I would suggest there. 
But I will also suggest to our docs folks, that we perhaps enhance the GitHub section in particular."
2250	When to use Bento ML instead of TensorFlow Serving?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
2251	When to use Bento ML instead of TensorFlow Serving?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
2252	When to use Bento ML instead of TensorFlow Serving?	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
2253	When to use Bento ML instead of TensorFlow Serving?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
2254	When to use Bento ML instead of TensorFlow Serving?	I wish Tim was here and would help me answer that because I don't know. I assume if you wanted to use Bento, you would probably use TensorFlow Lite – you don't want to use the usual TensorFlow for the same reasons that we talked about in the serverless module, because it's simply too large. Probably for Bento, you would use something like TensorFlow Lite and it will still work. Maybe another thing you can check – in our channel, we had a talk from one of the Bento folks, Building an ML Service Platform from the Ground Up. Here, Sean talks about the differences between TensorFlow Serving and Bento. With TensorFlow Serving, it's good – it's quite optimized for deep learning. There are some problems. First of all, it's written in C++ and expects your model in a certain format – this “saved model format”. Adding anything extra on top of that (like pre-processing, post-processing) becomes difficult, because you will need to have an extra step there. Bento makes it easier. But, again, I don't know. It's very difficult to give you a concrete recommendation without knowing what exactly you want to do. So it's case dependent. If you already use TensorFlow Serving in your company, then maybe it makes sense to stick to that. If we have Tim on Office Hours next week, maybe you can ask him that. He will be a better person to answer this question.
2255	Are all the homeworks/projects individual or do we have to form teams?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
2256	Are all the homeworks/projects individual or do we have to form teams?	"Alexey
Please check the FAQ. If you don't find the answer there, ask in Slack. If you get an answer in Slack, please put it into the FAQ."
2257	Are all the homeworks/projects individual or do we have to form teams?	Everything you do here is individual. You don't form teams.
2258	Are all the homeworks/projects individual or do we have to form teams?	Yes, you will have to come up with your own individual projects. It's not easy, I know. But we'll help you. We will help you to come up with a project idea. That's actually on purpose because we want every project to be unique. Then it becomes a part of your portfolio when you look for a job.
2259	Are all the homeworks/projects individual or do we have to form teams?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
2260	GCP is more recommended, but I’m saving it for data engineering bootcamp.	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
2261	GCP is more recommended, but I’m saving it for data engineering bootcamp.	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
2262	GCP is more recommended, but I’m saving it for data engineering bootcamp.	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
2263	GCP is more recommended, but I’m saving it for data engineering bootcamp.	I do sometimes. Not very often these days, but we have quite a few models that require deep learning. Usually, these models are related to images. We have some images on the platform, so when we need to understand what's happening on these images, we use deep learning.
2264	GCP is more recommended, but I’m saving it for data engineering bootcamp.	I guess not. That's the process of learning? Maybe more senior people don't… Yeah, I see where you're going with this. Senior people are not supposed to go bankrupt and default. They are probably supposed to pay back the debt. I guess these are the particularities of this dataset, I assume. Indeed, it's probably a data issue ,or not an issue, but a “characteristic” of this dataset.
2265	I’m working as a Java developer, but I'm much more interested in machine learning. Please suggest the best machine learning course online for me.	You are in the right place. I was a Java developer. I became interested in machine learning and this course is quintessence… basically all my experience and everything I needed to go through, to learn through, and I think that this is how I best learned, as a Java developer, as an engineer – by doing projects. This course was actually targeted at software engineers who wanted to become machine learning engineers. So you are in the best place.
2266	I’m working as a Java developer, but I'm much more interested in machine learning. Please suggest the best machine learning course online for me.	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2267	I’m working as a Java developer, but I'm much more interested in machine learning. Please suggest the best machine learning course online for me.	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
2268	I’m working as a Java developer, but I'm much more interested in machine learning. Please suggest the best machine learning course online for me.	Yeah. If it works for you, use it. I don't mind.
2269	I’m working as a Java developer, but I'm much more interested in machine learning. Please suggest the best machine learning course online for me.	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
2270	Can you paste the link of the taxi data or update the GitHub page?	"There are lots of different varieties of approaches here. One thing that you might do is use some kind of data ingestion tool, as I mentioned in the live FAQ discussion – something like Airbyte or Fivetran – if you have lots of data that you're ingesting, that can be a common way to stay on top of that. Or you could do it through a script, kind of like we do here. 
But what you might do is run your agent on a virtual environment in Google Cloud VM, for example, or in AWS, or somewhere else. You could run it in Kubernetes. There are a number of places where maybe you would have that running and have your infrastructure there, so that you were just pulling down the code not to your local machine, but it would just be in your VM. 
If you're actually going and using Pandas, you're putting the data into memory, so you do need to have some scripts that will do that. And you can go directly between Google Cloud Storage and BigQuery with the Prefect GCP library. So that Prefect GCP library that you installed, that you've been using, does have some options in there. I think it's in the BigQuery module to actually get data directly into BigQuery from GCS. So there are a lot of options there. Check out Prefect GCP to see how to do it."
2271	Can you paste the link of the taxi data or update the GitHub page?	No, you cannot.
2272	Can you paste the link of the taxi data or update the GitHub page?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
2273	Can you paste the link of the taxi data or update the GitHub page?	It’s already there. It has been there for quite some time. So if you go here, there is a note – and this is the link.
2274	Can you paste the link of the taxi data or update the GitHub page?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
2275	How does XGBoost or LightGBM improve the last tree and build a new tree and so on?	Yeah, that's what the lecture was about, wasn't it? I don't go into details about that. The theory is quite complicated there, to be honest. But there is a paper – this one. “Gradient boosting machines, a tutorial” by Alexey Natekin. This article talks about derivatives and the functional space. If you're into this kind of stuff, you can check out how exactly it works. But if you're not, don't worry. You can just skip that and focus on improving your validation score.
2276	How does XGBoost or LightGBM improve the last tree and build a new tree and so on?	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
2277	How does XGBoost or LightGBM improve the last tree and build a new tree and so on?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
2278	How does XGBoost or LightGBM improve the last tree and build a new tree and so on?	"We don't touch databases here. It's “semi-end-to-end,” if you will. [chuckles] If we think about the process for data projects – first, we have “business understanding” and “data understanding”. This course focuses more on this part:

Partly data preparation, and partly evaluation – maybe more deployment. There’s a focus on modeling and deployment. I would say that the focus is more on Data Preparation, Modeling and Deployment.

For example, Data Understanding and Data Preparation would be data engineering.

And then Evaluation and Deployment would be MLOps.

And then business understanding is more related to product management. There is also a thing called AI product management or ML product management, where they talk more about how exactly the process should look like and so on."
2279	How does XGBoost or LightGBM improve the last tree and build a new tree and so on?	I think for binary features, just one and zero makes more sense than OneHotEncoding. OneHotEncoding will take care of that but you will have two columns that are the exact opposite of each other. So that is a bit redundant. I think it's better just to convert it to a Boolean column with ones and zeros.
2280	For experiments, do we use the Conda environment and when we're ready for model deployment, we create another environment with Pipenv for the web app?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2281	For experiments, do we use the Conda environment and when we're ready for model deployment, we create another environment with Pipenv for the web app?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
2282	For experiments, do we use the Conda environment and when we're ready for model deployment, we create another environment with Pipenv for the web app?	"The way I usually do it is I have one Anaconda environment (I don't create a separate Conda environment for all experiments, I just have one Anaconda) [Terminal image as reference] I usually stay in the base environment. All the libraries I need for development are here, so I use them. And when I need to productionize it, then I create an environment with Pipenv. So this is how I usually do this. It doesn't mean that you have to do this the same way or that what I do is the “right” way of doing this. This is just the way I usually do it. 
Some of you might say, “But we need to have an environment from the very beginning.” And you will be right in this case. Ideally, for each project you start, you will start it with creating a fresh virtual environment and you install the packages there. Then you have no surprises when you move from the exploration and training phase to the deployment phase – all your dependencies are the same. But I'm lazy and I just use the environment I have for all the POCs – for all the, let's say, exploration projects. Then, when porting them to production, I create environments."
2283	For experiments, do we use the Conda environment and when we're ready for model deployment, we create another environment with Pipenv for the web app?	"You probably mean for logistic regression, right? For logistic regression, it's doing something very similar to what we saw in module 2. It is trying to minimize the… (I'm just trying to figure out how I can explain it without going into too many details and not to get lost myself and not to confuse you). It's probably a good idea to refer to some other course. I think I partly talked about this. Let me check. Here I talk a little bit about this, in ML Zoomcamp Office Hours week #4, where I explain a little bit how exactly it works. But it's a very superficial, let's say, overview of exactly what's happening there. 
If you want to learn in more detail how exactly the process works there, maybe you can just go to Google and search for “gradient descent, logistic regression,” or something like this. The first result will probably have a good explanation of what's happening under the hood. SciKit Learn does not use this exact method – it doesn't use gradient descent – it uses something more complex (more advanced) than that. But this is roughly what's happening under the hood. I hope you're satisfied by this answer. I don't think I can give you a better one with the time we have. It will be like a separate lecture. If you don't want to go into details, you can think of this as something that’s similar to logistic regression happening there but we add sigmoid on top of that."
2284	For experiments, do we use the Conda environment and when we're ready for model deployment, we create another environment with Pipenv for the web app?	"Tim 
Not really. We have users who use both Streamlit and Gradio. I think Streamlit and Gradio are a bit more about the presentation side. We do have a couple of users who have asked us about deeper Streamlit or Gradio support and I think that's something that's probably coming up in the next few months.
Alexey
For those who don't know what Streamlit or Gradio is – this is a way to create an interface for your services. Well, Gradio is focused on machine learning, meaning you create interfaces specifically for models. And then Streamlit is just a Python package for creating interfaces. We actually had a project at DataTalks.Club about Streamlit. I actually even have a tweet about this. This is what Streamlit looks like. In the video, you can see that you have this interface, and then based on the toggles that you move the core of your model changes. It gives you an easy way to build an interface like that. Meanwhile Bento focuses on quite a different use case, right? It’s not building interfaces, but serving models.
Tim 
Right, yeah. I think one of our users was asking for a really easy way, so that once you have your model and your Streamlit UI to be able to create a Bento deployment with one line in that way, anybody that you gave your Streamlit app to would be calling to that service and it could work anywhere. Which is a nice feature, I think.
Alexey 
Yeah, I think it is. I imagine if you have a Pydentic class there, then you can easily generate a Streamlit app for that.
Tim 
Right. Oh, that's a good idea. I hadn't thought of that. Yeah.
Alexey
And then in addition to that, the Swagger UI that you have, could be like having the actual interface. 
Tim
Oh, yeah, that would be cool. [chuckles]
Alexey
That could be a good Hackathon project, right?
Tim 
Yeah, for sure.
Alexey
From what I see – let's say you’ve built a model. Then you deployed this model and you have this nice API. You can query it with CURL. But what if you want to demo this project to your manager, or a manager of your manager, or somebody who does not necessarily have a technical background? If you give them the URL and say, “Okay, you need to execute the CURL command with a post request,” they would be like, “CURL what? What do you want from me? Just show me how it looks.” This is when tools like Streamlit come in handy. Instead of giving them a command line interface to query it, you just give them “Okay, this is the link. Play with this.” If Bento could do something like that – automatically generate things like that – many data scientists would thank you.
Tim
Yeah, I was just looking because I figured there's a library that turns Pydantic models into Streamlit apps. It looks like there is one that does that. I wonder if we could just plug into that app and then have that same thing. 
Alexey
That's cool. 
Tim
Yeah, that's awesome. That's kind of the cool part of open source – there are so many people out there with so many different projects and we get to collaborate with them and kind of build the best collaborations and combinations of these tools."
2285	Can Prefect be deployed to any Kubernetes platform like GKE or EKS?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
2286	Can Prefect be deployed to any Kubernetes platform like GKE or EKS?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
2287	Can Prefect be deployed to any Kubernetes platform like GKE or EKS?	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
2288	Can Prefect be deployed to any Kubernetes platform like GKE or EKS?	"Jeff
I'm not sure exactly what's being referenced here. But I'm guessing maybe the GitHub question from the homework – the GitHub block question turned out to be a little trickier than anticipated for folks. This is a great question. I got a couple of things on it here. One is that there is some stuff in the documentation, but maybe not a complete handhold of things. The storage doc does talk about code storage and flow storage in general here, and a little bit about how to do it. 
If you would like more detailed information, it’s always a great idea to look and think about me opening an issue. If you go to the GitHub repository for Prefect, you can click on “new issue”. If you want, you can propose a feature enhancement, if you want to call it that, or report a bug if you think you should have more information, and then put in what you would like to see. 
If you want to go one step further at something I really recommend, even like the next extension of Learning in Public is contributing to projects. This is an open source project here. You're welcome to contribute to it. You could go ahead and fork the repository, clone it down, and make a change to the documentation once you've figured things out there. Lots of lots of folks I know did get there with some support, through the FAQ and through Slack. It's partly what the videos are also for, in the course, to help you out as you're going. That's what I would suggest there. 
But I will also suggest to our docs folks, that we perhaps enhance the GitHub section in particular."
2289	Can Prefect be deployed to any Kubernetes platform like GKE or EKS?	"Jeff
Yes, absolutely. You can certainly deploy Prefect to those platforms. We also have a Helm chart that you can use. So you can check that out if you'd like. We do have people who run Prefect in lots of different workspaces, so it’s definitely possible."
2290	If someone asks you “What are the red flags when you see a dataset?” What would be the best way to answer this? This was actually an interview question.	"That's like a pretty generic question, so the answer will also be generic. They will look at the skills you have, they will look at the match – how the skills you have match what is needed. I guess that's it. This is what recruiters usually look for in candidates. If you're interested, another thing you can check out is our podcasts. 
We have an interview, Recruiting Data Professionals with Alicja Notowska. Alicja is a recruiter. She was working as a recruiter at Zalando, which is a very large company in Germany and she shared some tips from your point of view of a recruiter and what she looks for when hiring people. So check it out. It’s quite insightful. You will probably find useful things there."
2291	If someone asks you “What are the red flags when you see a dataset?” What would be the best way to answer this? This was actually an interview question.	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2292	If someone asks you “What are the red flags when you see a dataset?” What would be the best way to answer this? This was actually an interview question.	Well, a few things I always look at are missing values – that's the first thing. Then I also look at min/max values. Sometimes, the missing values are encoded with minus nine and nine or some other values – very low or very high values. You can see these if you do DataFrame.describe. You can also see if you have some categorical data in numerical columns or strings in numerical columns. Things like that. These things come to my mind immediately. Apart from that, it's just usually just doing exploratory data analysis and seeing if anything feels wrong. If something feels wrong, you just try to dig deeper until you understand what's going on there.
2293	If someone asks you “What are the red flags when you see a dataset?” What would be the best way to answer this? This was actually an interview question.	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
2294	If someone asks you “What are the red flags when you see a dataset?” What would be the best way to answer this? This was actually an interview question.	So for the first part “What should be my approach if I'm juggling between both?” Well, I don't know how much time you have. If you don't have a lot of time, I would suggest focusing on one and then come back to the other one. Actually, I need to make an announcement. As the course team from the Data Engineering Zoomcamp, we actually discussed it, and I'm happy to announce that there will be another iteration of Data Engineering Zoomcamp. Maybe what you can do now is just focus on ML Zoomcamp, and then come back to Data Engineering Zoomcamp when we launch it in January. There will be another announcement, but now you can know it. So, yes, we'll have another iteration in January.
2295	Somewhere, while talking about regularization, you mentioned a mathematical book, the name of which starts with “elementary” or “elements.”	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
2296	Somewhere, while talking about regularization, you mentioned a mathematical book, the name of which starts with “elementary” or “elements.”	That's a very similar question to when we should use the lambda and when we should use Kubernetes. If you want to use Kubernetes, then go with TensorFlow Serving. Otherwise, TensorFlow Lite. There is one thing where you have to use TensorFlow Lite – where you cannot use TensorFlow Serving – and this is when you want to deploy your model on a mobile device. I have a phone here. If you want to create an app with a model, then you will have to use TensorFlow Lite because you will not be able to use TensorFlow Serving. TensorFlow Serving also adds a bit of complexity. You will need to have this gateway service, you will need to have GRPC service, so that becomes more complex. If you have a lot of load, then you need to do this otherwise, TensorFlow Lite.
2297	Somewhere, while talking about regularization, you mentioned a mathematical book, the name of which starts with “elementary” or “elements.”	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2298	Somewhere, while talking about regularization, you mentioned a mathematical book, the name of which starts with “elementary” or “elements.”	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
2299	Somewhere, while talking about regularization, you mentioned a mathematical book, the name of which starts with “elementary” or “elements.”	It's “Elements of Statistical Learning.” This is the book. It's actually free. If you click on the link, you can get the book. It's legally free – it's published by the authors for free. Yeah, it's a great book. For example, we talked about the normal equation. It’s called “least squares”. Least squares is what we call a “normal equation”. They start with that. I will not spend time doing this now. But check out this book. It's quite mathematical, but you might like it.
2300	What should we do when there is high bias or high variance to improve the model? What about test error?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
2301	What should we do when there is high bias or high variance to improve the model? What about test error?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2302	What should we do when there is high bias or high variance to improve the model? What about test error?	"You can check out a few projects in this playlist. In the playlist, you can see some of the videos here, for example, there's a project from Alvaro. And it's not just Alvaro, there’s one from Alvaro and Ninad. You can just check out these projects – there’s a project from Carolina and Hamad, a project from Lisa, and a project from Timur. The project from Timor may actually fit what you want. Timur has done a lot of work and it's very interesting. So check it out. Actually, Timur’s project is not a midterm project. It's more like a capstone project – there is some deep learning there, he uses Karis. So maybe focus on the other three. 
You can also go to our course web page, and then go to midterm project (you need to use the 2021 folder) and then in the readme there should be some assignments. There will be some form that says “To find the projects you need to review, use this table.” Here, you can see all the projects from previous students’ midterm projects. You can pick some of them and check if you like them or not."
2303	What should we do when there is high bias or high variance to improve the model? What about test error?	"We don't cover high bias/high variance concepts. Again, check other theoretical courses from Andrew Ng. What you should do when you have errors is reduce the complexity of your models, for example. It really depends, right? Let's say you have a simple model, and you see that the simple model has this performance on your validation dataset. Then you can test that hypothesis, “Will getting a more complex model solve my problem or not?” Then you just try this more complex model and see what happens. Based on that, you make your decision. 
I don't know if I actually answered your question. Maybe a summary of my answer is “use validation and it will guide you in all these decisions.” And test error is, again, if you use your validation dataset many times, then at the end, you can't necessarily trust it because it might happen that the best model is the best just by pure luck. So that's why you have the test dataset to verify that it's not overfitting."
2304	What should we do when there is high bias or high variance to improve the model? What about test error?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
2305	Just to confirm, the best way to follow the content is through the GitHub repo named “Week X.”	No, you cannot.
2306	Just to confirm, the best way to follow the content is through the GitHub repo named “Week X.”	Yes.
2307	Just to confirm, the best way to follow the content is through the GitHub repo named “Week X.”	Yes, you will have to come up with your own individual projects. It's not easy, I know. But we'll help you. We will help you to come up with a project idea. That's actually on purpose because we want every project to be unique. Then it becomes a part of your portfolio when you look for a job.
2308	Just to confirm, the best way to follow the content is through the GitHub repo named “Week X.”	Our thoughts are that this is indeed an exciting area. This is an in-demand skill and that's why we have a section about analytics engineering in this course.
2309	Just to confirm, the best way to follow the content is through the GitHub repo named “Week X.”	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
2310	What if you have no Docker experience?	If it's a question for me, specifically – in Windows, I use a tool called Windows Terminal. This is the tool I use. In the settings, they have Git Bash, the usual command prompt, then they have multiple subsystems for Linux terminals. There also used to be PowerShell. I don't use PowerShell, so I don't have it here. This is what it looks like. I really like this. It’s just called Windows terminal. Check it out.
2311	What if you have no Docker experience?	"Alexey
I guess you refer to week 1, right? You do it in the same way as you do other things. There is one nuance,  one thing that you need to keep in mind, is that your service needs to wait till other things are ready. Perhaps you will need to add a bit of code there for that. If anyone knows a good example, please let us know and share it in Slack."
2312	What if you have no Docker experience?	No, you cannot.
2313	What if you have no Docker experience?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
2314	What if you have no Docker experience?	That's why we discussed Docker in the first week. We actually meant this week as a kind of Docker refresher. In the end, many people didn't have any Docker experience. They took it and it was maybe a bumpy ride, but it was fine. Many people did not have Docker experience and they successfully finished the first week. But it was probably harder than other weeks.
2315	How do I create a link where I can put my code to submit my homework?	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
2316	How do I create a link where I can put my code to submit my homework?	"Alexey
The best way to contribute is to take notes and include them to the notes section. And tell your friends about the course. As a part of your Learning in Public, spread the word. It will help us. And don't forget to give us a star if you have."
2317	How do I create a link where I can put my code to submit my homework?	"Victoria
I think it's also worth putting it as, it’s great, but why is it chosen as a cloud data platform? I saw that a lot of people were also talking about AWS and things like that. There’s even a channel of people that want to use Snowflake instead of GCP. The main reason why we chose GCP is because it has a very generous free tier that's $300. In the case of Snowflake, for example, it’s limited to one month. In the case of GCP it’s until you finish the money – you can use the backups and a lot of people use the virtual machines, and you can use BigQuery. It will cover all of that and you won’t spend anything from your money, which is the idea of the course. That's why we chose BigQuery. 
But the main goal that we have here is that you can apply the knowledge of a cloud data warehouse to any kind of data warehouse. The same with Prefect, you can also apply it to Airflow if you start working at a company that uses Airflow. Because at the end of the day, you should know how a cloud data warehouse and data warehousing works. In the case of specifics of BigQuery to something Snowflake – both our cloud data warehouses from my experience. BigQuery has a different way of connecting – it goes through an API – whereas in Snowflake you use the more well-known SQL. They both have their flavors or SQL. 
Big Query also has differences, for example, they call databases “projects,” they call schemas “datasets,” and things like that. So it changes terminology. They also have a different approach on cost. Something like, select, start, limit 10, for a BigQuery table could cost you a lot of money. But they tell you before that, how many bytes they'll scan and things like that. It’s just the smaller things, I would. Ankush, you're also very familiar with BigQuery. I've always used Snowflake or Redshift more, or Microsoft.
Ankush
I think the biggest reason for choosing BigQuery was that it just comes with Google Cloud Platform and it's free to use. That's one thing. I think, overall, Snowflake is generally more expensive than BigQuery, at least in my experience. But, of course, they’re super similar. If you look at Big Query optimizations, it's the same concept in Snowflake. 
The concepts, as Victoria said, are applicable to both the data warehouse solutions. It's just a flavor that we chose just because it's easy to have something like an inbuilt data warehouse where you actually start something on your own. This was one of the big reasons for choosing BigQuery.
Alexey
You're free to use AWS if you want, as well. I think that is also a channel where people want to use AWS for the course. We, unfortunately, will not be able to give you a lot of support there. Because with DBT, for example, I don't know how well it can connect to Redshift or Athena or whatever.
Victoria
Somebody is saying something in the chat. I think I was misunderstood. It says $300 is not a small amount. That's the amount of credits you get for free. You will not pay $300. The idea is that we chose GCP so you don't pay anything. You don't pay anything! [chuckles] 
Alexey
Yes, that's the main advantage. What you need is a Google account – you use it for registering at Google Cloud Platform. They will ask you for a credit card, but only to verify that you're a real human. This card is not used for a different Google Cloud Platform account. They will not charge you anything. To me, Google is trustworthy. I think you can trust it with your credit card. I mean, so far they didn’t violate the trust I have in them. It's a good deal, basically. You should take it. 
With AWS, most of the content we talk about – most of the things you will learn here about Google Cloud Platform – are easily transferable to AWS, like all these virtual machines, object storage, Spark. All these things work fine. In AWS, the buttons you need to click are different, or the Terraform script you need to write is different. But at the end, most of the concepts are still the same. You just need to map from Google Cloud Platform concept to AWS concept. But with AWS, you need to pay something."
2318	How do I create a link where I can put my code to submit my homework?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
2319	How do I create a link where I can put my code to submit my homework?	Put your code to GitHub. Let's say I have my code with my homework. Let's say your homework is in this directory. Copy this one and put it in the form. That's how you do this.
2320	Since Prefect is newer in comparison to Airflow, is it being used in the industry particularly by large/mid-scale companies?	No, you cannot.
2321	Since Prefect is newer in comparison to Airflow, is it being used in the industry particularly by large/mid-scale companies?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
2322	Since Prefect is newer in comparison to Airflow, is it being used in the industry particularly by large/mid-scale companies?	Well, what you can always do is create a new account. Don't forget to unlink your credit card if you want to use the same credit card. Unlink it from the billing from the old account, and then link it with a new one. Or if you have a different credit card, then you can just use that. For example, I use Revolut as a bank and in Revolut, you can create one-time virtual cards. That could be an option, too.
2323	Since Prefect is newer in comparison to Airflow, is it being used in the industry particularly by large/mid-scale companies?	I don't have an answer to that. I think all companies use it. It doesn't matter what size the company is. But maybe the fraction of companies that use Prefect is still smaller compared to Airflow.
2324	Since Prefect is newer in comparison to Airflow, is it being used in the industry particularly by large/mid-scale companies?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
2325	I'd like to review more than three projects. Is this welcomed and will it be rewarded with extra points?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
2326	I'd like to review more than three projects. Is this welcomed and will it be rewarded with extra points?	"You're more than welcome to do this, and your peers will appreciate it. But we capped the number of points you get for that at three projects. So you will not get more than three points for each project. You will get nine points for evaluating the projects, but you will not get 12 points. So it's more than welcome – you will learn a lot by doing this – but you will not get extra points. 
You shouldn't evaluate the projects just to get the points, you should evaluate them because you want to help your peers and you want to learn something new. This is a very good motivation and you will be rewarded with new knowledge. This is how I should have answered that. [chuckles] Not with points."
2327	I'd like to review more than three projects. Is this welcomed and will it be rewarded with extra points?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
2328	I'd like to review more than three projects. Is this welcomed and will it be rewarded with extra points?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
2329	I'd like to review more than three projects. Is this welcomed and will it be rewarded with extra points?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2330	How can linear regression predict anything at all? For example, what if there are two features which are strongly correlated (district and price for m2)?	"Yeah, that's an interesting question. This is something we do not cover in detail in this course. Again, maybe (if you want) you can refer to this Andrew Ng course for more theory. Here we don't cover a lot of theory – we focus more on practice. But from the theoretical point of view, linear regression is trying to optimize the so-called “loss function” and it's doing it by using some mathematics. A normal equation that we saw is one of the ways of finding the minimum for this loss function. Basically, the weights we get at the end are the best in terms of minimizing this loss function. The weights that we get, they make sure that this loss function is minimal. I don't know if it makes sense to you or not, but there is some mathematical foundation behind this. There is an explanation of why it happens this way. 
If you want to find out more about this, maybe you can check about a thing called “gradient descent,” which explains the loss function – we move down by tweaking our weights in such a way that, when we arrive at the end, the weights we have in our model, they are the best ones with respect to this particular loss function. I think we slightly cover loss functions minimization – I think maybe in neural networks. I don't remember. Here, we focus more on practical aspects. You kind of put some trust into the idea that the model gives you a good enough estimate, and then use it from a practical point of view. But if you want to learn more theory, then I think there is no better course than this one – Machine Learning by Andrew Ng. From my point of view, that was one of the best courses I took when it comes to theoretical machine learning."
2331	How can linear regression predict anything at all? For example, what if there are two features which are strongly correlated (district and price for m2)?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
2332	How can linear regression predict anything at all? For example, what if there are two features which are strongly correlated (district and price for m2)?	"You can hypothetically think about all the situations, but what I would do in your case is just set up a validation strategy and then test all your assumptions. Then, based on the performance of the model and the validation set, you can see if it's performing or not. It doesn't matter if there are correlated features, non-correlated features – you see the performance metric, (RMSE, in this case) and if this metric is good, you just go ahead and use this model. I hope that answered this question. 
If you want to have some sort of theoretical justification for how it works, then perhaps you should check some theoretical textbooks, like Elements of Statistical Learning or some other courses that focus on theory. Here, I want to show you how to use these things from a practical point of view and that's why I put so much emphasis on setting up this validation strategy. Because I think this is one of the most important things in machine learning. If you can reliably test your hypothesis, then you can do pretty much everything you want – anything you can imagine – you just go and try it on your validation dataset. And if it works, it works – you can just use it."
2333	How can linear regression predict anything at all? For example, what if there are two features which are strongly correlated (district and price for m2)?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
2334	How can linear regression predict anything at all? For example, what if there are two features which are strongly correlated (district and price for m2)?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2335	Do you feel any feature is missing in Prefect that is available in other tools like Airflow, Dagster, etc.?	"Ankush
I think if it's about finding a job, spend time learning AWS, because… come on – everybody's using AWS and you have a higher probability of finding a company which will have AWS. After that choose GCS or Azure or whatever you feel. But I think the main point is to learn the concepts. For example, let's talk about data warehouses for partitioning and clustering. In Prefect, it’s how to create your DAGs in particular structure or paralyzing your flow. I think if you learn this concept, it doesn't really matter which cloud platform you use at the end of the day, because all of their services will be pretty similar. I think once you grab the basic concepts and your base layer is good, it's very easy to adjust to both of the cloud environments. I have personally worked on AWS and Google Cloud. I've also worked a bit with Snowflake. Once you can get the ideas, or the concepts, it's really easy to make that switch.
Alexey
I'll add that my experience with Cloud is mostly AWS. Not mostly, I think, it’s only AWS. For this course, I used GCP for the first time and it wasn't that difficult. The UI is different. I actually think GCP has a better UI, in my opinion. It’s more intuitive. The only thing I needed to figure out was, “In AWS this thing is called this way. How was it called in GCP?” Then I would just Google it, find it, and then just use it. Most of the time, it was like that.
Ankush
I have a follow up question. What if ChatGPT is only integrated in Azure? Then what do we do? 
Alexey
Well… I'll have to use Azure. [chuckles]
Ankush
So we’ll need to migrate? [laughs]"
2336	Do you feel any feature is missing in Prefect that is available in other tools like Airflow, Dagster, etc.?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
2337	Do you feel any feature is missing in Prefect that is available in other tools like Airflow, Dagster, etc.?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
2338	Do you feel any feature is missing in Prefect that is available in other tools like Airflow, Dagster, etc.?	Yes, you will have to come up with your own individual projects. It's not easy, I know. But we'll help you. We will help you to come up with a project idea. That's actually on purpose because we want every project to be unique. Then it becomes a part of your portfolio when you look for a job.
2339	Do you feel any feature is missing in Prefect that is available in other tools like Airflow, Dagster, etc.?	"Jeff
No features are missing. Everything's there. Easy. 
Alexey
And more, right? [chuckles]
Jeff
Yeah, and way more. [chuckles] You know, I think there are always new things we can add. It's kind of nice to have multiple tools that are all working hard to compete with each other and up the game for workflow orchestration. I think the winners are everybody who are the users there. So we will keep doing that. As I mentioned earlier, we have more visualization tools in the works and things happening there. You always want to keep making things easier and easier to use. In this course, you got to see a number of things, but we didn't even touch on things like parallelizing, working with Dask and Ray. We have integrations for things like that. You can always just make it an easier process. There's a number of parts that you have to start for any of these services, whether it's Airflow, Dagster, or us. We want to do things like make that easier and easier. There are lots of things there. We have a free Python version just like everybody else, because we want people to go and use it and get value out of it. And that's great. Then for companies that need to have more enterprise features – you want single sign-on, you want to be able to control things when people leave your organization, all that kind of stuff. We got a company behind it. It's a win-win process for everybody and it allows us to get paid and work on Prefect and do things like teach courses like this. And it helps everyone do it. So I think it's good for all data engineering."
2340	How do the decision trees decide the split feature?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
2341	How do the decision trees decide the split feature?	Yeah. If it works for you, use it. I don't mind.
2342	How do the decision trees decide the split feature?	I think for binary features, just one and zero makes more sense than OneHotEncoding. OneHotEncoding will take care of that but you will have two columns that are the exact opposite of each other. So that is a bit redundant. I think it's better just to convert it to a Boolean column with ones and zeros.
2343	How do the decision trees decide the split feature?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
2344	How do the decision trees decide the split feature?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
2345	In the weekly projects that we implemented, we did not use scaling techniques like Standard Scaler. Isn't it important to use it before building the model?	I guess not. That's the process of learning? Maybe more senior people don't… Yeah, I see where you're going with this. Senior people are not supposed to go bankrupt and default. They are probably supposed to pay back the debt. I guess these are the particularities of this dataset, I assume. Indeed, it's probably a data issue ,or not an issue, but a “characteristic” of this dataset.
2346	In the weekly projects that we implemented, we did not use scaling techniques like Standard Scaler. Isn't it important to use it before building the model?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
2347	In the weekly projects that we implemented, we did not use scaling techniques like Standard Scaler. Isn't it important to use it before building the model?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
2348	In the weekly projects that we implemented, we did not use scaling techniques like Standard Scaler. Isn't it important to use it before building the model?	"Alexey
It is quite important. I wouldn't say it's a must for all the projects. For example, if you use something like a tree-based model, then you shouldn't worry about scaling. I think I shared links to our Office Hours from the previous year. If you go to cohorts, and the previous year, and then you go to midterm project – in week ten, we show you how to use SciKit Learn pipelines. This is an amazing concept. So far, what we've been doing is Dictionary Vectorizer (dict_vectorizer) and then it was followed by a model. In the case of Bento, we needed to save this Dictionary Vectorizer separately, as an artifact (as a binary) and then we needed to load this and then we needed to apply the model. But with a pipeline, you can actually train a pipeline and just use BentoML to save the pipeline. It can be saved, right Tim? You can just take a pipeline and do BentoML.sklean.save_model and then the entire pipeline will be saved. Right? [Tim agrees] This is a cool thing. I don't have an example, but you can include this feature scaler here in your pipeline. There should be an example somewhere. I think we did that last year. There is an example without the pipeline. It should be here – classification, and then notebooks scaling this one. In this notebook, we show how to scale your features with standard scaling, or min/max scaler. If you check Office Hours #3 from last year I spoke more about that. It is an important topic. We intentionally did not include this because we cannot include everything, but this is an important topic. I recommend that you check it out. And using pipelines is cool because you will not need to save all this Scaler, Dictionary Vectorizer – all these things separately. Let's say with Bento, you can just take one pipeline, save it with Bento, and then you will have one binary on model. It makes things easier.
Tim 
And if you do have any kind of custom Tokenizer, or Scaler, or Vectorizer – you can always add it to that “custom objects” field.
Alexey
We talked about MLflow. It's very annoying, to be honest. With MLflow, I need to choose to save this in a file. Then I have a Pickle file and I tell MLflow, “Okay, there is an artifact that is already in this file that I want to upload together with the model.” So I upload it together with the model and then when I want to use it, I need to download this from the registry, then I need again to use Pickle to load this into the memory. There is too much overhead. With Bento, I really like how easy it is to save extra stuff.
Tim
Right. In our first version of BentoML, we were just saving models but then we had so many users saying “Well, how do I save my Tokenizer along with the model?” A lot of the time, the Tokenizer, the Vectorizer, is sort of one-to-one with the model. You may have a version of that as well. So it's nice to be able to version it with the model."
2349	In the weekly projects that we implemented, we did not use scaling techniques like Standard Scaler. Isn't it important to use it before building the model?	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
2350	Why do we take full_train to identify the values for one-hot encoding. It could be that a value is only present in the test dataset. Why not use the full dataset?	"Actually, I don't remember exactly how I do this. The right approach would be to use only train data and not use validation data or test dataset. This is done on purpose for exactly this reason that you mentioned. We want to model a situation in which there is some unseen data – it's truly unseen, we do not see that all – and anything can happen to this unseen data, including the situation that you described, such as there is a value that is only present in the test data. 
Imagine that there is a new iPhone – when you train your model, this new iPhone didn't exist (you didn't have this category) but you apply this model, and then, all of a sudden, there is a new iPhone. How will your model react to this? You need to somehow model this situation. This is totally normal. This can and will happen in practice. We just want to understand what would be the effect of that. That's why we need to see this in training data. This is good and this is how it should be. I hope that is a good answer."
2351	Why do we take full_train to identify the values for one-hot encoding. It could be that a value is only present in the test dataset. Why not use the full dataset?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
2352	Why do we take full_train to identify the values for one-hot encoding. It could be that a value is only present in the test dataset. Why not use the full dataset?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
2353	Why do we take full_train to identify the values for one-hot encoding. It could be that a value is only present in the test dataset. Why not use the full dataset?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2354	Why do we take full_train to identify the values for one-hot encoding. It could be that a value is only present in the test dataset. Why not use the full dataset?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
2355	What aspects of real world data engineering are covered in this course, along with projects?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
2356	What aspects of real world data engineering are covered in this course, along with projects?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
2357	What aspects of real world data engineering are covered in this course, along with projects?	"Alexey
Pretty much every week is real world data engineering. I don't think we should spend too much time answering this."
2358	What aspects of real world data engineering are covered in this course, along with projects?	"There are lots of different varieties of approaches here. One thing that you might do is use some kind of data ingestion tool, as I mentioned in the live FAQ discussion – something like Airbyte or Fivetran – if you have lots of data that you're ingesting, that can be a common way to stay on top of that. Or you could do it through a script, kind of like we do here. 
But what you might do is run your agent on a virtual environment in Google Cloud VM, for example, or in AWS, or somewhere else. You could run it in Kubernetes. There are a number of places where maybe you would have that running and have your infrastructure there, so that you were just pulling down the code not to your local machine, but it would just be in your VM. 
If you're actually going and using Pandas, you're putting the data into memory, so you do need to have some scripts that will do that. And you can go directly between Google Cloud Storage and BigQuery with the Prefect GCP library. So that Prefect GCP library that you installed, that you've been using, does have some options in there. I think it's in the BigQuery module to actually get data directly into BigQuery from GCS. So there are a lot of options there. Check out Prefect GCP to see how to do it."
2359	What aspects of real world data engineering are covered in this course, along with projects?	Since we had more signups this year than last year, I hope it will be more than 100. But let's see.
2360	I’m not able to join Slack.	Yeah, I think one project can host several things. I don't know exactly, to be honest. I think there is probably an official recommendation from GCP. Check that out.
2361	I’m not able to join Slack.	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
2362	I’m not able to join Slack.	"Jeff
I'm not sure exactly what's being referenced here. But I'm guessing maybe the GitHub question from the homework – the GitHub block question turned out to be a little trickier than anticipated for folks. This is a great question. I got a couple of things on it here. One is that there is some stuff in the documentation, but maybe not a complete handhold of things. The storage doc does talk about code storage and flow storage in general here, and a little bit about how to do it. 
If you would like more detailed information, it’s always a great idea to look and think about me opening an issue. If you go to the GitHub repository for Prefect, you can click on “new issue”. If you want, you can propose a feature enhancement, if you want to call it that, or report a bug if you think you should have more information, and then put in what you would like to see. 
If you want to go one step further at something I really recommend, even like the next extension of Learning in Public is contributing to projects. This is an open source project here. You're welcome to contribute to it. You could go ahead and fork the repository, clone it down, and make a change to the documentation once you've figured things out there. Lots of lots of folks I know did get there with some support, through the FAQ and through Slack. It's partly what the videos are also for, in the course, to help you out as you're going. That's what I would suggest there. 
But I will also suggest to our docs folks, that we perhaps enhance the GitHub section in particular."
2363	I’m not able to join Slack.	I'll show you what to do in this case. When you go here to Slack, there is this form. Click on this form, fill it in, and we will invite you manually.
2364	I’m not able to join Slack.	No, you cannot.
2365	How do you recommend dealing with imbalance in classification data, especially when the data is small?	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
2366	How do you recommend dealing with imbalance in classification data, especially when the data is small?	"Well, there are multiple ways you can deal with this. First – get more data, if possible. If not, try to reduce the number of features you have. Sometimes, when the data set is small, if you have a lot of features, it might be confusing for the model. Or try to have a model with a lot of regularization. This should help. Here, the main thing is that you need to come up with a reliable way of evaluating the performance of your model. 
You really need to think about how you can set up your validation strategy in such a way that it's reliable. Once you have that, you can start experimenting with all these regularization things, with dropping some columns, with techniques like upsampling and downsampling and things like this. The important thing here is setting up your validation strategy. Once you have that, everything else will come. You will just need to experiment and the experiments will show what the best way is."
2367	How do you recommend dealing with imbalance in classification data, especially when the data is small?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2368	How do you recommend dealing with imbalance in classification data, especially when the data is small?	"One thing I would suggest is to check out Office Hours from last year (2021) if you go to the midterm project, there are these Office Hours. I think it's week nine, where I show how to use the count vectorizer for doing one hot encoding. With count vectorizer, you can apply some filtering. For example, you can say that “I'm only interested in categories that appear at least in 100 observations.” Then, instead of looking at all possible values, you look at only the frequent ones. So go through this document. 
I think there are a lot of different ways you can add filtering here. It could be minimal frequency, it could be something else. It's actually kind of misusing count vectorizer a bit, because count vectorizer is supposed to be used with text features, and not with usual categorical features. But you kind of hack it, in a way, if you say, “Okay, turn these categories into text, and then you train the count vectorizer on them.” So yeah, go through this thing. You can also find the video from that, where I do it live. That's very useful. Again, if you have a huge load of different categories – different values – then the count vectorizer will still require some memory to train. 
There is an alternative. It's called a hashing vectorizer. This one. It does not require training. So you can say, “Okay, I only want to have only like 10,000 features and not more than that.” And it will actually take a word, and it will compute a hash of this word, or value by category, and it will randomly put in one of the columns of this vectorizer. Not randomly – it will compute hash and then put in one of them. So it will be deterministic, of course. Sorry, I didn't choose the right words. 
This is a good way to save memory if you have a lot of categories. So, hashing vectorizer – it works in the same way as count vectorizer, except you don't need to do fit. Actually, you can actually just go through this and read it yourself. It explains everything that you need to know when you compare this one versus count vectorizer."
2369	How do you recommend dealing with imbalance in classification data, especially when the data is small?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
2370	What would you advise to learn between TensorFlow and PyTorch?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
2371	What would you advise to learn between TensorFlow and PyTorch?	I don't know. Since we covered Keras in the course, I'm kind of biased, because my advice would be to learn Keras. But I do admit that PyTorch is becoming more and more popular. Probably, if you look at the community size, you will find a lot more things in PyTorch than in TensorFlow. Also many new things are first implemented in PyTorch and then ported to TensorFlow. For example, Stable Diffusion was first implemented in PyTorch and then ported to TensorFlow. But in the end, it actually doesn't matter. They're quite similar. If you’re now learning TensorFlow or Keras, and then you join a company where they use PyTorch, you will just switch to PyTorch and the other way around. On a higher level, it doesn't really matter, but if you want to go with popularity, then maybe PyTorch is more popular for modeling purposes. But if we talk about model deployment and model serving, I think PyTorch is still getting there. Because of TensorFlow Serving and there’s also a thing called TensorFlow Extended and all that, it's backed by any data, but to me personally, it feels that TensorFlow is more mature when it comes to model deployment. I might be wrong, but this is my feeling.
2372	What would you advise to learn between TensorFlow and PyTorch?	Well, I'm glad you asked, because we have a data engineering course. If you take it, you will understand a bit more – we do not go into details about how exactly it connects with this course. If you remember, when we talked about CRISP DM, one of the steps there is data preparation and this is what data engineering covers. Once the data is prepared, once the data is in a data lake or data warehouse, then we, data scientists, machine learning engineers, get the data, train the model, deploy this model, and take it to the other steps of the process.
2373	What would you advise to learn between TensorFlow and PyTorch?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
2374	What would you advise to learn between TensorFlow and PyTorch?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2375	Is it a good idea to have knowledge of data engineering if we already have knowledge about ML and MLOps?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
2376	Is it a good idea to have knowledge of data engineering if we already have knowledge about ML and MLOps?	"Alexey
It's possible to apply to jobs right now. You don't have to take the course. Just start applying. Why do you need to take the course to start applying, right? It was possible to do it last year. Nobody is stopping you from doing this. But this course will give you some knowledge and add projects to your portfolio. That will be helpful in your job search. 
But I would encourage you to start applying to jobs right now to see what they actually want from you – what kind of topics come up, what kind of test assignments you get. You don't need to wait until the course is finished to start doing this. You can already learn a lot about the job market in your area or in the domain where you want to work.
Luis
I just want to add that when I did the 2022 cohort, I was in another company and the process of data engineering in that company was really similar to the Zoomcamp. What I started to notice was that the companies mostly use this architecture in engineering. So it's good for you to show yourself that you're starting to understand all this data engineering stuff. I think it's important.
Ankush
I just wanted to add. I don't know about applying for jobs, but once you have a job, if you want to get an increment, contact Michael. [chuckles]

Victoria
Michael moneymaker. [chuckles]"
2377	Is it a good idea to have knowledge of data engineering if we already have knowledge about ML and MLOps?	No, you cannot.
2378	Is it a good idea to have knowledge of data engineering if we already have knowledge about ML and MLOps?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
2379	Is it a good idea to have knowledge of data engineering if we already have knowledge about ML and MLOps?	"Alexey
No. I mean, we kind of told you. Yes, you maybe didn't see the intro video. But, sorry. No, it's late. We will actually recalculate the points for the second homework (HW 1B) because in the forum, it said that you can get up to one point, but we calculated that the cap there was seven. We are going to recalculate that. Thus, people who got a lot of points will get just one. Sorry about that. But don't focus on points. If you go to the leaderboard, we know that the person with this hash (722366d8b29ece9be3a7605363562c7c60d6918e) did seven links. Do you know who that is? Probably you don't. 
These points are virtual. It's nice to have a good score, but remember that these points mean whatever you want them to mean – not more than not less. Maybe it's okay if you get a few points less than others for the first homework, but the important thing here is consistency. If you just do your homework, even without doing any Learning in Public stuff, which I think you should do (you should post about the course). If you don't do this and you just do all the homework, by the end, you will be somewhere in the top 20. If you look at the leaderboard from the previous iteration, you will see that not everyone stuck around till the end. 
If you just do your homework, don't worry about these points, and you'll be fine. Sorry about that. I know that it can be discouraging that you didn't get as many points as others, but don't worry about that. Focus on learning. Maybe Michael has some other suggestions or some other things that I didn't mention, because he took part last year. But I think he was one of the people who did a few posts every week, right?
Michael
Yes. Actually, when I went through it, I missed the whole… you can get up to seven points, but I was just doing one every week and I just stuck with that. I was still near the top. I think the real value in that isn't necessarily the points but just being comfortable sharing – writing articles and making videos. That's the real value there. But everyone likes the clout value, too. As Alexey said, it will fluctuate, so if you keep up with it going forward, you'll probably be just fine."
2380	What happened with the Zillow failure. Is it a data science case?	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
2381	What happened with the Zillow failure. Is it a data science case?	I guess not. That's the process of learning? Maybe more senior people don't… Yeah, I see where you're going with this. Senior people are not supposed to go bankrupt and default. They are probably supposed to pay back the debt. I guess these are the particularities of this dataset, I assume. Indeed, it's probably a data issue ,or not an issue, but a “characteristic” of this dataset.
2382	What happened with the Zillow failure. Is it a data science case?	"It all depends on what you mean by “full stack ML engineer”. As you’ve noticed, data engineer Zoomcamp is not about machine learning – it's about data engineering. If in your opinion a “full stack ML engineer” needs to know data engineering, then you should do data engineering. If not, then no. From my point of view, there is a “full stack of data scientist”. What I put in the definition of a full stack data scientist is – a person who can do all the steps in the CRISP-DM process. They can help product managers with business understanding, they can work with analysts in data understanding, they can help data engineers in the data preparation step, they can do the modeling, and they can deploy the models. If you want to be that kind of person, then, of course, you also need to learn a bit of product management and analytics, which we don't have courses for. But data engineering – preparing data before it goes to a model – will certainly be helpful. 
Also, machine learning ops Zoomcamp (MLOps Zoomcamp) will be helpful as well. I would suggest, if you really want to focus on machine learning engineering, then probably doing MLOps Zoomcamp will make more sense for you. But data engineering Zoomcamp will be quite useful in the future, because data scientists and ML engineers tend to work on data pipelines as well. 
Maybe not all the content will be useful for you. For example, the content about data warehousing – as a data scientist, I don't find it super useful in my work to me personally. Also, the module about analytics engineering – it's nice, but it's not something I do day to day. But the modules about batch, the modules about Spark, the modules about streaming – this is something that can be useful, because this is something we could use in building machine learning pipelines as well. 
To put data in a machine learning model, we need to prepare it – we need to create data pipelines – and this is pretty relevant. But you can just check it out. All the content is already there. You can just go and check it out. If you like it, do it – if you don't, don’t. I would suggest maybe going with this one (ML Zoomcamp)."
2383	What happened with the Zillow failure. Is it a data science case?	I do sometimes. Not very often these days, but we have quite a few models that require deep learning. Usually, these models are related to images. We have some images on the platform, so when we need to understand what's happening on these images, we use deep learning.
2384	What happened with the Zillow failure. Is it a data science case?	I don't know much about this, to be honest. I know that their model for predicting the price of real estate went rogue. So it started predicting some… I won’t say more, because I'm not super sure about what exactly happened there. I wasn't really following. I know that the company lost a lot of money because of a rogue model. I think if you just Google that you will find an explanation.
2385	Can TensorFlow Lite be used in a lambda function without a Docker image? Only function-serverless?	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
2386	Can TensorFlow Lite be used in a lambda function without a Docker image? Only function-serverless?	I guess not. That's the process of learning? Maybe more senior people don't… Yeah, I see where you're going with this. Senior people are not supposed to go bankrupt and default. They are probably supposed to pay back the debt. I guess these are the particularities of this dataset, I assume. Indeed, it's probably a data issue ,or not an issue, but a “characteristic” of this dataset.
2387	Can TensorFlow Lite be used in a lambda function without a Docker image? Only function-serverless?	"Actually, I don't remember exactly how I do this. The right approach would be to use only train data and not use validation data or test dataset. This is done on purpose for exactly this reason that you mentioned. We want to model a situation in which there is some unseen data – it's truly unseen, we do not see that all – and anything can happen to this unseen data, including the situation that you described, such as there is a value that is only present in the test data. 
Imagine that there is a new iPhone – when you train your model, this new iPhone didn't exist (you didn't have this category) but you apply this model, and then, all of a sudden, there is a new iPhone. How will your model react to this? You need to somehow model this situation. This is totally normal. This can and will happen in practice. We just want to understand what would be the effect of that. That's why we need to see this in training data. This is good and this is how it should be. I hope that is a good answer."
2388	Can TensorFlow Lite be used in a lambda function without a Docker image? Only function-serverless?	It can be. Docker makes it a lot easier to do this. But it doesn't have to be Docker. With lambda, you will need to prepare a ZIP archive that has everything you need for deploying the lambda function. So you will need to put all your dependencies like NumPy, Pillow, and TensorFlow Lite inside a ZIP archive. You package everything in this ZIP archive, you put the ZIP archive to S3, and then you tell lambda, “Okay, this is where my files are. Get them.” You need to be careful when you prepare the ZIP archive. Let's say if you use Mac OS, and you're on the M1 chip then most likely, the ZIP archive you prepare on your Mac will not work on lambda. So you will probably need to do this inside Docker. Then inside Docker, you prepare a ZIP archive, and then you use the ZIP archive to upload this to AWS. If what I'm saying makes no sense to you, and it probably does not if it's the first time you use lambda, you can just, again, use Google, “AWS lambda python prepare a ZIP file with dependencies”. Yeah, I'm sure if you try this, this is what we need. When you do “pip install -r requirements” you put all the things in the requirements, and -t is the folder where it should install all the requirements. I would actually put it into a separate folder called “build,” for example. Then it will install all the dependencies to this build folder, and then you go to build and then you do ZIP to package everything. Then this is your ZIP file. That's what you need to do. I’m glad we talked about that. If it's not clear – if you're stuck – let's talk about this in Slack.
2389	Can TensorFlow Lite be used in a lambda function without a Docker image? Only function-serverless?	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
2390	How about starting a Kaggle competition?	"I wouldn't call it an issue, actually. It's more like a feature than a bug of RMSE. Let's say you have a model that predicts price and you want to know, on average, how wrong your model is in dollars. That's the purpose of RMSE. Or if you're predicting the age of somebody, then you want to know, on average, how wrong your model is in years. It gives us this understanding of how wrong the model is, on average, on the same scale as our target variable, while R-Squared and MAPE do not give us that. They serve different purposes. 
I never actually used R-Squared in practice. For me, it's always misleading. I cannot really interpret it quite well. But MAPE is a good one – it has its own problems and I think we talked about that in the last Office Hours. It's good if you look at multiple metrics at the same time – you look at RMSE, you look at MAPE, you perhaps look at mean absolute error (MAE) and then, based on that, you make some decisions whether to use this model or not."
2391	How about starting a Kaggle competition?	That's a very similar question to when we should use the lambda and when we should use Kubernetes. If you want to use Kubernetes, then go with TensorFlow Serving. Otherwise, TensorFlow Lite. There is one thing where you have to use TensorFlow Lite – where you cannot use TensorFlow Serving – and this is when you want to deploy your model on a mobile device. I have a phone here. If you want to create an app with a model, then you will have to use TensorFlow Lite because you will not be able to use TensorFlow Serving. TensorFlow Serving also adds a bit of complexity. You will need to have this gateway service, you will need to have GRPC service, so that becomes more complex. If you have a lot of load, then you need to do this otherwise, TensorFlow Lite.
2392	How about starting a Kaggle competition?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
2393	How about starting a Kaggle competition?	"That's a very good idea. We were actually thinking of doing something like this for December. December will be somewhat of a slow month – Christmas holidays and New Year holidays, we'll have a break. But we were thinking maybe we can start a competition at the beginning of December and finish it at the end of January, with other things. 
Actually, I even had an idea of using this generated dataset of dinosaurs and dragons. There are dinosaurs and you can also have dragons here – then you can build a model that tells the dinosaurs apart from the dragons. So that's one idea. Maybe it's boring. I don't know. We haven't come up with a better idea yet, so if you have some suggestions of what could be a good dataset of a good problem for this Kaggle competition, please let us know."
2394	How about starting a Kaggle competition?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2395	Any hint on when to use techniques to deal with class imbalance (undersampling, oversampling, and similar like SMOTE)?	"We had a talk about this on our YouTube channel. As far as I remember, it was called Machine Learning Design Patterns. These patterns were mostly about engineering – engineering patterns – but the first pattern, as far as I remember, was about rebalancing. So check it out. Sara explains when to use these techniques. Here, the important part is that you apply these techniques only to your training dataset. You keep your validation dataset intact – you don't change it – because you want to have a reliable way of evaluating the performance of your model. You apply different techniques to your training dataset and then you see how exactly it changes the performance on your validation dataset. 
So you apply these techniques to train, experiment, try different ones, and then go with the one that has the best uplift for your score on validation. That's usually how you do this. This will tell you if you actually need any of these techniques, or whether just throwing all the data that you have into the model is good enough, so you don't need to make the process more complicated than it should be."
2396	Any hint on when to use techniques to deal with class imbalance (undersampling, oversampling, and similar like SMOTE)?	Well, I’m glad you asked. We have a data engineering Zoomcamp, which starts in January and we have the MLOps Zoomcamp, which starts in May. The only thing that is missing here to become a full stack data scientist is the business understanding part. Remember, when we talked about the CRISP DM process, the first part was business understanding. This is business domain knowledge and things like that. So the only thing that is missing to become a full stack data scientist is picking up this part. For that, I don't know what the best way to learn it is, apart from just joining a company and then talking to stakeholders (to the users of your model) and trying to understand more and more from them. I do recommend doing this, but let's say if you’re not working yet and you want to be a generalist, taking all these three courses is fine. You don't have to do everything there. For example, in the data engineering course, maybe data warehousing is not as important for data scientists as for data engineers, so maybe you can skip that part. But chances are that, as a data scientist, you will need to work with data warehouses too. So you might as well just watch the whole thing.
2397	Any hint on when to use techniques to deal with class imbalance (undersampling, oversampling, and similar like SMOTE)?	Not always. For logistic regression, it does. For decision trees and random forest and XGBoost – not really. Then there is a thing called “calibration”. SciKit Learn calibration. You will need to check this out. I think there are also examples of how to do this with random forest. If you can't find examples here, there are examples on Kaggle. But practically speaking, I don't remember the last time I needed to use something like this. It's helpful when you want to train multiple models. Let's say you've been using logistic regression in production for some time and then you make some decisions based on this threshold. But then you want to deploy a new model (XGBoost) and you want to make sure that the ranges you have in the previous model are the same in the new model so the distribution looks kind of similar. This is where calibration helps. It's a useful thing, but not every application needs it.
2398	Any hint on when to use techniques to deal with class imbalance (undersampling, oversampling, and similar like SMOTE)?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
2399	Any hint on when to use techniques to deal with class imbalance (undersampling, oversampling, and similar like SMOTE)?	Rescale is already preprocessing. Actually, in exception, this is what they do. It's actually the scaling there. There are two operations, I think. I don't remember exactly, but it's rescaling plus something else. So rescaling is preprocessing.
2400	Where could we see other cohorts’ personal projects to get ideas for our own and see the bigger picture of what we'll achieve at the end of the course?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
2401	Where could we see other cohorts’ personal projects to get ideas for our own and see the bigger picture of what we'll achieve at the end of the course?	I would say to focus on projects, not on tools. In this course, you will get a chance to do this. If you open a job description for data engineers, you will see a lot, a lot, a lot of different technologies. Maybe think of a project that you can do for this particular company and do this project using, for example, the tools you learn here or the tools you want to learn, and build a portfolio. I guess this is more of a tip for getting a data engineering job, rather than for somebody who's starting their data engineering career. But if you already have a job, then maybe think of the business impact of what you need. I think I will not spend too much time talking about that because this requires a one hour long discussion. Perhaps we should have a podcast interview about that. If you can think of any potential guests for this podcast interview, you can let me know and we can try to invite these people and talk about this in more detail. One thing I want to share with you is that in our YouTube channel, we had a few interesting talks. We had this Getting a Data Engineering Job with Jeff talk. It’s a very nice presentation. Then we also had an interview with Jeff. So these two podcasts are quite interesting. You can also check out other ones. I think you'll find a lot of interesting content here.
2402	Where could we see other cohorts’ personal projects to get ideas for our own and see the bigger picture of what we'll achieve at the end of the course?	"Alexey
No. I mean, we kind of told you. Yes, you maybe didn't see the intro video. But, sorry. No, it's late. We will actually recalculate the points for the second homework (HW 1B) because in the forum, it said that you can get up to one point, but we calculated that the cap there was seven. We are going to recalculate that. Thus, people who got a lot of points will get just one. Sorry about that. But don't focus on points. If you go to the leaderboard, we know that the person with this hash (722366d8b29ece9be3a7605363562c7c60d6918e) did seven links. Do you know who that is? Probably you don't. 
These points are virtual. It's nice to have a good score, but remember that these points mean whatever you want them to mean – not more than not less. Maybe it's okay if you get a few points less than others for the first homework, but the important thing here is consistency. If you just do your homework, even without doing any Learning in Public stuff, which I think you should do (you should post about the course). If you don't do this and you just do all the homework, by the end, you will be somewhere in the top 20. If you look at the leaderboard from the previous iteration, you will see that not everyone stuck around till the end. 
If you just do your homework, don't worry about these points, and you'll be fine. Sorry about that. I know that it can be discouraging that you didn't get as many points as others, but don't worry about that. Focus on learning. Maybe Michael has some other suggestions or some other things that I didn't mention, because he took part last year. But I think he was one of the people who did a few posts every week, right?
Michael
Yes. Actually, when I went through it, I missed the whole… you can get up to seven points, but I was just doing one every week and I just stuck with that. I was still near the top. I think the real value in that isn't necessarily the points but just being comfortable sharing – writing articles and making videos. That's the real value there. But everyone likes the clout value, too. As Alexey said, it will fluctuate, so if you keep up with it going forward, you'll probably be just fine."
2403	Where could we see other cohorts’ personal projects to get ideas for our own and see the bigger picture of what we'll achieve at the end of the course?	Go to our date engineering course page. There should be a part with projects. Here, we have a peer review assignment. Here, you can actually see projects from the previous iteration. For example, this one. I think this is one actually from Lois who is one of our teaching assistants. This is his project. You can see what he did with this project. This is how you do this. You go through this, click on the project and you can see, for example, what Ilya (one of our students) did. You can see how they approached it. This is how you can find projects from the previous iteration.
2404	Where could we see other cohorts’ personal projects to get ideas for our own and see the bigger picture of what we'll achieve at the end of the course?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
2405	Can we get clarity on reproducibility requirements? It may not be practical for everyone to run some scripts on their machines, such as neural nets.	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
2406	Can we get clarity on reproducibility requirements? It may not be practical for everyone to run some scripts on their machines, such as neural nets.	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
2407	Can we get clarity on reproducibility requirements? It may not be practical for everyone to run some scripts on their machines, such as neural nets.	Yeah. If it works for you, use it. I don't mind.
2408	Can we get clarity on reproducibility requirements? It may not be practical for everyone to run some scripts on their machines, such as neural nets.	As I said last time, you don't have to run it. I do encourage you to run it, to try it, to learn as much as possible. But understand that we have lives and not everyone can have time, or can deal with, problems in somebody else's models. I don't require you to run everything, I encourage you to run. Here “reproducibility” just means checking the code for basic errors, checking that the data is there, checking that it's clear what to do in the readme, there are no errors in the code – maybe you see stack traces, which is a clear indication that there are errors. You can also see the Docker file and check that there are no errors and things like that. So you don't have to run it, but if you have time, if you want to learn, please do run it.
2409	Can we get clarity on reproducibility requirements? It may not be practical for everyone to run some scripts on their machines, such as neural nets.	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
2410	Any Flink materials in this course?	"Alexey
I don't know. Maybe I'll put this question to Ankush as well. But check the website. I think this is one of the good examples when you can just use Google to find the answer."
2411	Any Flink materials in this course?	"Alexey
Yeah, we talked about that in Slack, so please check it. I think this happens because some of the columns have missing values and when you use Pandas for converting CSV to Parquet, it reads columns with missing values as double. That's why this happens."
2412	Any Flink materials in this course?	"Alexey
I don't know when this question was asked, but we did extend it."
2413	Any Flink materials in this course?	No, we don't cover Flink
2414	Any Flink materials in this course?	This question is tricky, because I haven't yet published the homework for streaming. I think it's in progress right now. You can study streaming right now, but you will need to do your homework and submit the homework later, when it's released. But let's say you don't feel like studying data warehousing. Then just don't. I think you should, but let's say that you don't want to study DBT and you just want to do streaming. Feel free to skip DBT Batch and focus on streaming. That's fine. You don't have to do some models that you don't think are relevant for you.
2415	Can we get a brief description of how Prefect works architecture-wise? It will be helpful to solve some path errors that arise.	"Alexey
Google Cloud Platform is not mandatory. You can do most of the course without a cloud. You will not be able to use BigQuery, obviously, because it's a cloud offering. But you can use local postgres instead of that and I think you can run almost everything locally. For example, for streaming, instead of using Confluent Cloud, you can use Kafka. You can set it up locally.
Ankush
We also provide Docker images for everything, but the videos will be more on Confluent Cloud. But you can always change that.
Alexey  
I know that there are people from Nigeria, Iran, who cannot register in Google Cloud Platform. You can do things locally."
2416	Can we get a brief description of how Prefect works architecture-wise? It will be helpful to solve some path errors that arise.	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
2417	Can we get a brief description of how Prefect works architecture-wise? It will be helpful to solve some path errors that arise.	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
2418	Can we get a brief description of how Prefect works architecture-wise? It will be helpful to solve some path errors that arise.	"Jeff
See if this maybe helps a little bit. We have a diagram here talking about your execution environment. This is your infrastructure and your agent running (in this assumption) locally and using Prefect Cloud or the Prefect Orion server. There is an API here on the server. That's what you're interfacing with. And there's a UI that displays information from that API server. The agent opens a connection with the API server and scheduled flow runs get returned and all the information that is needed there and that schedule flow run. Then the agent kicks off the flow run with the flow code in the infrastructure that's specified. Then the results of how that goes gets sent to the API server and that information gets displayed in the cloud API server. I'm just guessing here – maybe a little bit of this question is coming from, (just one thing that came up a time or two) like, “The stuff’s on GitHub. Why do I also need to have a path to the local file to show where my flow code is when I'm making the deployment?” The reason for that is – just to make sure that all of the needed files are available in the place where the deployment is built. This is kind of how things are assumed and needed to work at this point with Prefect."
2419	Can we get a brief description of how Prefect works architecture-wise? It will be helpful to solve some path errors that arise.	No, you cannot.
2420	When I train a model in a certain size, when using the prediction, can I use it with a different one?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
2421	When I train a model in a certain size, when using the prediction, can I use it with a different one?	I guess this is related to the other question. I don't know. Just check it and then you'll see.
2422	When I train a model in a certain size, when using the prediction, can I use it with a different one?	"It all depends on what you mean by “full stack ML engineer”. As you’ve noticed, data engineer Zoomcamp is not about machine learning – it's about data engineering. If in your opinion a “full stack ML engineer” needs to know data engineering, then you should do data engineering. If not, then no. From my point of view, there is a “full stack of data scientist”. What I put in the definition of a full stack data scientist is – a person who can do all the steps in the CRISP-DM process. They can help product managers with business understanding, they can work with analysts in data understanding, they can help data engineers in the data preparation step, they can do the modeling, and they can deploy the models. If you want to be that kind of person, then, of course, you also need to learn a bit of product management and analytics, which we don't have courses for. But data engineering – preparing data before it goes to a model – will certainly be helpful. 
Also, machine learning ops Zoomcamp (MLOps Zoomcamp) will be helpful as well. I would suggest, if you really want to focus on machine learning engineering, then probably doing MLOps Zoomcamp will make more sense for you. But data engineering Zoomcamp will be quite useful in the future, because data scientists and ML engineers tend to work on data pipelines as well. 
Maybe not all the content will be useful for you. For example, the content about data warehousing – as a data scientist, I don't find it super useful in my work to me personally. Also, the module about analytics engineering – it's nice, but it's not something I do day to day. But the modules about batch, the modules about Spark, the modules about streaming – this is something that can be useful, because this is something we could use in building machine learning pipelines as well. 
To put data in a machine learning model, we need to prepare it – we need to create data pipelines – and this is pretty relevant. But you can just check it out. All the content is already there. You can just go and check it out. If you like it, do it – if you don't, don’t. I would suggest maybe going with this one (ML Zoomcamp)."
2423	When I train a model in a certain size, when using the prediction, can I use it with a different one?	I think for binary features, just one and zero makes more sense than OneHotEncoding. OneHotEncoding will take care of that but you will have two columns that are the exact opposite of each other. So that is a bit redundant. I think it's better just to convert it to a Boolean column with ones and zeros.
2424	When I train a model in a certain size, when using the prediction, can I use it with a different one?	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
2425	I have a question for you, Tim. If people have questions for you, what's the best way to reach out to you?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
2426	I have a question for you, Tim. If people have questions for you, what's the best way to reach out to you?	Yes… and no? Sometimes, yeah. I wouldn't say often – sometimes it happens, sometimes it doesn't. Usually, I often use interactions for – and here, I don't mean just linear regression or logistic regression. When you have categorical features, then for example, if you have “model” and “make,” you concatenate them and it gives you better granularity, let's say. If you do this – if you concatenate “model” and “make,” you will know what the effect is from the coefficients of linear regression, like how much the price is affected when we know what the particular model/make combination is. I hope I'm not confusing you even more.
2427	I have a question for you, Tim. If people have questions for you, what's the best way to reach out to you?	"Tim
Yeah, we have a lot of users who use MLflow and BentoML together. Once you have a model that’s sort of your “finalized” model – you save it to your MLflow registry and then (it depends on what your CI/CD pipeline looks like) but BentoML has a command to import from an MLflow registry. You can tell which MLflow model is the one that you want to deploy. It's very similar to save_model, except instead of pulling from your local environment, you're pulling from an MLflow registry. So you import the model, it automatically gets pulled into your Bento and then you can deploy it. On the BentoML documentation site, there's a big framework document on how to integrate with MLflow. 
Alexey
When searching, one of the suggestions was BentoML vs MLflow. I guess there are also some similarities because with MLflow, you can serve models. 
Tim
The thing about serving models is that it's a part of the pipeline that you just have to do. Every single framework out there has some way to serve a model – you train the model and then it just puts it up there to be able to serve. Typically, frameworks don't specialize in serving those. BentoML specializes in making the serving part really, really fast and really, really easy. What you find a lot of the time is, when you have serving logic in a really large end-to-end pipeline, it just puts the model there and then the only thing that you could submit to the model is the data. But we know in practice that there's usually business logic around this – pre-transformation logic, post-transformation logic, there's the version of the model that was saved. There are a lot of these other components that you want around the model, not just the model for inference. And of course there's the performance layer underneath. The BentoML architecture kind of brings that all together into one deployable, rather than just giving you one point to call inference for your model. 
Alexey
That is a comment in the live chat that says “There is PyCaret, too.” This is how you use PyCaret with Bento. 
Tim
Right. BentoML has lots and lots of integrations and we're only adding to them every day. The idea is to get the model from wherever you built it, bring it into your Bento, and then you can deploy it anywhere. Once it's in the Bento – in this standard packaging – it's nice because then we've got lots of different tools to deploy to Lambda, SageMaker, and lots of different places. And the places to deploy, we're adding to every week and month as well.
Alexey  
Since you can have a Docker container at the end – you do “bento_containerize” right? That's the command? And then you have a Docker container (Docker image) that you can deploy everywhere where you can deploy a Docker container, which is pretty much every place on the internet, right? 
Tim
Right. Some places, though, require specialized… 
Alexey
Like Lambda, right? 
Tim
Yeah, like Lambda won't accept just a normal Docker container. You have to actually create special endpoints. When we deploy to Lambda, we create those endpoints for you and make sure that it's behind an API gateway and things like that. For certain services, there are a lot of nuances that we kind of take care of for you – as long as you're in the Bento standard format.
Alexey 
For those who have no idea what MLflow is, and why we are talking about this – you don't need to worry about this right now. But at some point, this tool and other similar tools will be quite handy. You can check out our MLOps Zoomcamp course, after you finish this one. Don't try to do multiple courses at the same time. That could be too much. But of course, if you're curious, go check it out."
2428	I have a question for you, Tim. If people have questions for you, what's the best way to reach out to you?	"Tim
The best way to reach out to me is in the Bento ML Slack. Usually, you can join our Slack group and then you can just directly message me"
2429	I have a question for you, Tim. If people have questions for you, what's the best way to reach out to you?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2430	Data Analyst projects answer business problems or provide business insights. Pair your learning with Business Analysis.	Yeah. Thanks a lot for adding that. Pair your learning with business analysis. That's always a good suggestion. If you’re already working and have some colleagues with experience that you want to get, just talk to these colleagues. That's always a great idea.
2431	Data Analyst projects answer business problems or provide business insights. Pair your learning with Business Analysis.	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
2432	Data Analyst projects answer business problems or provide business insights. Pair your learning with Business Analysis.	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2433	Data Analyst projects answer business problems or provide business insights. Pair your learning with Business Analysis.	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
2434	Data Analyst projects answer business problems or provide business insights. Pair your learning with Business Analysis.	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
2435	What would be something that is unforgivable to a data scientist?	For me, as a Russian citizen, it was actually surprising to learn that the World Cup started, because I simply didn't know that until last week, when people all of a sudden started watching football. I see that Ukraine isn't playing there – they have other problems right now, I suspect. I would root for them if they played. I guess Poland. I know that they won a match recently against Saudi Arabia – so, good job. I didn't watch it though. But if you asked me which team I would root for, then maybe Poland.
2436	What would be something that is unforgivable to a data scientist?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
2437	What would be something that is unforgivable to a data scientist?	"Well, to me, you have to know how to set up a validation framework. You have to validate your data. If you asked me, “What is better – random forest or logistic regression?” I would say, “Okay, just do validation and check it.” If you don't know how to do this, I would suggest learning how to do this. Maybe “unforgivable” is a strong word. For example, if a candidate that I interview doesn't know about validation, then this candidate will probably be rejected. 
So I think this is one of the most important skills – knowing how to validate your model. Then once you know how to do this, you can answer any question by just testing it. If you have an environment where you can experiment – and you can experiment if you have a validation framework – then you can answer all the questions, like, “What if I increase the learning rate?” You just increase it and see what happens. Because it's all case-dependent, it's all data dependent. You need to know how to experiment and be comfortable with experimenting. I think this is the most important thing for a data scientist."
2438	What would be something that is unforgivable to a data scientist?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
2439	What would be something that is unforgivable to a data scientist?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2440	Do Office Hours focus on course chapters or is it more like an Q&A sessions on the videos?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
2441	Do Office Hours focus on course chapters or is it more like an Q&A sessions on the videos?	It's both. If you have any questions you want to ask, then you can use Office Hours for that.
2442	Do Office Hours focus on course chapters or is it more like an Q&A sessions on the videos?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
2443	Do Office Hours focus on course chapters or is it more like an Q&A sessions on the videos?	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
2444	Do Office Hours focus on course chapters or is it more like an Q&A sessions on the videos?	No. From what I understood, from what Ankush said, you don't need to know Java.
2445	What are your top three or more ethical guidelines while applying machine learning to solve real world problems? Predicting repeated crime, recommendation, etc.	"For each use case, indeed, you need to think about what kind of business problem you’re solving and then based on that, come up with a metric. In most cases, precision and recall and F1 score are useful. Accuracy – most of them are good. Maybe what you can do is go to Kaggle where there are a lot of competitions. You can go through these competitions and see what kind of evaluation metric they use. 
First you can try to understand what the problem is about and then you can see what kind of evaluation metric they use for this competition. This way, over time, you will build your intuition regarding what kind of metric to use in which situation? It's not always great. I think Kaggle has some limitations, at least it used to have them – it has a very limited set of metrics. Sometimes, competition organizers would use a metric that didn't make much sense. But it doesn't happen very often. I think right now, the platform supports very customized metrics for each use case. So check out Kaggle."
2446	What are your top three or more ethical guidelines while applying machine learning to solve real world problems? Predicting repeated crime, recommendation, etc.	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2447	What are your top three or more ethical guidelines while applying machine learning to solve real world problems? Predicting repeated crime, recommendation, etc.	"I am glad you asked. Project of the Week has nothing to do with the course to the extent that it's just different initiatives that are run in our community. We have a course and we also have Project of the Week. We also have webinars, we also have podcasts. They are all different activities. For the Project of the Week, this week, we'll have a project about recommender systems. The idea there is that every day, you get a set of tasks. On Wednesday, you get this set of tasks. You need to come up with an idea. You need to find a dataset for this idea, and you need to share your progress. Then on day two, you will need to go through the suggested materials and also find some materials on your own and do this thing – learn about the basics, do exploratory data analysis, understand this data that you found and share your progress. As you can see on the project page, every day, you get a set of tasks and you need to do them. So it's not like we're telling you exactly what to do like in the Zoomcamp (in the videos we tell you that “This is the exact sequence of actions you need to execute and this is the result you get.”) With the Project of the Week, it's more independent. We just give you four bullet points and then it's up to you to actually do these things. And then, of course, you share your progress. The idea behind this project is that, at the end, you have a project that you can add to your portfolio. This is not a project that you just took from the tutorial and followed along, but this is a project that you did with some guidance. These are two very different things. That's the idea behind Project of the Week. For the project that we'll have as a part of the course, the idea is somewhat similar, except you will get less guidance – you will have more independence, let's say. The guidance we have for the project in the course is the form of a bunch of criteria that you need to satisfy. Then it's up to you to come up with a plan. But here, for Project the Week, we have a suggested plan that you can follow and hopefully, at the end, come up with a result. Keep in mind that for Project of the Week, some things might not go according to plan and that’s fine. This is just a suggested plan. We don't know if it will actually work exactly like we outlined day by day. It's more just to give you some guidance, but it’s up to you how exactly to approach that. 
There is a comment that says “I did not know about this.” Yes, we have a channel in our slack. It's called #project-of-the-week and it starts this week Wednesday. You'll notice that there's the digit one in the name “2022-10-19-recommenders-1.md”. We'll also have a follow-up project about recommender systems. If you want to take part in this (by “take part” I mean, propose your own topic) for example, some of you were asking about time series, some of you were asking about NLP, some of you were asking about other things in the comments in live chat. Daniel mentioned audio – If you want to learn any of these libraries, we can do a Project of the Week to learn these libraries or methods or approaches or whatever."
2448	What are your top three or more ethical guidelines while applying machine learning to solve real world problems? Predicting repeated crime, recommendation, etc.	[chuckles] I'm not prepared to answer that question. I don't have a top three for guidelines. Maybe this is something we can discuss next time. I don't know how to answer that. Maybe what you can do is go to our YouTube channel (I know I keep doing this) where we just had this Responsible and Explainable AI Interview about that. So maybe check it out. I will mark this as “answered” and then maybe this is something we can go through next time, like we did last time – some of the questions I didn't answer and we moved it to this week. So probably we’re going to keep doing it like this.
2449	What are your top three or more ethical guidelines while applying machine learning to solve real world problems? Predicting repeated crime, recommendation, etc.	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
2450	What do we mean when we say that there are assumptions associated with a linear regression model?	Working, I guess. So yeah – work after courses. That's where I see you.
2451	What do we mean when we say that there are assumptions associated with a linear regression model?	There is a lecture about that. Just go and check this lecture. I think it was a decision tree learning algorithm and that unit describes how exactly this happens. I mean, there are some simplifications but overall, the algorithm is what I described in the video.
2452	What do we mean when we say that there are assumptions associated with a linear regression model?	"If you take statistics for machine learning education, the first lecture will be this – you'll probably derive the formula for logistic regression and there will be a slide with all these assumptions about the linear regression model. Here is the Google result: 
https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/assumptions-of-linear-regression/

Go through this. They are usually useful in practice, I guess. But what I typically do is train a model, and then rely on validation to tell me if my model is doing well or not. So if there is something wrong with my validation, I will see that the metric validation is bad. It's just a lot of trial and error, rather than figuring out all these theoretical assumptions. 
I don't want to say that all these assumptions are not relevant. If you work as a data scientist, they still are. But I also found that the more practical way to understand if you can apply linear regression to the data or not, is to just apply it and see if it works or not. If it works – if the results on the validation are good – then it means that you can apply your model to this dataset. 
Maybe some of the assumptions are violated. For example, multicollinearity – this is actually the case in the lectures, when we needed to regularize (add regularization to the model) this assumption was violated. You will see in the lectures what exactly happened. So check it out. I don't think I will be able to give a good answer, because I am not prepared to talk about this, but any statistics book or theoretical machine learning book explains this."
2453	What do we mean when we say that there are assumptions associated with a linear regression model?	"Alexey
CI stands for continuous integration. This is a way to – let's say you wrote some code, and then you push it to GitHub. Then on GitHub, there's GitHub actions, which is a way to automatically run some checks on your code – run tests, deploy things somewhere. Actually, again, I will do a shameless plug – in our MLOps Zoomcamp, we talked about GitHub actions in the best practices module. Right now, don't worry about this. Focus on your projects. But after you do your projects, it's really worth spending some time learning about best practices. These best practices include things like writing tests, using make files and also tools like CI/CD, infrastructure as code – we cover all that in the module. But if you are interested in these things, maybe it's actually worth taking that course too. We'll have another iteration in May. Not super soon – I’m just telling you that to get you excited."
2454	What do we mean when we say that there are assumptions associated with a linear regression model?	For me, as a Russian citizen, it was actually surprising to learn that the World Cup started, because I simply didn't know that until last week, when people all of a sudden started watching football. I see that Ukraine isn't playing there – they have other problems right now, I suspect. I would root for them if they played. I guess Poland. I know that they won a match recently against Saudi Arabia – so, good job. I didn't watch it though. But if you asked me which team I would root for, then maybe Poland.
2455	Do you recommend collecting our own data for the project?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2456	Do you recommend collecting our own data for the project?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
2457	Do you recommend collecting our own data for the project?	"There was no registration deadline at all for this course. So you can still register. You can still sign up and you can follow the course. You will not be able to submit the homework assignments where the due date is already over because there are already solutions posted, so it doesn't make much sense to submit homework when the solutions are already there. You can follow the course but you will not be able to submit the homework. You can check our frequently asked questions and I do recommend doing this. Please remember the rule – if you have a question, first go check the frequently asked questions, and if you don't find the question there, go and ask in Slack. 
For this particular question, it’s already there. “Don't worry, you can take the course.” Maybe right now it's becoming a little bit more challenging to catch up with everything, but note that you can skip the midterm project. Right now, if you want to catch up with everything, if you skip the midterm project, it's fine. You will be able to do two capstone projects and still get the certificate. Also, you can just take the course in self-paced mode. There is nothing wrong with that either."
2458	Do you recommend collecting our own data for the project?	That's a very good question. I do recommend it for your pet project. But for the midterm project, you will not have enough time to actually do this. For the midterm project, it’s better if you already have a dataset from somewhere. You will simply not have time (two weeks) to collect your dataset and then do the rest of things. You can start collecting your dataset right now. If this process will finish by the time the project starts. Otherwise, I would not recommend doing this. But if you want to do it for your portfolio project, for future employment, for a future job search, then it's a great idea and please do it. But be mindful of time, because you might not have enough time to finish the project.
2459	Do you recommend collecting our own data for the project?	"Tim
I think a couple people asked this in ML Zoomcamp Slack. I was thinking about this a little bit. I think it's sort of a philosophical thing [chuckles] where we think about models and Bento is sort of separate a little bit from code. So your project directory is where all the stuff that you'll commit to Git. I don't think we think that it's the right model to commit your models to Git, for example. We think of ourselves a little bit more like Docker. You don't have Docker images in your project directory, right? You just have your Docker file and that's what you commit to Git. In the same way, we store your models and your Bento in that dedicated directory, and then sort of leave it up to you to push it to different places where you might build or where you might need it
Alexey
Do you know why you decided to save it locally on your local file system? For example, what MLflow does – you can save it to S3 directly, but in the case of Bento, you always first save it to local storage and then you decide “This model I want to export, but for the rest, I don't care.”
Tim
Right. First, I think it’s because that's the easiest way to do it, but also because I think with BentoML, the workflow is that a lot of times, somebody's building a model on their laptop, or somebody's building a model in a slightly less distributed system and they're assembling their Bento there. So I think that the fastest way to access that model is locally. We do give you the ability to push that model and pull that model from other places. But ultimately, when you assemble your Bento and then you assemble your container, all that stuff has to be there. I don't think we wanted separate abstractions for when a person is training and saving, versus when a person is building a Bento and pulling in the model and assembling the container. It's just the same abstraction, which is just a local repository which contains models.
Alexey 
Maybe you don't want to save every model every single time you change one single parameter to the cloud. [Tim agrees] Maybe at the end, after all the experiments, you want to save the final one. Meanwhile, for MLflow, the use case is different – you actually want to save every single experiment because you want to see the results of these experiments.
Tim
Right."
2460	Can I use a dummy variable result? That is, I have two answers for a variable, but I see that one does not help me much so I only use the most relevant one.	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2461	Can I use a dummy variable result? That is, I have two answers for a variable, but I see that one does not help me much so I only use the most relevant one.	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
2462	Can I use a dummy variable result? That is, I have two answers for a variable, but I see that one does not help me much so I only use the most relevant one.	Again, set up your cross validation and then test it on your validation. If your validation says it's fine to drop it, just drop it.
2463	Can I use a dummy variable result? That is, I have two answers for a variable, but I see that one does not help me much so I only use the most relevant one.	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
2464	Can I use a dummy variable result? That is, I have two answers for a variable, but I see that one does not help me much so I only use the most relevant one.	"Yes, indeed there is. You need to get something like 50% in order to pass the project. I don’t remember the exact score we used last year. This is how we did it last year – we looked at the distribution of the scores. We didn’t want to mark many projects as failed, so we looked at the distribution and we made sure that most of you passed the project. I think the passing score was pretty low. Most of the students who did the project – who did put some effort there in doing the project and who met at least half of the criteria or most of the criteria but partly – they passed the project. It wasn't a big problem, so don't worry about that too much. 
As long as you put in some effort, then you'll pass it. The people who didn't pass last year submitted empty projects, or projects that were copied from somewhere, or projects where they just added some readme file and maybe copied some Kaggle kernel and that was it. So nothing was done in addition to that. There were people who passed, but they just didn't put any effort into attempting to do the project. If you put in some genuine effort, don't worry, you will pass the project."
2465	How do you emulate several terminals in the same session? What tool do you use?	"Alexey
No. I mean, we kind of told you. Yes, you maybe didn't see the intro video. But, sorry. No, it's late. We will actually recalculate the points for the second homework (HW 1B) because in the forum, it said that you can get up to one point, but we calculated that the cap there was seven. We are going to recalculate that. Thus, people who got a lot of points will get just one. Sorry about that. But don't focus on points. If you go to the leaderboard, we know that the person with this hash (722366d8b29ece9be3a7605363562c7c60d6918e) did seven links. Do you know who that is? Probably you don't. 
These points are virtual. It's nice to have a good score, but remember that these points mean whatever you want them to mean – not more than not less. Maybe it's okay if you get a few points less than others for the first homework, but the important thing here is consistency. If you just do your homework, even without doing any Learning in Public stuff, which I think you should do (you should post about the course). If you don't do this and you just do all the homework, by the end, you will be somewhere in the top 20. If you look at the leaderboard from the previous iteration, you will see that not everyone stuck around till the end. 
If you just do your homework, don't worry about these points, and you'll be fine. Sorry about that. I know that it can be discouraging that you didn't get as many points as others, but don't worry about that. Focus on learning. Maybe Michael has some other suggestions or some other things that I didn't mention, because he took part last year. But I think he was one of the people who did a few posts every week, right?
Michael
Yes. Actually, when I went through it, I missed the whole… you can get up to seven points, but I was just doing one every week and I just stuck with that. I was still near the top. I think the real value in that isn't necessarily the points but just being comfortable sharing – writing articles and making videos. That's the real value there. But everyone likes the clout value, too. As Alexey said, it will fluctuate, so if you keep up with it going forward, you'll probably be just fine."
2466	How do you emulate several terminals in the same session? What tool do you use?	If it's a question for me, specifically – in Windows, I use a tool called Windows Terminal. This is the tool I use. In the settings, they have Git Bash, the usual command prompt, then they have multiple subsystems for Linux terminals. There also used to be PowerShell. I don't use PowerShell, so I don't have it here. This is what it looks like. I really like this. It’s just called Windows terminal. Check it out.
2467	How do you emulate several terminals in the same session? What tool do you use?	Yes
2468	How do you emulate several terminals in the same session? What tool do you use?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
2469	How do you emulate several terminals in the same session? What tool do you use?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
2470	How do we decide if the best threshold for the classification, using the ROC curve or accuracy? As in the lesson we use 0.5 based on accuracy.	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
2471	How do we decide if the best threshold for the classification, using the ROC curve or accuracy? As in the lesson we use 0.5 based on accuracy.	"Yes, indeed there is. You need to get something like 50% in order to pass the project. I don’t remember the exact score we used last year. This is how we did it last year – we looked at the distribution of the scores. We didn’t want to mark many projects as failed, so we looked at the distribution and we made sure that most of you passed the project. I think the passing score was pretty low. Most of the students who did the project – who did put some effort there in doing the project and who met at least half of the criteria or most of the criteria but partly – they passed the project. It wasn't a big problem, so don't worry about that too much. 
As long as you put in some effort, then you'll pass it. The people who didn't pass last year submitted empty projects, or projects that were copied from somewhere, or projects where they just added some readme file and maybe copied some Kaggle kernel and that was it. So nothing was done in addition to that. There were people who passed, but they just didn't put any effort into attempting to do the project. If you put in some genuine effort, don't worry, you will pass the project."
2472	How do we decide if the best threshold for the classification, using the ROC curve or accuracy? As in the lesson we use 0.5 based on accuracy.	I have not used it, so I don't know.
2473	How do we decide if the best threshold for the classification, using the ROC curve or accuracy? As in the lesson we use 0.5 based on accuracy.	"With the ROC curve, it's not easy to find the best thresholds. I usually use accuracy or something else. First, you need to ask yourself what kind of metric you want to optimize. Usually, for example, if we want to optimize for precision and recall, we combine them in F1 score. And then, for example, I would evaluate my classifier against different thresholds and pick the one with the highest F1 score. 
It usually comes from your needs. If you want to have good precision, but not so bad recall, then you probably plot this precision versus recall on different thresholds, like you had to do for the homework. Based on that, you decide what the best threshold for your specific case is. This is how I usually do this."
2474	How do we decide if the best threshold for the classification, using the ROC curve or accuracy? As in the lesson we use 0.5 based on accuracy.	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
2475	I have a small dataset with 160 items. Any recommendations on train-test split? Cross-validation helps, right?	"Scikit Learn is not very different. This is not exactly how Scikit Learn is implemented. It's a little bit different, but the idea is very similar. In Scikit Learn, they use smarter ways of doing this. For example, in week two, we have a case when we need to apply regularization. When we have correlated or very close to correlated columns, then our matrix becomes very unstable, so we cannot easily invert it. 
If what I’m saying now doesn't make sense, it will make sense once you start watching the videos. Anyways, what I'm trying to say is that in Scikit Learn, they have a smart way of working around this, such that your weights – your vector with weights, the W vector – does not contain insanely large values. They have smarter methods for doing this. But apart from that, it's very similar and you should not get two different results. You can experiment, and I do recommend experimenting with Scikit Learn. We will actually do this in week three. In week three, we will use Scikit Learn for some of these things. 
In week two, we will implement things ourselves. But starting from week three, we will only use Scikit Learn and other libraries, so we will stop implementing things ourselves. Here's just to give you a taste of what machine learning is inside. It’s just a bunch of formulas – algorithms – there is no magic. Internally, Scikit Learn implements them too."
2476	I have a small dataset with 160 items. Any recommendations on train-test split? Cross-validation helps, right?	"Tim 
Not really. We have users who use both Streamlit and Gradio. I think Streamlit and Gradio are a bit more about the presentation side. We do have a couple of users who have asked us about deeper Streamlit or Gradio support and I think that's something that's probably coming up in the next few months.
Alexey
For those who don't know what Streamlit or Gradio is – this is a way to create an interface for your services. Well, Gradio is focused on machine learning, meaning you create interfaces specifically for models. And then Streamlit is just a Python package for creating interfaces. We actually had a project at DataTalks.Club about Streamlit. I actually even have a tweet about this. This is what Streamlit looks like. In the video, you can see that you have this interface, and then based on the toggles that you move the core of your model changes. It gives you an easy way to build an interface like that. Meanwhile Bento focuses on quite a different use case, right? It’s not building interfaces, but serving models.
Tim 
Right, yeah. I think one of our users was asking for a really easy way, so that once you have your model and your Streamlit UI to be able to create a Bento deployment with one line in that way, anybody that you gave your Streamlit app to would be calling to that service and it could work anywhere. Which is a nice feature, I think.
Alexey 
Yeah, I think it is. I imagine if you have a Pydentic class there, then you can easily generate a Streamlit app for that.
Tim 
Right. Oh, that's a good idea. I hadn't thought of that. Yeah.
Alexey
And then in addition to that, the Swagger UI that you have, could be like having the actual interface. 
Tim
Oh, yeah, that would be cool. [chuckles]
Alexey
That could be a good Hackathon project, right?
Tim 
Yeah, for sure.
Alexey
From what I see – let's say you’ve built a model. Then you deployed this model and you have this nice API. You can query it with CURL. But what if you want to demo this project to your manager, or a manager of your manager, or somebody who does not necessarily have a technical background? If you give them the URL and say, “Okay, you need to execute the CURL command with a post request,” they would be like, “CURL what? What do you want from me? Just show me how it looks.” This is when tools like Streamlit come in handy. Instead of giving them a command line interface to query it, you just give them “Okay, this is the link. Play with this.” If Bento could do something like that – automatically generate things like that – many data scientists would thank you.
Tim
Yeah, I was just looking because I figured there's a library that turns Pydantic models into Streamlit apps. It looks like there is one that does that. I wonder if we could just plug into that app and then have that same thing. 
Alexey
That's cool. 
Tim
Yeah, that's awesome. That's kind of the cool part of open source – there are so many people out there with so many different projects and we get to collaborate with them and kind of build the best collaborations and combinations of these tools."
2477	I have a small dataset with 160 items. Any recommendations on train-test split? Cross-validation helps, right?	"Yeah, you definitely need to use cross-validation here. Tree models overfit. That's why you need to use cross-validation – to make sure that this doesn't happen. Usually, when you have such a small dataset, it helps to use very simple models. Trees – when you let them grow indefinitely deep, they do tend to overfit. 
So maybe you stick to the depth of three, or use something like linear regression or logistic regression with very few features. Because you have a small dataset, you need to be very careful. I guess, that’s the general rule – use simpler models and then use cross-validation."
2478	I have a small dataset with 160 items. Any recommendations on train-test split? Cross-validation helps, right?	I guess not. That's the process of learning? Maybe more senior people don't… Yeah, I see where you're going with this. Senior people are not supposed to go bankrupt and default. They are probably supposed to pay back the debt. I guess these are the particularities of this dataset, I assume. Indeed, it's probably a data issue ,or not an issue, but a “characteristic” of this dataset.
2479	I have a small dataset with 160 items. Any recommendations on train-test split? Cross-validation helps, right?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
2480	Could you share with us some real-world cleaning strategies, good practices, checklists, and what to look for when at the “transform” step?	"Alexey
The first thing about the dataset – what kind of dataset do you want to use? Or what kind of problem do you want to solve? Once you figure this out, then you're basically ready to start working on a project. Then in the project, you need to decide if you want to do streaming or batch. For batch, it's using things like Prefect, Spark, or DBT. For streaming, it’s using the materials from the last lecture (week 2). Once you decide that, you will just implement this and you will find all the information you need here in the week 7 project repo. Just go through this and if you have any questions left, let us know. Keep in mind that these are the criteria that other people (your peers) will use when evaluating your project. Perhaps you can already think about that and how you want to implement your project in such a way that you maximize the score you get from these criteria."
2481	Could you share with us some real-world cleaning strategies, good practices, checklists, and what to look for when at the “transform” step?	We’re still in progress (as of January 26, 2023). I thought we would do this earlier. But it's still not finished.
2482	Could you share with us some real-world cleaning strategies, good practices, checklists, and what to look for when at the “transform” step?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
2483	Could you share with us some real-world cleaning strategies, good practices, checklists, and what to look for when at the “transform” step?	No, you cannot.
2484	Could you share with us some real-world cleaning strategies, good practices, checklists, and what to look for when at the “transform” step?	"Jeff
There are lots of different things there. This is something I used to teach folks doing data science-type work and it applies for data engineering, too. I don't have a good checklist at the tip of my fingers, but you do want to look at things like “Are your data in the right type? Do you have missing values, libraries, rate expectations (which we have integration for)?” That can help with trying to make sure your data looks how you think it'll look, in terms of the statistical properties of it. Hypothesis can also generate some ideas there, or some other Python library. Pandera for everything being how you expect, values matching what you expect. So there are a lot of different tools in Python that you can use there. It's a good question. It can be a lot of different things."
2485	I lost five weeks of ML Zoomcamp, including the midterm project and the last two homework assignments, but I'm catching up. Is it fine if I do both capstone projects?	Yes, that's the main reason we have two capstone projects. This is for those who are catching up to be able to get the certificate. Now there are holidays, like Christmas holidays and then January holidays in some countries – like in Russia, for example, people usually take holidays not during the end of December, but at the beginning of January. No matter in which part of the world you are in, you still have one month to catch up (November) and then start working on the capstone. Actually, I’ve now realized that the capstone starts quite soon. But for capstone 1, you only need the first six weeks or so, and then you can finish this project. Then during the holidays, you can watch Kubernetes, neural networks and so on, if you want. Then you can do capstone 2. That's probably better. But yeah, it's fine. You should do 2 capstones.
2486	I lost five weeks of ML Zoomcamp, including the midterm project and the last two homework assignments, but I'm catching up. Is it fine if I do both capstone projects?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2487	I lost five weeks of ML Zoomcamp, including the midterm project and the last two homework assignments, but I'm catching up. Is it fine if I do both capstone projects?	Well, I'm glad you asked, because we have a data engineering course. If you take it, you will understand a bit more – we do not go into details about how exactly it connects with this course. If you remember, when we talked about CRISP DM, one of the steps there is data preparation and this is what data engineering covers. Once the data is prepared, once the data is in a data lake or data warehouse, then we, data scientists, machine learning engineers, get the data, train the model, deploy this model, and take it to the other steps of the process.
2488	I lost five weeks of ML Zoomcamp, including the midterm project and the last two homework assignments, but I'm catching up. Is it fine if I do both capstone projects?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
2489	I lost five weeks of ML Zoomcamp, including the midterm project and the last two homework assignments, but I'm catching up. Is it fine if I do both capstone projects?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
2490	I have used Python for scripting before but not full-on development. Is it a good Idea to pursue this course?	No, you cannot.
2491	I have used Python for scripting before but not full-on development. Is it a good Idea to pursue this course?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
2492	I have used Python for scripting before but not full-on development. Is it a good Idea to pursue this course?	Yes, definitely. Many people who did Python took the course and succeeded at the end.
2493	I have used Python for scripting before but not full-on development. Is it a good Idea to pursue this course?	"Jeff
I'm not sure exactly what's being referenced here. But I'm guessing maybe the GitHub question from the homework – the GitHub block question turned out to be a little trickier than anticipated for folks. This is a great question. I got a couple of things on it here. One is that there is some stuff in the documentation, but maybe not a complete handhold of things. The storage doc does talk about code storage and flow storage in general here, and a little bit about how to do it. 
If you would like more detailed information, it’s always a great idea to look and think about me opening an issue. If you go to the GitHub repository for Prefect, you can click on “new issue”. If you want, you can propose a feature enhancement, if you want to call it that, or report a bug if you think you should have more information, and then put in what you would like to see. 
If you want to go one step further at something I really recommend, even like the next extension of Learning in Public is contributing to projects. This is an open source project here. You're welcome to contribute to it. You could go ahead and fork the repository, clone it down, and make a change to the documentation once you've figured things out there. Lots of lots of folks I know did get there with some support, through the FAQ and through Slack. It's partly what the videos are also for, in the course, to help you out as you're going. That's what I would suggest there. 
But I will also suggest to our docs folks, that we perhaps enhance the GitHub section in particular."
2494	I have used Python for scripting before but not full-on development. Is it a good Idea to pursue this course?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
2495	Sounds like you're a recovering data scientist.	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
2496	Sounds like you're a recovering data scientist.	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2497	Sounds like you're a recovering data scientist.	An article is not a substitute for a project. An article is a bonus thing. But in the end, to get a certificate, you need to do two projects. If you want to have more fun, you can do an article as well.
2498	Sounds like you're a recovering data scientist.	Maybe. I don't know what that means. But yeah. Maybe?
2499	Sounds like you're a recovering data scientist.	"Actually, I don't remember exactly how I do this. The right approach would be to use only train data and not use validation data or test dataset. This is done on purpose for exactly this reason that you mentioned. We want to model a situation in which there is some unseen data – it's truly unseen, we do not see that all – and anything can happen to this unseen data, including the situation that you described, such as there is a value that is only present in the test data. 
Imagine that there is a new iPhone – when you train your model, this new iPhone didn't exist (you didn't have this category) but you apply this model, and then, all of a sudden, there is a new iPhone. How will your model react to this? You need to somehow model this situation. This is totally normal. This can and will happen in practice. We just want to understand what would be the effect of that. That's why we need to see this in training data. This is good and this is how it should be. I hope that is a good answer."
2500	I used a couple of code snippets from online. That's okay, right?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
2501	I used a couple of code snippets from online. That's okay, right?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2502	I used a couple of code snippets from online. That's okay, right?	It is possible to do everything locally. You will need to do it by… I think you will need to do it in module five when we deploy something. Or actually, you will probably need to do it when we do your project. Yeah. It's fine to do everything locally. But if you have access to cloud and you want to practice with this, I do recommend practicing, because cloud is one of the skills that employers are looking for. They really need it. So if you want to be more employable, I do recommend learning about cloud.
2503	I used a couple of code snippets from online. That's okay, right?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
2504	I used a couple of code snippets from online. That's okay, right?	That is definitely okay. As a data scientist, I often Google. I end up on StackOverflow and I see a code snippet – I copy it and use it in my work. I see nothing wrong if you do the same. That's a usual thing. But if you copy the entire project, that's a different situation, right? [chuckles]
2505	I have missed three weeks. What should I do? I’m a beginner and it takes me some time to grasp the lessons. I’m so confused on what to do right now.	Yeah. When it comes to documentation, the good thing about AWS is the community. It's much, much more popular, widespread, common – more companies use it – and more people share their knowledge about AWS. You will find a lot of resources online, even if it's not official documentation. Chances are, if you want to learn a specific service from AWS, you will find a ton of YouTube videos about it, you will find a ton of articles about it, and there will be also official docs, of course, which might be confusing sometimes (most of the time [chuckles]). But then there are also 1000s of other resources that are often better than the official docs. I'm not sure if that's the case with GCP as well. I don't use GCP at work. My exposure to the GCP is more like just playing around with it rather than using it professionally.
2506	I have missed three weeks. What should I do? I’m a beginner and it takes me some time to grasp the lessons. I’m so confused on what to do right now.	Just take your time. Go through the materials at your own pace. If we talk about the syllabus, you will only need the second, third, fourth – up to the midterm project. This should be sufficient – up to the seventh module. This should be sufficient to finish the midterm project and the capstone project. You can treat the modules after 7 as extra. They are useful – they are very useful. But if you're really short on time, you can just skip them and use the materials from the first seven modules to complete your capstone project. Once you complete your capstone project, once you must submit everything you did, then you can take the modules after 8 at your own pace. That could be one approach. Another approach could be just taking them without caring about the certificate. Just focus on learning and do things at your own pace. Don't rush.
2507	I have missed three weeks. What should I do? I’m a beginner and it takes me some time to grasp the lessons. I’m so confused on what to do right now.	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
2508	I have missed three weeks. What should I do? I’m a beginner and it takes me some time to grasp the lessons. I’m so confused on what to do right now.	I guess not. That's the process of learning? Maybe more senior people don't… Yeah, I see where you're going with this. Senior people are not supposed to go bankrupt and default. They are probably supposed to pay back the debt. I guess these are the particularities of this dataset, I assume. Indeed, it's probably a data issue ,or not an issue, but a “characteristic” of this dataset.
2509	I have missed three weeks. What should I do? I’m a beginner and it takes me some time to grasp the lessons. I’m so confused on what to do right now.	"Actually, I don't remember exactly how I do this. The right approach would be to use only train data and not use validation data or test dataset. This is done on purpose for exactly this reason that you mentioned. We want to model a situation in which there is some unseen data – it's truly unseen, we do not see that all – and anything can happen to this unseen data, including the situation that you described, such as there is a value that is only present in the test data. 
Imagine that there is a new iPhone – when you train your model, this new iPhone didn't exist (you didn't have this category) but you apply this model, and then, all of a sudden, there is a new iPhone. How will your model react to this? You need to somehow model this situation. This is totally normal. This can and will happen in practice. We just want to understand what would be the effect of that. That's why we need to see this in training data. This is good and this is how it should be. I hope that is a good answer."
2510	Are the MLOps concepts important to know for a data scientist job?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
2511	Are the MLOps concepts important to know for a data scientist job?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2512	Are the MLOps concepts important to know for a data scientist job?	"I guess? I think yes, because data scientists do not live in isolation from the rest of the company – the rest of the team. The article I referred to actually describes how people work in a team. MLOps is not just about tools, it's also about processes – how exactly your work should be organized in such a way that you can easily maintain it, scale it, and so on. 
This is why data scientists should know some things about this. I guess that answers your question? There are also tools for experiment tracking, so data scientists definitely need to use them. I think I'm just trying to say “Yes, it is important.”"
2513	Are the MLOps concepts important to know for a data scientist job?	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
2514	Are the MLOps concepts important to know for a data scientist job?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
2515	What's the difference between Docker and Kubernetes?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2516	What's the difference between Docker and Kubernetes?	"There was no registration deadline at all for this course. So you can still register. You can still sign up and you can follow the course. You will not be able to submit the homework assignments where the due date is already over because there are already solutions posted, so it doesn't make much sense to submit homework when the solutions are already there. You can follow the course but you will not be able to submit the homework. You can check our frequently asked questions and I do recommend doing this. Please remember the rule – if you have a question, first go check the frequently asked questions, and if you don't find the question there, go and ask in Slack. 
For this particular question, it’s already there. “Don't worry, you can take the course.” Maybe right now it's becoming a little bit more challenging to catch up with everything, but note that you can skip the midterm project. Right now, if you want to catch up with everything, if you skip the midterm project, it's fine. You will be able to do two capstone projects and still get the certificate. Also, you can just take the course in self-paced mode. There is nothing wrong with that either."
2517	What's the difference between Docker and Kubernetes?	Docker allows you to run a Docker image – a set of prepared instructions – this is what you put inside a Docker file. Then like when you do Docker build, Docker takes the Docker file and then creates a Docker image and then you can run this image. You can run this image with Docker locally and that's fine. But what Kubernetes does is it allows you to run many different Docker images. Kubernetes is a container orchestration platform. It allows you to take all these Docker images that you’ve built on your local machine with Docker and it allows them to execute somewhere in a cluster. It doesn't have to be on your computer. It can automatically provision more machines, if you're running out of machines and all that. It's not 100% correct what I will say, but you can think about this as “Kubernetes orchestrates Docker images.” You have some images and then you just put them in Kubernetes, and Kubernetes figures out on which machine to run it, how many resources to give it and then it just runs (orchestrates) all these things. So you can think about this as Docker is for local use and Kubernetes is for deployment in the cloud.
2518	What's the difference between Docker and Kubernetes?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
2519	What's the difference between Docker and Kubernetes?	"Tim
I think a couple people asked this in ML Zoomcamp Slack. I was thinking about this a little bit. I think it's sort of a philosophical thing [chuckles] where we think about models and Bento is sort of separate a little bit from code. So your project directory is where all the stuff that you'll commit to Git. I don't think we think that it's the right model to commit your models to Git, for example. We think of ourselves a little bit more like Docker. You don't have Docker images in your project directory, right? You just have your Docker file and that's what you commit to Git. In the same way, we store your models and your Bento in that dedicated directory, and then sort of leave it up to you to push it to different places where you might build or where you might need it
Alexey
Do you know why you decided to save it locally on your local file system? For example, what MLflow does – you can save it to S3 directly, but in the case of Bento, you always first save it to local storage and then you decide “This model I want to export, but for the rest, I don't care.”
Tim
Right. First, I think it’s because that's the easiest way to do it, but also because I think with BentoML, the workflow is that a lot of times, somebody's building a model on their laptop, or somebody's building a model in a slightly less distributed system and they're assembling their Bento there. So I think that the fastest way to access that model is locally. We do give you the ability to push that model and pull that model from other places. But ultimately, when you assemble your Bento and then you assemble your container, all that stuff has to be there. I don't think we wanted separate abstractions for when a person is training and saving, versus when a person is building a Bento and pulling in the model and assembling the container. It's just the same abstraction, which is just a local repository which contains models.
Alexey 
Maybe you don't want to save every model every single time you change one single parameter to the cloud. [Tim agrees] Maybe at the end, after all the experiments, you want to save the final one. Meanwhile, for MLflow, the use case is different – you actually want to save every single experiment because you want to see the results of these experiments.
Tim
Right."
2520	In which cases should you apply logarithm to the target variable?	"Tim 
Not really. We have users who use both Streamlit and Gradio. I think Streamlit and Gradio are a bit more about the presentation side. We do have a couple of users who have asked us about deeper Streamlit or Gradio support and I think that's something that's probably coming up in the next few months.
Alexey
For those who don't know what Streamlit or Gradio is – this is a way to create an interface for your services. Well, Gradio is focused on machine learning, meaning you create interfaces specifically for models. And then Streamlit is just a Python package for creating interfaces. We actually had a project at DataTalks.Club about Streamlit. I actually even have a tweet about this. This is what Streamlit looks like. In the video, you can see that you have this interface, and then based on the toggles that you move the core of your model changes. It gives you an easy way to build an interface like that. Meanwhile Bento focuses on quite a different use case, right? It’s not building interfaces, but serving models.
Tim 
Right, yeah. I think one of our users was asking for a really easy way, so that once you have your model and your Streamlit UI to be able to create a Bento deployment with one line in that way, anybody that you gave your Streamlit app to would be calling to that service and it could work anywhere. Which is a nice feature, I think.
Alexey 
Yeah, I think it is. I imagine if you have a Pydentic class there, then you can easily generate a Streamlit app for that.
Tim 
Right. Oh, that's a good idea. I hadn't thought of that. Yeah.
Alexey
And then in addition to that, the Swagger UI that you have, could be like having the actual interface. 
Tim
Oh, yeah, that would be cool. [chuckles]
Alexey
That could be a good Hackathon project, right?
Tim 
Yeah, for sure.
Alexey
From what I see – let's say you’ve built a model. Then you deployed this model and you have this nice API. You can query it with CURL. But what if you want to demo this project to your manager, or a manager of your manager, or somebody who does not necessarily have a technical background? If you give them the URL and say, “Okay, you need to execute the CURL command with a post request,” they would be like, “CURL what? What do you want from me? Just show me how it looks.” This is when tools like Streamlit come in handy. Instead of giving them a command line interface to query it, you just give them “Okay, this is the link. Play with this.” If Bento could do something like that – automatically generate things like that – many data scientists would thank you.
Tim
Yeah, I was just looking because I figured there's a library that turns Pydantic models into Streamlit apps. It looks like there is one that does that. I wonder if we could just plug into that app and then have that same thing. 
Alexey
That's cool. 
Tim
Yeah, that's awesome. That's kind of the cool part of open source – there are so many people out there with so many different projects and we get to collaborate with them and kind of build the best collaborations and combinations of these tools."
2521	In which cases should you apply logarithm to the target variable?	Yeah. If it works for you, use it. I don't mind.
2522	In which cases should you apply logarithm to the target variable?	Maybe go check out module two, where we talked about that. I think during Office Hours, we also covered that. I don't remember, to be honest, what I answered to that, but usually, when you have a long tail that goes to plus infinity (to the right) this is when you want to apply a logarithmic transformation and usually when you don't have negative values in your original data  is when you use it.
2523	In which cases should you apply logarithm to the target variable?	I'm not sure I understand the question. The question is “Why test_train_split splits in two, but here, we split into three parts? For that there is a video in week one that explains why we need three datasets, not two. So you can go check it out. There is a problem called “multiple comparison problem” and that's why we use test/split to account for that. You will see later, in module three – there we can also use SciKit Learn train_test_split to split our original dataset into three parts and this is exactly what we will do.
2524	In which cases should you apply logarithm to the target variable?	I think for binary features, just one and zero makes more sense than OneHotEncoding. OneHotEncoding will take care of that but you will have two columns that are the exact opposite of each other. So that is a bit redundant. I think it's better just to convert it to a Boolean column with ones and zeros.
2525	How to get out of tutorial hell?	"You will get out of tutorial hell when you start doing the project in this course, because it will be just a set of guidelines – this matrix – but you will have to do the rest yourself. You will have to find the problem yourself, you will have to do exploratory data analysis yourself, you will have to do data preparation yourself, you will have to do everything yourself – there will be no homework with the exact steps that you need to do to have a project. And this is how you get out of tutorial hell. 
Let's not think about this particular course, but in any setting – you want to find the problem that you want to solve. Once you have the problem, then try to think “What is the best way to solve this? What is the shortest way to solve it?” Or, at least, “What is the next step I need to take to solve it?” And then try to work your way through solving this problem. So focus on the problem. If you just do tutorials, you're not solving problems – you're just doing tutorials. So focus on the problem."
2526	How to get out of tutorial hell?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2527	How to get out of tutorial hell?	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
2528	How to get out of tutorial hell?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
2529	How to get out of tutorial hell?	Yeah. If it works for you, use it. I don't mind.
2530	Can you explain what exactly model.fit(x_train, y_train) does? Thanks.	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
2531	Can you explain what exactly model.fit(x_train, y_train) does? Thanks.	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
2532	Can you explain what exactly model.fit(x_train, y_train) does? Thanks.	I haven't read this one, but I heard it's good. It's called Learn Kubernetes in a Month of Lunches. I heard that this is a good book. I have not tried it, but my understanding is that for each day, they give you a small chunk to implement. You implement this and then you learn Kubernetes this way.
2533	Can you explain what exactly model.fit(x_train, y_train) does? Thanks.	"You probably mean for logistic regression, right? For logistic regression, it's doing something very similar to what we saw in module 2. It is trying to minimize the… (I'm just trying to figure out how I can explain it without going into too many details and not to get lost myself and not to confuse you). It's probably a good idea to refer to some other course. I think I partly talked about this. Let me check. Here I talk a little bit about this, in ML Zoomcamp Office Hours week #4, where I explain a little bit how exactly it works. But it's a very superficial, let's say, overview of exactly what's happening there. 
If you want to learn in more detail how exactly the process works there, maybe you can just go to Google and search for “gradient descent, logistic regression,” or something like this. The first result will probably have a good explanation of what's happening under the hood. SciKit Learn does not use this exact method – it doesn't use gradient descent – it uses something more complex (more advanced) than that. But this is roughly what's happening under the hood. I hope you're satisfied by this answer. I don't think I can give you a better one with the time we have. It will be like a separate lecture. If you don't want to go into details, you can think of this as something that’s similar to logistic regression happening there but we add sigmoid on top of that."
2534	Can you explain what exactly model.fit(x_train, y_train) does? Thanks.	Okay. Yeah
2535	What to do after courses? Where do you see us after courses?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
2536	What to do after courses? Where do you see us after courses?	Just take your time. Go through the materials at your own pace. If we talk about the syllabus, you will only need the second, third, fourth – up to the midterm project. This should be sufficient – up to the seventh module. This should be sufficient to finish the midterm project and the capstone project. You can treat the modules after 7 as extra. They are useful – they are very useful. But if you're really short on time, you can just skip them and use the materials from the first seven modules to complete your capstone project. Once you complete your capstone project, once you must submit everything you did, then you can take the modules after 8 at your own pace. That could be one approach. Another approach could be just taking them without caring about the certificate. Just focus on learning and do things at your own pace. Don't rush.
2537	What to do after courses? Where do you see us after courses?	There is another metric called MAPE (mean absolute percentage error). This will give you a percentage, but there are some problems with this. You can read about the problems of this formula – of this metric. There are some problems with this metric, and usually, what you do is calculate both mean/average precision, RMSE, then you can also look at mean average error, then you can also look at some other business metrics that are important to you. Usually you don't look at just one number.
2538	What to do after courses? Where do you see us after courses?	I'm happy to hear that you liked it. I cannot talk more about using GCP for completing the course. But, by all means, use it. I don't mind. I actually encourage you to try different things outside of the course. To me, actually, when I look at the user interface of Google Cloud Platform, it feels more “friendly”. It feels like [chuckles] they actually care about developers – they care about aesthetics – while in AWS, it's more like developers creating interfaces. It’s different, right? [chuckles] So, by all means, use GCP if you like it. Maybe you can also write some tutorials and then include that in the notes. Other students will appreciate that, too.
2539	What to do after courses? Where do you see us after courses?	Working, I guess. So yeah – work after courses. That's where I see you.
2540	What would be the main skills/algorithms/tools that I need to demonstrate in my portfolio to get a computer vision job? Some key factors to highlight?	"To be honest, I don't know what the typical requirements for computer vision jobs are. They can vary from one company to another. I will give you a general piece of advice that I would give in any other case. You need to do some research about what the company is doing and understand what their problems are. What kinds of problems are they solving? Then try to solve a similar problem. For example, if it's a company like where I work – online classifieds – you can go to Kaggle and look at what kind of computer vision problems companies in the online classifieds domain are putting out there. 
Or just go through the tech blog and see what kind of articles that this company is publishing. Then try to build the project around that. For example, let's take OLX, the company where I work. You can find an article here, Fighting fraud with Triplet Loss. In this article, we show how we use computer vision to find duplicates – how to find image duplicates – and then this article describes that. So if you do something similar in your project, you can just add it to your CV and then talk about this during your interview. That's much better than just a random project. It will certainly help to pique attention. I guess that's the most important factor, try to do some research, find what kind of problems the company is interested in solving and then solve these problems. Then use it to build your portfolio."
2541	What would be the main skills/algorithms/tools that I need to demonstrate in my portfolio to get a computer vision job? Some key factors to highlight?	Yeah, it's probably not the best practice. I wouldn't do this in a real-life situation. In a real-life situation, we simply will not have unseen data. But the idea here was to get a dataset that is already familiar to you – the dataset we used in the previous homework – and just convert it to a binary problem. That was the goal we had in mind – how can we make it easier for you to learn these things without overloading you with a new dataset? That was the simplest approach we decided to take to actually do it here. Another alternative could have been, instead of asking you to compute the mean, just give you the number, but with the mean, it's a little bit more interesting, I think.
2542	What would be the main skills/algorithms/tools that I need to demonstrate in my portfolio to get a computer vision job? Some key factors to highlight?	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
2543	What would be the main skills/algorithms/tools that I need to demonstrate in my portfolio to get a computer vision job? Some key factors to highlight?	Yes, but I cannot guarantee that you will arrive at the same answer as without it. To be on the safe side, it’s better to use the materials from the course. Nothing is stopping you from trying it with Scikit Learn as well. If you think it's more useful for you to do it this way, then by all means do it. Again, homework is just for checking your understanding of the course content. You decide what you do with your time. I would recommend first understanding how to implement linear regression with NumPy. Then in week three, when we start covering SciKit Learn, you can just use it from that point on.
2544	What would be the main skills/algorithms/tools that I need to demonstrate in my portfolio to get a computer vision job? Some key factors to highlight?	"One thing I would suggest is to check out Office Hours from last year (2021) if you go to the midterm project, there are these Office Hours. I think it's week nine, where I show how to use the count vectorizer for doing one hot encoding. With count vectorizer, you can apply some filtering. For example, you can say that “I'm only interested in categories that appear at least in 100 observations.” Then, instead of looking at all possible values, you look at only the frequent ones. So go through this document. 
I think there are a lot of different ways you can add filtering here. It could be minimal frequency, it could be something else. It's actually kind of misusing count vectorizer a bit, because count vectorizer is supposed to be used with text features, and not with usual categorical features. But you kind of hack it, in a way, if you say, “Okay, turn these categories into text, and then you train the count vectorizer on them.” So yeah, go through this thing. You can also find the video from that, where I do it live. That's very useful. Again, if you have a huge load of different categories – different values – then the count vectorizer will still require some memory to train. 
There is an alternative. It's called a hashing vectorizer. This one. It does not require training. So you can say, “Okay, I only want to have only like 10,000 features and not more than that.” And it will actually take a word, and it will compute a hash of this word, or value by category, and it will randomly put in one of the columns of this vectorizer. Not randomly – it will compute hash and then put in one of them. So it will be deterministic, of course. Sorry, I didn't choose the right words. 
This is a good way to save memory if you have a lot of categories. So, hashing vectorizer – it works in the same way as count vectorizer, except you don't need to do fit. Actually, you can actually just go through this and read it yourself. It explains everything that you need to know when you compare this one versus count vectorizer."
2545	Must we really use Java in part 6? And will there be a part in our project that will require us to write Java?	No, you cannot.
2546	Must we really use Java in part 6? And will there be a part in our project that will require us to write Java?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
2547	Must we really use Java in part 6? And will there be a part in our project that will require us to write Java?	We currently do not have any course partners yet. If you work at a company and you want to partner with us, please reach out and we will see how we can do it. In one of our courses, which was our first iteration of the Machine Learning Zoomcamp, we partnered with a company called Delphi. They got two interns from our course and they were quite satisfied with the outcome. You can actually read more about the work of these interns in our articles – Interview with Valerii Chetvertakov and then another one, Interview with Ken Wu. You can learn more about the interns and if you think that, at your company, you need good interns or juniors or you want to partner in any other way, please reach out and we can see how to make it work.
2548	Must we really use Java in part 6? And will there be a part in our project that will require us to write Java?	I will defer this question to Ankush. He's unfortunately not here right now, so maybe ask him in Slack. From what I understood, it better describes the internals of Kafka. You don't need to know any Java. He explains everything you need. No, you will not need to use Java for your project. You can, but you don't have to.
2549	Must we really use Java in part 6? And will there be a part in our project that will require us to write Java?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
2550	Why Web to GCS to BigQuery, and not Web to BigQuery?	"Alexey
I guess that's a question about our ETL that we wrote – why we wrote first to GCS and didn’t skip it.
Jeff
I think this comes up at some point when talking about having your data lakes. A lot of times, it's good to put the raw (or fairly close to raw) data into a data lake and then you have the opportunity to go and get it if you need to do something with it. That’s kind of the really raw unprocessed data. And then you can take it from there into BigQuery and use it in BigQuery for people who want to go and do some analytics work with it. There are a lot of different workflows these days. A lot of things are happening more directly with BigQuery. Things going right into data warehouses or “data lake houses” as are often referred to. These are both Google products – GCS and BigQuery – and they're just getting more and more tightly integrated as time goes by. We're able to do more and more and just actually store BigQuery data in Google Cloud Storage, and just kind of access it directly. There are just so many different ways that it happens, but a common workflow has been to first put the fairly raw data into the lake and then do things with it later in the warehouse. That's the flow that we're showing here. Ankush, any comments?
Ankush
I think, as you said, this is just about different patterns. One pattern is obviously writing directly, one is writing to GCS and then to BigQuery. And obviously, everything has its advantages and disadvantages. But writing to a data lake always gives you a certain backup or an advantage, per se. Because now if you want to move away from BigQuery and use Postgres or use Snowflake, you don't really have to care – you can start from GCS all over again. What happens if BigQuery is not accessible or it's getting expensive for machine learning load, and then you can directly read from GCS other than actually doing it through BigQuery. Right now, our example is very small and it does make sense to directly load it to BigQuery. 
But what happens if your data size is increasing on a daily basis, or a weekly basis, and you just cannot put that into BigQuery? All your data is very unstructured and you need to spend the time to figure out what kind of data you want to put into BigQuery. So all of these questions really come up when you have a lot of data coming and a big variety of data coming in. Using something like a GCS bucket, where you can just dump your unstructured raw data format, and then later on, maybe six months down the line, you figure out, “Hey, this data is really important. I want to put it into analytics. Let me pull this into BigQuery with some work.” And I think that's a great pattern to start with. I would always suggest looking into this pattern this way, but obviously, there are also companies and some data sources, where it does make sense to directly put into your data warehouse or BigQuery."
2551	Why Web to GCS to BigQuery, and not Web to BigQuery?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
2552	Why Web to GCS to BigQuery, and not Web to BigQuery?	"Jeff
I don't know. When hosting meetups, I'll just draw on [a board]. It's great to have hybrid options for folks. People who can be remote, people can be in-person – it definitely adds complexity in how things are done. There is a lot more with logistics and things. The short answer is no. But if people are interested in some kind of online Prefect-specific course, send me a message in Slack. I'm there. Let me know about your interest level. It sounds like there's some interest.
Alexey
At least six people are interested. 
Jeff
[chuckles] At least six. 
Alexey
Maybe more, but they just didn’t know that they could vote."
2553	Why Web to GCS to BigQuery, and not Web to BigQuery?	For Spark, we use Python. So it's PySpark. So what Java aspects will be covered? Ankush already uploaded the videos for week 6, just go and check that out.
2554	Why Web to GCS to BigQuery, and not Web to BigQuery?	I will not answer this right now. We did not plan anything like that. I think Anna prepared some materials about how to deploy Prefect, not with Terraform, but with some Google Cloud Platform stuff. I might be mistaken, but in practice, I think we use something like Kubernetes. Kubernetes is not managed through Terraform. I will leave this question answered and in the next Office Hours when we'll talk more about Prefect. Somebody from the Prefect team will answer this question. Probably Jeff will be in our next Office Hours.
2555	How can we contribute to the course?	I think I already showed you. If you go to our YouTube channel, and search for “data engineering,” you will see a lot of interesting videos. There’s this Getting a Data Engineering Job video and then this one could also be quite relevant.
2556	How can we contribute to the course?	I will defer this question to Ankush. He's unfortunately not here right now, so maybe ask him in Slack. From what I understood, it better describes the internals of Kafka. You don't need to know any Java. He explains everything you need. No, you will not need to use Java for your project. You can, but you don't have to.
2557	How can we contribute to the course?	Since we had more signups this year than last year, I hope it will be more than 100. But let's see.
2558	How can we contribute to the course?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
2559	How can we contribute to the course?	"Alexey
The best way to contribute is to take notes and include them to the notes section. And tell your friends about the course. As a part of your Learning in Public, spread the word. It will help us. And don't forget to give us a star if you have."
2560	How should I organize my data lake – by departments, methodology? What are the best practices or any article to guide me?	"Jeff
I'm not sure exactly what's being referenced here. But I'm guessing maybe the GitHub question from the homework – the GitHub block question turned out to be a little trickier than anticipated for folks. This is a great question. I got a couple of things on it here. One is that there is some stuff in the documentation, but maybe not a complete handhold of things. The storage doc does talk about code storage and flow storage in general here, and a little bit about how to do it. 
If you would like more detailed information, it’s always a great idea to look and think about me opening an issue. If you go to the GitHub repository for Prefect, you can click on “new issue”. If you want, you can propose a feature enhancement, if you want to call it that, or report a bug if you think you should have more information, and then put in what you would like to see. 
If you want to go one step further at something I really recommend, even like the next extension of Learning in Public is contributing to projects. This is an open source project here. You're welcome to contribute to it. You could go ahead and fork the repository, clone it down, and make a change to the documentation once you've figured things out there. Lots of lots of folks I know did get there with some support, through the FAQ and through Slack. It's partly what the videos are also for, in the course, to help you out as you're going. That's what I would suggest there. 
But I will also suggest to our docs folks, that we perhaps enhance the GitHub section in particular."
2561	How should I organize my data lake – by departments, methodology? What are the best practices or any article to guide me?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
2562	How should I organize my data lake – by departments, methodology? What are the best practices or any article to guide me?	BigQuery is a data warehouse and it's optimized. It's usually faster. AWS Athena is a data lake. You will see the difference in week 2 – there is a video that explains what a data lake is. Then in week 3, you will see what the data warehouse is. But AWS Athena is more like a data lake. You can still run all these queries, but maybe they will be slower and I also think they will be cheaper. For analytical queries that – for queries where you want to get results quickly, I usually use BigQuery. Again, like these are two different clouds, two different technologies. The counterpart of BigQuery would be Redshift. But I think Redshift is slower than BigQuery as well.
2563	How should I organize my data lake – by departments, methodology? What are the best practices or any article to guide me?	"Alexey
Ankush will probably be a better person to answer that. Maybe ask that in Slack and see what the answer is. From what I know (the way we organize it where my work is) each department or team has its own S3 bucket. Inside this S3 bucket, we have different folders, or prefixes, for different data sources – for different projects, let's say. And then inside each project, there could be different specific data sources. And in these data sources, data is usually partitioned by day, but there could be other partitions, too. So there are partitions inside these folders and then, eventually, there are parquet files. I hope that answers your question. There are probably articles about this. What I described is something that our data engineers did. I did not take part in that. I just use it. Maybe if anyone knows any good articles about that, please share them in Slack."
2564	How should I organize my data lake – by departments, methodology? What are the best practices or any article to guide me?	No, you cannot.
2565	What Java aspects will be covered in the course? Will it be Java + Spark?	The easiest way to contribute is to just give us a star, share the course with your network on LinkedIn, Twitter, other social media channels. Another way is to take notes and contribute the notes to the course. You can also create time codes. I think there is a video about that. If you go to issues there are open issues for things without time codes. We will also need to add new videos here, for which we don't have time codes yet. You can contribute by creating time codes there. In the video, there is an explanation of what time codes are. Also, when you do projects, you can also do a demo of your project – that would be awesome. Another thing you can do is contribute to our list of frequently asked questions. Some of the questions that you see in Slack are not there. It still happens even though our list is quite comprehensive. If you see that there is a question that is not there, please add it there. It will help us a lot.
2566	What Java aspects will be covered in the course? Will it be Java + Spark?	Our thoughts are that this is indeed an exciting area. This is an in-demand skill and that's why we have a section about analytics engineering in this course.
2567	What Java aspects will be covered in the course? Will it be Java + Spark?	For Spark, we use Python. So it's PySpark. So what Java aspects will be covered? Ankush already uploaded the videos for week 6, just go and check that out.
2568	What Java aspects will be covered in the course? Will it be Java + Spark?	I don't know. I never tried to do it there. Maybe you can just go and try it. If it works, let us know. And if it doesn't, also let us know so we know the answer.
2569	What Java aspects will be covered in the course? Will it be Java + Spark?	"Alexey
The best way to contribute is to take notes and include them to the notes section. And tell your friends about the course. As a part of your Learning in Public, spread the word. It will help us. And don't forget to give us a star if you have."
2570	The retries of tasks can be notified? Or do we have to do this logic manually?	Everything you do here is individual. You don't form teams.
2571	The retries of tasks can be notified? Or do we have to do this logic manually?	You can follow the same setup here. Apart from creating a virtual machine and doing port forwarding, everything else that you do in this video also applies to a local Linux environment. Maybe you will also need to install Google Cloud SDK, because on a virtual machine, you already have it – you don't need to install it. Locally, you will need to install it. Apart from that, you can just follow the same stuff for a Linux computer. For Windows or Mac, I don't think we have a guideline.
2572	The retries of tasks can be notified? Or do we have to do this logic manually?	"Alexey
I guess this is related to Prefect, right? Please ask this in Slack. Jeff is monitoring Slack, so he will answer that."
2573	The retries of tasks can be notified? Or do we have to do this logic manually?	Yeah. I am a data scientist and, as a part of my role, I quite often needed to create data pipelines. Knowing data engineering, knowing Spark, and things like workflow orchestrators was really helpful, especially when I worked in a startup and we didn't have a data engineering team that could support us. We needed to do everything on our own and this is how I picked up these data engineering skills and they were immensely useful. If you're in data science and machine learning, knowing how to build data pipelines is extremely useful. Maybe knowledge of data warehousing and analytics engineering and dashboarding is a bit less relevant, at least to what I do as a data scientist, but I think it could still be useful.
2574	The retries of tasks can be notified? Or do we have to do this logic manually?	"Jeff
There is a link here, as I shared in Slack. If you click on the Blocks page, anywhere in the UI here, there’s Prefect docs right here on the bottom left, if you're logged into Prefect Cloud."
2575	Can you explain regularization better? I did not really understand it from the course video. Thank you	"Because we were a bit delayed with the BentoML module, we will also give a few days of extension for the project. But if you cannot submit your project by that time either, don't worry. We will have two more projects. We will have a capstone project and capstone project 2. If you didn't finish or you failed the midterm project, you get to resubmit it one more time. And then you do your capstone project and capstone project 2. 
At the end, to pass the course (to get the certificate) you need to pass two projects out of three. You have this option to try it one more time. If you open the deadline calendar you can see that if we extend it by a couple of days, let’s say Thursday – we will probably also extend the evaluation by a little bit. It's not a problem to extend it. I just need to see how exactly it affects the overall plan. You'll have at least a couple of days to wrap it up. If you haven't started deploying your model, do it now. Stop everything you’re doing and take care of model deployments. Don't do it on the last day. This is an important part. 
I know from my experience, talking to students last year, many of the students procrastinated on that – they were leaving it to the last day. Then on the last day, many things didn't work out. These things take a bit of time, so you need to be ready that you will have some surprises and that not everything will work from the first attempt. So it’s better to drop everything you’re working on now, take whatever best model you have so far and do the rest of these criteria. You get my point. Start working on deploying your model now."
2576	Can you explain regularization better? I did not really understand it from the course video. Thank you	Go to LinkedIn, find a job that you like, click “apply” and that's all you need. [chuckles] I don’t know what you are waiting for.
2577	Can you explain regularization better? I did not really understand it from the course video. Thank you	Why do you think it will be irrelevant? Do you think we'll just identify all churned users, bribe them with discounts, and live happily ever after? At some point, other users will consider leaving too, so we actually need to run this model regularly. We will need to update this model regularly – retrain – because the model will probably make mistakes. So it's probably a never-ending process here. We will always need to keep an eye on this model – probably in an automated way. Maybe we have a process that automatically retrains it every half a year or something. But I don't think just deploying this model and using it will solve the churn problem. Maybe I misunderstood your question. I don't know.
2578	Can you explain regularization better? I did not really understand it from the course video. Thank you	Yes, that's the main reason we have two capstone projects. This is for those who are catching up to be able to get the certificate. Now there are holidays, like Christmas holidays and then January holidays in some countries – like in Russia, for example, people usually take holidays not during the end of December, but at the beginning of January. No matter in which part of the world you are in, you still have one month to catch up (November) and then start working on the capstone. Actually, I’ve now realized that the capstone starts quite soon. But for capstone 1, you only need the first six weeks or so, and then you can finish this project. Then during the holidays, you can watch Kubernetes, neural networks and so on, if you want. Then you can do capstone 2. That's probably better. But yeah, it's fine. You should do 2 capstones.
2579	Can you explain regularization better? I did not really understand it from the course video. Thank you	"I need more information to actually help you here. I don't think I will have enough time to explain it here. What you can do is maybe check out “regularization Andrew Ng” via Google. He is really good at explaining regularization. He's explaining it from a different point of view, but the explanation he gives is really good. So check it out. 
The problem we had here in this course is – sometimes you have correlated variables, when you have one variable and another variable mean the same thing. When this happens, you cannot invert the matrix. That's why we use regularization here. But it has a much wider scope. And Andrew Ng’s explanation will help you. If you have specific things that you did not understand, please ask in Slack, or maybe next time. But this is such a broad question that I do not know how to best answer and help you."
