	sentence1	sentence2	label
0	plans another zoomcamp near future	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
1	plans another zoomcamp near future	alexey 33 submissions think decision extend deadline good one hopefully end week see higher number understand project much difficult watching course following along 33 people – probably put lot effort thanks saw one person write put 100 hours already project outstanding sorry cruel 100 hours lot	0
2	plans another zoomcamp near future	alexey question data engineering zoomcamp zoomcamp zoomcamp want mlops zoomcamp near future month two probably comes data engineering zoomcamp still need talk probably next year happens next year sejal data engineering zoomcamp 's possibility may revise recordings videos example airflow setup little time-consuming also little effortful compute resources side maybe might find alternative setup instructions works smoothly currently plans refactor existing zoomcamp makes easy next iterations would otherwise release project-based iteration think recordings would still remain could new project cycle alexey 're talking finished project right another iteration projects reason n't able work project right another iteration immediately let 's say finish reviewing week next immediately 'll start another cohort projects	1
3	plans another zoomcamp near future	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
4	plans another zoomcamp near future	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
5	jenkins gitlab used instead airflow	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
6	jenkins gitlab used instead airflow	victoria alexey extent think like general purpose ci/cd tools n't use data workflows principle things like airflow workflow engines tailored specific use case nice things victoria definitely different use case overlap fit different use cases alexey must admit use jenkins work n't chuckles please n't regret chuckles victoria said legacy so… alexey yeah still legacy need maintain nobody wants touch n't know happens nobody wants spend time moving elsewhere kind runs personally would n't come anywhere near jenkins like jenkins	1
7	jenkins gitlab used instead airflow	victoria 'm sure answer mean yes definitely adding part project least workshop reason want people learn also consider people entirely new concept thought would much overhead try cover things already quite complex day-to-day even though work daily dbt work slow-changing dimensions daily basis example 's also think 's something look moment use definitely dbt especially deletions implementing load step done hooks think 's section advanced knowledge link hooks like incremental models mentioned changing dimensions n't link could link something could use snapshots think added concept updates guess could also pre-hook post-hook also something would incremental model incremental model loads new chunk data use merge depending workers use use could insert update 's something would work unique key project also final project 's understand complex go 's limit hope answers	0
8	jenkins gitlab used instead airflow	alexey well good job keep working almost finished first week finish start week two finish week three chuckles follow sequence videos playlist github sufficient think questions go slack ankush keep going alexey chuckles yes exactly ’ almost finished first week good sign first week year pretty tough many people lot problems docker whatnot 're almost finished good sign ’ good track keep working	0
9	jenkins gitlab used instead airflow	alexey add comment least workplace airflow instance set us data scientist would go instance airflow managed data engineers devops engineers would use never needed actually run docker compose moment multiple dags orchestration tool quite useful would personally maybe go luigi favorite one need think people well since people know luigi many like maybe airflow would actually better choice consider sejal would also like add question docker compose used docker compose airflow style use official setup seeing first time well 's unnecessarily complicated especially xcom variables forth generally docker compose acts wrapper docker containers production workflows docker containers used integration ci/cd pipelines wherever 're going deploy docker compose docker compose part helpful development environments case let 's say mock version airflow environment production want test dags local development environment use docker compose make things easier running simple commands like ‘ docker compose ’ would make fine instead running ‘ docker run container name etc whatever want ’ terms convenience terms usage ’ generally used development environments production alexey think first part question “ close usual data engineer role workflows ” workflows see work complex steps probably also case need start something already – would n't call complex pipeline – three four steps already something eventually might grow bigger bigger add steps ankush would also like add 're cloud maybe use airflow deploy docker maybe actually use service provided cloud provider one similar airflow think airflow would used internally like cloud composite google cloud platform would using terraform sejal yeah used aws step functions aws setup step functions combination lambda place airflow ankush said also something wanted say really depends production self-hosted cluster kubernetes wherever want use pure airflow version company could kind setup use native airflow cluster could cloud-based setup convenient option use managed services cloud composer aws also version called managed airflow	0
10	train.py train model save disk required show score validation data test data models script	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
11	train.py train model save disk required show score validation data test data models script	n't think takes lot time first need record videos prepare homework imagine adding collaboration startups well maybe sometime future n't know somebody startup listening right want hire people graduate zoomcamp please reach let 's arrange something	0
12	train.py train model save disk required show score validation data test data models script	's actually think 's useful n't train.py produce pickle file let 's say want retrain model using different parameters – 's useful see changed score real life like projects work train.py 's always one file sometimes 's bunch files somehow log performance think 's useful n't want reason n't	1
13	train.py train model save disk required show score validation data test data models script	took course ago back useful think something use python immediately could helpful 'm sure would good recommendation current course course good would n't compare stanford course andrew ng simply different course 'll get lot theory one practical hands-on – 's mostly writing code together maybe take	0
14	train.py train model save disk required show score validation data test data models script	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
15	please clarify use from_logits=true	classification classification let 's say n't predicting price let 's say predicting 's average number hours image 1 case would need another layer – rather layer 's called activation n't activation 're still… ’ trying formulate way 's confusing think 's better go lecture rewatch ’ essentially retelling thing need from_logits price variable loss mean squared error logits loss categorical cross-entropy categorical cross entropy usually used multiple categories need use logits 's numerically stable n't use – stay activation softmax n't use legit hope 's clear regression n't need use homework need use from_logits either homework two classes 's binary classification problem multiple classes – let 's say three classes four classes five classes – need use logits loss categorical cross-entropy loss categorical cross-entropy 's recommended use from_logits=true 's numerically stable	1
16	please clarify use from_logits=true	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
17	please clarify use from_logits=true	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
18	please clarify use from_logits=true	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
19	please clarify use from_logits=true	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
20	understand growing importance data cataloging think 's beneficial put something together data cataloging tools openmetadata etc	alexey think one topics relates first question regarding important outside scope course data governance tools could one things n't know 're data engineering per se 're related data engineering think general data governance – organize data manage data company definitely important thing n't know us actually put something together one know anything openmetadata etc. victoria work lot trying select data catalog n't used openmetadata anything like would suggest time yes try 's also something quite new maybe advanced judging size modern data stack 's probably something company ask n't feel pressured 'm trying say yes 's important – would quite nice read understand concepts time try openmetadata open source data catalogs 's great feel pressure 'll probably get 's also data engineering side would say probably like glue crawler something like 's also maybe data engineer would participate regarding data catalog also kind saw data catalog dbt part covers part 's bit think whole pipeline everyone made project maybe 'll something passwords time alexey use internal data catalog 's pretty convenient n't think 'll able record anything actually maybe 's good idea invite somebody webinar knows someone talk tools maybe connect us webinar victoria 've episode data governance right long time ago alexey yes victoria ca n't remember name book alexey yes book victoria book read project around selecting data catalog alexey one like discussion need data governance data catalog think one points data governance data catalog nice episode 's theoretical 're looking hands-on practice right place definitely interesting place least start understand data governance yes book check	1
21	understand growing importance data cataloging think 's beneficial put something together data cataloging tools openmetadata etc	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
22	understand growing importance data cataloging think 's beneficial put something together data cataloging tools openmetadata etc	alexey think 's helpful 's necessary helpful happens dags run airflow dags tasks tasks often kubernetes jobs something like infra – many companies infrastructure managed kubernetes 's quite simple get new container execute new job kubernetes n't think 's necessary think 's helpful 'm data engineer – 'm data scientist data scientist knowledge kubernetes quite helpful could debug jobs 'm running airflow run kubernetes could see logs would say want learn hurt get hired without kubernetes knowledge materials course sufficient get hired pick kubernetes work example agree victoria victoria yeah think kubernetes getting popular ca n't hurt	0
23	understand growing importance data cataloging think 's beneficial put something together data cataloging tools openmetadata etc	ankush yes yes think transfer service right yes question data using aws want run test around data source feel free also leave comment maybe even know goes azure blob storage	0
24	understand growing importance data cataloging think 's beneficial put something together data cataloging tools openmetadata etc	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
25	somehow learn many points homework etc currently	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
26	somehow learn many points homework etc currently	honest n't used uvicorn used gunicorn n't know matters n't actually checked benchmarks use gunicorn also use others – n't remember ones reason basically	0
27	somehow learn many points homework etc currently	think around 200 204 think something like	0
28	somehow learn many points homework etc currently	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
29	somehow learn many points homework etc currently	yes leaderboard think shared link couple weeks ago slack maybe actually need add link course repo 's time look slack course ml zoomcamp channel find	1
30	must-have resources/materials beginners ml/ai	building locally fine	0
31	must-have resources/materials beginners ml/ai	n't think takes lot time first need record videos prepare homework imagine adding collaboration startups well maybe sometime future n't know somebody startup listening right want hire people graduate zoomcamp please reach let 's arrange something	0
32	must-have resources/materials beginners ml/ai	's broad question really depends background let 's say 're data analyst probably learn things differently software engineer ’ come background probably want take different approach really depends background helpful course coursera andrew ng ’ old – around 2012. one first courses took another thing helped kaggle course current course based book machine learning bookcamp hope point somebody say must-have resource beginner machine learning ai 'm bit biased course might say n't know 's necessarily true 'll leave decide whether 's case	1
33	must-have resources/materials beginners ml/ai	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
34	must-have resources/materials beginners ml/ai	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
35	host ml models github pages	's really case-dependent saw slack remember correctly carlos asked needs drop features – seemed okay drop highly correlated feature assistant name target variable churn basically people work one particular person tend turn less people work somebody else think case seems like relevant feature include n't know – 's really case-dependent usually need sort domain expertise decide feature introduces leakage carlos ’ case think 's safe leave drop see performance validation dataset changes gives idea important feature	0
36	host ml models github pages	honest n't used uvicorn used gunicorn n't know matters n't actually checked benchmarks use gunicorn also use others – n't remember ones reason basically	0
37	host ml models github pages	yes submit anywhere want say 20 pt n't remember yeah work good article “ good ” meaning 's copy/pasting code nothing else actually need explain 's going also article – closer time working article guidelines well	0
38	host ml models github pages	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
39	host ml models github pages	n't think actually need web server github pages serve static pages like html unfortunately host machine learning models github pages	1
40	way know exactly many bins needed set histplot make proper visualization data price var homework 2 case	alexey question lecture use exception output exception n't include top 5x5x2048 long long long tube – three-dimensional thing dmitry know 's 5x5 think convolutional layers 1x1 – quite end 2000-something filters result applying filters still lot feature maps 'm completely sure know 's going dmitry happens exception last layer dmitry think 's filters alexey yeah bunch of… dmitry convolutional nature alexey yeah something specified coming pretrained network	0
41	way know exactly many bins needed set histplot make proper visualization data price var homework 2 case	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
42	way know exactly many bins needed set histplot make proper visualization data price var homework 2 case	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
43	way know exactly many bins needed set histplot make proper visualization data price var homework 2 case	dmitry think really depends use case distribution data also depends would like receive end – purpose plots different types plots different purposes alexey value always start dmitry think start 10 usually go direction ’ needed alexey usually start 50. n't know habit guess dmitry ’ usually 10 15	1
44	way know exactly many bins needed set histplot make proper visualization data price var homework 2 case	use cross-validation check validation get better scores – drop one drop one go sometimes makes sense drop even decrease score simply maybe one feature complex compute another one	0
45	build run docker container switches root /app 'm sure next suggestions	took course ago back useful think something use python immediately could helpful 'm sure would good recommendation current course course good would n't compare stanford course andrew ng simply different course 'll get lot theory one practical hands-on – 's mostly writing code together maybe take	0
46	build run docker container switches root /app 'm sure next suggestions	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
47	build run docker container switches root /app 'm sure next suggestions	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
48	build run docker container switches root /app 'm sure next suggestions	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
49	build run docker container switches root /app 'm sure next suggestions	idea mean mean “ switches root /app ” maybe ask slack put docker file looks like command used building image hope watched video docker couple weeks ago n't long time ago maybe rewatch maybe clarify things please share docker file commands used building docker image let 's try figure 's wrong	1
50	suggest interesting datasets try data pipelines	alexey ideas sejal sejal yeah think updated ... maybe move section main readme page know part additional things could setup writing unit tests ci/cd framework forth think 's “ project ” section maybe move parent branch things cover bit would say make complete packaged environment including test cases automatic deployment pipelines alexey yeah think 's good idea move project like “ next steps ” right sejal exactly something based question see many things automated might cases run scheduled pipelines airflow whichever tool 're using order generate frequent reports maybe weekly reports results need daily basis let 's say forecasting pipeline 're estimating stock prices week maybe scheduled cron job based 's something dashboard reflect predictions real-time stock market prices also weekly basis daily basis also app create idea alexey 'm thinking things n't cover example n't cover serverless quite interesting concept also stream processing let 's say stream lambda function whatever n't remember called google function use consuming stream something putting another stream build quite complex pipelines streams without writing consumer code write lambda function applied stream something internally handles everything need handle consume quite cool concept – serverless another thing n't cover could useful data engineers kubernetes similar thing example using aws batch amazon using kubernetes jobs something like also could quite useful sejal yeah +1 kubernetes think companies using preferring use kubernetes choice cloud cluster instead aws settled hosted solutions kubernetes definitely plus definitely recommend learning alexey another thing would like recommend learning something n't cover serverless thing mentioned – kind covered little bit streaming week kubernetes kind covered little bit batch week least mentioned use kubernetes job batch jobs serverless streaming n't cover concept data monitoring data quality checks things like nice tools example tomorrow datatalks.club webinar workshop data monitoring whylogs whylogs tool data monitoring nice tools example maybe 've heard great expectations soda sql – quite something n't cover well maybe little bit dbt week covered testing bit think deserves attention something many companies looking definitely worth checking victoria maybe also ideas victoria definitely data quality agree one dbt lot bunch packages well course implementation observability 'm sure mentioned guess well alexey mentions monitoring observability also good tools alexey something also n't cover specifically mentioned times tools like fivetran think 're gaining popularity airbyte open source alternative fivetran also worth checking see many many many things n't make course look quite useful might think “ okay need learn five six topics. ” maybe n't try learn much possible goal let 's say work data engineer already start applying data engineering positions see ask – kind things need n't know yet thousands things potentially learn 's difficult select next one maybe base decision whatever hear potential employers	0
51	suggest interesting datasets try data pipelines	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
52	suggest interesting datasets try data pipelines	alexey discussion/thread announcement channel week ago – lot datasets shared n't know many cool websites datasets check victoria put announcements alexey also want slack dump datatalks.club also use play around dataset see 's bunch json files build data pipelines well	1
53	suggest interesting datasets try data pipelines	alexey 's early talk future iterations data engineering zoomcamp think chuckles ml camp yes – videos difference homework 's still discussion n't know 'll see ankush yeah think slight discussion met sure going repeat ml de zoomcamp let 's see response participation feel definitely success definitely feel free pass around course give us feedback maybe make better decision alexey re-recording videos… 's much effort n't think 'll want let 's say ml zoomcamp example much effort 'd rather reuse videos instead making new ones hope videos good decide rerun need re-record yeah 'll see	0
54	suggest interesting datasets try data pipelines	alexey yes let tell “ ” first use peer reviewing project deadline already peers review project let 's say “ support ” course forever 's want get project reviewed need need submit within deadline peers review give feedback “ ” also said “ yes ” yes might remember weeks ago made announcement another project cohort right first one finish project another project immediately war ukraine people work project right everyone else work project right reason 're giving chance work let 's say joined course catch everything submit project second “ trial/iteration/cohort ” – basically one month three weeks another project cohort submit project way submit project	0
55	data engineer freelancing jobs remote worldwide projects compared web software engineering	alexey 'm sure able parse question understanding “ data engineering freelance jobs web software engineering ” n't know monitoring gut feeling many freelance jobs compared web software engineering simply many people need website like wordpress website compared data pipeline – maybe many people need still 's sufficient find couple gigs 'm freelancer 'm speculating ankush neither i. tell – good definitely find freelancing job data engineering ’ pretty sure alexey 's fair compare web software engineering demand simple website high think popular engineering kind remote job	1
56	data engineer freelancing jobs remote worldwide projects compared web software engineering	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
57	data engineer freelancing jobs remote worldwide projects compared web software engineering	victoria alexey extent think like general purpose ci/cd tools n't use data workflows principle things like airflow workflow engines tailored specific use case nice things victoria definitely different use case overlap fit different use cases alexey must admit use jenkins work n't chuckles please n't regret chuckles victoria said legacy so… alexey yeah still legacy need maintain nobody wants touch n't know happens nobody wants spend time moving elsewhere kind runs personally would n't come anywhere near jenkins like jenkins	0
58	data engineer freelancing jobs remote worldwide projects compared web software engineering	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	0
59	data engineer freelancing jobs remote worldwide projects compared web software engineering	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
60	use pytorch instead tensorflow	yes	1
61	use pytorch instead tensorflow	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
62	use pytorch instead tensorflow	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
63	use pytorch instead tensorflow	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
64	use pytorch instead tensorflow	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
65	aws datasets already s3 bucket bring redshift need bring bucket first	victoria 'm sure answer mean yes definitely adding part project least workshop reason want people learn also consider people entirely new concept thought would much overhead try cover things already quite complex day-to-day even though work daily dbt work slow-changing dimensions daily basis example 's also think 's something look moment use definitely dbt especially deletions implementing load step done hooks think 's section advanced knowledge link hooks like incremental models mentioned changing dimensions n't link could link something could use snapshots think added concept updates guess could also pre-hook post-hook also something would incremental model incremental model loads new chunk data use merge depending workers use use could insert update 's something would work unique key project also final project 's understand complex go 's limit hope answers	0
66	aws datasets already s3 bucket bring redshift need bring bucket first	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
67	aws datasets already s3 bucket bring redshift need bring bucket first	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
68	aws datasets already s3 bucket bring redshift need bring bucket first	ankush n't know 's example project alexey well sure good example projects project right chuckles create great projects example projects right think good project open recommend must projects n't know “ ideal pipeline ” vague thing ankush yeah really depends use case ideal pipeline would one least manual steps requires least intervention even adding new data sources let 's say similar kinds data consume producing data scale accordingly minimum intervention side possible data engineer would good pipeline would say alexey definition ideal pipeline stitch together open source tools like airflow spark ankush chuckles 'm pretty sure product manager come new use case ideal pipeline would longer ideal laughs alexey think tools like fivetran never used closer ideal one case fivetran works said product manager comes use case longer fits closed-source platform therefore need figure actually make things work experience working things n't know olx love stitch together open source products hope best ankush going open source think tools discussed used ones alexey ideal would n't say work ankush think enterprise ones much better – much easier – less flexible well basic use cases 's perfectly fine want something specific would stuck might need pay ask improvements might take months come kinds things always really depend upon kind majority company kind use cases	0
69	aws datasets already s3 bucket bring redshift need bring bucket first	ankush n't know think aws possible without restrictions alexey probably public open bucket like taxi trips theoretically thing would private s3 buckets ankush yeah think problem would open source data google cloud storage definitely need move s3 bucket	1
70	week ’ prerequisites recommended “ setting memory docker engine minimum 5 gb. ” airflow webserver going	alexey think 's helpful 's necessary helpful happens dags run airflow dags tasks tasks often kubernetes jobs something like infra – many companies infrastructure managed kubernetes 's quite simple get new container execute new job kubernetes n't think 's necessary think 's helpful 'm data engineer – 'm data scientist data scientist knowledge kubernetes quite helpful could debug jobs 'm running airflow run kubernetes could see logs would say want learn hurt get hired without kubernetes knowledge materials course sufficient get hired pick kubernetes work example agree victoria victoria yeah think kubernetes getting popular ca n't hurt	0
71	week ’ prerequisites recommended “ setting memory docker engine minimum 5 gb. ” airflow webserver going	victoria ’ think need 've berlin three years speak german work depend go startup course 're going safe try make sure ask happen end team lot germans experience feel like need speak german order integrate 's thing would recommend though outside learning part – live germany try learn german actually integrate least case feel like helped lot feel country able go doctor receive calls stuff like speak least bit german n't speak super-fluidly quite helpful work alexey maybe 's also helpful separate berlin rest germany berlin 's international many companies berlin go start speaking german people look say “ sorry n't understand you. ” munich german city speak german german quite bad ’ lived like five half six seven years lost count ’ long start feeling ashamed still n't speak german chuckles victoria everything said disclaimer – berlin 's true even sometimes go coffee shops reply english person n't speak german quite crazy german capital alexey couriers deliver things also n't speak german hear way speak n't speak german 's good n't speak chuckles switch english victoria yeah 's true get everything english also one small part question “ need visa ” depend italian citizenship n't need visa example every country would say yes 's european alexey n't need german visa already work contract use contract 's enough get blue card visa “ blue card ” special program want get job seeker visa think need show level proficiency german n't remember ’ a2 b1 one n't need spend lot learning get level victoria job seeker well someone like data engineer think requirements less strict alexey think maybe anymore five years ago think requirement get least a1 level victoria anymore know lot people job seeker one definitely n't speak german alexey okay ignore said look victoria yeah definitely look chuckles also depend country kind degree something like may also change things alexey right blue card need degree without degree get program check internet	0
72	week ’ prerequisites recommended “ setting memory docker engine minimum 5 gb. ” airflow webserver going	sejal 'm sure 've seen video 've already explained use docker setup actually change configuration increase memory 're using docker setup special instructions especially linux vms update faq order able alternatively maxed memory usage let 's say machine provides 4 gb extend suggest using frills version using cloud vm able use wo n't deal manually configuring things alexey 's controlled much resources wsl 2 resources backend says “ resources limits managed windows ” configure windows n't problems think way works ubuntu docker get everything host machine think windows wsl 2 ’ similar macos saw videos also computer n't like docker compose ’ bit heavy sejal read somewhere docker compose level maturity compatibility windows wsl think github issues link sent yesterday also explicitly stated alexey ’ windows haters write sejal laughs made fake accounts laughs ankush definitely one alexey laughs well maybe ’ fake accounts	1
73	week ’ prerequisites recommended “ setting memory docker engine minimum 5 gb. ” airflow webserver going	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
74	week ’ prerequisites recommended “ setting memory docker engine minimum 5 gb. ” airflow webserver going	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	0
75	many ways write codes short codes better actual jobs machine learning beginner focus writing concise codes production code	building locally fine	0
76	many ways write codes short codes better actual jobs machine learning beginner focus writing concise codes production code	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
77	many ways write codes short codes better actual jobs machine learning beginner focus writing concise codes production code	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
78	many ways write codes short codes better actual jobs machine learning beginner focus writing concise codes production code	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
79	many ways write codes short codes better actual jobs machine learning beginner focus writing concise codes production code	really depends actually n't think short codes better long codes long codes usually verbose example look wrote image reference – terrible code n't write like maybe makes sense first select subset select subset columns drop duplicates – actually split multiple rows image reference look immediately understand 's going previous solution one line lot stuff happening difficult understand even though concise second one verbose personally think latter way writing code better simply look 's easier understand 's happening n't think colleagues like produce concise code question production code – ’ code running production chuckles recursive definition code wrote affects customers 's production code code running laptop nobody run 's production code	1
80	distant data engineer ’ day-to-day work looks like main differences	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
81	distant data engineer ’ day-to-day work looks like main differences	alexey guess question asking difference course actual work data engineer victoria problems chuckles troubleshooting probably ankush think course made way set becoming data engineer tackling problems data engineer mostly revolve around technologies talking alexey main differences except troubleshooting imagined analysts come people ad hoc queries saying “ hey data table ” help main differences ankush would say complexity definitely alexey yeah relatively simple case right need one join well two joins bunch tables n't need join big tables join location table pretty small practice often two big tables need join victoria yeah n't complex business logic plus n't really setup time set bigquery maintain probably ’ dealing service account much alexey ideally may team deals example work team manages airflow n't need worry write dags commit 's never set airflow locally worry things 's managed think ’ data warehouse things ankush think one thing everybody data basically communicate outwards meaning different columns ratio versus ratio personally lot 's experience like victoria would say especially business stakeholders dbt also data catalog part data team helps quite lot also look work selection actual data catalog hopefully ’ many questions alexey case questions catalog team internal catalog tool get questions data engineers	1
82	distant data engineer ’ day-to-day work looks like main differences	victoria would say yes go-to dbt yes dbt enforces go read viewpoint 'll explain also models kind structure mentioned modern concepts concept comes 80s kimball defined kitchen analogy find useful normally go course complex gets may need steps 's okay always try separated – presentation source need typecasting like example maybe duplication middle may models go data pipeline let ’ say flow less would look like alexey kitchen analogy victoria kitchen analogy way model data similar restaurants model food warehouse raw food source data everyone allowed fact 's actually dangerous 's security measures actually go warehouse stored stored things like kitchen people cook raw food make food 're going serve allowed would data engineers data analysts maybe analytics engineers would data warehouse trying process raw data end part restaurant people eat – dining hall presentation layer data warehouse fact tables look like data marts 're building data marts business stakeholders everyone allowed get foot let ’ say restrictions 's already presented 're presenting raw alexey think tableau self-service tool go cubes presentation layer right victoria yeah technically bi tools self-serve could everything could expose everything right external tables create tables inaudible create section model create factories could everything could even ones development exposed self-service bi tool like tableau looker whatever analogy says use presentation layer parts present raw data people would n't know use 's going like “ oh n't find phones ones maybe 's transformation ” things like alexey nice analogy	0
83	distant data engineer ’ day-to-day work looks like main differences	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
84	distant data engineer ’ day-to-day work looks like main differences	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
85	try linux mint well	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
86	try linux mint well	alexey one thing maybe n't clear enough – please always include code solution maybe people n't watch first said n't submit code homework get zero points saw submissions people put dot empty put link towards data science article example please n't please put code n't 'll get zero points entire week 'll make explicit form week three well please n't forget put code	0
87	try linux mint well	let 's go notes course repo course let 's take say linear regression notes – says “ add notes video prs welcome ” mean – let 's say watch video took notes want share links notes fellow learners taking notes putting somewhere notion medium github whatever prefer – online page put create pull request link notes somebody maybe n't like watching videos go notes read 's idea even better instead adding links add notes directly people need go external site – everything repo ’ idea n't time make notes 's added notes time want contribute notes contribute notes help everyone	0
88	try linux mint well	maybe n't know actually started like windows editing videos linux terrible windows 's much easier bunch stuff comes development n't think 's anything better linux comes simple day-to-day stuff like editing video editing image n't think linux best environment	1
89	try linux mint well	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
90	guidance many .py files notebook broken deploying	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
91	guidance many .py files notebook broken deploying	building locally fine	0
92	guidance many .py files notebook broken deploying	n't think 's rule thumb notebook multiple things deploy… basically python file ideally one single thing one python file training example one python file testing model – something like n't think particular rule thumb sometimes makes sense larger files sometimes smaller files	1
93	guidance many .py files notebook broken deploying	answer – depends dataset – typically train/validation split first try scaling try without scaling see happens think something talked last week actually notebook prepared last week go classification notebook notebook scaling shows scale features use standard scaler use min max scaler – use two different scalars shows particular dataset turned little bit better experiment see scaling makes sense linear models sometimes 's actually good idea add extra complexity pipeline instead dictionary vectorizer whatnot 'll also need add next step scaling basically steps prediction pipeline becomes bit difficult maintain would say try scale see improvement get significant maybe 's worth extra complexity get use cross-validation use best judgment see makes sense	0
94	guidance many .py files notebook broken deploying	n't think takes lot time first need record videos prepare homework imagine adding collaboration startups well maybe sometime future n't know somebody startup listening right want hire people graduate zoomcamp please reach let 's arrange something	0
95	anything else aware midterm	nothing comes mind maybe 're uncertain something please ask	1
96	anything else aware midterm	think people good intentions n't think somebody intentionally give bad score somebody wants intentionally give bad scores please n't would seriously please n't 're learn somebody 's giving best see matrix use evaluating give score person deserves n't link coursera actually research peer reviewing found peer reviews actually work quite well people n't give bad scores sake another reason three people reviewing one project exactly avoid one person giving everyone bad scores 'll take median avoid 's three scores also think opportunity get feedback people write something saying “ hey problems running this. ” 's good somebody tries actually run project n't answer apart please bit trust people think good intentions	0
97	anything else aware midterm	really depends cloud showed elastic beanstalk n't think needed change much anything n't remember changing anything thing changed url service test.py script thing needed change think heroku 's similar actually couple tutorials using pythonanywhere heroku probably explain need change go see need change anything probably much	0
98	anything else aware midterm	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
99	anything else aware midterm	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
100	article mean published one linkedin post	n't think takes lot time first need record videos prepare homework imagine adding collaboration startups well maybe sometime future n't know somebody startup listening right want hire people graduate zoomcamp please reach let 's arrange something	0
101	article mean published one linkedin post	flask – let draw let 's say want create web service image 5 web service gets requests web service responds something could predictions could something else – n't related machine learning implement web service use flask use fastapi use framework creating web services use flask use something else – flask allows use post example let 's say “ get ” usually parse parameters query let 's say go google put “ test ” see q=test https //www.google.com/search q=test oq=test aqs=chrome.0.69i59l2j69i65l3j69i60j69i61l2.1264j0j1 sourceid=chrome ie=utf-8 image 6 parameter pass “ get ” request “ get ” request sometimes let 's say want score customer n't want put gender=female contract=one year string send json post post also add body request generally use post general use case flask able create web service	0
102	article mean published one linkedin post	soft skills resources career questions would recommend actually go datatalks.club slack channel called career questions go ask questions use slido link course way separate two sessions talk course career slack talk careers	0
103	article mean published one linkedin post	means published one publish medium example blog whatever linkedin posts – n't think put nice long article linkedin post think linkedin articles n't code – embed code would n't recommend using linkedin articles medium good blog use blog	1
104	article mean published one linkedin post	case everyone uses docker problem n't use docker n't get point using docker ’ run write “ sorry run this. ” hope n't happen 's actually whole idea behind docker – 's transferable different environments let 's say use gunicorn n't use docker able test case use scoring matrix evaluate accordingly gunicorn tick box would encourage still try learn submission even run reason think reason run something windows ubuntu gunicorn everything runnable	0
105	ever issues dealing hype mitigate hype mindset workplace	alexey ankush like scala right chuckles ankush think scala definitely one best languages data engineers chuckles biggest part 's typesafe 's heavy verbose java scala perfect combination least coming java background love scala would say yes 's kind important language know data engineer especially spark written scala sometimes see errors spark- java-native might get confused knowledge like scala personally use developing pipelines also use write code spark sometimes also beam actually need really know python well enough proficient python get away minimum knowledge java scala would say unfortunately would say like scala personally like definitely go ahead learn 's still good language alexey think get performance benefits python extra layer abstraction top spark adds bit overhead cases want control things right ankush personal experience whenever written batch jobs – 'm talking real-time 'm talking batch jobs n't matter much time takes – always half hour 10-15 minutes minimum adding another 30 seconds one minute python performance n't really matter real-time streaming yes would huge loss n't think 's case pyspark real-time batch n't really matter alexey think serialization better use scala use “ case classes ” whatever called easily turn rdd case class think uses efficient serialization mechanism python sometimes difference minutes remember case difference like 20 minutes took pipeline rewrote scala difference big ankush would say spark 1.0-something data frames 's also really useful anymore alexey data frames yeah 1.2. remember used python every step pipeline found one job could optimize scala whole pipeline lot faster something needed scala year using spark chuckles think important – know java scala data engineer ankush good python would use already know java ’ need scala n't know either learn scala scala would much easier learn application especially spark alexey argue n't lot time chuckles cross-talk ankush preferences biased really like scala answers neutral definitely biased towards scala alexey victoria n't want add anything victoria n't know n't used scala several years reason people offer jobs java development linkedin chuckles alexey use victoria used university prove get title would use	0
106	ever issues dealing hype mitigate hype mindset workplace	victoria 'm sure answer mean yes definitely adding part project least workshop reason want people learn also consider people entirely new concept thought would much overhead try cover things already quite complex day-to-day even though work daily dbt work slow-changing dimensions daily basis example 's also think 's something look moment use definitely dbt especially deletions implementing load step done hooks think 's section advanced knowledge link hooks like incremental models mentioned changing dimensions n't link could link something could use snapshots think added concept updates guess could also pre-hook post-hook also something would incremental model incremental model loads new chunk data use merge depending workers use use could insert update 's something would work unique key project also final project 's understand complex go 's limit hope answers	0
107	ever issues dealing hype mitigate hype mindset workplace	victoria would say ’ also 's different set products example data warehouse start data tech stack scratch would definitely use redshift cost lower end need probably would need migrate something like bigquery snowflake lot documentation 's pretty easy set 's around longer – think reason broader adoption mainly around things alexey seems aws popular well least berlin look companies use cloud maybe 70 people use aws 20 may use gcp remaining 10 use azure 's roughly see maybe 'm wrong course also got use gcp bit seems ui nicer go web console 's little bit nicer think comes tools 's bit difficult example aws pip install aws cli google cloud 's bit difficult maybe 's one reason aws popular – ’ mature maybe think gcp catching nice interface sometimes things cheaper bigquery also think big advantage maybe clouds n't really victoria plus aws around 10 years thesis using aws thing gcp yet 's lot people go 're familiar outside stability alexey think cloud services services quite similar ’ used aws many things gcp straightforward n't typical less think many concepts map one another	0
108	ever issues dealing hype mitigate hype mindset workplace	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
109	ever issues dealing hype mitigate hype mindset workplace	alexey maybe like every time heard new language new tool new something wanted try realized new tools usually pretty raw look shiny start using lot problems come sometimes 's better use time-proven tools bit conservative something happened experience trying tools seeing even though look shiny still bugs especially 's new tool mitigate mindset workplace n't know guess depends use case ask team “ much new tool actually bring pros cons much work add integrate new tool potential benefits ” benefits outweigh headaches go usually analysis see maybe n't actually need new tool old one fine victoria critical mindset go something 's hype always go read everything kind see “ something 'm already using add much value much headaches add ” also 's another tool maintain consider critical mindset definitely	1
110	possible get email confirmation submitting homework	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
111	possible get email confirmation submitting homework	lisa kinds things missing data text data sometimes need cleaning like street written “ st ” “ street ” might want combine 's missing data might want take average median ’ outliers depending makes sense might want throw say elon musk 's salary dataset might good model go chuckles alexey think salary zero right wrote recently tweet n't salary shares lisa agrees laughs would outlier different way lisa direction actually laughs things like would want look might use inner core tiles cut tops bottoms lot different techniques use alexey data cleaning broad also abstract unfortunately 's manual magic button like datarobot would nice give bunch dirty data magic wand button click data becomes clean would nice datarobot maybe two buttons – “ make data nice `` “ make data clean. ” “ okay find best model data. ” maybe research automated data cleaning well must something lisa companies companies buying consolidating trend seems end-to-end also deploying models machine learning ops make sure n't data drift things like 's always robust might want particular situation try generalize build tools	0
112	possible get email confirmation submitting homework	probably honest 'm big fan test-driven development way mean tests important way tdd works first come test fails implement things fix make test green iteration n't find super useful like 's personal preference usually implement something test like general software engineering practices ’ talking test driven development machine learning way first write test implement something – ’ know ’ talking model say “ okay model kind accuracy ” train model 's way expected maybe n't make much sense hand lot code around model needs testing example things showed today – may fail behave way expect tests around think quite important want make sure things control example could actually good test example transform want make sure neighborhood fordham n't appear categories could good test maybe actually makes sense write tests – actual machine learning thing model – things around model data preparation pipeline probably want test get results post-processing results want test testing model could tricky	0
113	possible get email confirmation submitting homework	n't know n't checked honest took – took data preparation step – n't see data preparation step time maybe categorical variable	0
114	possible get email confirmation submitting homework	alexey possible gmail account google forms option exist gmail account many people take part course icloud yandex yahoo email provider ’ gmail 's seems 's really possible requires logged google automatically captures email sends result know maybe 'm wrong know make way everyone gmail users get confirmation n't force others use gmail let know	1
115	change vector pil image maybe	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
116	change vector pil image maybe	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
117	change vector pil image maybe	multi-classification – yes clustering – 'm sure probably good idea n't cover hard – know evaluate peers know better clustering	0
118	change vector pil image maybe	dmitry think really depends use case distribution data also depends would like receive end – purpose plots different types plots different purposes alexey value always start dmitry think start 10 usually go direction ’ needed alexey usually start 50. n't know habit guess dmitry ’ usually 10 15	0
119	change vector pil image maybe	alexey sure maybe answer thing talked regarding filters well use matplotlib take filter then… dmitry think pil interesting option 's images guess alexey yeah pil 's like want save jpeg example want plot see looks matplotlib probably sufficient homework also use matplotlib n't use pil 48:08 0 upvotes use dogs vs cats main capstone project better performance dmitry guess maybe datasets alexey yeah datasets dmitry mean lot open datasets guess alexey cats dogs yeah dmitry mean think cats dogs another time would n't interesting alexey yeah add data interesting dmitry maybe cats versus dogs versus something else alexey wombats cats versus wombats chuckles think image net must wombats right dmitry yeah alexey take part image net get wombats dmitry fun alexey laughs maybe wombats versus… similar wombats wombats cats quite different right different environments 's easy task dmitry maybe wildcats alexey yeah saw wildcat zoo looked like normal cat suspect maybe put normal cat said 's wildcat chuckles dmitry better marketing chuckles	1
120	algorithms/models allowed use example gradient boosting use lightgbm	yes use whatever want sure document	1
121	algorithms/models allowed use example gradient boosting use lightgbm	case everyone uses docker problem n't use docker n't get point using docker ’ run write “ sorry run this. ” hope n't happen 's actually whole idea behind docker – 's transferable different environments let 's say use gunicorn n't use docker able test case use scoring matrix evaluate accordingly gunicorn tick box would encourage still try learn submission even run reason think reason run something windows ubuntu gunicorn everything runnable	0
122	algorithms/models allowed use example gradient boosting use lightgbm	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
123	algorithms/models allowed use example gradient boosting use lightgbm	n't think takes lot time first need record videos prepare homework imagine adding collaboration startups well maybe sometime future n't know somebody startup listening right want hire people graduate zoomcamp please reach let 's arrange something	0
124	algorithms/models allowed use example gradient boosting use lightgbm	flask – let draw let 's say want create web service image 5 web service gets requests web service responds something could predictions could something else – n't related machine learning implement web service use flask use fastapi use framework creating web services use flask use something else – flask allows use post example let 's say “ get ” usually parse parameters query let 's say go google put “ test ” see q=test https //www.google.com/search q=test oq=test aqs=chrome.0.69i59l2j69i65l3j69i60j69i61l2.1264j0j1 sourceid=chrome ie=utf-8 image 6 parameter pass “ get ” request “ get ” request sometimes let 's say want score customer n't want put gender=female contract=one year string send json post post also add body request generally use post general use case flask able create web service	0
125	model building features bigquery aws azure provides efficient capabilities compared building machine learning models ab initio jupyter	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
126	model building features bigquery aws azure provides efficient capabilities compared building machine learning models ab initio jupyter	alexey week 2 yes – upload video solution also update leaderboard probably take time depending clean data – need lot data cleaning take lot time please n't put things n't look like emails email field fill please careful takes time clean later	0
127	model building features bigquery aws azure provides efficient capabilities compared building machine learning models ab initio jupyter	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
128	model building features bigquery aws azure provides efficient capabilities compared building machine learning models ab initio jupyter	alexey think question asking model building features bigquery good think good n't done understand ’ pretty basic “ ab initio ” something latin right victoria “ starting based first principles. ” sounds like spanish 's latin wanted check chuckles 's usually used legal terminology alexey maybe 's coming lawyer wants become data engineer anyway model building features pretty basic 's good start try switch jupyter notebook advanced things let ’ say – need flexibility think n't really experience bigquery aws 's little bit different aware similar method aws athena similar bigquery n't know create model athena maybe aws would use sagemaker sagemaker jupyter notebooks works 's good need complexity switch jupyter notebooks	1
129	model building features bigquery aws azure provides efficient capabilities compared building machine learning models ab initio jupyter	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
130	've completed entire course homework way get certificate 're interested project	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
131	've completed entire course homework way get certificate 're interested project	victoria 'm sure answer mean yes definitely adding part project least workshop reason want people learn also consider people entirely new concept thought would much overhead try cover things already quite complex day-to-day even though work daily dbt work slow-changing dimensions daily basis example 's also think 's something look moment use definitely dbt especially deletions implementing load step done hooks think 's section advanced knowledge link hooks like incremental models mentioned changing dimensions n't link could link something could use snapshots think added concept updates guess could also pre-hook post-hook also something would incremental model incremental model loads new chunk data use merge depending workers use use could insert update 's something would work unique key project also final project 's understand complex go 's limit hope answers	0
132	've completed entire course homework way get certificate 're interested project	alexey think 's helpful 's necessary helpful happens dags run airflow dags tasks tasks often kubernetes jobs something like infra – many companies infrastructure managed kubernetes 's quite simple get new container execute new job kubernetes n't think 's necessary think 's helpful 'm data engineer – 'm data scientist data scientist knowledge kubernetes quite helpful could debug jobs 'm running airflow run kubernetes could see logs would say want learn hurt get hired without kubernetes knowledge materials course sufficient get hired pick kubernetes work example agree victoria victoria yeah think kubernetes getting popular ca n't hurt	0
133	've completed entire course homework way get certificate 're interested project	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
134	've completed entire course homework way get certificate 're interested project	alexey way thought – watching course enough maybe watch course feel like know everything try something realize 's actually easy thought want experience want sit try apply knowledge picked 's decided way get certificate end project 's purpose 's enough watch need things answer way need project 're interested certificate	1
135	best practices ingest data scale example log file getting created every minute api would guys use	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	0
136	best practices ingest data scale example log file getting created every minute api would guys use	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
137	best practices ingest data scale example log file getting created every minute api would guys use	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
138	best practices ingest data scale example log file getting created every minute api would guys use	sejal something planning cover code video week videos updated yet actually multiple ways – use cloud services auto-scaling mode provide kind usability auto-scale clusters ec2 clusters way local project still want run scale data scientists might say need better hardware chuckles like gpus example think question little broad covering different use cases course material ankush yeah think lot things missing question taking example – really depends ingesting consuming consuming real time totally different architecture 're consuming batch care consuming every minute kinds questions would way much easier answer know upfront actually encourage put engineering channel slack hope somebody pick also pick also suggest think end usage ingesting using also determine architecture look like	1
139	best practices ingest data scale example log file getting created every minute api would guys use	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
140	model really bad accuracy generates random images tested identified cats dogs	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
141	model really bad accuracy generates random images tested identified cats dogs	building locally fine	0
142	model really bad accuracy generates random images tested identified cats dogs	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
143	model really bad accuracy generates random images tested identified cats dogs	dmitry 's fine showcase purpose said optimize number books example validation steps also architecture bit changed sure create much better model alexey model guess wanted build cat vs dog classifier works well would use pre-trained neural network fine tune right dmitry yeah ’ one options alexey get lot different pictures cats dogs train scratch options dmitry options would try tweak parameters things alexey think train model scratch – one – without using pre-trained neural network decent accuracy dmitry yeah 's also question “ decent ” alexey least 80 example dmitry around 80 think yes need remember pretrained always benefit alexey think – right model accuracy 65 validation slightly better random guess well 's slightly better 's 10-15 better chances pick random images cases incorrect 's 65 right dmitry yeah n't any… alexey yeah 'm saying happen could reason – 65 best accuracy world	1
144	model really bad accuracy generates random images tested identified cats dogs	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
145	somebody gcp account wants run project created vice versa – need correct aws azure-based project	alexey add comment least workplace airflow instance set us data scientist would go instance airflow managed data engineers devops engineers would use never needed actually run docker compose moment multiple dags orchestration tool quite useful would personally maybe go luigi favorite one need think people well since people know luigi many like maybe airflow would actually better choice consider sejal would also like add question docker compose used docker compose airflow style use official setup seeing first time well 's unnecessarily complicated especially xcom variables forth generally docker compose acts wrapper docker containers production workflows docker containers used integration ci/cd pipelines wherever 're going deploy docker compose docker compose part helpful development environments case let 's say mock version airflow environment production want test dags local development environment use docker compose make things easier running simple commands like ‘ docker compose ’ would make fine instead running ‘ docker run container name etc whatever want ’ terms convenience terms usage ’ generally used development environments production alexey think first part question “ close usual data engineer role workflows ” workflows see work complex steps probably also case need start something already – would n't call complex pipeline – three four steps already something eventually might grow bigger bigger add steps ankush would also like add 're cloud maybe use airflow deploy docker maybe actually use service provided cloud provider one similar airflow think airflow would used internally like cloud composite google cloud platform would using terraform sejal yeah used aws step functions aws setup step functions combination lambda place airflow ankush said also something wanted say really depends production self-hosted cluster kubernetes wherever want use pure airflow version company could kind setup use native airflow cluster could cloud-based setup convenient option use managed services cloud composer aws also version called managed airflow	0
146	somebody gcp account wants run project created vice versa – need correct aws azure-based project	alexey 's early talk future iterations data engineering zoomcamp think chuckles ml camp yes – videos difference homework 's still discussion n't know 'll see ankush yeah think slight discussion met sure going repeat ml de zoomcamp let 's see response participation feel definitely success definitely feel free pass around course give us feedback maybe make better decision alexey re-recording videos… 's much effort n't think 'll want let 's say ml zoomcamp example much effort 'd rather reuse videos instead making new ones hope videos good decide rerun need re-record yeah 'll see	0
147	somebody gcp account wants run project created vice versa – need correct aws azure-based project	victoria would say ’ also 's different set products example data warehouse start data tech stack scratch would definitely use redshift cost lower end need probably would need migrate something like bigquery snowflake lot documentation 's pretty easy set 's around longer – think reason broader adoption mainly around things alexey seems aws popular well least berlin look companies use cloud maybe 70 people use aws 20 may use gcp remaining 10 use azure 's roughly see maybe 'm wrong course also got use gcp bit seems ui nicer go web console 's little bit nicer think comes tools 's bit difficult example aws pip install aws cli google cloud 's bit difficult maybe 's one reason aws popular – ’ mature maybe think gcp catching nice interface sometimes things cheaper bigquery also think big advantage maybe clouds n't really victoria plus aws around 10 years thesis using aws thing gcp yet 's lot people go 're familiar outside stability alexey think cloud services services quite similar ’ used aws many things gcp straightforward n't typical less think many concepts map one another	0
148	somebody gcp account wants run project created vice versa – need correct aws azure-based project	alexey well use judgment want try force register account azure run want n't maybe look code see errors 's really ankush maybe matching ask criteria – like used – maybe match corresponding people alexey 's nice idea let 's see 'm afraid complicate things bit already script matching n't use criteria 's random ankush need modify script one column alexey yeah perhaps let 's see ankush easier enough group different things alexey create form submitting ask cloud used right ankush exactly right alexey makes sense machine learning zoomcamp – wanted re-execute whole thing re-executed 's really want learn project – execute grading also learning – want learn really want learn things work execute learn n't time since executing different cloud might take lot time – n't much time really ask invest something like five hours figuring azure works use judgment guess 's answer try matching minimize	1
149	somebody gcp account wants run project created vice versa – need correct aws azure-based project	victoria n't used anaconda think 's also documentation install dbt locally general 're using anaconda 're using environment install use homebrew 're using mac also use pip install pip install dbt postgres example need adapter dbt snowflake dbt big query – whatever want use locally 's 're installed set profiles.yml show videos create project clone whatever 's 's need use locally never install using anaconda 'm guessing 's probably something similar probably conda forge install like 's possible otherwise would go pip install alexey anaconda usually pip victoria yeah 're environment install like 's	0
150	many students stay active learners week 1 2 3	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
151	many students stay active learners week 1 2 3	alexey 's want look project 're behind right n't worry take time decide whether give certificate based projects homework 's – catch week 3 lighter week 1 maybe fast-forward directly docker compose run week 2 depending much time maybe go solution published today give everything need week 3 use transfer service video already watching video probably learn lot already like homework solution	0
152	many students stay active learners week 1 2 3	alexey think 's helpful 's necessary helpful happens dags run airflow dags tasks tasks often kubernetes jobs something like infra – many companies infrastructure managed kubernetes 's quite simple get new container execute new job kubernetes n't think 's necessary think 's helpful 'm data engineer – 'm data scientist data scientist knowledge kubernetes quite helpful could debug jobs 'm running airflow run kubernetes could see logs would say want learn hurt get hired without kubernetes knowledge materials course sufficient get hired pick kubernetes work example agree victoria victoria yeah think kubernetes getting popular ca n't hurt	0
153	many students stay active learners week 1 2 3	alexey week 1 400 submissions week 2 let check one time – 200 submissions week two quite difficult well 's 227 responses week 2. duplicates week 3 's early say 's yet week 2 yet either n't think number increase much see – number views videos – also decay videos week 1 watched like 5000 times week 2 's 1000. week 3 think ’ like 500. 's also going hope 's difficult maybe docker airflow stuff much must admit real life unfortunately also deal – things like setting environment 's annoying feels like 're learning anything need focus setting airflow thing instead working creating airflow dags useful skill hope difficulties encountered discouraging n't kill us makes us stronger right good skill able figure things surely need work – figuring tool n't work way want deal kind stuff pretty much every day – every day per month sure	1
154	many students stay active learners week 1 2 3	victoria 'm sure answer mean yes definitely adding part project least workshop reason want people learn also consider people entirely new concept thought would much overhead try cover things already quite complex day-to-day even though work daily dbt work slow-changing dimensions daily basis example 's also think 's something look moment use definitely dbt especially deletions implementing load step done hooks think 's section advanced knowledge link hooks like incremental models mentioned changing dimensions n't link could link something could use snapshots think added concept updates guess could also pre-hook post-hook also something would incremental model incremental model loads new chunk data use merge depending workers use use could insert update 's something would work unique key project also final project 's understand complex go 's limit hope answers	0
155	data engineering market berlin someone land de job berlin	alexey well good job keep working almost finished first week finish start week two finish week three chuckles follow sequence videos playlist github sufficient think questions go slack ankush keep going alexey chuckles yes exactly ’ almost finished first week good sign first week year pretty tough many people lot problems docker whatnot 're almost finished good sign ’ good track keep working	0
156	data engineering market berlin someone land de job berlin	victoria think 's lot 's little bit hard get data engineer 's also would assume lot openings lot options someone land job go meetups contacts 's probably good way apply sejal agree actually plenty data engineering openings data engineer previous role used get invitations job interviews almost every week recruiters 's abundance role market definitely try alexey since changed title ml engineer get fewer invitations n't get anything sejal oddly chuckles alexey chuckles okay see data engineer actually better terms market demand sejal yeah think engineering skills way demand data science machine learning skills alexey yeah 's interesting happened “ sexiest job 21st century ” nobody wants hire chuckles interesting discussion one podcasts ellen ellen data scientist became data engineer berlin think also talks get job data engineer berlin specifically think recommendation talk consulting companies sort programs coaching juniors business model hire many juniors possible 're cheap sell client lot money need training programs juniors effective recommendation try find consultancy company learn hope 's accurate summary episodes maybe 'm misinterpreting something victoria yeah think come non-data background 're starting course bear mind 'll apply lot probably lot rejections least 's beginning moved berlin everything kind new n't job time moved berlin looked one try get many interviews possible challenges train lot also help know benchmark kind yeah lot offers start applying start applying start talking people probably data engineers well 's also good consultancy n't know 've never used anything like alexey worked outsourcing company moved berlin confirm company worked quite good training process would coach assigned would help everything also sort bootcamp program would try sell client would coach pass interview client client likes start working quite common think outsourcing consulting companies long time ago n't know actually works chuckles sejal 's actually interesting model 's good companies actually putting much effort coaching employees honed certain projects 's nice alexey yeah thing business model buying somebody cheap selling lot money taking margin unfortunately sometimes least case salary pretty low remember n't much left paying flat careful chuckles least good experience one year could sell experience “ market standards ” let 's say depending young much want get experience think sometimes 's worth actually agreeing lower salary exchange experience get experience value market becomes higher another shameless plug something actually talked juan pablo suggested looking small gigs n't pay lot experience sell afterwards somewhat similar mentioned even though 's analytics data engineering think tips shared looking job – think 're pretty universal also funny story actually driving uber able survive trying study things order switch 's cool one well	1
157	data engineering market berlin someone land de job berlin	victoria 's buzzwords right alexey yeah go repo see like practical week like five 10 minute video explaining use 's planned ankush maybe spend couple minutes today well data mesh basically concept idea – let 's go back say “ data pipelines data engineering teams built different companies right ” people generating data people consuming data one big team middle called data engineering team dataops team whatever want call different companies different names teams responsible ingesting data transforming putting making way 's consumable right way right fashion data mesh wants basically decentralize particular role big team wants give power let 's say responsibility team generating data teams consuming data therefore ’ layer instead team taking whole role 's kind better idea build services give responsibility give services software teams generating data consuming basically going higher level respect case happens team ’ generating data responsible quality data answering different questions data things 's really useful multiple teams handling data idea data knowledge data lost best person best team answer questions best team knowledge data team 's generating also responsible putting data data lake data warehouse also responsible queries attaches whole thing together 's kind rough idea data mesh alexey ’ abstract concept actually go youtube channel longer explanation data mesh “ longer ” mean one hour long – hour six minutes chuckles 're kind stuff… way dataops 101 also quite nice one check well one modern data stack analytics engineering also good basically check three – 're good 're interested data mesh specifically first one goes lot detail second one – bit detail ankush 's also nice blog post alexey zhamak actually writing book right think 's early release 's written five six chapters n't remember 's progress 's going quite big book right ankush blog post like today 's date read couple days impossible read one go 's huge victoria think also download first two chapters something book give quick overview well ankush 's also another book o'reilly called data mesh practice max schultze alexey one free right ankush think free 's free	0
158	data engineering market berlin someone land de job berlin	victoria 'm sure answer mean yes definitely adding part project least workshop reason want people learn also consider people entirely new concept thought would much overhead try cover things already quite complex day-to-day even though work daily dbt work slow-changing dimensions daily basis example 's also think 's something look moment use definitely dbt especially deletions implementing load step done hooks think 's section advanced knowledge link hooks like incremental models mentioned changing dimensions n't link could link something could use snapshots think added concept updates guess could also pre-hook post-hook also something would incremental model incremental model loads new chunk data use merge depending workers use use could insert update 's something would work unique key project also final project 's understand complex go 's limit hope answers	0
159	data engineering market berlin someone land de job berlin	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
160	extracting weekly reddit data project appending data warehouse table however 'm sure kind dbt transformations required	ankush planning syllabus actually removed nosql databases wanted focus important tools data engineering 's covering spark data warehouse kafka dbt airflow nosql definitely important part data engineers know community really wants maybe develop couple videos nosql use – maybe give example cassandra something alexey also ran survey started course eight different topics concerning community wants hear analytics engineering top one terms preference nosql last one whether internals nosql n't remember based decisions survey well seemed like much interest nosql databases lot interest analytics engineering affected way decided come syllabus course maybe also let us know specifically mind nosql broad term talking redis talking mongo cassandra dynamo exactly	0
161	extracting weekly reddit data project appending data warehouse table however 'm sure kind dbt transformations required	alexey week 2 yes – upload video solution also update leaderboard probably take time depending clean data – need lot data cleaning take lot time please n't put things n't look like emails email field fill please careful takes time clean later	0
162	extracting weekly reddit data project appending data warehouse table however 'm sure kind dbt transformations required	alexey depends exactly want reddit data right ankush yeah exactly would focus final goal let 's say “ want calculate reddit data. ” might want say “ keywords used ” something like case transformations directly depend alexey something like “ active number posts per day. ” ankush exactly “ per hour distribution posts. ” 'm pretty sure us goes daytime goes night trying answer 's first question would focus know answer know transformations apply alexey yeah 's end goal project dashboard “ exactly want put dashboard ” go “ dashboard data use kind transformations need order go data dashboard ” become clear need group-by need join table	1
163	extracting weekly reddit data project appending data warehouse table however 'm sure kind dbt transformations required	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
164	extracting weekly reddit data project appending data warehouse table however 'm sure kind dbt transformations required	alexey add comment least workplace airflow instance set us data scientist would go instance airflow managed data engineers devops engineers would use never needed actually run docker compose moment multiple dags orchestration tool quite useful would personally maybe go luigi favorite one need think people well since people know luigi many like maybe airflow would actually better choice consider sejal would also like add question docker compose used docker compose airflow style use official setup seeing first time well 's unnecessarily complicated especially xcom variables forth generally docker compose acts wrapper docker containers production workflows docker containers used integration ci/cd pipelines wherever 're going deploy docker compose docker compose part helpful development environments case let 's say mock version airflow environment production want test dags local development environment use docker compose make things easier running simple commands like ‘ docker compose ’ would make fine instead running ‘ docker run container name etc whatever want ’ terms convenience terms usage ’ generally used development environments production alexey think first part question “ close usual data engineer role workflows ” workflows see work complex steps probably also case need start something already – would n't call complex pipeline – three four steps already something eventually might grow bigger bigger add steps ankush would also like add 're cloud maybe use airflow deploy docker maybe actually use service provided cloud provider one similar airflow think airflow would used internally like cloud composite google cloud platform would using terraform sejal yeah used aws step functions aws setup step functions combination lambda place airflow ankush said also something wanted say really depends production self-hosted cluster kubernetes wherever want use pure airflow version company could kind setup use native airflow cluster could cloud-based setup convenient option use managed services cloud composer aws also version called managed airflow	0
165	got know course today possible begin	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
166	got know course today possible begin	alexey think still two days finish first homework although maybe could bit challenging considering people already spent 10 hours last week need catch maybe take bit easier join maybe n't rush try homework 1 homework 2 want say end say finished course look project completed project considered completed course n't stress much homework ankush think 's definitely doable even start 's still doable would also encourage look topics see “ hey know terraform something want learn right now. ” maybe want skip ’ docker particular technology think comfortable – skip videos already running late topics	1
167	got know course today possible begin	victoria 'm sure answer mean yes definitely adding part project least workshop reason want people learn also consider people entirely new concept thought would much overhead try cover things already quite complex day-to-day even though work daily dbt work slow-changing dimensions daily basis example 's also think 's something look moment use definitely dbt especially deletions implementing load step done hooks think 's section advanced knowledge link hooks like incremental models mentioned changing dimensions n't link could link something could use snapshots think added concept updates guess could also pre-hook post-hook also something would incremental model incremental model loads new chunk data use merge depending workers use use could insert update 's something would work unique key project also final project 's understand complex go 's limit hope answers	0
168	got know course today possible begin	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
169	got know course today possible begin	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
170	think us grip airflow difficult part camp	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
171	think us grip airflow difficult part camp	victoria next week difficult one right chuckles alexey think dbt difficult one victoria 'm already dbt week meant week five alexey week five n't know think kafka difficult one terms setup give problems every time need set kafka 's nightmare n't really needed set airflow locally think 's similar let 's see hope easier week bigquery relatively easy 's managed n't need infra setup machine dbt part also relatively simple maybe end course 'll survey 'll see difficult one hope airflow hope kafka spark simpler	1
172	think us grip airflow difficult part camp	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
173	think us grip airflow difficult part camp	alexey think 's helpful 's necessary helpful happens dags run airflow dags tasks tasks often kubernetes jobs something like infra – many companies infrastructure managed kubernetes 's quite simple get new container execute new job kubernetes n't think 's necessary think 's helpful 'm data engineer – 'm data scientist data scientist knowledge kubernetes quite helpful could debug jobs 'm running airflow run kubernetes could see logs would say want learn hurt get hired without kubernetes knowledge materials course sufficient get hired pick kubernetes work example agree victoria victoria yeah think kubernetes getting popular ca n't hurt	0
174	think us grip airflow difficult part camp	alexey 's want look project 're behind right n't worry take time decide whether give certificate based projects homework 's – catch week 3 lighter week 1 maybe fast-forward directly docker compose run week 2 depending much time maybe go solution published today give everything need week 3 use transfer service video already watching video probably learn lot already like homework solution	0
175	respect encoding encode splitting split encoding entire dataset	yes	0
176	respect encoding encode splitting split encoding entire dataset	said take test data put away – n't look validation look use encoding use anything want apply learned training dataset let draw image 7 one “ test ” take hide forget “ validation ” transformations things like fit transform never fit anything validation dataset fit things “ train. ” fit model fit transformers like dictionary vectorizer transformer transform method transform validation dataset course test also transform n't train hide away put away n't look	1
177	respect encoding encode splitting split encoding entire dataset	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
178	respect encoding encode splitting split encoding entire dataset	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
179	respect encoding encode splitting split encoding entire dataset	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
180	theory homework really challenging data scientist 'd like stats ml math skip kubernetes part would enough find data scientist job	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
181	theory homework really challenging data scientist 'd like stats ml math skip kubernetes part would enough find data scientist job	think around 200 204 think something like	0
182	theory homework really challenging data scientist 'd like stats ml math skip kubernetes part would enough find data scientist job	yes would say course machine learning engineers data scientists want work machine learning engineer ’ suggest skipping kubernetes part would actually say quite important part n't skip machine learning engineer data scientist 's okay skip still think beneficial help stand data scientist example olx open new position make job posting within couple days get several hundred applications 's difficult select candidates many think want stand kind volume – among hundreds people ’ likely know use kubernetes therefore ’ suggest skipping would suggest still course feel free – like enjoy process n't like kubernetes 's fine think 're gon na find job n't need worry	1
183	theory homework really challenging data scientist 'd like stats ml math skip kubernetes part would enough find data scientist job	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
184	theory homework really challenging data scientist 'd like stats ml math skip kubernetes part would enough find data scientist job	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
185	code review routine midterm projects organized users involved process	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
186	code review routine midterm projects organized users involved process	ideally said take dataset – leave test dataset aside n't touch final model n't eda test dataset train full train – n't training dataset accidentally see pattern try use maybe build feature around pattern meanwhile pattern might true general case try – 's called data snooping look data might accidentally see something something may seem important accidentally overfit try avoid looking test dataset	0
187	code review routine midterm projects organized users involved process	alexey sure maybe answer thing talked regarding filters well use matplotlib take filter then… dmitry think pil interesting option 's images guess alexey yeah pil 's like want save jpeg example want plot see looks matplotlib probably sufficient homework also use matplotlib n't use pil 48:08 0 upvotes use dogs vs cats main capstone project better performance dmitry guess maybe datasets alexey yeah datasets dmitry mean lot open datasets guess alexey cats dogs yeah dmitry mean think cats dogs another time would n't interesting alexey yeah add data interesting dmitry maybe cats versus dogs versus something else alexey wombats cats versus wombats chuckles think image net must wombats right dmitry yeah alexey take part image net get wombats dmitry fun alexey laughs maybe wombats versus… similar wombats wombats cats quite different right different environments 's easy task dmitry maybe wildcats alexey yeah saw wildcat zoo looked like normal cat suspect maybe put normal cat said 's wildcat chuckles dmitry better marketing chuckles	0
188	code review routine midterm projects organized users involved process	yes everyone submitted homework midterm project review others believe answered elsewhere something clear talk slack	1
189	code review routine midterm projects organized users involved process	yes would say course machine learning engineers data scientists want work machine learning engineer ’ suggest skipping kubernetes part would actually say quite important part n't skip machine learning engineer data scientist 's okay skip still think beneficial help stand data scientist example olx open new position make job posting within couple days get several hundred applications 's difficult select candidates many think want stand kind volume – among hundreds people ’ likely know use kubernetes therefore ’ suggest skipping would suggest still course feel free – like enjoy process n't like kubernetes 's fine think 're gon na find job n't need worry	0
190	please explain use cases splunk snowflake	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
191	please explain use cases splunk snowflake	alexey feel sorry need deal work chuckles n't really experience sejal used working oracle like long time ago ’ assuming person wrote question still working legacy databases like oracle really n't answer either chuckles guess 'll talk generic terms ci/cd pipelines wherever 're deploying infrastructure basically wherever 're deploying schemas think would better place keep things ankush maybe using git alexey think issue code stored database easily version control somebody point thought good idea let databases run arbitrary code like triggers sejal possibly victoria would able answer case coupling ci/cd pipelines dbt pipelines n't know alexey think usually people days try extract logic put backend – move database put backend version controlled	0
192	please explain use cases splunk snowflake	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
193	please explain use cases splunk snowflake	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
194	please explain use cases splunk snowflake	alexey splunk – ’ know 's data warehouse victoria know victoria snowflake integration splunk splunk data observability platform ’ never used n't feel confident enough explain use case know monte carlo example know used alexey confirms think 's kind monte carlo 's observability also provides security features around data data warehouse use tools project mean use n't fully anything taught way around far know 's kind like layer top would say adds information would probably end using company 's want try think 's good opportunity since already project ’ necessary plus guess 'll use snowflake 'm sure ’ warehouse agnostic 's snowflake part 'm missing alexey trial version snowflake victoria yes 300 credits well could use use warehouse cross-talk alexey snowflake aws google cloud platform cloud victoria yeah 'm entirely sure free trial kind limitation host buy choose definitely – host location everything free trial ’ remember know 's 300 alexey basically use case snowflake – alternative bigquery would use way would use bigquery 's data warehouse another alternative could redshift example aws ’ firebolt think something like victoria yeah firebolt ’ entirely… firebolt oriented performance example 've heard interesting use cases people use snowflake everything like transformation connected tool firebolt apparently query fast n't think could use firebolt entirely alternative snowflake bigquery example guess 's like everything else 're project time 're confident enough everything want try 're sure time everything try use new tools always another perfect even continue one otherwise ’ overcomplicate even though 's alternative even though ’ still data warehouse still learn alexey correctly understand splunk – know data observability general let 's say streaming pipeline data going pipeline batch pipeline data tools give possibility monitor data quality every day see many records many records values certain columns give nice dashboards covered data quality bit dbt splunk advanced data quality tool would say example tool called great expectations also data quality tool another interesting one soda sql victoria actually great expectations 's package dbt ’ also interested alexey check 're open source also tool called whylogs also good 's focused machine learning machine learning data pipelines big part machine learning pipelines also data pipelines example support spark well quite bunch tools cover tools touch upon data quality little bit dbt maybe 's bit advanced work real data pipeline quality checks good idea	1
195	limit number options categorical values one hot encoding implementation	reason like fastapi go	0
196	limit number options categorical values one hot encoding implementation	alexey think lectures used five actually next lecture – week – cover classification one lessons one hot encoding see actually use values top 10 top 5. dmitry think question regarding discussed yesterday alexey oh okay variable 20 options think one hot encoding still good let 's say variable 100 1000 options – think dmitry one hot encoding still good option dmitry usually 're talking linear model ’ sure mean work ensembles trees example random forest talking linear models good situation use example label encoder basically instead creating n numbers columns let 's say zero one use one column specify numbers let 's say 1000 different values 1000 different numbers label encoder works alexey well think said linear models it's… maybe got confused think tree-based models way encoding label encoder good linears 's dmitry think also kind depends curves dimensionality big feature set end	1
197	limit number options categorical values one hot encoding implementation	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
198	limit number options categorical values one hot encoding implementation	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
199	limit number options categorical values one hot encoding implementation	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
