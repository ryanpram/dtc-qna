would say dbt next big thing data stack	alexey example spark google “ spark gcp bigquery connector ” good article shows need need jar need use jar submit spark job let 's say files stored google cloud storage something files multiple options first option save back google cloud storage create external table something like like saw week 3. another option would saving directly bigquery using say “ write ” say want write bigquery uses connector connect bigquery write n't know permissions actually work – maybe also need specify key somehow probably 're already dataproc already permissions need order write bigquery order read google cloud storage use dataproc theoretically easy – add extra jar write bigquery data data warehouse use materials dbt week analytics engineering week actually consume content data warehouse visualize	0
would say dbt next big thing data stack	ankush yes yes think transfer service right yes question data using aws want run test around data source feel free also leave comment maybe even know goes azure blob storage	0
would say dbt next big thing data stack	victoria would say ’ also 's different set products example data warehouse start data tech stack scratch would definitely use redshift cost lower end need probably would need migrate something like bigquery snowflake lot documentation 's pretty easy set 's around longer – think reason broader adoption mainly around things alexey seems aws popular well least berlin look companies use cloud maybe 70 people use aws 20 may use gcp remaining 10 use azure 's roughly see maybe 'm wrong course also got use gcp bit seems ui nicer go web console 's little bit nicer think comes tools 's bit difficult example aws pip install aws cli google cloud 's bit difficult maybe 's one reason aws popular – ’ mature maybe think gcp catching nice interface sometimes things cheaper bigquery also think big advantage maybe clouds n't really victoria plus aws around 10 years thesis using aws thing gcp yet 's lot people go 're familiar outside stability alexey think cloud services services quite similar ’ used aws many things gcp straightforward n't typical less think many concepts map one another	0
would say dbt next big thing data stack	victoria yes	1
would say dbt next big thing data stack	alexey maybe like every time heard new language new tool new something wanted try realized new tools usually pretty raw look shiny start using lot problems come sometimes 's better use time-proven tools bit conservative something happened experience trying tools seeing even though look shiny still bugs especially 's new tool mitigate mindset workplace n't know guess depends use case ask team “ much new tool actually bring pros cons much work add integrate new tool potential benefits ” benefits outweigh headaches go usually analysis see maybe n't actually need new tool old one fine victoria critical mindset go something 's hype always go read everything kind see “ something 'm already using add much value much headaches add ” also 's another tool maintain consider critical mindset definitely	0
know homework score submission	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
know homework score submission	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
know homework score submission	still finish grading homework hopefully 'll soon sorry n't time last week grade soon n't worry 've saved submissions need couple tweaks python script grading 's 's taking long n't done things yet	1
know homework score submission	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
know homework score submission	building locally fine	0
clustering problem allowed midterm project multi-classification	let 's say two dimensional thing 2d want turn 1d case need use pooling2d 'm sure right want quickly check reason remember showing usually go defining model build sort layer layer every time add one layer model predicts see output based see kind pooling layer example need let 's say want turn 2d 1d think need use 2d pooling 'm exactly sure whether 's 2d 1d think 1d pooling needed something one dimensional want turn one value use 1d pooling 3d pooling three dimensional thing want turn one-dimensional thing image 2 things always confuse honest 's follow step step try different poolings want make sure convert image vector presentation something one-dimensional usually size number images times something 's like 2d array image one-dimensional vector based try different poolings sometimes also flatten flatten takes whatever – let 's say kd – want turn 1d use flatten many different options think 's clear difference might remember exactly use 1d 2d difference kind input take kind output produce 's cube 's 2d 's hypercube three dimensions 's something else	0
clustering problem allowed midterm project multi-classification	classification classification let 's say n't predicting price let 's say predicting 's average number hours image 1 case would need another layer – rather layer 's called activation n't activation 're still… ’ trying formulate way 's confusing think 's better go lecture rewatch ’ essentially retelling thing need from_logits price variable loss mean squared error logits loss categorical cross-entropy categorical cross entropy usually used multiple categories need use logits 's numerically stable n't use – stay activation softmax n't use legit hope 's clear regression n't need use homework need use from_logits either homework two classes 's binary classification problem multiple classes – let 's say three classes four classes five classes – need use logits loss categorical cross-entropy loss categorical cross-entropy 's recommended use from_logits=true 's numerically stable	0
clustering problem allowed midterm project multi-classification	yes good references – course course called machine learning zoomcamp cover parts except conda git assume midterm project use git conda – use conda python interpreter install pipenv manage dependencies n't know good references honest maybe n't know answered question hope references course good enough ’ please let know think improved think missing	0
clustering problem allowed midterm project multi-classification	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
clustering problem allowed midterm project multi-classification	multi-classification – yes clustering – 'm sure probably good idea n't cover hard – know evaluate peers know better clustering	1
'm struggling run python script tensorflow tensorflow lite runtime wsl ubuntu 18.04 python 3.8. leads	alexey one thing maybe n't clear enough – please always include code solution maybe people n't watch first said n't submit code homework get zero points saw submissions people put dot empty put link towards data science article example please n't please put code n't 'll get zero points entire week 'll make explicit form week three well please n't forget put code	0
'm struggling run python script tensorflow tensorflow lite runtime wsl ubuntu 18.04 python 3.8. leads	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
'm struggling run python script tensorflow tensorflow lite runtime wsl ubuntu 18.04 python 3.8. leads	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
'm struggling run python script tensorflow tensorflow lite runtime wsl ubuntu 18.04 python 3.8. leads	alexey question lecture use exception output exception n't include top 5x5x2048 long long long tube – three-dimensional thing dmitry know 's 5x5 think convolutional layers 1x1 – quite end 2000-something filters result applying filters still lot feature maps 'm completely sure know 's going dmitry happens exception last layer dmitry think 's filters alexey yeah bunch of… dmitry convolutional nature alexey yeah something specified coming pretrained network	0
'm struggling run python script tensorflow tensorflow lite runtime wsl ubuntu 18.04 python 3.8. leads	would suggest try python 3.7 ubuntu either upgrade ubuntu 20.04 downgrade python 3.7. laptop exact setup python 3.8 n't work run docker run docker wo n't problems really want run host machine ubuntu python either upgrade ubuntu downgrade python	1
project group individual work	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
project group individual work	alexey week 2 yes – upload video solution also update leaderboard probably take time depending clean data – need lot data cleaning take lot time please n't put things n't look like emails email field fill please careful takes time clean later	0
project group individual work	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
project group individual work	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	0
project group individual work	alexey individual groups think becomes trickier victoria yes guess always discuss slack someone else could using dataset alexey yeah think becomes group project case right dataset different use cases end	1
scope two projects developed course	really depends cloud showed elastic beanstalk n't think needed change much anything n't remember changing anything thing changed url service test.py script thing needed change think heroku 's similar actually couple tutorials using pythonanywhere heroku probably explain need change go see need change anything probably much	0
scope two projects developed course	project next week something similar lectures homework need find dataset – propose datasets also find dataset need explain kind thing want predict regression classification – kind problem want solve need sort exploratory data analysis feature importance analysis need course data preparation need train model actually multiple models – 're week six learn use tree based models matter 're solving regression classification problems use linear model linear regression logistic regression able use tree based model like tree decision tree classifier decision tree regressor example need try multiple models deploy least locally rough outline project 's main idea learned lot already learned basically covered far 's time without guidance – without repeating videos project want peer reviewing also need review answers peers way ’ also learn	1
scope two projects developed course	use cross-validation check validation get better scores – drop one drop one go sometimes makes sense drop even decrease score simply maybe one feature complex compute another one	0
scope two projects developed course	far week two week three follow simple approach simple approach described take entire dataset split three parts train validation test 'll use week two week three week four talk cross validation – explain right want say approach validation sufficient many cases think 's pretty safe say many many many applications split enough cross validation course nicer learn week four right go detail try explain wait video “ select approach cross validation ” would say multiple options k-fold… 'm sure many people understand n't covered yet basically dataset big n't need k-fold dataset smaller use k-fold “ leave p ” – 've never used personally	0
scope two projects developed course	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
creating top 100 leaderboard talked first livestream last livestream said n't permissions publish names	multi-classification – yes clustering – 'm sure probably good idea n't cover hard – know evaluate peers know better clustering	0
creating top 100 leaderboard talked first livestream last livestream said n't permissions publish names	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
creating top 100 leaderboard talked first livestream last livestream said n't permissions publish names	's actually think 's useful n't train.py produce pickle file let 's say want retrain model using different parameters – 's useful see changed score real life like projects work train.py 's always one file sometimes 's bunch files somehow log performance think 's useful n't want reason n't	0
creating top 100 leaderboard talked first livestream last livestream said n't permissions publish names	alexey one individually contact 100 people ask permission probably links like linkedin github whatever want include 're one top 100 want also tell put name	1
creating top 100 leaderboard talked first livestream last livestream said n't permissions publish names	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
two numerical features highly correlated shall drop one	use cross-validation check validation get better scores – drop one drop one go sometimes makes sense drop even decrease score simply maybe one feature complex compute another one	1
two numerical features highly correlated shall drop one	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
two numerical features highly correlated shall drop one	's actually think 's useful n't train.py produce pickle file let 's say want retrain model using different parameters – 's useful see changed score real life like projects work train.py 's always one file sometimes 's bunch files somehow log performance think 's useful n't want reason n't	0
two numerical features highly correlated shall drop one	think showed multiple times lessons use validation dataset compare different models validation dataset one best performance final model good rule thumb 's always end story remember talked cross-validation let 's say cross-validation see model good performance also high standard deviation 's great model probably want model lower standard deviation even performance slightly worse right might much information stick rule whatever best performance validation dataset wins later work develop intuition actually pick best model	0
two numerical features highly correlated shall drop one	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
number images generated augmentation configure	let 's say two dimensional thing 2d want turn 1d case need use pooling2d 'm sure right want quickly check reason remember showing usually go defining model build sort layer layer every time add one layer model predicts see output based see kind pooling layer example need let 's say want turn 2d 1d think need use 2d pooling 'm exactly sure whether 's 2d 1d think 1d pooling needed something one dimensional want turn one value use 1d pooling 3d pooling three dimensional thing want turn one-dimensional thing image 2 things always confuse honest 's follow step step try different poolings want make sure convert image vector presentation something one-dimensional usually size number images times something 's like 2d array image one-dimensional vector based try different poolings sometimes also flatten flatten takes whatever – let 's say kd – want turn 1d use flatten many different options think 's clear difference might remember exactly use 1d 2d difference kind input take kind output produce 's cube 's 2d 's hypercube three dimensions 's something else	0
number images generated augmentation configure	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
number images generated augmentation configure	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
number images generated augmentation configure	dmitry basically apply images understand question correctly alexey get image iterate dataset load image apply random mutation image basically one image create one new image go dataset 10 times network sees 10 times slightly different variation original image guess configure – specifying many epochs many times want go dataset could guess dmitry agrees	1
number images generated augmentation configure	flask – let draw let 's say want create web service image 5 web service gets requests web service responds something could predictions could something else – n't related machine learning implement web service use flask use fastapi use framework creating web services use flask use something else – flask allows use post example let 's say “ get ” usually parse parameters query let 's say go google put “ test ” see q=test https //www.google.com/search q=test oq=test aqs=chrome.0.69i59l2j69i65l3j69i60j69i61l2.1264j0j1 sourceid=chrome ie=utf-8 image 6 parameter pass “ get ” request “ get ” request sometimes let 's say want score customer n't want put gender=female contract=one year string send json post post also add body request generally use post general use case flask able create web service	0
’ using docker setup locally downloading lots data week 3 possible set everything gcp vm use transfer service	alexey well ’ aws account go transfer service 's probably easiest need click buttons want set gcp virtual machine need spend time setting everything 's worth weeks – spark probably kafka well – useful probably worth time investment want quickly copy files transfer service probably easiest way	1
’ using docker setup locally downloading lots data week 3 possible set everything gcp vm use transfer service	alexey 's want look project 're behind right n't worry take time decide whether give certificate based projects homework 's – catch week 3 lighter week 1 maybe fast-forward directly docker compose run week 2 depending much time maybe go solution published today give everything need week 3 use transfer service video already watching video probably learn lot already like homework solution	0
’ using docker setup locally downloading lots data week 3 possible set everything gcp vm use transfer service	alexey 'm sure last part –how docker provided terraform ankush run stuff terraform think always ... set airflow cluster terraform n't think execute job alexey 's something like use terraform prepare environment prepare thing run need actually run something least two separate commands first “ terraform apply ” “ python run ” something like 50:02 1 upvotes allowed contact instructors directly discuss projects things unsure regarding project 'm using new tools alexey think 's best ask things slack channel others also interested learning answer scales better instead everyone writing us directly – answer everyone sees answer b others answer available right somebody knows tool maybe n't example pulsar one students might know able help 's best use channel contact us directly	0
’ using docker setup locally downloading lots data week 3 possible set everything gcp vm use transfer service	alexey add comment least workplace airflow instance set us data scientist would go instance airflow managed data engineers devops engineers would use never needed actually run docker compose moment multiple dags orchestration tool quite useful would personally maybe go luigi favorite one need think people well since people know luigi many like maybe airflow would actually better choice consider sejal would also like add question docker compose used docker compose airflow style use official setup seeing first time well 's unnecessarily complicated especially xcom variables forth generally docker compose acts wrapper docker containers production workflows docker containers used integration ci/cd pipelines wherever 're going deploy docker compose docker compose part helpful development environments case let 's say mock version airflow environment production want test dags local development environment use docker compose make things easier running simple commands like ‘ docker compose ’ would make fine instead running ‘ docker run container name etc whatever want ’ terms convenience terms usage ’ generally used development environments production alexey think first part question “ close usual data engineer role workflows ” workflows see work complex steps probably also case need start something already – would n't call complex pipeline – three four steps already something eventually might grow bigger bigger add steps ankush would also like add 're cloud maybe use airflow deploy docker maybe actually use service provided cloud provider one similar airflow think airflow would used internally like cloud composite google cloud platform would using terraform sejal yeah used aws step functions aws setup step functions combination lambda place airflow ankush said also something wanted say really depends production self-hosted cluster kubernetes wherever want use pure airflow version company could kind setup use native airflow cluster could cloud-based setup convenient option use managed services cloud composer aws also version called managed airflow	0
’ using docker setup locally downloading lots data week 3 possible set everything gcp vm use transfer service	alexey n't think really understand question ankush 're vpc matter alexey cluster – spark cluster cluster kafka cluster ankush 're running vpc able access 's ip right alexey yeah oh okay externally ip guess 're running inside network use internal ip 's inside kubernetes refer names deployments n't know ankush also n't know 's really specific particular use case need give us maybe slack explain bit – running kind machines exactly exposing terms external ip alexey usually kafka cluster kafka machines spark cluster spark cluster connects kafka reads data ankush depends vpc running 're running different vpcs need expose separately need expose ip running vpc matter able access internally might wrong 'm devops chuckles alexey usually take things granted maybe good thing work grateful data team makes tools work often n't ask “ okay actually configured ” works n't – n't go support channel ask “ hey please fix ” ankush think 's also data team 's also maybe devops team embedded inside data team 're working olx pretty big company maybe many smaller teams inside big teams guess 's devops topic	0
mean “ feel free submit prs links notes ”	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
mean “ feel free submit prs links notes ”	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
mean “ feel free submit prs links notes ”	yes submit anywhere want say 20 pt n't remember yeah work good article “ good ” meaning 's copy/pasting code nothing else actually need explain 's going also article – closer time working article guidelines well	0
mean “ feel free submit prs links notes ”	let 's go notes course repo course let 's take say linear regression notes – says “ add notes video prs welcome ” mean – let 's say watch video took notes want share links notes fellow learners taking notes putting somewhere notion medium github whatever prefer – online page put create pull request link notes somebody maybe n't like watching videos go notes read 's idea even better instead adding links add notes directly people need go external site – everything repo ’ idea n't time make notes 's added notes time want contribute notes contribute notes help everyone	1
mean “ feel free submit prs links notes ”	really depends cloud showed elastic beanstalk n't think needed change much anything n't remember changing anything thing changed url service test.py script thing needed change think heroku 's similar actually couple tutorials using pythonanywhere heroku probably explain need change go see need change anything probably much	0
randomly assigned projects review many points need get order pass project get certificate	alexey yes previous question showed review assignment process passing grade see 'll take look course say right “ okay order pass course need get 30 points. ” say two three people pass situation want chuckles want first look projects come fair threshold see actually pass	1
randomly assigned projects review many points need get order pass project get certificate	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	0
randomly assigned projects review many points need get order pass project get certificate	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
randomly assigned projects review many points need get order pass project get certificate	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
randomly assigned projects review many points need get order pass project get certificate	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
use data science project structure template like cookiecutter storing projects real tasks	flask – let draw let 's say want create web service image 5 web service gets requests web service responds something could predictions could something else – n't related machine learning implement web service use flask use fastapi use framework creating web services use flask use something else – flask allows use post example let 's say “ get ” usually parse parameters query let 's say go google put “ test ” see q=test https //www.google.com/search q=test oq=test aqs=chrome.0.69i59l2j69i65l3j69i60j69i61l2.1264j0j1 sourceid=chrome ie=utf-8 image 6 parameter pass “ get ” request “ get ” request sometimes let 's say want score customer n't want put gender=female contract=one year string send json post post also add body request generally use post general use case flask able create web service	0
use data science project structure template like cookiecutter storing projects real tasks	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
use data science project structure template like cookiecutter storing projects real tasks	honest n't used uvicorn used gunicorn n't know matters n't actually checked benchmarks use gunicorn also use others – n't remember ones reason basically	0
use data science project structure template like cookiecutter storing projects real tasks	yes work olx quite extensively use cookiecutter internal structures example thing called aws patch way deploy models use often example cookiecutter template kick starts project aws batch already template use yeah cookiecutter great tool n't heard tool suggest checking sometimes know “ okay need folder folder folder team everybody makes projects way. ” sometimes informal guidelines well	1
use data science project structure template like cookiecutter storing projects real tasks	took course ago back useful think something use python immediately could helpful 'm sure would good recommendation current course course good would n't compare stanford course andrew ng simply different course 'll get lot theory one practical hands-on – 's mostly writing code together maybe take	0
possible etl process data lakes etl data warehouse etl data lake	alexey add comment least workplace airflow instance set us data scientist would go instance airflow managed data engineers devops engineers would use never needed actually run docker compose moment multiple dags orchestration tool quite useful would personally maybe go luigi favorite one need think people well since people know luigi many like maybe airflow would actually better choice consider sejal would also like add question docker compose used docker compose airflow style use official setup seeing first time well 's unnecessarily complicated especially xcom variables forth generally docker compose acts wrapper docker containers production workflows docker containers used integration ci/cd pipelines wherever 're going deploy docker compose docker compose part helpful development environments case let 's say mock version airflow environment production want test dags local development environment use docker compose make things easier running simple commands like ‘ docker compose ’ would make fine instead running ‘ docker run container name etc whatever want ’ terms convenience terms usage ’ generally used development environments production alexey think first part question “ close usual data engineer role workflows ” workflows see work complex steps probably also case need start something already – would n't call complex pipeline – three four steps already something eventually might grow bigger bigger add steps ankush would also like add 're cloud maybe use airflow deploy docker maybe actually use service provided cloud provider one similar airflow think airflow would used internally like cloud composite google cloud platform would using terraform sejal yeah used aws step functions aws setup step functions combination lambda place airflow ankush said also something wanted say really depends production self-hosted cluster kubernetes wherever want use pure airflow version company could kind setup use native airflow cluster could cloud-based setup convenient option use managed services cloud composer aws also version called managed airflow	0
possible etl process data lakes etl data warehouse etl data lake	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
possible etl process data lakes etl data warehouse etl data lake	alexey 's want look project 're behind right n't worry take time decide whether give certificate based projects homework 's – catch week 3 lighter week 1 maybe fast-forward directly docker compose run week 2 depending much time maybe go solution published today give everything need week 3 use transfer service video already watching video probably learn lot already like homework solution	0
possible etl process data lakes etl data warehouse etl data lake	ankush part next week ’ week 3 course look first video find answer	1
possible etl process data lakes etl data warehouse etl data lake	victoria yes	0
data engineer need know algorithms data structures data engineers still face software engineering tests tech interviews	victoria 's buzzwords right alexey yeah go repo see like practical week like five 10 minute video explaining use 's planned ankush maybe spend couple minutes today well data mesh basically concept idea – let 's go back say “ data pipelines data engineering teams built different companies right ” people generating data people consuming data one big team middle called data engineering team dataops team whatever want call different companies different names teams responsible ingesting data transforming putting making way 's consumable right way right fashion data mesh wants basically decentralize particular role big team wants give power let 's say responsibility team generating data teams consuming data therefore ’ layer instead team taking whole role 's kind better idea build services give responsibility give services software teams generating data consuming basically going higher level respect case happens team ’ generating data responsible quality data answering different questions data things 's really useful multiple teams handling data idea data knowledge data lost best person best team answer questions best team knowledge data team 's generating also responsible putting data data lake data warehouse also responsible queries attaches whole thing together 's kind rough idea data mesh alexey ’ abstract concept actually go youtube channel longer explanation data mesh “ longer ” mean one hour long – hour six minutes chuckles 're kind stuff… way dataops 101 also quite nice one check well one modern data stack analytics engineering also good basically check three – 're good 're interested data mesh specifically first one goes lot detail second one – bit detail ankush 's also nice blog post alexey zhamak actually writing book right think 's early release 's written five six chapters n't remember 's progress 's going quite big book right ankush blog post like today 's date read couple days impossible read one go 's huge victoria think also download first two chapters something book give quick overview well ankush 's also another book o'reilly called data mesh practice max schultze alexey one free right ankush think free 's free	0
data engineer need know algorithms data structures data engineers still face software engineering tests tech interviews	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
data engineer need know algorithms data structures data engineers still face software engineering tests tech interviews	alexey depends exactly want reddit data right ankush yeah exactly would focus final goal let 's say “ want calculate reddit data. ” might want say “ keywords used ” something like case transformations directly depend alexey something like “ active number posts per day. ” ankush exactly “ per hour distribution posts. ” 'm pretty sure us goes daytime goes night trying answer 's first question would focus know answer know transformations apply alexey yeah 's end goal project dashboard “ exactly want put dashboard ” go “ dashboard data use kind transformations need order go data dashboard ” become clear need group-by need join table	0
data engineer need know algorithms data structures data engineers still face software engineering tests tech interviews	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	1
data engineer need know algorithms data structures data engineers still face software engineering tests tech interviews	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
original plan project going start week three weeks think going less current plan zoomcamp	alexey well think finished kafka week – think called modules weeks n't really correspond one-to-one physical calendar weeks finish kafka week 'll start project think three weeks reasonable n't want spend much working working projects instead ambitious maybe think “ okay actually two weeks ” third week reviewing peers let 's say right – today deadline spark kafka let 's say deadline next week march 27 finish project one last week march peer reviewing 's roughly plan course see goes maybe adjust 's plan	1
original plan project going start week three weeks think going less current plan zoomcamp	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
original plan project going start week three weeks think going less current plan zoomcamp	victoria set warehouse end define connection difference would use adapter dbt several adapters would first want use dbt go check adapters make sure 's supported several official example redshift could use community-supported azure alexey yeah aws athena azure athena redshift presto well example presto cluster use aws – within aws presto cluster potentially similar setup connect dbt presto victoria general add little bit answer let 's say project right bigquery would change profile 's set connection may make changes 's something sql supports bigquery way around able run project minutes	0
original plan project going start week three weeks think going less current plan zoomcamp	alexey think 's helpful 's necessary helpful happens dags run airflow dags tasks tasks often kubernetes jobs something like infra – many companies infrastructure managed kubernetes 's quite simple get new container execute new job kubernetes n't think 's necessary think 's helpful 'm data engineer – 'm data scientist data scientist knowledge kubernetes quite helpful could debug jobs 'm running airflow run kubernetes could see logs would say want learn hurt get hired without kubernetes knowledge materials course sufficient get hired pick kubernetes work example agree victoria victoria yeah think kubernetes getting popular ca n't hurt	0
original plan project going start week three weeks think going less current plan zoomcamp	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
important docker use data engineering example installing hadoop spark docker	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
important docker use data engineering example installing hadoop spark docker	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	0
important docker use data engineering example installing hadoop spark docker	victoria set warehouse end define connection difference would use adapter dbt several adapters would first want use dbt go check adapters make sure 's supported several official example redshift could use community-supported azure alexey yeah aws athena azure athena redshift presto well example presto cluster use aws – within aws presto cluster potentially similar setup connect dbt presto victoria general add little bit answer let 's say project right bigquery would change profile 's set connection may make changes 's something sql supports bigquery way around able run project minutes	0
important docker use data engineering example installing hadoop spark docker	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
important docker use data engineering example installing hadoop spark docker	ankush think today 's time 're sort software engineering data engineering back end engineering need knowledge docker world moved around us way need build application need deploy application need maintain application easiest way use docker would say definitely important would install hadoop spark docker maybe maybe use dedicated hardware docker learning docker definitely way around think need least basic knowledge docker expert docker feel knowing basics knowing get around knowing basic commands really help grow career alexey maybe could add point view data scientist data science often need use libraries things like xgboost require sort native dependencies native dependencies things install aptget install linux ubuntu like openmp things like let 's say want use xgboost spark lot trouble like “ prepare package dependencies way submit spark cluster n't fail ” found spark actually submit docker containers – prepare package dependencies docker submit spark spark would run spark would use dependencies specified docker – life became lot easier n't even need worry exactly prepared zip archive n't know spark maybe 'm saying n't make much sense believe life became easier prepare docker file instead figuring exactly run aptget spark nodes quite helpful personally sejal 'm sure must read advantages docker useful every stage software engineering backend frontend even infrastructure lot advantages terms usability portability also really downsizes effort terms preparing deployment-based code low maintenance mock production environment onto docker version test things without deploy production would say ’ important	1
best strategy update records parquet file bigquery appending data parquet files	victoria 'm sure answer mean yes definitely adding part project least workshop reason want people learn also consider people entirely new concept thought would much overhead try cover things already quite complex day-to-day even though work daily dbt work slow-changing dimensions daily basis example 's also think 's something look moment use definitely dbt especially deletions implementing load step done hooks think 's section advanced knowledge link hooks like incremental models mentioned changing dimensions n't link could link something could use snapshots think added concept updates guess could also pre-hook post-hook also something would incremental model incremental model loads new chunk data use merge depending workers use use could insert update 's something would work unique key project also final project 's understand complex go 's limit hope answers	0
best strategy update records parquet file bigquery appending data parquet files	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
best strategy update records parquet file bigquery appending data parquet files	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	0
best strategy update records parquet file bigquery appending data parquet files	ankush append parquet file sejal think probably meant – think alexey also talking – incremental loads since end result week 2 bigquery external table think default 's append-only mode right alexey alexey executing saw… executed bunch dags runs went bigquery see results table created first dag run basically n't append table reason n't know think saw week 3 videos way create statement create table needs use kind sort wildcard say “ 's 2019 minus 10 ” bigquery picks files pattern probably typically n't know – 'm bigquery expert least week 2 need upload data google cloud storage need create bigquery tables create bigquery table week 3 ankush show – saw video sql statement creating table ankush bigquery yes need use wildcards pick multiple files 's aim would still append parquet file parquet files meant purpose 're append-only kinds file 're like columnar storage basically append need rewrite whole thing case would suggest basically keep open-ended close file 're done writing probably let bigquery read side writing bigquery table internally think 's also covered later videos bigquery internally figures couple writes let 's say hour figures imbalance internal file structure reclusters automatically reshuffles data creates new file n't really worry cost side 's internally bigquery question alexey maybe add words way see usually happens folder data lake specific date let 's say first structure name whatever table year month day within folder bunch parquet files want add information add another parquet file folder way get data partition	1
best strategy update records parquet file bigquery appending data parquet files	alexey well guess want dashboard aggregates hard time trying imagine dataset take raw form put dashboard maybe 's already grouped 's already aggregated 's probably small dataset 's suitable course think maybe select reasonably big dataset require transformation 're doubt ask slack really n't know kind dataset put dashboard without transformations ankush yeah minimum transformations would required alexey counts group-bys maybe joins ankush exactly think 's better choose dataset allow	0
even n't turn project within two weeks still get project reviewed feedback	alexey 's want look project 're behind right n't worry take time decide whether give certificate based projects homework 's – catch week 3 lighter week 1 maybe fast-forward directly docker compose run week 2 depending much time maybe go solution published today give everything need week 3 use transfer service video already watching video probably learn lot already like homework solution	0
even n't turn project within two weeks still get project reviewed feedback	alexey add comment least workplace airflow instance set us data scientist would go instance airflow managed data engineers devops engineers would use never needed actually run docker compose moment multiple dags orchestration tool quite useful would personally maybe go luigi favorite one need think people well since people know luigi many like maybe airflow would actually better choice consider sejal would also like add question docker compose used docker compose airflow style use official setup seeing first time well 's unnecessarily complicated especially xcom variables forth generally docker compose acts wrapper docker containers production workflows docker containers used integration ci/cd pipelines wherever 're going deploy docker compose docker compose part helpful development environments case let 's say mock version airflow environment production want test dags local development environment use docker compose make things easier running simple commands like ‘ docker compose ’ would make fine instead running ‘ docker run container name etc whatever want ’ terms convenience terms usage ’ generally used development environments production alexey think first part question “ close usual data engineer role workflows ” workflows see work complex steps probably also case need start something already – would n't call complex pipeline – three four steps already something eventually might grow bigger bigger add steps ankush would also like add 're cloud maybe use airflow deploy docker maybe actually use service provided cloud provider one similar airflow think airflow would used internally like cloud composite google cloud platform would using terraform sejal yeah used aws step functions aws setup step functions combination lambda place airflow ankush said also something wanted say really depends production self-hosted cluster kubernetes wherever want use pure airflow version company could kind setup use native airflow cluster could cloud-based setup convenient option use managed services cloud composer aws also version called managed airflow	0
even n't turn project within two weeks still get project reviewed feedback	ankush yes yes think transfer service right yes question data using aws want run test around data source feel free also leave comment maybe even know goes azure blob storage	0
even n't turn project within two weeks still get project reviewed feedback	alexey yes let tell “ ” first use peer reviewing project deadline already peers review project let 's say “ support ” course forever 's want get project reviewed need need submit within deadline peers review give feedback “ ” also said “ yes ” yes might remember weeks ago made announcement another project cohort right first one finish project another project immediately war ukraine people work project right everyone else work project right reason 're giving chance work let 's say joined course catch everything submit project second “ trial/iteration/cohort ” – basically one month three weeks another project cohort submit project way submit project	1
even n't turn project within two weeks still get project reviewed feedback	ankush n't know think aws possible without restrictions alexey probably public open bucket like taxi trips theoretically thing would private s3 buckets ankush yeah think problem would open source data google cloud storage definitely need move s3 bucket	0
enable package info pop-ups jupyter notebook	flask app produce result actually return prediction 're good n't need fix answer question regarding many points deducted think whole project kind assumes managed deploy flask n't able put docker able deploy cloud lose like five six points something along lines think 's good idea make sure flask returns something problems please share code think know asking question talked already slack maybe error somewhere predict.py script need make sure fix error work perhaps take code lectures – think xgboost model logistic regression need put x variable t-matrix xgboost instead using predict_proba need use predict think 's – n't work please write slack let 's try figure 's working	0
enable package info pop-ups jupyter notebook	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
enable package info pop-ups jupyter notebook	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
enable package info pop-ups jupyter notebook	yes could soon post message slack need find time couple tweaks script	0
enable package info pop-ups jupyter notebook	use tab n't think jupyter running computer need start maybe let show ipython – “ import numpy ” tab press tab shows press tab shows kind things put ’ jupyter notebook go inside parenthesis press tab see different options shown	1
possible bypass pipenv/any python virtual environments go straight docker development	example go kaggle see number notebooks available particular competition could good indicator 's good dataset n't really talk multi-class classification think 's okay want multi-class classification first dataset probably either binary classification see target clearly zero one think go already know deal numerical variables know deal categorical variables idea deal missing data see dataset things 's similar dataset good one 're sure ask slack help	0
possible bypass pipenv/any python virtual environments go straight docker development	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
possible bypass pipenv/any python virtual environments go straight docker development	mean another iteration course yes go course repository see answer first take course self-paced mode videos available except last videos kserve start watching 's important first attempt homework without looking answer check solutions least one project 's also important n't follow videos sure learn something 's better solidify learning also practicing least one project 're taking course self-paced mode want get feedback project also share link slack happy give feedback project answer actual question regarding next cohort – start september cohort started september probably start bit earlier started middle september time probably start maybe one week earlier want notified informed link form click put email send email course starts	0
possible bypass pipenv/any python virtual environments go straight docker development	course 's possible let 's say want work multiple projects computer host machine laptop n't want put everything docker every time docker adds overhead need build image – 's much overhead 's something lightweight separate different environments different python projects think good idea course docker local development local testing think 's better without docker deploy think would suggest use environment manager	1
possible bypass pipenv/any python virtual environments go straight docker development	really depends cloud showed elastic beanstalk n't think needed change much anything n't remember changing anything thing changed url service test.py script thing needed change think heroku 's similar actually couple tutorials using pythonanywhere heroku probably explain need change go see need change anything probably much	0
docker windows nightmare way make easier	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
docker windows nightmare way make easier	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
docker windows nightmare way make easier	yes go ubuntu work chuckles n't know tablet docker docker n't work arch 64 architecture windows people maybe use new chip mac know 'm talking basically ’ know install docker windows tablet 'm sure regular laptops think 's best install linux use	1
docker windows nightmare way make easier	flask app produce result actually return prediction 're good n't need fix answer question regarding many points deducted think whole project kind assumes managed deploy flask n't able put docker able deploy cloud lose like five six points something along lines think 's good idea make sure flask returns something problems please share code think know asking question talked already slack maybe error somewhere predict.py script need make sure fix error work perhaps take code lectures – think xgboost model logistic regression need put x variable t-matrix xgboost instead using predict_proba need use predict think 's – n't work please write slack let 's try figure 's working	0
docker windows nightmare way make easier	alexey one thing maybe n't clear enough – please always include code solution maybe people n't watch first said n't submit code homework get zero points saw submissions people put dot empty put link towards data science article example please n't please put code n't 'll get zero points entire week 'll make explicit form week three well please n't forget put code	0
reluctant use spark ml lib bad experience using building machine learning big data	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
reluctant use spark ml lib bad experience using building machine learning big data	victoria would say ’ also 's different set products example data warehouse start data tech stack scratch would definitely use redshift cost lower end need probably would need migrate something like bigquery snowflake lot documentation 's pretty easy set 's around longer – think reason broader adoption mainly around things alexey seems aws popular well least berlin look companies use cloud maybe 70 people use aws 20 may use gcp remaining 10 use azure 's roughly see maybe 'm wrong course also got use gcp bit seems ui nicer go web console 's little bit nicer think comes tools 's bit difficult example aws pip install aws cli google cloud 's bit difficult maybe 's one reason aws popular – ’ mature maybe think gcp catching nice interface sometimes things cheaper bigquery also think big advantage maybe clouds n't really victoria plus aws around 10 years thesis using aws thing gcp yet 's lot people go 're familiar outside stability alexey think cloud services services quite similar ’ used aws many things gcp straightforward n't typical less think many concepts map one another	0
reluctant use spark ml lib bad experience using building machine learning big data	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
reluctant use spark ml lib bad experience using building machine learning big data	alexey 'm sure last part –how docker provided terraform ankush run stuff terraform think always ... set airflow cluster terraform n't think execute job alexey 's something like use terraform prepare environment prepare thing run need actually run something least two separate commands first “ terraform apply ” “ python run ” something like 50:02 1 upvotes allowed contact instructors directly discuss projects things unsure regarding project 'm using new tools alexey think 's best ask things slack channel others also interested learning answer scales better instead everyone writing us directly – answer everyone sees answer b others answer available right somebody knows tool maybe n't example pulsar one students might know able help 's best use channel contact us directly	0
reluctant use spark ml lib bad experience using building machine learning big data	experience spark ml lib added spark afterthought 's focus spark thought “ okay would cool show also machine learning. ” models get spark ml lib really good usually python use library called scikit learn training machine learning models typically lot better spark models 're also faster train explanation ml lib added spark show 's possible – like proof concept – really n't invest much time n't know 's actively maintained experience n't good way usually use spark preparing data dataset parquet file would use something else – spark – load data train model applying model already released video 's last one map partition typically apply machine learning models spark check ’ using building machine learning big data – use big machine lot memory lot ram load dataset use something like scikit learn train model	1
andrew ng ’ course use python still recommend taking ml mainly works python	means published one publish medium example blog whatever linkedin posts – n't think put nice long article linkedin post think linkedin articles n't code – embed code would n't recommend using linkedin articles medium good blog use blog	0
andrew ng ’ course use python still recommend taking ml mainly works python	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
andrew ng ’ course use python still recommend taking ml mainly works python	took course ago back useful think something use python immediately could helpful 'm sure would good recommendation current course course good would n't compare stanford course andrew ng simply different course 'll get lot theory one practical hands-on – 's mostly writing code together maybe take	1
andrew ng ’ course use python still recommend taking ml mainly works python	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
andrew ng ’ course use python still recommend taking ml mainly works python	probably honest 'm big fan test-driven development way mean tests important way tdd works first come test fails implement things fix make test green iteration n't find super useful like 's personal preference usually implement something test like general software engineering practices ’ talking test driven development machine learning way first write test implement something – ’ know ’ talking model say “ okay model kind accuracy ” train model 's way expected maybe n't make much sense hand lot code around model needs testing example things showed today – may fail behave way expect tests around think quite important want make sure things control example could actually good test example transform want make sure neighborhood fordham n't appear categories could good test maybe actually makes sense write tests – actual machine learning thing model – things around model data preparation pipeline probably want test get results post-processing results want test testing model could tricky	0
talk little bit ridge regression regression types popular data science	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
talk little bit ridge regression regression types popular data science	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
talk little bit ridge regression regression types popular data science	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
talk little bit ridge regression regression types popular data science	ridge regression implementation regression linear regression regression without regularization regression regression regularization ridge regression similar 've previous lesson 's optimized regression types regression models let 's say tree-based models cover soon neural networks also solve regression problems definitely many models use solving regression think maybe week train tree-based models also solve regression problem using trees	1
talk little bit ridge regression regression types popular data science	flask app produce result actually return prediction 're good n't need fix answer question regarding many points deducted think whole project kind assumes managed deploy flask n't able put docker able deploy cloud lose like five six points something along lines think 's good idea make sure flask returns something problems please share code think know asking question talked already slack maybe error somewhere predict.py script need make sure fix error work perhaps take code lectures – think xgboost model logistic regression need put x variable t-matrix xgboost instead using predict_proba need use predict think 's – n't work please write slack let 's try figure 's working	0
explain apply/organize version control stored procedures functions views existing databases coupled code	alexey 's want look project 're behind right n't worry take time decide whether give certificate based projects homework 's – catch week 3 lighter week 1 maybe fast-forward directly docker compose run week 2 depending much time maybe go solution published today give everything need week 3 use transfer service video already watching video probably learn lot already like homework solution	0
explain apply/organize version control stored procedures functions views existing databases coupled code	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
explain apply/organize version control stored procedures functions views existing databases coupled code	alexey 33 submissions think decision extend deadline good one hopefully end week see higher number understand project much difficult watching course following along 33 people – probably put lot effort thanks saw one person write put 100 hours already project outstanding sorry cruel 100 hours lot	0
explain apply/organize version control stored procedures functions views existing databases coupled code	victoria every table materialized table ’ ankush exactly answered previous question external tables think 's meaning question basically external tables versus internal tables alexey okay also concept materialized view view view nothing sql query executed every time want something table view 're kind creating query way speed – materialize view meaning create table data instead querying query query actual data get right ankush victoria agree	0
explain apply/organize version control stored procedures functions views existing databases coupled code	alexey feel sorry need deal work chuckles n't really experience sejal used working oracle like long time ago ’ assuming person wrote question still working legacy databases like oracle really n't answer either chuckles guess 'll talk generic terms ci/cd pipelines wherever 're deploying infrastructure basically wherever 're deploying schemas think would better place keep things ankush maybe using git alexey think issue code stored database easily version control somebody point thought good idea let databases run arbitrary code like triggers sejal possibly victoria would able answer case coupling ci/cd pipelines dbt pipelines n't know alexey think usually people days try extract logic put backend – move database put backend version controlled	1
popular applications logistic regression data science	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
popular applications logistic regression data science	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
popular applications logistic regression data science	one thing immediately comes mind click prediction let 's say go website banner happens behind banner model predicts “ probability particular user comes webpage meaning click banner ” usually sort linear logistic regression hood pretty fast advertisements 're used quite lot especially talk real-time advertisements enter website immediately get ad linear models logistic regression used	1
popular applications logistic regression data science	multi-classification – yes clustering – 'm sure probably good idea n't cover hard – know evaluate peers know better clustering	0
popular applications logistic regression data science	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
software engineering utilizes test driven development since machine learning n't rule-based kind tests written ml projects	example model image reference first write integration tests put docker set lives machine start hitting different https requests get results want first test model n't take long respond want look numbers maybe numbers look mean prediction maximum prediction things like maybe cars n't specific number let 's say expect car like worth 50k maybe price 40k 50k model sudden stops producing price car within age range look try figure happened basically bunch test cases want test predictions model tests break want take look understand happening mainly two course models – example talked bugs – maybe bug nothing model 's data pre-processing issue write usual unit tests basically machine learning write tests write usual software except 're maybe little bit different nature expect ranges always answer	1
software engineering utilizes test driven development since machine learning n't rule-based kind tests written ml projects	n't think takes lot time first need record videos prepare homework imagine adding collaboration startups well maybe sometime future n't know somebody startup listening right want hire people graduate zoomcamp please reach let 's arrange something	0
software engineering utilizes test driven development since machine learning n't rule-based kind tests written ml projects	's actually think 's useful n't train.py produce pickle file let 's say want retrain model using different parameters – 's useful see changed score real life like projects work train.py 's always one file sometimes 's bunch files somehow log performance think 's useful n't want reason n't	0
software engineering utilizes test driven development since machine learning n't rule-based kind tests written ml projects	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
software engineering utilizes test driven development since machine learning n't rule-based kind tests written ml projects	deploy model – let 's say model deployed web service meaning phone old iphone button details listing car like title model make want predict price car right phone sends request web service idea eventually request ends model model replies gets information car like model make age – things – replies back predictions “ car costs much ” recommend user let 's say 50k model – say 's deployed production client talk model communicate model using web service let 's say use https communicate model deployment image reference monitoring quality means – let 's say deployed model one year car one year ago cost 50k probably costs 40k cars change new cars appear cars expensive time ago become less expensive maybe cars become expensive – things change need able detect – changes call model monitoring need see things call changes distributions “ drifts distributions ” things change need able detect let 's say retrain model scalability – let 's say ’ one phone millions one simple web service handle much load need deploy one instance web service let 's say 100 100 together deal 1 million requests time one single one mean scalability scale web service add instances model able process traffic maintainability means – let 's say bug model want fix bug easy us fix bug need debug model maintainability easy us move around code base web service usually follow best engineering practices 's easier something cover course probably need course software engineering practices actually talked model monitoring also simple monitoring like “ many requests per second model getting ” things like also need set monitoring	0
know free ways deploy models	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
know free ways deploy models	flask – let draw let 's say want create web service image 5 web service gets requests web service responds something could predictions could something else – n't related machine learning implement web service use flask use fastapi use framework creating web services use flask use something else – flask allows use post example let 's say “ get ” usually parse parameters query let 's say go google put “ test ” see q=test https //www.google.com/search q=test oq=test aqs=chrome.0.69i59l2j69i65l3j69i60j69i61l2.1264j0j1 sourceid=chrome ie=utf-8 image 6 parameter pass “ get ” request “ get ” request sometimes let 's say want score customer n't want put gender=female contract=one year string send json post post also add body request generally use post general use case flask able create web service	0
know free ways deploy models	yes submit anywhere want say 20 pt n't remember yeah work good article “ good ” meaning 's copy/pasting code nothing else actually need explain 's going also article – closer time working article guidelines well	0
know free ways deploy models	's really case-dependent saw slack remember correctly carlos asked needs drop features – seemed okay drop highly correlated feature assistant name target variable churn basically people work one particular person tend turn less people work somebody else think case seems like relevant feature include n't know – 's really case-dependent usually need sort domain expertise decide feature introduces leakage carlos ’ case think 's safe leave drop see performance validation dataset changes gives idea important feature	0
know free ways deploy models	showed pythonanywhere heroku also free aws create new account get another free year maybe ’ need different credit card n't know	1
students post project ’ code public github repos projects change next zoomcamp run	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
students post project ’ code public github repos projects change next zoomcamp run	alexey well 's really anonymous asked submit link github n't know need ask questions previous course think ask contact information project description n't know ankush think encourage might unfair different people first maybe 're available answer questions let 's put like grading maybe 's unfair want learn 's definitely helpful think grading done based upon submitted 's also really important document steps alexey yeah guess need contact author means really give four points reproducibility ankush case maybe instructions clear report via contact alexey yeah exactly n't remember problem previous course maybe also create issue github n't know	0
students post project ’ code public github repos projects change next zoomcamp run	alexey projects individual one need come project end end 's 's really problem next round concept – need find project solve help find datasets something end end 's 's problem projects stay github maybe question also homework –whether homework stay something us figure right every one posts solution homework github theoretically 's possible find answers use would really question would would find answers online put form actually give learning 're get course certificate hope 're learning scores ankush plan next zoomcamp alexey let 's see overwhelmed end one chuckles energy left ankush next one least easier since n't create videos chuckles alexey 's plan machine learning zoomcamp want restart september videos need figure exactly 're going homework – whether new ones question slack whether 's problem people starting share answers github homework somebody reuse n't reuse pinky swear n't know get santa claus ’ naughty list receive gift chuckles 's best suggest things complicate process ankush message live chat “ homework points leaderboard real benefit. ” think 's true plus seen use hashes 's next impossible know whose email id actually fame alexey actually wanted create page github ml zoomcamp top 100 people amount fame end people agree 'll ask explicit permission like “ allow put name ” would question motivation really want get leaderboard without completing homework 'm talking anyone specific even ask question remember zoomcamp question “ cheat get caught ” chuckles cheat 's problem right 's problem	1
students post project ’ code public github repos projects change next zoomcamp run	alexey yes think n't think draw anywhere let 's say one machine spark another machine airflow machine airflow needs able send request spark spark submit need live network case sparks submit save url ip address spark computer 's enough one thing might need though probably need install spark airflow order able maybe sejal correct think need need java need spark submit script order able actually send jar network spark master need modify airflow container – used docker images course need modify need install java need install spark spark submit say “ -- master ” specify master virtual machine spark would actually instead running spark virtual machine somewhere would use dataproc relatively simple run need click buttons wait five minutes creates cluster cluster 's pretty convenient	0
students post project ’ code public github repos projects change next zoomcamp run	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
going tackle kubernetes tensorflow serving kubeflow kubeflow serving	like said 's always problem- data-specific datasets get good performance	0
going tackle kubernetes tensorflow serving kubeflow kubeflow serving	think people good intentions n't think somebody intentionally give bad score somebody wants intentionally give bad scores please n't would seriously please n't 're learn somebody 's giving best see matrix use evaluating give score person deserves n't link coursera actually research peer reviewing found peer reviews actually work quite well people n't give bad scores sake another reason three people reviewing one project exactly avoid one person giving everyone bad scores 'll take median avoid 's three scores also think opportunity get feedback people write something saying “ hey problems running this. ” 's good somebody tries actually run project n't answer apart please bit trust people think good intentions	0
going tackle kubernetes tensorflow serving kubeflow kubeflow serving	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
going tackle kubernetes tensorflow serving kubeflow kubeflow serving	yes	1
going tackle kubernetes tensorflow serving kubeflow kubeflow serving	really depends cloud showed elastic beanstalk n't think needed change much anything n't remember changing anything thing changed url service test.py script thing needed change think heroku 's similar actually couple tutorials using pythonanywhere heroku probably explain need change go see need change anything probably much	0
kubernetes spin containers needed use case terraform	sejal maybe someone help answer really expert kubernetes side terraform lets build static code templates infrastructure iec-style manner would work cases want static-based infrastructure cases set infrastructure resources destroyed point time want restore certain image certain state – terraform useful case kubernetes 's actually like spin-up cluster deploy services onto kubernetes cluster use cases really different case maybe ankush alexey add kubernetes ankush yeah definitely think kubernetes terraform different think kubernetes thinking deploying microservices sort applications done terraform using aws fargate aws ecs solutions terraform much let ’ assume kubernetes use terraform set kubernetes cluster also use terraform set let 's say s3 buckets next week ’ course see use terraform set transfer service google cloud platform possible kubernetes yes argue certain services like spark flink coming new solutions run kubernetes still example say want run kafka clusters cases kubernetes good solution would like stable solution stable infrastructure case 's terraform would definitely help set kinds clusters alexey person quite far infrastructure would add usually people dedicated company terraform helps go web ui click things instead going clicking file say “ bunch resources set cloud. ” let 's say need move one account another whatever reason – happened multiple times work needed migrate different account terraform destroy one account actually would destroy later terraform “ plan apply ” one account go back old account terraform destroy course still takes time everything code know exactly kind services use example kind buckets use aws us would kind lambda functions basically resources kubernetes could one resource terraform file comment says “ terraform infrastructure code kubernetes infrastructure. ” think 's quite concise way summarizing	1
kubernetes spin containers needed use case terraform	alexey depends exactly want reddit data right ankush yeah exactly would focus final goal let 's say “ want calculate reddit data. ” might want say “ keywords used ” something like case transformations directly depend alexey something like “ active number posts per day. ” ankush exactly “ per hour distribution posts. ” 'm pretty sure us goes daytime goes night trying answer 's first question would focus know answer know transformations apply alexey yeah 's end goal project dashboard “ exactly want put dashboard ” go “ dashboard data use kind transformations need order go data dashboard ” become clear need group-by need join table	0
kubernetes spin containers needed use case terraform	victoria 'm sure answer mean yes definitely adding part project least workshop reason want people learn also consider people entirely new concept thought would much overhead try cover things already quite complex day-to-day even though work daily dbt work slow-changing dimensions daily basis example 's also think 's something look moment use definitely dbt especially deletions implementing load step done hooks think 's section advanced knowledge link hooks like incremental models mentioned changing dimensions n't link could link something could use snapshots think added concept updates guess could also pre-hook post-hook also something would incremental model incremental model loads new chunk data use merge depending workers use use could insert update 's something would work unique key project also final project 's understand complex go 's limit hope answers	0
kubernetes spin containers needed use case terraform	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
kubernetes spin containers needed use case terraform	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
victoria said must use one two approaches project exactly talking batch vs steam	alexey week 2 yes – upload video solution also update leaderboard probably take time depending clean data – need lot data cleaning take lot time please n't put things n't look like emails email field fill please careful takes time clean later	0
victoria said must use one two approaches project exactly talking batch vs steam	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
victoria said must use one two approaches project exactly talking batch vs steam	ankush yes yes think transfer service right yes question data using aws want run test around data source feel free also leave comment maybe even know goes azure blob storage	0
victoria said must use one two approaches project exactly talking batch vs steam	victoria example weeks kind overlap one another maybe overlap could use dbt could use spark mean one two approaches choose use everything ’ learned alexey also pipeline indeed batch versus stream also option exactly data ends data warehouse do… somehow need put data data warehouse batch steam data stuff top transform data example use dbt something else indeed many ways juggle maybe – also option probably go one victoria mean things 's much option example bigquery need google cloud storage things like unless 're using local version guess	1
victoria said must use one two approaches project exactly talking batch vs steam	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
document files write github readme files resources	guidelines resources write something 's way write document – use markdown	1
document files write github readme files resources	mean another iteration course yes go course repository see answer first take course self-paced mode videos available except last videos kserve start watching 's important first attempt homework without looking answer check solutions least one project 's also important n't follow videos sure learn something 's better solidify learning also practicing least one project 're taking course self-paced mode want get feedback project also share link slack happy give feedback project answer actual question regarding next cohort – start september cohort started september probably start bit earlier started middle september time probably start maybe one week earlier want notified informed link form click put email send email course starts	0
document files write github readme files resources	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
document files write github readme files resources	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
document files write github readme files resources	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
remind us prize first 100 leaderboard	glory 's prize course 're one 100 want put name create page course page name yeah 's name link want like linkedin twitter whatever 's prize everyone took two projects get certificate 's first 100 anyone completed projects get certificate	1
remind us prize first 100 leaderboard	let 's say two dimensional thing 2d want turn 1d case need use pooling2d 'm sure right want quickly check reason remember showing usually go defining model build sort layer layer every time add one layer model predicts see output based see kind pooling layer example need let 's say want turn 2d 1d think need use 2d pooling 'm exactly sure whether 's 2d 1d think 1d pooling needed something one dimensional want turn one value use 1d pooling 3d pooling three dimensional thing want turn one-dimensional thing image 2 things always confuse honest 's follow step step try different poolings want make sure convert image vector presentation something one-dimensional usually size number images times something 's like 2d array image one-dimensional vector based try different poolings sometimes also flatten flatten takes whatever – let 's say kd – want turn 1d use flatten many different options think 's clear difference might remember exactly use 1d 2d difference kind input take kind output produce 's cube 's 2d 's hypercube three dimensions 's something else	0
remind us prize first 100 leaderboard	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
remind us prize first 100 leaderboard	alexey one thing maybe n't clear enough – please always include code solution maybe people n't watch first said n't submit code homework get zero points saw submissions people put dot empty put link towards data science article example please n't please put code n't 'll get zero points entire week 'll make explicit form week three well please n't forget put code	0
remind us prize first 100 leaderboard	use cross-validation check validation get better scores – drop one drop one go sometimes makes sense drop even decrease score simply maybe one feature complex compute another one	0
possible increase deadline project might struggle two weeks	victoria 'm sure answer mean yes definitely adding part project least workshop reason want people learn also consider people entirely new concept thought would much overhead try cover things already quite complex day-to-day even though work daily dbt work slow-changing dimensions daily basis example 's also think 's something look moment use definitely dbt especially deletions implementing load step done hooks think 's section advanced knowledge link hooks like incremental models mentioned changing dimensions n't link could link something could use snapshots think added concept updates guess could also pre-hook post-hook also something would incremental model incremental model loads new chunk data use merge depending workers use use could insert update 's something would work unique key project also final project 's understand complex go 's limit hope answers	0
possible increase deadline project might struggle two weeks	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
possible increase deadline project might struggle two weeks	alexey well good job keep working almost finished first week finish start week two finish week three chuckles follow sequence videos playlist github sufficient think questions go slack ankush keep going alexey chuckles yes exactly ’ almost finished first week good sign first week year pretty tough many people lot problems docker whatnot 're almost finished good sign ’ good track keep working	0
possible increase deadline project might struggle two weeks	alexey want keep two weeks order ... give get ambitious might decide “ okay use pulsar use prefect use azure ” might go crazy might work well n't want ambitious want select something simple end-to-end workflow – something two weeks 's purposely made two weeks give time ... come something simple ankush idea full-fledged production-ready system keep simple also use technologies see grades given develop maybe bit sophisticated solution think reasonable two weeks alexey yeah course problems 're struggling something slack ask questions people helpful slack usually check channel see questions already answered really amazing thanks guys help 'm pretty sure get stuck ask question people help 'll also try help course n't afraid ask help n't get ambitious try something simple keep simple fine two weeks	1
possible increase deadline project might struggle two weeks	alexey well guess want dashboard aggregates hard time trying imagine dataset take raw form put dashboard maybe 's already grouped 's already aggregated 's probably small dataset 's suitable course think maybe select reasonably big dataset require transformation 're doubt ask slack really n't know kind dataset put dashboard without transformations ankush yeah minimum transformations would required alexey counts group-bys maybe joins ankush exactly think 's better choose dataset allow	0
submitted homework sent link github okay	yeah guess okay	1
submitted homework sent link github okay	yes submit anywhere want say 20 pt n't remember yeah work good article “ good ” meaning 's copy/pasting code nothing else actually need explain 's going also article – closer time working article guidelines well	0
submitted homework sent link github okay	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
submitted homework sent link github okay	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
submitted homework sent link github okay	like said 's always problem- data-specific datasets get good performance	0
following techniques compare hyperparameter tuning 1 tuning features singly 2 tuning together using gridsearch	n't think actually need web server github pages serve static pages like html unfortunately host machine learning models github pages	0
following techniques compare hyperparameter tuning 1 tuning features singly 2 tuning together using gridsearch	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
following techniques compare hyperparameter tuning 1 tuning features singly 2 tuning together using gridsearch	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
following techniques compare hyperparameter tuning 1 tuning features singly 2 tuning together using gridsearch	lisa well thought singly little hands-on lot times would try gridsearch think trying demonstrate “ oh skills actually try these. ” used things like datarobot things like great “ okay let try 100 models ” usually business sense subject matter expertise kind know “ okay well maybe influences things more. ” something like either gridsearch datarobot auto-machine learning think good places start would n't use also 's nice datarobot put models chuckles 's really handy terms saving time saw long notebook got tried “ okay let tune learning rate ” “ let tune different things. ” gridsearch lot faster auto-machine learning dump “ 's chart 's comparison ’ leaderboard. ” ’ much faster could code think place depends use case guess – fast need get something done goal kind thing alexey yeah datarobot give dataset press button magic gives best model right works ’ lisa pretty much one dataset everything prepared go dataset tune bunch things 's nice intern run phd intern may know much subject matter work think use kinds tools yeah big button laughs start alexey way would answer question – course advantages disadvantages deserve try would go gridsearch know dataset small try fit lot models let 's say couple thousand observations dataset training single model n't take one second two go crazy test many combinations want gridsearch – 's trying every single combination let 's say c parameter random forest try bootstrap without try number estimators try max depth basically creates like 50 100 1000 different combinations dataset small fitting one model n't take lot time go see usually relatively large dataset like 20-30 thousand fitting one model takes time 's one second – 's 10 seconds 20 seconds 30 seconds trying gridsearch takes forever would actually go try tune features singly exactly like showed course example xgboost showed like heuristic usually works xgboost n't give best parameters necessarily maybe gridsearch give enough compute gridsearch would find better parameters turn singly find ok solution faster gridsearch takes lot time finds better solution optimal one tuning features singly takes less time 's menial large dataset arrive ok model faster	1
following techniques compare hyperparameter tuning 1 tuning features singly 2 tuning together using gridsearch	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
many submissions week	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
many submissions week	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
many submissions week	alexey 246 150 less last week last week 404. ’ know homework maybe bit difficult next one also fun think liked homework prepared week next one also fun right dmitry dmitry yeah sure	1
many submissions week	building locally fine	0
many submissions week	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
reason technical perspective would rather choose gunicorn compared server managers like uvicorn example	honest n't used uvicorn used gunicorn n't know matters n't actually checked benchmarks use gunicorn also use others – n't remember ones reason basically	1
reason technical perspective would rather choose gunicorn compared server managers like uvicorn example	yes n't written script yet actually want – many people asked see results made mistake beginning letting people register n't add checkbox saying 's okay use data like first name last name public really public leaderboard everyone go check score simply n't ask permission want instead write web service ’ email field go put email get scores idea hope 'll able implement hopefully n't difficult able get scores way yes grade homework store scores course end remember first video talked public leaderboard end course 100 people leaderboard ask permission publish names 'll need figure exactly communicate results separate homework grading actually difficult want write script	0
reason technical perspective would rather choose gunicorn compared server managers like uvicorn example	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
reason technical perspective would rather choose gunicorn compared server managers like uvicorn example	think use full train dataset let look gridsearchcv ’ know gridsearchcv way parameter tuning selecting best parameters internally cross-validation find best parameters think must example somewhere ’ good example – parameter estimating using cross-validation check example tuning support vector machine classifier ’ tuning parameters trying different values c. 're trying different values gamma linear kernel – internal details svm n't matter 're trying different sets parameters – train/test dataset separation call full train fit full train dataset n't split train/validation 're using full train dataset gridsearchcv split internally think cool thing	0
reason technical perspective would rather choose gunicorn compared server managers like uvicorn example	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
data engineering skills needed machine learning engineer	victoria alexey extent think like general purpose ci/cd tools n't use data workflows principle things like airflow workflow engines tailored specific use case nice things victoria definitely different use case overlap fit different use cases alexey must admit use jenkins work n't chuckles please n't regret chuckles victoria said legacy so… alexey yeah still legacy need maintain nobody wants touch n't know happens nobody wants spend time moving elsewhere kind runs personally would n't come anywhere near jenkins like jenkins	0
data engineering skills needed machine learning engineer	sejal yes really kubernetes expert maybe alexey help alexey bit googling see kubernetes operator probably answers yeah operator use kubernetes internally think create kubernetes job execute typically use kubernetes operators use aws batch operators essentially thing except 're running kubernetes aws batch see usually pattern exactly use airflow use python operators bash operators – execute things workers – usually make workers pretty dumb n't lot resources instead delegate external compute environment like kubernetes aws batch maybe ecs alternatives google cloud azure state much resources need specific job want run docker image parameters executed somewhere – worker pattern see usually 's either sql operators execute bigquery athena presto something else kinds aws batch kubernetes jobs spark example	0
data engineering skills needed machine learning engineer	alexey yeah think 're helpful example machine learning engineering often requires building data pipelines model sometimes 's done data engineers sometimes 's done data scientists sometimes 's done by… basically everyone take part team n't dedicated data engineer somebody still needs – machine learning engineer data scientist –knowing tools like airflow spark maybe dbt quite helpful prepare data form useful machine learning models	1
data engineering skills needed machine learning engineer	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
data engineering skills needed machine learning engineer	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
november 1 november 8 review projects	alexey one thing maybe n't clear enough – please always include code solution maybe people n't watch first said n't submit code homework get zero points saw submissions people put dot empty put link towards data science article example please n't please put code n't 'll get zero points entire week 'll make explicit form week three well please n't forget put code	0
november 1 november 8 review projects	's actually think 's useful n't train.py produce pickle file let 's say want retrain model using different parameters – 's useful see changed score real life like projects work train.py 's always one file sometimes 's bunch files somehow log performance think 's useful n't want reason n't	0
november 1 november 8 review projects	correct yes	1
november 1 november 8 review projects	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
november 1 november 8 review projects	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
homework assignments deadlines found lot issues week 1 terms dependencies software versions working	alexey example spark google “ spark gcp bigquery connector ” good article shows need need jar need use jar submit spark job let 's say files stored google cloud storage something files multiple options first option save back google cloud storage create external table something like like saw week 3. another option would saving directly bigquery using say “ write ” say want write bigquery uses connector connect bigquery write n't know permissions actually work – maybe also need specify key somehow probably 're already dataproc already permissions need order write bigquery order read google cloud storage use dataproc theoretically easy – add extra jar write bigquery data data warehouse use materials dbt week analytics engineering week actually consume content data warehouse visualize	0
homework assignments deadlines found lot issues week 1 terms dependencies software versions working	alexey yes deadlines keep focused help stay focused however said feel overwhelmed finish homework time n't worry – still get certificate complete project even though said certificates n't matter one maybe also n't matter much matters knowledge get end n't hard miss deadline 's fine terms scores get points homework ’ finish time cares long learn something new 's important thing sorry hear found lot issues hopefully able resolve	1
homework assignments deadlines found lot issues week 1 terms dependencies software versions working	victoria would say ’ also 's different set products example data warehouse start data tech stack scratch would definitely use redshift cost lower end need probably would need migrate something like bigquery snowflake lot documentation 's pretty easy set 's around longer – think reason broader adoption mainly around things alexey seems aws popular well least berlin look companies use cloud maybe 70 people use aws 20 may use gcp remaining 10 use azure 's roughly see maybe 'm wrong course also got use gcp bit seems ui nicer go web console 's little bit nicer think comes tools 's bit difficult example aws pip install aws cli google cloud 's bit difficult maybe 's one reason aws popular – ’ mature maybe think gcp catching nice interface sometimes things cheaper bigquery also think big advantage maybe clouds n't really victoria plus aws around 10 years thesis using aws thing gcp yet 's lot people go 're familiar outside stability alexey think cloud services services quite similar ’ used aws many things gcp straightforward n't typical less think many concepts map one another	0
homework assignments deadlines found lot issues week 1 terms dependencies software versions working	alexey sure answer question actually something think spoiler homework solution chuckles using airflow helps use airflow 's 've whole week week 2 using airflow first download data ny taxi website parquetize upload google cloud storage n't s3 oh see question coming guess go ny taxi data website – transfer service – one videos transfer service transfer service moving data s3 google cloud storage understand question data already s3 – look urls see url contains “ s3 amazon aws ” name packet data already s3 n't need probably long answer short data already s3 n't need	0
homework assignments deadlines found lot issues week 1 terms dependencies software versions working	alexey 's early talk future iterations data engineering zoomcamp think chuckles ml camp yes – videos difference homework 's still discussion n't know 'll see ankush yeah think slight discussion met sure going repeat ml de zoomcamp let 's see response participation feel definitely success definitely feel free pass around course give us feedback maybe make better decision alexey re-recording videos… 's much effort n't think 'll want let 's say ml zoomcamp example much effort 'd rather reuse videos instead making new ones hope videos good decide rerun need re-record yeah 'll see	0
write article save github share linkedin	alexey one thing maybe n't clear enough – please always include code solution maybe people n't watch first said n't submit code homework get zero points saw submissions people put dot empty put link towards data science article example please n't please put code n't 'll get zero points entire week 'll make explicit form week three well please n't forget put code	0
write article save github share linkedin	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
write article save github share linkedin	yes think writing article github good basically article would suggest trying medium 's difficult seems medium also start recommending article people see 's github fine want actually turn github repo website 's called github pages take look maybe instead putting article git repo 's possible turn git page blog page need follow instruction 's difficult ’ come engineering background n't done 're comfortable using command line take time actually figure think 's worth investing time seeing works easily website free – entirely free n't need anything else actually secret 's secret actually datatalks.club site actually page run github pages everything see website github use github pages host 's free need pay domain – datatalks.club – rest github provides free hope convinced try	1
write article save github share linkedin	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
write article save github share linkedin	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
would know open model2.bin n't know exists	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
would know open model2.bin n't know exists	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
would know open model2.bin n't know exists	let 's say “ import numpy np ” let 's say “ np.random.exponential ” noticed press shift+tab press shift+tab shows docstring multiple times press shift+tab expands see funny thing n't even realize muscle memory 's n't saying loud know go shift+tab small thing open twice expand another thing put question mark instead parentheses “ np.random.exponential ” show docstring another thing think put “ help np.random.exponential ” print docstring	0
would know open model2.bin n't know exists	think mentioned homework saw created docker file see exists	1
would know open model2.bin n't know exists	building locally fine	0
good references conda git pipenv flask gunicorn docker work together	example go kaggle see number notebooks available particular competition could good indicator 's good dataset n't really talk multi-class classification think 's okay want multi-class classification first dataset probably either binary classification see target clearly zero one think go already know deal numerical variables know deal categorical variables idea deal missing data see dataset things 's similar dataset good one 're sure ask slack help	0
good references conda git pipenv flask gunicorn docker work together	far week two week three follow simple approach simple approach described take entire dataset split three parts train validation test 'll use week two week three week four talk cross validation – explain right want say approach validation sufficient many cases think 's pretty safe say many many many applications split enough cross validation course nicer learn week four right go detail try explain wait video “ select approach cross validation ” would say multiple options k-fold… 'm sure many people understand n't covered yet basically dataset big n't need k-fold dataset smaller use k-fold “ leave p ” – 've never used personally	0
good references conda git pipenv flask gunicorn docker work together	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
good references conda git pipenv flask gunicorn docker work together	alexey one thing maybe n't clear enough – please always include code solution maybe people n't watch first said n't submit code homework get zero points saw submissions people put dot empty put link towards data science article example please n't please put code n't 'll get zero points entire week 'll make explicit form week three well please n't forget put code	0
good references conda git pipenv flask gunicorn docker work together	yes good references – course course called machine learning zoomcamp cover parts except conda git assume midterm project use git conda – use conda python interpreter install pipenv manage dependencies n't know good references honest maybe n't know answered question hope references course good enough ’ please let know think improved think missing	1
book based de zoomcamp like book ml zoomcamp	ankush avro parquet totally different things avro row-based parquet columnar something like batch processing data-analytical kind work parquet perfect solution use spark something 's super fast calculate something like sum account parquet file read particular column rather whole column whole row avro hand strong providing flexibility terms backward forward compatibility 're using avro kafka using something like protobuf kafka would really useful think like different technologies altogether use different use cases alexey avro guess use case would streaming parquet batch jobs ankush yeah think definitely beginning would basic use cases yes alexey also something called ocr ankush 's also columnar storage alexey oh 's different orc chuckles remember watching presentation berlin buzzwords format remember seeing cool presentation never actually tried ankush neither i. alexey 's optimized row columnar interesting never knew stood widely used people go parquet ankush seen parquet guess companies definitely using 's bad technology think 's parquet famous compatible solutions parquet people tend use often	0
book based de zoomcamp like book ml zoomcamp	alexey today laughs ankush laughs maybe long break alexey thing ml zoomcamp already book thought “ okay maybe course. ” 's different actually .. sejal good idea though alexey think maybe alvaro 're watching – think could quite good writing books notes repo quite comprehensive take markdown go o'reilly tell “ hey look have. ” chuckles anyway maybe n't go o'reilly think notes alvaro quite good notes well quite good ’ alvaro quite also taking lot notes course 's always first submit notes pull requests thanks great job notes ankush feel free use share royalties us chuckles	1
book based de zoomcamp like book ml zoomcamp	alexey add comment least workplace airflow instance set us data scientist would go instance airflow managed data engineers devops engineers would use never needed actually run docker compose moment multiple dags orchestration tool quite useful would personally maybe go luigi favorite one need think people well since people know luigi many like maybe airflow would actually better choice consider sejal would also like add question docker compose used docker compose airflow style use official setup seeing first time well 's unnecessarily complicated especially xcom variables forth generally docker compose acts wrapper docker containers production workflows docker containers used integration ci/cd pipelines wherever 're going deploy docker compose docker compose part helpful development environments case let 's say mock version airflow environment production want test dags local development environment use docker compose make things easier running simple commands like ‘ docker compose ’ would make fine instead running ‘ docker run container name etc whatever want ’ terms convenience terms usage ’ generally used development environments production alexey think first part question “ close usual data engineer role workflows ” workflows see work complex steps probably also case need start something already – would n't call complex pipeline – three four steps already something eventually might grow bigger bigger add steps ankush would also like add 're cloud maybe use airflow deploy docker maybe actually use service provided cloud provider one similar airflow think airflow would used internally like cloud composite google cloud platform would using terraform sejal yeah used aws step functions aws setup step functions combination lambda place airflow ankush said also something wanted say really depends production self-hosted cluster kubernetes wherever want use pure airflow version company could kind setup use native airflow cluster could cloud-based setup convenient option use managed services cloud composer aws also version called managed airflow	0
book based de zoomcamp like book ml zoomcamp	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
book based de zoomcamp like book ml zoomcamp	alexey well good job keep working almost finished first week finish start week two finish week three chuckles follow sequence videos playlist github sufficient think questions go slack ankush keep going alexey chuckles yes exactly ’ almost finished first week good sign first week year pretty tough many people lot problems docker whatnot 're almost finished good sign ’ good track keep working	0
project need orchestrate everything airflow use airflow parts rest “ manually ”	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
project need orchestrate everything airflow use airflow parts rest “ manually ”	victoria 's buzzwords right alexey yeah go repo see like practical week like five 10 minute video explaining use 's planned ankush maybe spend couple minutes today well data mesh basically concept idea – let 's go back say “ data pipelines data engineering teams built different companies right ” people generating data people consuming data one big team middle called data engineering team dataops team whatever want call different companies different names teams responsible ingesting data transforming putting making way 's consumable right way right fashion data mesh wants basically decentralize particular role big team wants give power let 's say responsibility team generating data teams consuming data therefore ’ layer instead team taking whole role 's kind better idea build services give responsibility give services software teams generating data consuming basically going higher level respect case happens team ’ generating data responsible quality data answering different questions data things 's really useful multiple teams handling data idea data knowledge data lost best person best team answer questions best team knowledge data team 's generating also responsible putting data data lake data warehouse also responsible queries attaches whole thing together 's kind rough idea data mesh alexey ’ abstract concept actually go youtube channel longer explanation data mesh “ longer ” mean one hour long – hour six minutes chuckles 're kind stuff… way dataops 101 also quite nice one check well one modern data stack analytics engineering also good basically check three – 're good 're interested data mesh specifically first one goes lot detail second one – bit detail ankush 's also nice blog post alexey zhamak actually writing book right think 's early release 's written five six chapters n't remember 's progress 's going quite big book right ankush blog post like today 's date read couple days impossible read one go 's huge victoria think also download first two chapters something book give quick overview well ankush 's also another book o'reilly called data mesh practice max schultze alexey one free right ankush think free 's free	0
project need orchestrate everything airflow use airflow parts rest “ manually ”	alexey need use sort orchestrator whether 's airflow something else things manually – n't think 's good idea project still finish project n't started working dimensions assessing project one criteria evaluating project whether use workflow orchestrator decide use workflow orchestrator get points criterion maybe 'll lose two three points still need finalize 's idea ’ better use real life need use orchestrator also depends exactly mean “ rest ” maybe give us bit details let 's say slack answer yeah – 's better use victoria ’ probably also worth mentioning something mentioned could use could use another scheduler n't like airflow make sure clarify project details person evaluate project peer review able understand 've done may experience particular scheduler could also alternate way 's find easier	1
project need orchestrate everything airflow use airflow parts rest “ manually ”	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
project need orchestrate everything airflow use airflow parts rest “ manually ”	alexey add comment least workplace airflow instance set us data scientist would go instance airflow managed data engineers devops engineers would use never needed actually run docker compose moment multiple dags orchestration tool quite useful would personally maybe go luigi favorite one need think people well since people know luigi many like maybe airflow would actually better choice consider sejal would also like add question docker compose used docker compose airflow style use official setup seeing first time well 's unnecessarily complicated especially xcom variables forth generally docker compose acts wrapper docker containers production workflows docker containers used integration ci/cd pipelines wherever 're going deploy docker compose docker compose part helpful development environments case let 's say mock version airflow environment production want test dags local development environment use docker compose make things easier running simple commands like ‘ docker compose ’ would make fine instead running ‘ docker run container name etc whatever want ’ terms convenience terms usage ’ generally used development environments production alexey think first part question “ close usual data engineer role workflows ” workflows see work complex steps probably also case need start something already – would n't call complex pipeline – three four steps already something eventually might grow bigger bigger add steps ankush would also like add 're cloud maybe use airflow deploy docker maybe actually use service provided cloud provider one similar airflow think airflow would used internally like cloud composite google cloud platform would using terraform sejal yeah used aws step functions aws setup step functions combination lambda place airflow ankush said also something wanted say really depends production self-hosted cluster kubernetes wherever want use pure airflow version company could kind setup use native airflow cluster could cloud-based setup convenient option use managed services cloud composer aws also version called managed airflow	0
even extra jars needed could write tables read process	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
even extra jars needed could write tables read process	alexey 33 submissions think decision extend deadline good one hopefully end week see higher number understand project much difficult watching course following along 33 people – probably put lot effort thanks saw one person write put 100 hours already project outstanding sorry cruel 100 hours lot	0
even extra jars needed could write tables read process	alexey 's want look project 're behind right n't worry take time decide whether give certificate based projects homework 's – catch week 3 lighter week 1 maybe fast-forward directly docker compose run week 2 depending much time maybe go solution published today give everything need week 3 use transfer service video already watching video probably learn lot already like homework solution	0
even extra jars needed could write tables read process	alexey think saw thing back talking dataproc bigquery spark connector probably unfortunately n't used tool use dataproc n't know reason could 're using vm reason could vm maybe permissions write bigquery need check configure connector order able use key suspect could problem also make sure role use accessing google cloud storage actually write bigquery think 's “ bigquery admin ” something like make sure permission 's called “ service account ” think service account right permissions case created separate service account make sure permission unfortunately working example share right something want check weekend maybe record video time please feel free share slack think students help well think managed make work used spark gcp please share problem slack perhaps somebody solved problem already help	1
even extra jars needed could write tables read process	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
think skills analytics engineers need useful data engineers example knowing dbt quite useful data engineers maybe 's main focus	victoria would say 's main focus us example company data engineers n't really use dbt everyone office access 's something analytics engineer data engineer mainly focuses happens warehouse right kafka s3 buckets pipelines think 's still good know 's going data going data engineer let 's say stakeholder happens analytics engineer think 's good knowledge know actually happening alexey reason mentioned ankush one year ago maybe first met still working different company data engineer mentioned tool dbt idea tool looked told something data engineers use company first time heard apparently data engineers also need use tool sometimes victoria think many companies n't analytics engineers 's quite new role either data engineer pick data analyst 'd say makes sense data engineer picks think companies use dbt already quite familiar role analytics engineer makes sense would person would take dbt alexey think ’ asked podcast analytics engineering data engineers transition analytics engineering sometimes right victoria yeah analytics engineer person year ago something else – either data engineering data analysis either go get little bit technical go get little bit business focus yes	1
think skills analytics engineers need useful data engineers example knowing dbt quite useful data engineers maybe 's main focus	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
think skills analytics engineers need useful data engineers example knowing dbt quite useful data engineers maybe 's main focus	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
think skills analytics engineers need useful data engineers example knowing dbt quite useful data engineers maybe 's main focus	alexey discussion/thread announcement channel week ago – lot datasets shared n't know many cool websites datasets check victoria put announcements alexey also want slack dump datatalks.club also use play around dataset see 's bunch json files build data pipelines well	0
think skills analytics engineers need useful data engineers example knowing dbt quite useful data engineers maybe 's main focus	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
changes required host port settings python script flask requests script deploy container cloud i.e heroku	building locally fine	0
changes required host port settings python script flask requests script deploy container cloud i.e heroku	flask – let draw let 's say want create web service image 5 web service gets requests web service responds something could predictions could something else – n't related machine learning implement web service use flask use fastapi use framework creating web services use flask use something else – flask allows use post example let 's say “ get ” usually parse parameters query let 's say go google put “ test ” see q=test https //www.google.com/search q=test oq=test aqs=chrome.0.69i59l2j69i65l3j69i60j69i61l2.1264j0j1 sourceid=chrome ie=utf-8 image 6 parameter pass “ get ” request “ get ” request sometimes let 's say want score customer n't want put gender=female contract=one year string send json post post also add body request generally use post general use case flask able create web service	0
changes required host port settings python script flask requests script deploy container cloud i.e heroku	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
changes required host port settings python script flask requests script deploy container cloud i.e heroku	really depends cloud showed elastic beanstalk n't think needed change much anything n't remember changing anything thing changed url service test.py script thing needed change think heroku 's similar actually couple tutorials using pythonanywhere heroku probably explain need change go see need change anything probably much	1
changes required host port settings python script flask requests script deploy container cloud i.e heroku	case everyone uses docker problem n't use docker n't get point using docker ’ run write “ sorry run this. ” hope n't happen 's actually whole idea behind docker – 's transferable different environments let 's say use gunicorn n't use docker able test case use scoring matrix evaluate accordingly gunicorn tick box would encourage still try learn submission even run reason think reason run something windows ubuntu gunicorn everything runnable	0
could get homework could go around setting environments	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
could get homework could go around setting environments	mae – talk great detail mae rmse metrics evaluating quality regression models course use rmse mae alternative one rmse formula mae formula little bit different instead looking squared error look absolute error absolute means use bars formula 're little bit different rmse penalizes big errors let 's say prediction 0.1 actual prediction 100. rmse get higher penalty 's 99.9 squared huge huge penalty mae 99.9. pros cons – use n't choose one always report numbers	0
could get homework could go around setting environments	say want work machine learning engineer need able set environments practice makes perfect need train eventually able	1
could get homework could go around setting environments	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
could get homework could go around setting environments	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
using cross-validation	well 's honestly want use cross-validation think also use usual train/validation split cross-validation bonus see stable model – standard deviation usually 's useful would suggest require cross-validation useful must form validation without able select best model	1
using cross-validation	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
using cross-validation	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
using cross-validation	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
using cross-validation	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
give us tour slack channel	victoria would say yes go-to dbt yes dbt enforces go read viewpoint 'll explain also models kind structure mentioned modern concepts concept comes 80s kimball defined kitchen analogy find useful normally go course complex gets may need steps 's okay always try separated – presentation source need typecasting like example maybe duplication middle may models go data pipeline let ’ say flow less would look like alexey kitchen analogy victoria kitchen analogy way model data similar restaurants model food warehouse raw food source data everyone allowed fact 's actually dangerous 's security measures actually go warehouse stored stored things like kitchen people cook raw food make food 're going serve allowed would data engineers data analysts maybe analytics engineers would data warehouse trying process raw data end part restaurant people eat – dining hall presentation layer data warehouse fact tables look like data marts 're building data marts business stakeholders everyone allowed get foot let ’ say restrictions 's already presented 're presenting raw alexey think tableau self-service tool go cubes presentation layer right victoria yeah technically bi tools self-serve could everything could expose everything right external tables create tables inaudible create section model create factories could everything could even ones development exposed self-service bi tool like tableau looker whatever analogy says use presentation layer parts present raw data people would n't know use 's going like “ oh n't find phones ones maybe 's transformation ” things like alexey nice analogy	0
give us tour slack channel	victoria set warehouse end define connection difference would use adapter dbt several adapters would first want use dbt go check adapters make sure 's supported several official example redshift could use community-supported azure alexey yeah aws athena azure athena redshift presto well example presto cluster use aws – within aws presto cluster potentially similar setup connect dbt presto victoria general add little bit answer let 's say project right bigquery would change profile 's set connection may make changes 's something sql supports bigquery way around able run project minutes	0
give us tour slack channel	alexey 's early talk future iterations data engineering zoomcamp think chuckles ml camp yes – videos difference homework 's still discussion n't know 'll see ankush yeah think slight discussion met sure going repeat ml de zoomcamp let 's see response participation feel definitely success definitely feel free pass around course give us feedback maybe make better decision alexey re-recording videos… 's much effort n't think 'll want let 's say ml zoomcamp example much effort 'd rather reuse videos instead making new ones hope videos good decide rerun need re-record yeah 'll see	0
give us tour slack channel	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
give us tour slack channel	alexey slack see default channels course-related questions go course-data-engineering channel need click plus icon “ browse channel ” write “ course ” search look relevant course channel course-data-engineering click “ join ” channel use asking course-related questions lot stuff happening community example welcome introduce think course-ml-zoomcamp channel course click browse channels see example channel engineering discussing engineering things channel data science another cool channel called book-of-the-week invite book authors come answer questions check well register slack still n't done go datatalks.club leave email main page prompt click “ join ” receive link n't receive link reach twitter linkedin – whichever way prefer give email use different way adding slack	1
eda first split data split data eda data need order eda train full train full data	's interesting one let 's say numpy array image reference numbers – 1 6 – compute mean mean 3.5. add 3.5 multiple times mean n't change happens nan pandas simply ignores – n't look pretends 's basically pandas ignores nans looks numbers replace nans mean case 3.5 get data effectively mean n't change try show formula let 's say bit math let 's say four cities couple n/as basically part usual values part n/as missing part let 's say want compute mean .mean pandas happens let 's say n numbers like k missing values total series k+n – total length compute mean part get 1/nσxi=x̄ mean case 3.5. let 's replace n/as x̄ happens instead looking 1/n look 1/n+k total number elements already filled sum another sum x̄ k n summed k times x̄ becomes kx̄ formula like multiply n/n n/n simply 1. 's multiplying thing one get n x̄ part mean image reference clue 'm talking bear 's almost result 1/ n+k n x̄+kx̄ leave x equation 1/ n+k n+k x̄=x̄ basically shows add many means like result always x̄ 'm sure everyone enjoyed little proof image reference prove saw question meeting got curious like “ show mathematically ” spent like 10 minutes trying figure nice exercise brain 'm sure would able show proof spot honest	0
eda first split data split data eda data need order eda train full train full data	classification classification let 's say n't predicting price let 's say predicting 's average number hours image 1 case would need another layer – rather layer 's called activation n't activation 're still… ’ trying formulate way 's confusing think 's better go lecture rewatch ’ essentially retelling thing need from_logits price variable loss mean squared error logits loss categorical cross-entropy categorical cross entropy usually used multiple categories need use logits 's numerically stable n't use – stay activation softmax n't use legit hope 's clear regression n't need use homework need use from_logits either homework two classes 's binary classification problem multiple classes – let 's say three classes four classes five classes – need use logits loss categorical cross-entropy loss categorical cross-entropy 's recommended use from_logits=true 's numerically stable	0
eda first split data split data eda data need order eda train full train full data	ideally said take dataset – leave test dataset aside n't touch final model n't eda test dataset train full train – n't training dataset accidentally see pattern try use maybe build feature around pattern meanwhile pattern might true general case try – 's called data snooping look data might accidentally see something something may seem important accidentally overfit try avoid looking test dataset	1
eda first split data split data eda data need order eda train full train full data	flask – let draw let 's say want create web service image 5 web service gets requests web service responds something could predictions could something else – n't related machine learning implement web service use flask use fastapi use framework creating web services use flask use something else – flask allows use post example let 's say “ get ” usually parse parameters query let 's say go google put “ test ” see q=test https //www.google.com/search q=test oq=test aqs=chrome.0.69i59l2j69i65l3j69i60j69i61l2.1264j0j1 sourceid=chrome ie=utf-8 image 6 parameter pass “ get ” request “ get ” request sometimes let 's say want score customer n't want put gender=female contract=one year string send json post post also add body request generally use post general use case flask able create web service	0
eda first split data split data eda data need order eda train full train full data	’ two weeks october 18 2021 meaning first november 2022	0
able kick-start entire data pipeline including airflow using terraform enabling docker provided terraform	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
able kick-start entire data pipeline including airflow using terraform enabling docker provided terraform	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
able kick-start entire data pipeline including airflow using terraform enabling docker provided terraform	alexey way thought – watching course enough maybe watch course feel like know everything try something realize 's actually easy thought want experience want sit try apply knowledge picked 's decided way get certificate end project 's purpose 's enough watch need things answer way need project 're interested certificate	0
able kick-start entire data pipeline including airflow using terraform enabling docker provided terraform	alexey 'm sure last part –how docker provided terraform ankush run stuff terraform think always ... set airflow cluster terraform n't think execute job alexey 's something like use terraform prepare environment prepare thing run need actually run something least two separate commands first “ terraform apply ” “ python run ” something like 50:02 1 upvotes allowed contact instructors directly discuss projects things unsure regarding project 'm using new tools alexey think 's best ask things slack channel others also interested learning answer scales better instead everyone writing us directly – answer everyone sees answer b others answer available right somebody knows tool maybe n't example pulsar one students might know able help 's best use channel contact us directly	1
able kick-start entire data pipeline including airflow using terraform enabling docker provided terraform	alexey think 's helpful 's necessary helpful happens dags run airflow dags tasks tasks often kubernetes jobs something like infra – many companies infrastructure managed kubernetes 's quite simple get new container execute new job kubernetes n't think 's necessary think 's helpful 'm data engineer – 'm data scientist data scientist knowledge kubernetes quite helpful could debug jobs 'm running airflow run kubernetes could see logs would say want learn hurt get hired without kubernetes knowledge materials course sufficient get hired pick kubernetes work example agree victoria victoria yeah think kubernetes getting popular ca n't hurt	0
another session starting soon	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
another session starting soon	mean another iteration course yes go course repository see answer first take course self-paced mode videos available except last videos kserve start watching 's important first attempt homework without looking answer check solutions least one project 's also important n't follow videos sure learn something 's better solidify learning also practicing least one project 're taking course self-paced mode want get feedback project also share link slack happy give feedback project answer actual question regarding next cohort – start september cohort started september probably start bit earlier started middle september time probably start maybe one week earlier want notified informed link form click put email send email course starts	1
another session starting soon	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
another session starting soon	classification classification let 's say n't predicting price let 's say predicting 's average number hours image 1 case would need another layer – rather layer 's called activation n't activation 're still… ’ trying formulate way 's confusing think 's better go lecture rewatch ’ essentially retelling thing need from_logits price variable loss mean squared error logits loss categorical cross-entropy categorical cross entropy usually used multiple categories need use logits 's numerically stable n't use – stay activation softmax n't use legit hope 's clear regression n't need use homework need use from_logits either homework two classes 's binary classification problem multiple classes – let 's say three classes four classes five classes – need use logits loss categorical cross-entropy loss categorical cross-entropy 's recommended use from_logits=true 's numerically stable	0
another session starting soon	let 's say two dimensional thing 2d want turn 1d case need use pooling2d 'm sure right want quickly check reason remember showing usually go defining model build sort layer layer every time add one layer model predicts see output based see kind pooling layer example need let 's say want turn 2d 1d think need use 2d pooling 'm exactly sure whether 's 2d 1d think 1d pooling needed something one dimensional want turn one value use 1d pooling 3d pooling three dimensional thing want turn one-dimensional thing image 2 things always confuse honest 's follow step step try different poolings want make sure convert image vector presentation something one-dimensional usually size number images times something 's like 2d array image one-dimensional vector based try different poolings sometimes also flatten flatten takes whatever – let 's say kd – want turn 1d use flatten many different options think 's clear difference might remember exactly use 1d 2d difference kind input take kind output produce 's cube 's 2d 's hypercube three dimensions 's something else	0
big project take time work week	victoria would say ’ also 's different set products example data warehouse start data tech stack scratch would definitely use redshift cost lower end need probably would need migrate something like bigquery snowflake lot documentation 's pretty easy set 's around longer – think reason broader adoption mainly around things alexey seems aws popular well least berlin look companies use cloud maybe 70 people use aws 20 may use gcp remaining 10 use azure 's roughly see maybe 'm wrong course also got use gcp bit seems ui nicer go web console 's little bit nicer think comes tools 's bit difficult example aws pip install aws cli google cloud 's bit difficult maybe 's one reason aws popular – ’ mature maybe think gcp catching nice interface sometimes things cheaper bigquery also think big advantage maybe clouds n't really victoria plus aws around 10 years thesis using aws thing gcp yet 's lot people go 're familiar outside stability alexey think cloud services services quite similar ’ used aws many things gcp straightforward n't typical less think many concepts map one another	0
big project take time work week	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
big project take time work week	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
big project take time work week	victoria 'm sure answer mean yes definitely adding part project least workshop reason want people learn also consider people entirely new concept thought would much overhead try cover things already quite complex day-to-day even though work daily dbt work slow-changing dimensions daily basis example 's also think 's something look moment use definitely dbt especially deletions implementing load step done hooks think 's section advanced knowledge link hooks like incremental models mentioned changing dimensions n't link could link something could use snapshots think added concept updates guess could also pre-hook post-hook also something would incremental model incremental model loads new chunk data use merge depending workers use use could insert update 's something would work unique key project also final project 's understand complex go 's limit hope answers	0
big project take time work week	ankush think try keep project minimum definitely need spend time ’ like deadline one week would giving deadline two weeks community wants extension possibility well encourage take time work project definitely spending time would help advance career sejal add self paced course uploading recordings videos taking 's live course please take time based capacity available resources terms time effort n't really squeeze things timeline make work alexey course everyone deadlines project make possible actually peer reviewing 're behind videos still project ask feedback project – post link slack channel say “ project somebody please take look evaluate ” 'm sure somebody glad learn	1
much time submit final project	victoria 'm sure answer mean yes definitely adding part project least workshop reason want people learn also consider people entirely new concept thought would much overhead try cover things already quite complex day-to-day even though work daily dbt work slow-changing dimensions daily basis example 's also think 's something look moment use definitely dbt especially deletions implementing load step done hooks think 's section advanced knowledge link hooks like incremental models mentioned changing dimensions n't link could link something could use snapshots think added concept updates guess could also pre-hook post-hook also something would incremental model incremental model loads new chunk data use merge depending workers use use could insert update 's something would work unique key project also final project 's understand complex go 's limit hope answers	0
much time submit final project	alexey mean much time get deadline would two weeks 28th march 2022. one week peer reviewing	1
much time submit final project	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
much time submit final project	alexey well good job keep working almost finished first week finish start week two finish week three chuckles follow sequence videos playlist github sufficient think questions go slack ankush keep going alexey chuckles yes exactly ’ almost finished first week good sign first week year pretty tough many people lot problems docker whatnot 're almost finished good sign ’ good track keep working	0
much time submit final project	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	0
slides week 9 serverless videos	really depends cloud showed elastic beanstalk n't think needed change much anything n't remember changing anything thing changed url service test.py script thing needed change think heroku 's similar actually couple tutorials using pythonanywhere heroku probably explain need change go see need change anything probably much	0
slides week 9 serverless videos	honest n't used uvicorn used gunicorn n't know matters n't actually checked benchmarks use gunicorn also use others – n't remember ones reason basically	0
slides week 9 serverless videos	n't think slides n't remember maybe one two drawings maybe n't see point uploading mostly hands-on without slides	1
slides week 9 serverless videos	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
slides week 9 serverless videos	yes good references – course course called machine learning zoomcamp cover parts except conda git assume midterm project use git conda – use conda python interpreter install pipenv manage dependencies n't know good references honest maybe n't know answered question hope references course good enough ’ please let know think improved think missing	0
finish project 1 time still want feedback submit incomplete project	yes please submit incomplete project actually n't mention 15 points max amount – pass paths probably need something like 10 points 7. actually take look many points get see kind cutoff makes sense order pass project n't score twos every metric pass project example problems running docker get two points – get 1 world stop still learn something let 's say finished point first part n't deploy get 7 points example still learn something still get feedback regarding passing criteria 'll need take look actual scores end nobody gets 15 points nobody would pass hope n't happen ’ see want look course first	1
finish project 1 time still want feedback submit incomplete project	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
finish project 1 time still want feedback submit incomplete project	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
finish project 1 time still want feedback submit incomplete project	took course ago back useful think something use python immediately could helpful 'm sure would good recommendation current course course good would n't compare stanford course andrew ng simply different course 'll get lot theory one practical hands-on – 's mostly writing code together maybe take	0
finish project 1 time still want feedback submit incomplete project	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
kafka running virtual machine 1 spark streaming virtual machine 2 use external ip connect cluster settings	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
kafka running virtual machine 1 spark streaming virtual machine 2 use external ip connect cluster settings	alexey well 's really anonymous asked submit link github n't know need ask questions previous course think ask contact information project description n't know ankush think encourage might unfair different people first maybe 're available answer questions let 's put like grading maybe 's unfair want learn 's definitely helpful think grading done based upon submitted 's also really important document steps alexey yeah guess need contact author means really give four points reproducibility ankush case maybe instructions clear report via contact alexey yeah exactly n't remember problem previous course maybe also create issue github n't know	0
kafka running virtual machine 1 spark streaming virtual machine 2 use external ip connect cluster settings	alexey n't think really understand question ankush 're vpc matter alexey cluster – spark cluster cluster kafka cluster ankush 're running vpc able access 's ip right alexey yeah oh okay externally ip guess 're running inside network use internal ip 's inside kubernetes refer names deployments n't know ankush also n't know 's really specific particular use case need give us maybe slack explain bit – running kind machines exactly exposing terms external ip alexey usually kafka cluster kafka machines spark cluster spark cluster connects kafka reads data ankush depends vpc running 're running different vpcs need expose separately need expose ip running vpc matter able access internally might wrong 'm devops chuckles alexey usually take things granted maybe good thing work grateful data team makes tools work often n't ask “ okay actually configured ” works n't – n't go support channel ask “ hey please fix ” ankush think 's also data team 's also maybe devops team embedded inside data team 're working olx pretty big company maybe many smaller teams inside big teams guess 's devops topic	1
kafka running virtual machine 1 spark streaming virtual machine 2 use external ip connect cluster settings	alexey well use judgment want try force register account azure run want n't maybe look code see errors 's really ankush maybe matching ask criteria – like used – maybe match corresponding people alexey 's nice idea let 's see 'm afraid complicate things bit already script matching n't use criteria 's random ankush need modify script one column alexey yeah perhaps let 's see ankush easier enough group different things alexey create form submitting ask cloud used right ankush exactly right alexey makes sense machine learning zoomcamp – wanted re-execute whole thing re-executed 's really want learn project – execute grading also learning – want learn really want learn things work execute learn n't time since executing different cloud might take lot time – n't much time really ask invest something like five hours figuring azure works use judgment guess 's answer try matching minimize	0
kafka running virtual machine 1 spark streaming virtual machine 2 use external ip connect cluster settings	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
please make video docker	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
please make video docker	alexey 'm glad asked think actually videos week one check	1
please make video docker	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
please make video docker	victoria 'm sure answer mean yes definitely adding part project least workshop reason want people learn also consider people entirely new concept thought would much overhead try cover things already quite complex day-to-day even though work daily dbt work slow-changing dimensions daily basis example 's also think 's something look moment use definitely dbt especially deletions implementing load step done hooks think 's section advanced knowledge link hooks like incremental models mentioned changing dimensions n't link could link something could use snapshots think added concept updates guess could also pre-hook post-hook also something would incremental model incremental model loads new chunk data use merge depending workers use use could insert update 's something would work unique key project also final project 's understand complex go 's limit hope answers	0
please make video docker	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
reasoning using auc comparing features rather feature target mathematical reason/intuitive notion behind	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
reasoning using auc comparing features rather feature target mathematical reason/intuitive notion behind	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
reasoning using auc comparing features rather feature target mathematical reason/intuitive notion behind	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
reasoning using auc comparing features rather feature target mathematical reason/intuitive notion behind	one think 's good way checking feature importance mathematical/intuitive reason behind let 's say even train first model see features already give good separation positive negative classes remember attrition auc interpretation auc probability randomly selected positive example higher score randomly selected negative example score also mean feature ’ randomly selected positive example value feature higher randomly selected negative example think probably makes sense try let 's say seniority – negative correlation maybe 's little bit complex already see important features maybe features already good enough maybe n't even need model use feature prediction roll second iteration proper model 's think 's good idea 's added exercise	1
reasoning using auc comparing features rather feature target mathematical reason/intuitive notion behind	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
things look order select good dataset	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
things look order select good dataset	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
things look order select good dataset	example go kaggle see number notebooks available particular competition could good indicator 's good dataset n't really talk multi-class classification think 's okay want multi-class classification first dataset probably either binary classification see target clearly zero one think go already know deal numerical variables know deal categorical variables idea deal missing data see dataset things 's similar dataset good one 're sure ask slack help	1
things look order select good dataset	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
things look order select good dataset	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
need submit midterm project data code deployment files etc	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
need submit midterm project data code deployment files etc	multi-classification – yes clustering – 'm sure probably good idea n't cover hard – know evaluate peers know better clustering	0
need submit midterm project data code deployment files etc	think answered previous question make sure document well file 'll put deliverables	1
need submit midterm project data code deployment files etc	flask – let draw let 's say want create web service image 5 web service gets requests web service responds something could predictions could something else – n't related machine learning implement web service use flask use fastapi use framework creating web services use flask use something else – flask allows use post example let 's say “ get ” usually parse parameters query let 's say go google put “ test ” see q=test https //www.google.com/search q=test oq=test aqs=chrome.0.69i59l2j69i65l3j69i60j69i61l2.1264j0j1 sourceid=chrome ie=utf-8 image 6 parameter pass “ get ” request “ get ” request sometimes let 's say want score customer n't want put gender=female contract=one year string send json post post also add body request generally use post general use case flask able create web service	0
need submit midterm project data code deployment files etc	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
tutorial install dbt locally anaconda found someone explained slack still working run dbt	victoria n't used anaconda think 's also documentation install dbt locally general 're using anaconda 're using environment install use homebrew 're using mac also use pip install pip install dbt postgres example need adapter dbt snowflake dbt big query – whatever want use locally 's 're installed set profiles.yml show videos create project clone whatever 's 's need use locally never install using anaconda 'm guessing 's probably something similar probably conda forge install like 's possible otherwise would go pip install alexey anaconda usually pip victoria yeah 're environment install like 's	1
tutorial install dbt locally anaconda found someone explained slack still working run dbt	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
tutorial install dbt locally anaconda found someone explained slack still working run dbt	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
tutorial install dbt locally anaconda found someone explained slack still working run dbt	alexey well good job keep working almost finished first week finish start week two finish week three chuckles follow sequence videos playlist github sufficient think questions go slack ankush keep going alexey chuckles yes exactly ’ almost finished first week good sign first week year pretty tough many people lot problems docker whatnot 're almost finished good sign ’ good track keep working	0
tutorial install dbt locally anaconda found someone explained slack still working run dbt	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
could n't go ahead model deployment steps first steps saving model adequate pass midterm	yes could soon post message slack need find time couple tweaks script	0
could n't go ahead model deployment steps first steps saving model adequate pass midterm	multi-classification – yes clustering – 'm sure probably good idea n't cover hard – know evaluate peers know better clustering	0
could n't go ahead model deployment steps first steps saving model adequate pass midterm	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
could n't go ahead model deployment steps first steps saving model adequate pass midterm	tell 're really missing 're deployment step one important steps machine learning project train model finish –you notebook pickle file – model useless nobody use model rely somebody else – need give model somebody else say “ hey please deploy model ” reply “ yeah maybe next year year 'm busy stuff sorry. ” think ability able deploy model one important skills suggest maybe midterm project general – 're serious data science think learn docker learn flask learn tools useful please actually answer question – could n't finish maybe n't enough time 's understandable complex understand especially 're first time docker stuff new sufficient pass midterm project know yet want first look distribution scores let 's say lot people example 90 people submitted midterm project n't step 'm sure 's okay say 's adequate basically want decide threshold passing project seeing scores probably better said – strongly encourage still couple days good tutorial ninat slack shows use google cloud shell docker stuff n't even need install docker computer use gmail account google account stuff take time go tutorials still two days think 'll able 're still training model stop try deployment please grateful learn	1
could n't go ahead model deployment steps first steps saving model adequate pass midterm	answer – depends dataset – typically train/validation split first try scaling try without scaling see happens think something talked last week actually notebook prepared last week go classification notebook notebook scaling shows scale features use standard scaler use min max scaler – use two different scalars shows particular dataset turned little bit better experiment see scaling makes sense linear models sometimes 's actually good idea add extra complexity pipeline instead dictionary vectorizer whatnot 'll also need add next step scaling basically steps prediction pipeline becomes bit difficult maintain would say try scale see improvement get significant maybe 's worth extra complexity get use cross-validation use best judgment see makes sense	0
may able submit time see people 's work learn projects	think many people actually shared midterm projects linkedin twitter perhaps follow mlzoomcamp hashtag see kind projects think 's also good idea midterm project peer reviewing ask everyone wants share work – share able look n't really want share form maybe people n't want share work everyone somebody wants share post links slack	1
may able submit time see people 's work learn projects	think people good intentions n't think somebody intentionally give bad score somebody wants intentionally give bad scores please n't would seriously please n't 're learn somebody 's giving best see matrix use evaluating give score person deserves n't link coursera actually research peer reviewing found peer reviews actually work quite well people n't give bad scores sake another reason three people reviewing one project exactly avoid one person giving everyone bad scores 'll take median avoid 's three scores also think opportunity get feedback people write something saying “ hey problems running this. ” 's good somebody tries actually run project n't answer apart please bit trust people think good intentions	0
may able submit time see people 's work learn projects	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
may able submit time see people 's work learn projects	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
may able submit time see people 's work learn projects	alexey one thing maybe n't clear enough – please always include code solution maybe people n't watch first said n't submit code homework get zero points saw submissions people put dot empty put link towards data science article example please n't please put code n't 'll get zero points entire week 'll make explicit form week three well please n't forget put code	0
ideal pipeline look like connect everything make reproducible minimal effort example project anywhere	alexey well use judgment want try force register account azure run want n't maybe look code see errors 's really ankush maybe matching ask criteria – like used – maybe match corresponding people alexey 's nice idea let 's see 'm afraid complicate things bit already script matching n't use criteria 's random ankush need modify script one column alexey yeah perhaps let 's see ankush easier enough group different things alexey create form submitting ask cloud used right ankush exactly right alexey makes sense machine learning zoomcamp – wanted re-execute whole thing re-executed 's really want learn project – execute grading also learning – want learn really want learn things work execute learn n't time since executing different cloud might take lot time – n't much time really ask invest something like five hours figuring azure works use judgment guess 's answer try matching minimize	0
ideal pipeline look like connect everything make reproducible minimal effort example project anywhere	alexey n't think really understand question ankush 're vpc matter alexey cluster – spark cluster cluster kafka cluster ankush 're running vpc able access 's ip right alexey yeah oh okay externally ip guess 're running inside network use internal ip 's inside kubernetes refer names deployments n't know ankush also n't know 's really specific particular use case need give us maybe slack explain bit – running kind machines exactly exposing terms external ip alexey usually kafka cluster kafka machines spark cluster spark cluster connects kafka reads data ankush depends vpc running 're running different vpcs need expose separately need expose ip running vpc matter able access internally might wrong 'm devops chuckles alexey usually take things granted maybe good thing work grateful data team makes tools work often n't ask “ okay actually configured ” works n't – n't go support channel ask “ hey please fix ” ankush think 's also data team 's also maybe devops team embedded inside data team 're working olx pretty big company maybe many smaller teams inside big teams guess 's devops topic	0
ideal pipeline look like connect everything make reproducible minimal effort example project anywhere	alexey add comment least workplace airflow instance set us data scientist would go instance airflow managed data engineers devops engineers would use never needed actually run docker compose moment multiple dags orchestration tool quite useful would personally maybe go luigi favorite one need think people well since people know luigi many like maybe airflow would actually better choice consider sejal would also like add question docker compose used docker compose airflow style use official setup seeing first time well 's unnecessarily complicated especially xcom variables forth generally docker compose acts wrapper docker containers production workflows docker containers used integration ci/cd pipelines wherever 're going deploy docker compose docker compose part helpful development environments case let 's say mock version airflow environment production want test dags local development environment use docker compose make things easier running simple commands like ‘ docker compose ’ would make fine instead running ‘ docker run container name etc whatever want ’ terms convenience terms usage ’ generally used development environments production alexey think first part question “ close usual data engineer role workflows ” workflows see work complex steps probably also case need start something already – would n't call complex pipeline – three four steps already something eventually might grow bigger bigger add steps ankush would also like add 're cloud maybe use airflow deploy docker maybe actually use service provided cloud provider one similar airflow think airflow would used internally like cloud composite google cloud platform would using terraform sejal yeah used aws step functions aws setup step functions combination lambda place airflow ankush said also something wanted say really depends production self-hosted cluster kubernetes wherever want use pure airflow version company could kind setup use native airflow cluster could cloud-based setup convenient option use managed services cloud composer aws also version called managed airflow	0
ideal pipeline look like connect everything make reproducible minimal effort example project anywhere	ankush n't know 's example project alexey well sure good example projects project right chuckles create great projects example projects right think good project open recommend must projects n't know “ ideal pipeline ” vague thing ankush yeah really depends use case ideal pipeline would one least manual steps requires least intervention even adding new data sources let 's say similar kinds data consume producing data scale accordingly minimum intervention side possible data engineer would good pipeline would say alexey definition ideal pipeline stitch together open source tools like airflow spark ankush chuckles 'm pretty sure product manager come new use case ideal pipeline would longer ideal laughs alexey think tools like fivetran never used closer ideal one case fivetran works said product manager comes use case longer fits closed-source platform therefore need figure actually make things work experience working things n't know olx love stitch together open source products hope best ankush going open source think tools discussed used ones alexey ideal would n't say work ankush think enterprise ones much better – much easier – less flexible well basic use cases 's perfectly fine want something specific would stuck might need pay ask improvements might take months come kinds things always really depend upon kind majority company kind use cases	1
ideal pipeline look like connect everything make reproducible minimal effort example project anywhere	alexey well good job keep working almost finished first week finish start week two finish week three chuckles follow sequence videos playlist github sufficient think questions go slack ankush keep going alexey chuckles yes exactly ’ almost finished first week good sign first week year pretty tough many people lot problems docker whatnot 're almost finished good sign ’ good track keep working	0
summaries videos like course notes jupyter notebooks etc	alexey google form works fine n't worry clicked “ submit ” worked accepted guess question scores want see answers correct show solution wednesday 'll upload video solution also sort leaderboard maybe actually show already something similar course leaderboard shows many points get homework n't show email course 's hash email see many points got question result get leaderboard total amount points know soon answers correct	0
summaries videos like course notes jupyter notebooks etc	ankush planning syllabus actually removed nosql databases wanted focus important tools data engineering 's covering spark data warehouse kafka dbt airflow nosql definitely important part data engineers know community really wants maybe develop couple videos nosql use – maybe give example cassandra something alexey also ran survey started course eight different topics concerning community wants hear analytics engineering top one terms preference nosql last one whether internals nosql n't remember based decisions survey well seemed like much interest nosql databases lot interest analytics engineering affected way decided come syllabus course maybe also let us know specifically mind nosql broad term talking redis talking mongo cassandra dynamo exactly	0
summaries videos like course notes jupyter notebooks etc	alexey n't think actually time summarize everything course got inspired sejal – prepares notes 's recording video based notes share notes also take notes share notes fellow students way learn public – sort regurgitate content produce something new – useful useful others promise summaries encourage take notes share others jupyter books think something publish particular week jupyter notebook code snippets course share ankush definitely share code snippets code use particular week definitely shared github	1
summaries videos like course notes jupyter notebooks etc	alexey example spark google “ spark gcp bigquery connector ” good article shows need need jar need use jar submit spark job let 's say files stored google cloud storage something files multiple options first option save back google cloud storage create external table something like like saw week 3. another option would saving directly bigquery using say “ write ” say want write bigquery uses connector connect bigquery write n't know permissions actually work – maybe also need specify key somehow probably 're already dataproc already permissions need order write bigquery order read google cloud storage use dataproc theoretically easy – add extra jar write bigquery data data warehouse use materials dbt week analytics engineering week actually consume content data warehouse visualize	0
summaries videos like course notes jupyter notebooks etc	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	0
please recommend books read along course	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
please recommend books read along course	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
please recommend books read along course	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
please recommend books read along course	ankush one recommendation think designing data-intensive applications martin kleppmann definitely one books every data engineer read another one favorite database internals one also really nice first recommendation definitely top priority alexey would like mention designing data-intensive applications really great book might look intimidating simply looking sheer volume book expect read book cover cover something buy reference go throughout data engineering career even 're data engineer – 'm data scientist find book useful keep reference	1
please recommend books read along course	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
could update current leader board results	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
could update current leader board results	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
could update current leader board results	yes could soon post message slack need find time couple tweaks script	1
could update current leader board results	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
could update current leader board results	answer – depends dataset – typically train/validation split first try scaling try without scaling see happens think something talked last week actually notebook prepared last week go classification notebook notebook scaling shows scale features use standard scaler use min max scaler – use two different scalars shows particular dataset turned little bit better experiment see scaling makes sense linear models sometimes 's actually good idea add extra complexity pipeline instead dictionary vectorizer whatnot 'll also need add next step scaling basically steps prediction pipeline becomes bit difficult maintain would say try scale see improvement get significant maybe 's worth extra complexity get use cross-validation use best judgment see makes sense	0
think 2021 data engineering roadmap good guideline follow	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
think 2021 data engineering roadmap good guideline follow	alexey 33 submissions think decision extend deadline good one hopefully end week see higher number understand project much difficult watching course following along 33 people – probably put lot effort thanks saw one person write put 100 hours already project outstanding sorry cruel 100 hours lot	0
think 2021 data engineering roadmap good guideline follow	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	0
think 2021 data engineering roadmap good guideline follow	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
think 2021 data engineering roadmap good guideline follow	victoria think 's ne someone already shared slack 've seen past really like n't know – 'm data engineer think 's complete go website actually link resources learn would n't able talk content roadmap 's content 'm super familiar think 's complete alexey think good note “ beginners ’ feel overwhelmed vast number tools frameworks listed typical data engineer would master subset tools throughout several years depending his/her company career choices. ” see roadmap like n't feel overwhelmed think learning things take couple years right victoria agrees feels like university curriculum one document computer science fundamentals take one year master victoria 's thing courses maybe one saw several courses also icons remember seen actually links alexey actually multiple roadmaps like maybe different one saw example would go little bit lean say – maybe n't need data structures immediately maybe pick important thing would basic terminal usage bit linux git well python sql maybe pick one thing go entire thing repeat victoria green check marks general things hearts personal cloud icons cloud-based alexey ’ nice roadmap many things see overwhelming example jenkins – n't know think companies still use jenkins right victoria yes n't seen think alexey use legacy legacy thing stays legacy long somebody needs support 're kinds things jenkins useful also use gitlab ci/cd ci/cd section think 's cool terraform victoria example ci/cd would say 's good know concepts 's something would focus first year 's good know 's something would stable project something like even project even knowledge pick alexey looks nice – picture ’ high quality taste 's much stuff victoria 'm going look one mentioning 'm going share find bunch different courses information difficult things like ’ https //awesomedataengineering.com/ also github repo actually one based repo – one talked analytics engineering think 's linear n't much lot things find nice defines coverage depth also whether 's free pay – focus free resources alexey tells need study also need read pick topic might useful victoria find one useful first one pretty nice 's maybe much said also probably oriented courses alexey see suggest covering workflow management one last topics sure ’ right place would put lot earlier actually chuckles yeah looks like good map oh even click get free resources victoria see books see courses want courses ’ quite nice even something best books – three designing data intensive applications data warehouse toolkit one use refresher review data modeling concepts seven databases seven weeks alexey data warehouse toolkit classic book 's pretty old recognize kimball name person created lot stuff victoria 's one fathers data warehouse along umair imam think first version 80s	1
experience data engineering planning focus writing good unit test cases	victoria would say yes go-to dbt yes dbt enforces go read viewpoint 'll explain also models kind structure mentioned modern concepts concept comes 80s kimball defined kitchen analogy find useful normally go course complex gets may need steps 's okay always try separated – presentation source need typecasting like example maybe duplication middle may models go data pipeline let ’ say flow less would look like alexey kitchen analogy victoria kitchen analogy way model data similar restaurants model food warehouse raw food source data everyone allowed fact 's actually dangerous 's security measures actually go warehouse stored stored things like kitchen people cook raw food make food 're going serve allowed would data engineers data analysts maybe analytics engineers would data warehouse trying process raw data end part restaurant people eat – dining hall presentation layer data warehouse fact tables look like data marts 're building data marts business stakeholders everyone allowed get foot let ’ say restrictions 's already presented 're presenting raw alexey think tableau self-service tool go cubes presentation layer right victoria yeah technically bi tools self-serve could everything could expose everything right external tables create tables inaudible create section model create factories could everything could even ones development exposed self-service bi tool like tableau looker whatever analogy says use presentation layer parts present raw data people would n't know use 's going like “ oh n't find phones ones maybe 's transformation ” things like alexey nice analogy	0
experience data engineering planning focus writing good unit test cases	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
experience data engineering planning focus writing good unit test cases	ankush n't think 's focus course think focus course learn data engineering basics learn develop software kafka spark might cover test cases yes extensive writing good unit test cases course	1
experience data engineering planning focus writing good unit test cases	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
experience data engineering planning focus writing good unit test cases	victoria would say ’ also 's different set products example data warehouse start data tech stack scratch would definitely use redshift cost lower end need probably would need migrate something like bigquery snowflake lot documentation 's pretty easy set 's around longer – think reason broader adoption mainly around things alexey seems aws popular well least berlin look companies use cloud maybe 70 people use aws 20 may use gcp remaining 10 use azure 's roughly see maybe 'm wrong course also got use gcp bit seems ui nicer go web console 's little bit nicer think comes tools 's bit difficult example aws pip install aws cli google cloud 's bit difficult maybe 's one reason aws popular – ’ mature maybe think gcp catching nice interface sometimes things cheaper bigquery also think big advantage maybe clouds n't really victoria plus aws around 10 years thesis using aws thing gcp yet 's lot people go 're familiar outside stability alexey think cloud services services quite similar ’ used aws many things gcp straightforward n't typical less think many concepts map one another	0
'm aware logs simplify computations involving products np also prone flt pt errors	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
'm aware logs simplify computations involving products np also prone flt pt errors	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
'm aware logs simplify computations involving products np also prone flt pt errors	honest idea question means… oh flt pt floating pointers okay yes numpy floating point errors n't know – computers let 's say multiply two numbers operations take two numbers see instead 3 3 followed lot zeros 4 image reference floating point operations precise computers 's summing multiplying little floating point arithmetic error numpy 's running computer 's also prone guess question “ keep using logs data exploration ” 're probably referring week 2 think use logarithm price main reason numerical instability – ’ floating point errors – 'll probably learn video simply distributions long tails please refer eda video probably second third explain need 's keep using logs n't apply logarithm 's difficult machine learning models actually learn data	1
'm aware logs simplify computations involving products np also prone flt pt errors	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
'm aware logs simplify computations involving products np also prone flt pt errors	’ two weeks october 18 2021 meaning first november 2022	0
many project submissions time	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	1
many project submissions time	let 's go notes course repo course let 's take say linear regression notes – says “ add notes video prs welcome ” mean – let 's say watch video took notes want share links notes fellow learners taking notes putting somewhere notion medium github whatever prefer – online page put create pull request link notes somebody maybe n't like watching videos go notes read 's idea even better instead adding links add notes directly people need go external site – everything repo ’ idea n't time make notes 's added notes time want contribute notes contribute notes help everyone	0
many project submissions time	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
many project submissions time	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
many project submissions time	n't think 's rule thumb notebook multiple things deploy… basically python file ideally one single thing one python file training example one python file testing model – something like n't think particular rule thumb sometimes makes sense larger files sometimes smaller files	0
need scale numerical features logistic regression	two people want work dataset individually please feel free imagine way control go kaggle select competition even ’ accidentally happen long 's exactly code – long n't copy – ’ fine yeah please n't copy code	0
need scale numerical features logistic regression	answer – depends dataset – typically train/validation split first try scaling try without scaling see happens think something talked last week actually notebook prepared last week go classification notebook notebook scaling shows scale features use standard scaler use min max scaler – use two different scalars shows particular dataset turned little bit better experiment see scaling makes sense linear models sometimes 's actually good idea add extra complexity pipeline instead dictionary vectorizer whatnot 'll also need add next step scaling basically steps prediction pipeline becomes bit difficult maintain would say try scale see improvement get significant maybe 's worth extra complexity get use cross-validation use best judgment see makes sense	1
need scale numerical features logistic regression	multi-classification – yes clustering – 'm sure probably good idea n't cover hard – know evaluate peers know better clustering	0
need scale numerical features logistic regression	flask app produce result actually return prediction 're good n't need fix answer question regarding many points deducted think whole project kind assumes managed deploy flask n't able put docker able deploy cloud lose like five six points something along lines think 's good idea make sure flask returns something problems please share code think know asking question talked already slack maybe error somewhere predict.py script need make sure fix error work perhaps take code lectures – think xgboost model logistic regression need put x variable t-matrix xgboost instead using predict_proba need use predict think 's – n't work please write slack let 's try figure 's working	0
need scale numerical features logistic regression	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
certain model results great think requirement model outcome data	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
certain model results great think requirement model outcome data	alexey one thing maybe n't clear enough – please always include code solution maybe people n't watch first said n't submit code homework get zero points saw submissions people put dot empty put link towards data science article example please n't please put code n't 'll get zero points entire week 'll make explicit form week three well please n't forget put code	0
certain model results great think requirement model outcome data	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
certain model results great think requirement model outcome data	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
certain model results great think requirement model outcome data	like said 's always problem- data-specific datasets get good performance	1
restartability incremental loads usually made bigquery 'm familiar redshift use stored procedure chain steps	alexey 'm curious know exactly difficulties knowing next knowing start maybe ’ take stab first go github repo github repo things structured weeks click week 1 example want learn materials week 1. week 2 're preparing right look different finish look like week 1 – concise know details need go everything need starting introduction code docker gcp terraform setting environment actually environment setup n't problems installing docker installing terraform installing google cloud ignore thing reason example trouble running postgres docker check video 's step-by-step video shows set environment using virtual machine google cloud video like bonus watch point follow order see page playlist actually google cloud intro first n't think actually matters introduction google cloud platform short video n't matter watch docker try follow format weeks 'll multiple sections week example week two three sections like data lake orchestration first one video data lakes basics orchestration bunch videos airflow answer question – go github repo follow approach suggested difficulties feel free go slack ask difficulties something particular going slack channels looking kind problems people 's likely somebody else problem solution one threads since 're talking 's amazing see helpful 's big heart thanks lot n't know personally 's overwhelming open slack channel see like 50 new messages 'm happy see already helped basically resolve issues big big thank 's helpful us 'm sure would able answer questions thanks lot sejal yeah plus one alexey said thank much helping pressing questions definitely answer definitely looking threads ’ wo n't constantly available answer possible alexey also wanted say need refresher github maybe google “ github tutorial ” “ getting started github ” 'll find lot amazing resources 'm sure able better available sources ankush using git continuously videos obviously would using commands hope might help refreshing github knowledge best way would small youtube search github 'm pretty sure really good videos alexey anyone comes across nice github tutorial helpful please share rest students	0
restartability incremental loads usually made bigquery 'm familiar redshift use stored procedure chain steps	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
restartability incremental loads usually made bigquery 'm familiar redshift use stored procedure chain steps	alexey feel sorry need deal work chuckles n't really experience sejal used working oracle like long time ago ’ assuming person wrote question still working legacy databases like oracle really n't answer either chuckles guess 'll talk generic terms ci/cd pipelines wherever 're deploying infrastructure basically wherever 're deploying schemas think would better place keep things ankush maybe using git alexey think issue code stored database easily version control somebody point thought good idea let databases run arbitrary code like triggers sejal possibly victoria would able answer case coupling ci/cd pipelines dbt pipelines n't know alexey think usually people days try extract logic put backend – move database put backend version controlled	0
restartability incremental loads usually made bigquery 'm familiar redshift use stored procedure chain steps	alexey must admit understand word means collectively – maybe ’ much data scientist answer bad ankush maybe would able answer victoria victoria would say also familiar bigquery ’ worked bigquery outside project particular would imagine 's data warehouse could use key start limit loading timestamp something like	1
restartability incremental loads usually made bigquery 'm familiar redshift use stored procedure chain steps	alexey week 2 yes – upload video solution also update leaderboard probably take time depending clean data – need lot data cleaning take lot time please n't put things n't look like emails email field fill please careful takes time clean later	0
could high level overview week 2 homework main points getting	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
could high level overview week 2 homework main points getting	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	1
could high level overview week 2 homework main points getting	victoria set warehouse end define connection difference would use adapter dbt several adapters would first want use dbt go check adapters make sure 's supported several official example redshift could use community-supported azure alexey yeah aws athena azure athena redshift presto well example presto cluster use aws – within aws presto cluster potentially similar setup connect dbt presto victoria general add little bit answer let 's say project right bigquery would change profile 's set connection may make changes 's something sql supports bigquery way around able run project minutes	0
could high level overview week 2 homework main points getting	alexey 's early talk future iterations data engineering zoomcamp think chuckles ml camp yes – videos difference homework 's still discussion n't know 'll see ankush yeah think slight discussion met sure going repeat ml de zoomcamp let 's see response participation feel definitely success definitely feel free pass around course give us feedback maybe make better decision alexey re-recording videos… 's much effort n't think 'll want let 's say ml zoomcamp example much effort 'd rather reuse videos instead making new ones hope videos good decide rerun need re-record yeah 'll see	0
could high level overview week 2 homework main points getting	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
failed deliver homework far hence missed deadline	alexey 's want look project 're behind right n't worry take time decide whether give certificate based projects homework 's – catch week 3 lighter week 1 maybe fast-forward directly docker compose run week 2 depending much time maybe go solution published today give everything need week 3 use transfer service video already watching video probably learn lot already like homework solution	1
failed deliver homework far hence missed deadline	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
failed deliver homework far hence missed deadline	alexey example spark google “ spark gcp bigquery connector ” good article shows need need jar need use jar submit spark job let 's say files stored google cloud storage something files multiple options first option save back google cloud storage create external table something like like saw week 3. another option would saving directly bigquery using say “ write ” say want write bigquery uses connector connect bigquery write n't know permissions actually work – maybe also need specify key somehow probably 're already dataproc already permissions need order write bigquery order read google cloud storage use dataproc theoretically easy – add extra jar write bigquery data data warehouse use materials dbt week analytics engineering week actually consume content data warehouse visualize	0
failed deliver homework far hence missed deadline	alexey ankush like scala right chuckles ankush think scala definitely one best languages data engineers chuckles biggest part 's typesafe 's heavy verbose java scala perfect combination least coming java background love scala would say yes 's kind important language know data engineer especially spark written scala sometimes see errors spark- java-native might get confused knowledge like scala personally use developing pipelines also use write code spark sometimes also beam actually need really know python well enough proficient python get away minimum knowledge java scala would say unfortunately would say like scala personally like definitely go ahead learn 's still good language alexey think get performance benefits python extra layer abstraction top spark adds bit overhead cases want control things right ankush personal experience whenever written batch jobs – 'm talking real-time 'm talking batch jobs n't matter much time takes – always half hour 10-15 minutes minimum adding another 30 seconds one minute python performance n't really matter real-time streaming yes would huge loss n't think 's case pyspark real-time batch n't really matter alexey think serialization better use scala use “ case classes ” whatever called easily turn rdd case class think uses efficient serialization mechanism python sometimes difference minutes remember case difference like 20 minutes took pipeline rewrote scala difference big ankush would say spark 1.0-something data frames 's also really useful anymore alexey data frames yeah 1.2. remember used python every step pipeline found one job could optimize scala whole pipeline lot faster something needed scala year using spark chuckles think important – know java scala data engineer ankush good python would use already know java ’ need scala n't know either learn scala scala would much easier learn application especially spark alexey argue n't lot time chuckles cross-talk ankush preferences biased really like scala answers neutral definitely biased towards scala alexey victoria n't want add anything victoria n't know n't used scala several years reason people offer jobs java development linkedin chuckles alexey use victoria used university prove get title would use	0
failed deliver homework far hence missed deadline	alexey add comment least workplace airflow instance set us data scientist would go instance airflow managed data engineers devops engineers would use never needed actually run docker compose moment multiple dags orchestration tool quite useful would personally maybe go luigi favorite one need think people well since people know luigi many like maybe airflow would actually better choice consider sejal would also like add question docker compose used docker compose airflow style use official setup seeing first time well 's unnecessarily complicated especially xcom variables forth generally docker compose acts wrapper docker containers production workflows docker containers used integration ci/cd pipelines wherever 're going deploy docker compose docker compose part helpful development environments case let 's say mock version airflow environment production want test dags local development environment use docker compose make things easier running simple commands like ‘ docker compose ’ would make fine instead running ‘ docker run container name etc whatever want ’ terms convenience terms usage ’ generally used development environments production alexey think first part question “ close usual data engineer role workflows ” workflows see work complex steps probably also case need start something already – would n't call complex pipeline – three four steps already something eventually might grow bigger bigger add steps ankush would also like add 're cloud maybe use airflow deploy docker maybe actually use service provided cloud provider one similar airflow think airflow would used internally like cloud composite google cloud platform would using terraform sejal yeah used aws step functions aws setup step functions combination lambda place airflow ankush said also something wanted say really depends production self-hosted cluster kubernetes wherever want use pure airflow version company could kind setup use native airflow cluster could cloud-based setup convenient option use managed services cloud composer aws also version called managed airflow	0
relevant terraform data engineering job would getting terraform associate certificate make sense junior de	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	1
relevant terraform data engineering job would getting terraform associate certificate make sense junior de	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
relevant terraform data engineering job would getting terraform associate certificate make sense junior de	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
relevant terraform data engineering job would getting terraform associate certificate make sense junior de	ankush avro parquet totally different things avro row-based parquet columnar something like batch processing data-analytical kind work parquet perfect solution use spark something 's super fast calculate something like sum account parquet file read particular column rather whole column whole row avro hand strong providing flexibility terms backward forward compatibility 're using avro kafka using something like protobuf kafka would really useful think like different technologies altogether use different use cases alexey avro guess use case would streaming parquet batch jobs ankush yeah think definitely beginning would basic use cases yes alexey also something called ocr ankush 's also columnar storage alexey oh 's different orc chuckles remember watching presentation berlin buzzwords format remember seeing cool presentation never actually tried ankush neither i. alexey 's optimized row columnar interesting never knew stood widely used people go parquet ankush seen parquet guess companies definitely using 's bad technology think 's parquet famous compatible solutions parquet people tend use often	0
relevant terraform data engineering job would getting terraform associate certificate make sense junior de	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
regarding regularization – would better analyze data model remove redundant features	alexey one thing maybe n't clear enough – please always include code solution maybe people n't watch first said n't submit code homework get zero points saw submissions people put dot empty put link towards data science article example please n't please put code n't 'll get zero points entire week 'll make explicit form week three well please n't forget put code	0
regarding regularization – would better analyze data model remove redundant features	far week two week three follow simple approach simple approach described take entire dataset split three parts train validation test 'll use week two week three week four talk cross validation – explain right want say approach validation sufficient many cases think 's pretty safe say many many many applications split enough cross validation course nicer learn week four right go detail try explain wait video “ select approach cross validation ” would say multiple options k-fold… 'm sure many people understand n't covered yet basically dataset big n't need k-fold dataset smaller use k-fold “ leave p ” – 've never used personally	0
regarding regularization – would better analyze data model remove redundant features	alexey think dmitry dmitry thinking think 's referring one questions usually real task need everything together combined process n't go step step basically one purposes eda step give bridge feature selection process – thus helping understand feature use use model decide need decide whether 's okay okay go regularization basically step step alexey also n't hurt try regularization well even though seems like redundancy features – sudden regularization helps need test dmitry ’ help alexey yes n't know way find except try	1
regarding regularization – would better analyze data model remove redundant features	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
regarding regularization – would better analyze data model remove redundant features	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
opinion topic requires effort understand properly someone brand new data engineering	ankush really depends upon interest kind knowledge already example lot sql knowledge understanding dbt would super easy maybe spark kafka streaming would something hard 're coming software background 've software development especially backend engineering time working distributed databases sort streaming switching kafka spark would relatively easier really depends upon background would say generally topics relatively intermediate 'm pretty sure videos able explain way understand alexey maybe would add internals databases pretty complex topic cover detail books suggested – easy interesting example martin kleppmann 's book need reread chapters worth n't easy worth ankush definitely think finished whole book couple years basically picked topics interesting went back parts later	1
opinion topic requires effort understand properly someone brand new data engineering	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
opinion topic requires effort understand properly someone brand new data engineering	victoria would say ’ also 's different set products example data warehouse start data tech stack scratch would definitely use redshift cost lower end need probably would need migrate something like bigquery snowflake lot documentation 's pretty easy set 's around longer – think reason broader adoption mainly around things alexey seems aws popular well least berlin look companies use cloud maybe 70 people use aws 20 may use gcp remaining 10 use azure 's roughly see maybe 'm wrong course also got use gcp bit seems ui nicer go web console 's little bit nicer think comes tools 's bit difficult example aws pip install aws cli google cloud 's bit difficult maybe 's one reason aws popular – ’ mature maybe think gcp catching nice interface sometimes things cheaper bigquery also think big advantage maybe clouds n't really victoria plus aws around 10 years thesis using aws thing gcp yet 's lot people go 're familiar outside stability alexey think cloud services services quite similar ’ used aws many things gcp straightforward n't typical less think many concepts map one another	0
opinion topic requires effort understand properly someone brand new data engineering	alexey example spark google “ spark gcp bigquery connector ” good article shows need need jar need use jar submit spark job let 's say files stored google cloud storage something files multiple options first option save back google cloud storage create external table something like like saw week 3. another option would saving directly bigquery using say “ write ” say want write bigquery uses connector connect bigquery write n't know permissions actually work – maybe also need specify key somehow probably 're already dataproc already permissions need order write bigquery order read google cloud storage use dataproc theoretically easy – add extra jar write bigquery data data warehouse use materials dbt week analytics engineering week actually consume content data warehouse visualize	0
opinion topic requires effort understand properly someone brand new data engineering	alexey 's early talk future iterations data engineering zoomcamp think chuckles ml camp yes – videos difference homework 's still discussion n't know 'll see ankush yeah think slight discussion met sure going repeat ml de zoomcamp let 's see response participation feel definitely success definitely feel free pass around course give us feedback maybe make better decision alexey re-recording videos… 's much effort n't think 'll want let 's say ml zoomcamp example much effort 'd rather reuse videos instead making new ones hope videos good decide rerun need re-record yeah 'll see	0
trying catch almost finished first week	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
trying catch almost finished first week	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
trying catch almost finished first week	alexey well good job keep working almost finished first week finish start week two finish week three chuckles follow sequence videos playlist github sufficient think questions go slack ankush keep going alexey chuckles yes exactly ’ almost finished first week good sign first week year pretty tough many people lot problems docker whatnot 're almost finished good sign ’ good track keep working	1
trying catch almost finished first week	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
trying catch almost finished first week	ankush yes definitely explore aws azure comfortable using gcp course using gcp tools like bigquery might available platforms might similarities alternatives might touch course course based upon google cloud platform one biggest reasons create new account get 300 credit 's really useful everybody 's familiar cloud create free account course ideally spend money platform services alexey think another reason decided go gcp connection dbt bigquery athena think 's bit tricky want go snowflake dbt 's super expensive something want pay course 's thinking whether go aws gcp – gcp gives free credits b dbt works bigquery ’ led decision gcp sejal mentioned able run everything locally except bigquery part course bigquery lives cloud rest stuff runnable locally	0
planning touch mlops topics	alexey n't think actually time summarize everything course got inspired sejal – prepares notes 's recording video based notes share notes also take notes share notes fellow students way learn public – sort regurgitate content produce something new – useful useful others promise summaries encourage take notes share others jupyter books think something publish particular week jupyter notebook code snippets course share ankush definitely share code snippets code use particular week definitely shared github	0
planning touch mlops topics	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
planning touch mlops topics	alexey week 2 yes – upload video solution also update leaderboard probably take time depending clean data – need lot data cleaning take lot time please n't put things n't look like emails email field fill please careful takes time clean later	0
planning touch mlops topics	sejal initially planning decided move think better separate course alexey sharing details anything specifically planned maybe alexey something add alexey well course finishes 'll another one called might guess mlops zoomcamp maybe wondering “ zoomcamp ” ’ actually using zoom right show four us rest watch youtube 's funny name think zooming topics also works ankush think started “ boot camp ” “ zoom camp ” meaning ’ going online alexey zoom limit 100 people per call cheap plans 's machine learning zoomcamp decided go youtube kind stayed youtube 600 people would n't able fit one zoom call name think kind got stuck 's funny ankush interested mlops take part datatalks slack group couple months release information – details maybe structure	1
planning touch mlops topics	alexey add comment least workplace airflow instance set us data scientist would go instance airflow managed data engineers devops engineers would use never needed actually run docker compose moment multiple dags orchestration tool quite useful would personally maybe go luigi favorite one need think people well since people know luigi many like maybe airflow would actually better choice consider sejal would also like add question docker compose used docker compose airflow style use official setup seeing first time well 's unnecessarily complicated especially xcom variables forth generally docker compose acts wrapper docker containers production workflows docker containers used integration ci/cd pipelines wherever 're going deploy docker compose docker compose part helpful development environments case let 's say mock version airflow environment production want test dags local development environment use docker compose make things easier running simple commands like ‘ docker compose ’ would make fine instead running ‘ docker run container name etc whatever want ’ terms convenience terms usage ’ generally used development environments production alexey think first part question “ close usual data engineer role workflows ” workflows see work complex steps probably also case need start something already – would n't call complex pipeline – three four steps already something eventually might grow bigger bigger add steps ankush would also like add 're cloud maybe use airflow deploy docker maybe actually use service provided cloud provider one similar airflow think airflow would used internally like cloud composite google cloud platform would using terraform sejal yeah used aws step functions aws setup step functions combination lambda place airflow ankush said also something wanted say really depends production self-hosted cluster kubernetes wherever want use pure airflow version company could kind setup use native airflow cluster could cloud-based setup convenient option use managed services cloud composer aws also version called managed airflow	0
facing problems data cleaning techniques good data cleaning large amounts data	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
facing problems data cleaning techniques good data cleaning large amounts data	yes submit anywhere want say 20 pt n't remember yeah work good article “ good ” meaning 's copy/pasting code nothing else actually need explain 's going also article – closer time working article guidelines well	0
facing problems data cleaning techniques good data cleaning large amounts data	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
facing problems data cleaning techniques good data cleaning large amounts data	dmitry think really depends use case distribution data also depends would like receive end – purpose plots different types plots different purposes alexey value always start dmitry think start 10 usually go direction ’ needed alexey usually start 50. n't know habit guess dmitry ’ usually 10 15	0
facing problems data cleaning techniques good data cleaning large amounts data	lisa kinds things missing data text data sometimes need cleaning like street written “ st ” “ street ” might want combine 's missing data might want take average median ’ outliers depending makes sense might want throw say elon musk 's salary dataset might good model go chuckles alexey think salary zero right wrote recently tweet n't salary shares lisa agrees laughs would outlier different way lisa direction actually laughs things like would want look might use inner core tiles cut tops bottoms lot different techniques use alexey data cleaning broad also abstract unfortunately 's manual magic button like datarobot would nice give bunch dirty data magic wand button click data becomes clean would nice datarobot maybe two buttons – “ make data nice `` “ make data clean. ” “ okay find best model data. ” maybe research automated data cleaning well must something lisa companies companies buying consolidating trend seems end-to-end also deploying models machine learning ops make sure n't data drift things like 's always robust might want particular situation try generalize build tools	1
would mind clarifying evaluation test validation sets give example tried different regularizers values pretty close	building locally fine	0
would mind clarifying evaluation test validation sets give example tried different regularizers values pretty close	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
would mind clarifying evaluation test validation sets give example tried different regularizers values pretty close	alexey guess question need evaluation test validation sets dmitry think 's obvious question yeah 'll try answer maybe 'm answering completely wrong question sorry 's difficult understand validation talked example n't know whether need regularization r-value kind regularization parameter need kind features useful kind features useful whether need fill missing values zero whether need fill mean whether need fill something else etc questions need answer answer using validation dataset three-way split train validation test test hypothesis validation dataset n't want throw away – still want use training final model 's combine train set validation dataset one like saw last question today 's homework use data available train final model final evaluation test dataset n't know actually answered question asked question invented	1
would mind clarifying evaluation test validation sets give example tried different regularizers values pretty close	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
would mind clarifying evaluation test validation sets give example tried different regularizers values pretty close	n't think takes lot time first need record videos prepare homework imagine adding collaboration startups well maybe sometime future n't know somebody startup listening right want hire people graduate zoomcamp please reach let 's arrange something	0
project got 0.9963 roc_auc_score train 0.9303 validation overfitting bit much	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
project got 0.9963 roc_auc_score train 0.9303 validation overfitting bit much	would n't look train honest sometimes look think important thing validation result 93 validation seems high way check 're overfitting use test dataset tuning model selecting best one use test dataset see number close 're overfitting see 70 probably accidentally overfit reason n't know maybe make sense choose different split try figure problem	1
project got 0.9963 roc_auc_score train 0.9303 validation overfitting bit much	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
project got 0.9963 roc_auc_score train 0.9303 validation overfitting bit much	flask app produce result actually return prediction 're good n't need fix answer question regarding many points deducted think whole project kind assumes managed deploy flask n't able put docker able deploy cloud lose like five six points something along lines think 's good idea make sure flask returns something problems please share code think know asking question talked already slack maybe error somewhere predict.py script need make sure fix error work perhaps take code lectures – think xgboost model logistic regression need put x variable t-matrix xgboost instead using predict_proba need use predict think 's – n't work please write slack let 's try figure 's working	0
project got 0.9963 roc_auc_score train 0.9303 validation overfitting bit much	took course ago back useful think something use python immediately could helpful 'm sure would good recommendation current course course good would n't compare stanford course andrew ng simply different course 'll get lot theory one practical hands-on – 's mostly writing code together maybe take	0
use infrastructure code tools set data warehouse ui	alexey week 2 yes – upload video solution also update leaderboard probably take time depending clean data – need lot data cleaning take lot time please n't put things n't look like emails email field fill please careful takes time clean later	0
use infrastructure code tools set data warehouse ui	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
use infrastructure code tools set data warehouse ui	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
use infrastructure code tools set data warehouse ui	alexey let go see criteria one criteria cloud get zero points use cloud get two points use cloud use ui setting things get four points use cloud use infrastructures code tools yes use ui n't use terraform anything similar use get two points	1
use infrastructure code tools set data warehouse ui	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
project suggestions spark	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
project suggestions spark	alexey think 's helpful 's necessary helpful happens dags run airflow dags tasks tasks often kubernetes jobs something like infra – many companies infrastructure managed kubernetes 's quite simple get new container execute new job kubernetes n't think 's necessary think 's helpful 'm data engineer – 'm data scientist data scientist knowledge kubernetes quite helpful could debug jobs 'm running airflow run kubernetes could see logs would say want learn hurt get hired without kubernetes knowledge materials course sufficient get hired pick kubernetes work example agree victoria victoria yeah think kubernetes getting popular ca n't hurt	0
project suggestions spark	alexey depends exactly want reddit data right ankush yeah exactly would focus final goal let 's say “ want calculate reddit data. ” might want say “ keywords used ” something like case transformations directly depend alexey something like “ active number posts per day. ” ankush exactly “ per hour distribution posts. ” 'm pretty sure us goes daytime goes night trying answer 's first question would focus know answer know transformations apply alexey yeah 's end goal project dashboard “ exactly want put dashboard ” go “ dashboard data use kind transformations need order go data dashboard ” become clear need group-by need join table	0
project suggestions spark	ankush would pick big dataset project spark see scale work 100 nodes one go ’ likely spend credits still scale massively spark choose project suggestion directly project would suggest go least minimum 200-300 gb maybe even one terabyte data see power spark shining actually comparison maybe normal script sequentially actually see power something distributed handle kinds features alexey 's good thing try scale 'm sure actually scale project environment would work would pretty difficult think maybe also start small – “ small ” n't mean titanic dataset iris dataset need something preferably larger one kilobyte – least couple gigs – play taxi dataset think 's still quite small spark need something bigger actually feel – see need tune garbage collector things suggestion close need deal work look ad click datasets advertisements usually lot data example kaggle couple click-through rate prediction competitions pretty large datasets n't know one actually large yeah 's large larger ones also look datasets criteo – example 1 tb click logs similar ones look ad click datasets find lot relatively big datasets also difficult understand information click device click comes roughly understand 's happening another dataset probably could challenging 's called common crawl basically copy internet crawl internet save everything s3 get started even take couple parts dataset let 's say january enough play data 's difficult contains natural text contains lot pornography copy internet need careful exactly process 's internet – copy also good dataset play around necessarily spark general want try processing large amounts data also good list suggestions slack check 'll probably find good datasets another maybe complex example – graph datasets snap stanford find good big graphs could also quite interesting process especially spark spark graphx thing think works scala though n't know works python want experiment library n't found anything better processing graphs library also could nice project sejal think also spark ml right alexey data scientist would recommend using chuckles maybe something quick think reason exists marketing say “ hey this. ” n't think people put enough effort ankush 's powerful enough alexey models spark ml n't really work well usually data science use spark preparing dataset get beefy machine load data use simple python script training model still use spark materialize data prepared spark save s3 script would load data train model prepared dataset ankush hopefully time data already smaller size use important information alexey yeah 's already prepared 's parquet example 's fast download load sejal also use spark training alexey training use preparing data use python script spark applying model usually use spark map partition example still need finish last video still process two videos one rdds map filter reduce one map partition map partition quite useful operation use quite often applying machine learning models give time finish videos able see sejal nice think helpful well chuckles alexey give raw unedited version 's bad get interrupted forget say needs editing	1
project suggestions spark	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
could create two slido sessions – one soft skills/resources/career questions one course content	flask – let draw let 's say want create web service image 5 web service gets requests web service responds something could predictions could something else – n't related machine learning implement web service use flask use fastapi use framework creating web services use flask use something else – flask allows use post example let 's say “ get ” usually parse parameters query let 's say go google put “ test ” see q=test https //www.google.com/search q=test oq=test aqs=chrome.0.69i59l2j69i65l3j69i60j69i61l2.1264j0j1 sourceid=chrome ie=utf-8 image 6 parameter pass “ get ” request “ get ” request sometimes let 's say want score customer n't want put gender=female contract=one year string send json post post also add body request generally use post general use case flask able create web service	0
could create two slido sessions – one soft skills/resources/career questions one course content	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
could create two slido sessions – one soft skills/resources/career questions one course content	think around 200 204 think something like	0
could create two slido sessions – one soft skills/resources/career questions one course content	mean another iteration course yes go course repository see answer first take course self-paced mode videos available except last videos kserve start watching 's important first attempt homework without looking answer check solutions least one project 's also important n't follow videos sure learn something 's better solidify learning also practicing least one project 're taking course self-paced mode want get feedback project also share link slack happy give feedback project answer actual question regarding next cohort – start september cohort started september probably start bit earlier started middle september time probably start maybe one week earlier want notified informed link form click put email send email course starts	0
could create two slido sessions – one soft skills/resources/career questions one course content	soft skills resources career questions would recommend actually go datatalks.club slack channel called career questions go ask questions use slido link course way separate two sessions talk course career slack talk careers	1
suggestions reinforcing learned able get everything working want things stick	victoria 'm sure answer mean yes definitely adding part project least workshop reason want people learn also consider people entirely new concept thought would much overhead try cover things already quite complex day-to-day even though work daily dbt work slow-changing dimensions daily basis example 's also think 's something look moment use definitely dbt especially deletions implementing load step done hooks think 's section advanced knowledge link hooks like incremental models mentioned changing dimensions n't link could link something could use snapshots think added concept updates guess could also pre-hook post-hook also something would incremental model incremental model loads new chunk data use merge depending workers use use could insert update 's something would work unique key project also final project 's understand complex go 's limit hope answers	0
suggestions reinforcing learned able get everything working want things stick	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
suggestions reinforcing learned able get everything working want things stick	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
suggestions reinforcing learned able get everything working want things stick	sejal project end last three weeks n't exactly finalized idea project covering modules covered previous six weeks build project around alexey still need think exactly organize project 's still finished already try start looking dataset want process think exactly want dataset already start exploring dataset basically try course dataset another thing reinforce learned take notes – ’ take notes notebook actually share notes – put github example notion 's time consuming know personally notes took like five years ago still remember things contrast things ’ take notes n't remember personally taking notes processing regurgitating putting helpful information stay brain would one recommendation 's time consuming ankush also another recommendation already data engineering backend development job see opportunity work docker terraform take learned first week try real life production instances working right maybe ’ student try build docker image simple hello world overcomplicate think stick forever build something like docker file scratch google around trying figure best approach deploy successful really stick anything else goes terraform google cloud service account terraform work know run create another storage bucket create another transfer service google around would easiest resource create google cloud services think things hands-on really stick alexey yeah 's probably best advice get hands dirty	1
suggestions reinforcing learned able get everything working want things stick	alexey add comment least workplace airflow instance set us data scientist would go instance airflow managed data engineers devops engineers would use never needed actually run docker compose moment multiple dags orchestration tool quite useful would personally maybe go luigi favorite one need think people well since people know luigi many like maybe airflow would actually better choice consider sejal would also like add question docker compose used docker compose airflow style use official setup seeing first time well 's unnecessarily complicated especially xcom variables forth generally docker compose acts wrapper docker containers production workflows docker containers used integration ci/cd pipelines wherever 're going deploy docker compose docker compose part helpful development environments case let 's say mock version airflow environment production want test dags local development environment use docker compose make things easier running simple commands like ‘ docker compose ’ would make fine instead running ‘ docker run container name etc whatever want ’ terms convenience terms usage ’ generally used development environments production alexey think first part question “ close usual data engineer role workflows ” workflows see work complex steps probably also case need start something already – would n't call complex pipeline – three four steps already something eventually might grow bigger bigger add steps ankush would also like add 're cloud maybe use airflow deploy docker maybe actually use service provided cloud provider one similar airflow think airflow would used internally like cloud composite google cloud platform would using terraform sejal yeah used aws step functions aws setup step functions combination lambda place airflow ankush said also something wanted say really depends production self-hosted cluster kubernetes wherever want use pure airflow version company could kind setup use native airflow cluster could cloud-based setup convenient option use managed services cloud composer aws also version called managed airflow	0
've heard airbyte fit pipeline	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	0
've heard airbyte fit pipeline	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
've heard airbyte fit pipeline	alexey used airbyte ankush ankush alexey airbyte something similar fivetran open source also like pipeline tool yeah open source data integration modern data teams 's similar fivetran 's open source open source connectors n't used want learn – want try 's unlikely use work already existing data infrastructure looks pretty interesting ankush think 's like data engineering template right choose technologies fit alexey want try see works ankush powerful alexey maybe want explore project could good idea hand n't spend much time want give try remember 're time-bounded see figure make work move use tool course make sure finish project	1
've heard airbyte fit pipeline	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
've heard airbyte fit pipeline	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
deadline midterm project	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
deadline midterm project	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
deadline midterm project	mean another iteration course yes go course repository see answer first take course self-paced mode videos available except last videos kserve start watching 's important first attempt homework without looking answer check solutions least one project 's also important n't follow videos sure learn something 's better solidify learning also practicing least one project 're taking course self-paced mode want get feedback project also share link slack happy give feedback project answer actual question regarding next cohort – start september cohort started september probably start bit earlier started middle september time probably start maybe one week earlier want notified informed link form click put email send email course starts	0
deadline midterm project	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
deadline midterm project	’ two weeks october 18 2021 meaning first november 2022	1
preference os operating system use course	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
preference os operating system use course	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
preference os operating system use course	alexey week 2 yes – upload video solution also update leaderboard probably take time depending clean data – need lot data cleaning take lot time please n't put things n't look like emails email field fill please careful takes time clean later	0
preference os operating system use course	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
preference os operating system use course	ankush operating systems among four us guess 'm using mac guess alexey 're using linux alexey windows linux ankush see different operating systems 's pretty open operating systems generally think supported ones would mac windows linux victoria lot things cloud anyways big part would n't even dependent os 's also nice know sejal instructions available alternative os 're using example part gcp setup may work standardized fashion 're using windows alexey prepared special instructions well updated materials specific kind os 'd using possible restrict top three chuckles alexey yeah decided actually course windows prepare materials windows previous course students windows trouble trying figure things n't work 's one decided windows way already know answers questions something n't work hopefully	1
week 3 homework need “ yellow taxi ” data 2019 2020 “ hire vehicles ” 2020	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
week 3 homework need “ yellow taxi ” data 2019 2020 “ hire vehicles ” 2020	alexey homework need “ hire vehicles data ” 2020 lectures follow along material ankush uses 2019 2020 “ yellow taxi ” data able follow along videos execute queries need “ yellow taxi ” data two years homework ’ 2020. wanted mention “ green taxi ” data victoria victoria yeah think 'll need 2019 2020 long green tax data project work alexey 're talking week 4 – analytics engineering – also use green data homework solution show upload “ green taxi ” data well quite small change already know upload “ yellow taxi ” data need change lines code basically find/replace “ yellow ” “ green ” 's pretty much urls pattern week 4. victoria summarize week four “ yellow taxi ” data 2019 2020 use videos follow ankush alexey “ hire vehicles ” two options – one high volume low volume need “ hire vehicles ” “ high volume ” victoria “ hire vehicles ” 2019. use get green taxes data 2019 2020 able week 4	1
week 3 homework need “ yellow taxi ” data 2019 2020 “ hire vehicles ” 2020	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
week 3 homework need “ yellow taxi ” data 2019 2020 “ hire vehicles ” 2020	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
week 3 homework need “ yellow taxi ” data 2019 2020 “ hire vehicles ” 2020	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	0
resources .collect method applied query pyspark could please explain simple words purpose method	alexey week 2 yes – upload video solution also update leaderboard probably take time depending clean data – need lot data cleaning take lot time please n't put things n't look like emails email field fill please careful takes time clean later	0
resources .collect method applied query pyspark could please explain simple words purpose method	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
resources .collect method applied query pyspark could please explain simple words purpose method	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
resources .collect method applied query pyspark could please explain simple words purpose method	alexey purpose method let 's say this… well 'm going say rdd n't released videos rdd 'll try explain briefly rdd distributed dataset let 's say usual python list usual python – list distributed across many machines data frames data frames also sort distributed collection consists many partitions operations data frame rdd invoke .collect get everything data frame rdd contains memory object kind move think distributed collection turn collection python driver node maybe long explanation maybe shorter one ankush ankush want count elements need put one node count 's use .collect remember use .collect large datasets also run memory careful using aware 're 's really helpful want collect write one file helpful careful using alexey transformation talking actions transformations operations spark transformations actions example actions talked “ show ” talked “ take ” talked “ head ” also talked “ write ” “ collect ” also action triggers entire execution instead saving somewhere get object driver node	1
resources .collect method applied query pyspark could please explain simple words purpose method	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	0
dbt organized transformation stage- processing- presentations architecture typically used layers	alexey 's early talk future iterations data engineering zoomcamp think chuckles ml camp yes – videos difference homework 's still discussion n't know 'll see ankush yeah think slight discussion met sure going repeat ml de zoomcamp let 's see response participation feel definitely success definitely feel free pass around course give us feedback maybe make better decision alexey re-recording videos… 's much effort n't think 'll want let 's say ml zoomcamp example much effort 'd rather reuse videos instead making new ones hope videos good decide rerun need re-record yeah 'll see	0
dbt organized transformation stage- processing- presentations architecture typically used layers	alexey example spark google “ spark gcp bigquery connector ” good article shows need need jar need use jar submit spark job let 's say files stored google cloud storage something files multiple options first option save back google cloud storage create external table something like like saw week 3. another option would saving directly bigquery using say “ write ” say want write bigquery uses connector connect bigquery write n't know permissions actually work – maybe also need specify key somehow probably 're already dataproc already permissions need order write bigquery order read google cloud storage use dataproc theoretically easy – add extra jar write bigquery data data warehouse use materials dbt week analytics engineering week actually consume content data warehouse visualize	0
dbt organized transformation stage- processing- presentations architecture typically used layers	victoria would say yes go-to dbt yes dbt enforces go read viewpoint 'll explain also models kind structure mentioned modern concepts concept comes 80s kimball defined kitchen analogy find useful normally go course complex gets may need steps 's okay always try separated – presentation source need typecasting like example maybe duplication middle may models go data pipeline let ’ say flow less would look like alexey kitchen analogy victoria kitchen analogy way model data similar restaurants model food warehouse raw food source data everyone allowed fact 's actually dangerous 's security measures actually go warehouse stored stored things like kitchen people cook raw food make food 're going serve allowed would data engineers data analysts maybe analytics engineers would data warehouse trying process raw data end part restaurant people eat – dining hall presentation layer data warehouse fact tables look like data marts 're building data marts business stakeholders everyone allowed get foot let ’ say restrictions 's already presented 're presenting raw alexey think tableau self-service tool go cubes presentation layer right victoria yeah technically bi tools self-serve could everything could expose everything right external tables create tables inaudible create section model create factories could everything could even ones development exposed self-service bi tool like tableau looker whatever analogy says use presentation layer parts present raw data people would n't know use 's going like “ oh n't find phones ones maybe 's transformation ” things like alexey nice analogy	1
dbt organized transformation stage- processing- presentations architecture typically used layers	alexey feel sorry need deal work chuckles n't really experience sejal used working oracle like long time ago ’ assuming person wrote question still working legacy databases like oracle really n't answer either chuckles guess 'll talk generic terms ci/cd pipelines wherever 're deploying infrastructure basically wherever 're deploying schemas think would better place keep things ankush maybe using git alexey think issue code stored database easily version control somebody point thought good idea let databases run arbitrary code like triggers sejal possibly victoria would able answer case coupling ci/cd pipelines dbt pipelines n't know alexey think usually people days try extract logic put backend – move database put backend version controlled	0
dbt organized transformation stage- processing- presentations architecture typically used layers	alexey week 3 yellow taxi data use 2018-2020. frequently/high use think 2019. zones 's one file victoria week 4 use one month right victoria whatever load basically week 3. would understand one 2019-2020 right since 're going building model would independent – 'll use whatever table n't get homework something like least one month enough run models alexey 'll try update description 's less confusing yellow taxis ’ 2019-2020 for-hire vehicles 2019. n't use green taxis week 3	0
scoring system available midterm project	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
scoring system available midterm project	yes said provide matrix multiple dimensions like parameter tuning data prep etc dimensions criteria saying example “ 0 parameter tuning whatsoever ” “ 1 model tuned ” “ 2 person tuned multiple models selected best one. ” parameter tuning tried multiple models selected best parameter get score 2. whoever reviewing three options form say “ parameter tuning ” option one option two option three relevant points attached click correct option get score	1
scoring system available midterm project	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
scoring system available midterm project	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
scoring system available midterm project	think around 200 204 think something like	0
issues airflow “ docker-compose ” command removing triggerer appears worked could explain could	alexey well good job keep working almost finished first week finish start week two finish week three chuckles follow sequence videos playlist github sufficient think questions go slack ankush keep going alexey chuckles yes exactly ’ almost finished first week good sign first week year pretty tough many people lot problems docker whatnot 're almost finished good sign ’ good track keep working	0
issues airflow “ docker-compose ” command removing triggerer appears worked could explain could	sejal official airflow setup like mentioned documentation quite overwhelming decided provide default level services required multi-node setup workshop using single machine multi-node setup compatible scalable clusters example suitable single-machine configuration especially working something like 8gb ram also made recent announcements created frills version removed unnecessary services redis queue enables multi-node setup also triggerer workers etc main things required actually scheduler executor threaded executor multi-node executor instead single-node executor local executor along web server want use gui-based platform airflow ram cpu usage conserved worked alexey comes smaller version airflow problems like maybe 's worth trying give try run cloud virtual machine n't problems see questions airflow memory-intensive resource best way solved personally using cloud virtual machine windows computer chrome open time 's ideal choose either chrome docker compose airflow chrome quite useful 's opted virtual machine maybe 'll work well ankush looks like 's time buy new machine alexey yeah think bought computer something like couple months ago 's already like blame chrome n't know matter good computer moment open couple tabs chrome says “ okay ram let take it. ” much left rest processes	1
issues airflow “ docker-compose ” command removing triggerer appears worked could explain could	alexey add comment least workplace airflow instance set us data scientist would go instance airflow managed data engineers devops engineers would use never needed actually run docker compose moment multiple dags orchestration tool quite useful would personally maybe go luigi favorite one need think people well since people know luigi many like maybe airflow would actually better choice consider sejal would also like add question docker compose used docker compose airflow style use official setup seeing first time well 's unnecessarily complicated especially xcom variables forth generally docker compose acts wrapper docker containers production workflows docker containers used integration ci/cd pipelines wherever 're going deploy docker compose docker compose part helpful development environments case let 's say mock version airflow environment production want test dags local development environment use docker compose make things easier running simple commands like ‘ docker compose ’ would make fine instead running ‘ docker run container name etc whatever want ’ terms convenience terms usage ’ generally used development environments production alexey think first part question “ close usual data engineer role workflows ” workflows see work complex steps probably also case need start something already – would n't call complex pipeline – three four steps already something eventually might grow bigger bigger add steps ankush would also like add 're cloud maybe use airflow deploy docker maybe actually use service provided cloud provider one similar airflow think airflow would used internally like cloud composite google cloud platform would using terraform sejal yeah used aws step functions aws setup step functions combination lambda place airflow ankush said also something wanted say really depends production self-hosted cluster kubernetes wherever want use pure airflow version company could kind setup use native airflow cluster could cloud-based setup convenient option use managed services cloud composer aws also version called managed airflow	0
issues airflow “ docker-compose ” command removing triggerer appears worked could explain could	ankush avro parquet totally different things avro row-based parquet columnar something like batch processing data-analytical kind work parquet perfect solution use spark something 's super fast calculate something like sum account parquet file read particular column rather whole column whole row avro hand strong providing flexibility terms backward forward compatibility 're using avro kafka using something like protobuf kafka would really useful think like different technologies altogether use different use cases alexey avro guess use case would streaming parquet batch jobs ankush yeah think definitely beginning would basic use cases yes alexey also something called ocr ankush 's also columnar storage alexey oh 's different orc chuckles remember watching presentation berlin buzzwords format remember seeing cool presentation never actually tried ankush neither i. alexey 's optimized row columnar interesting never knew stood widely used people go parquet ankush seen parquet guess companies definitely using 's bad technology think 's parquet famous compatible solutions parquet people tend use often	0
issues airflow “ docker-compose ” command removing triggerer appears worked could explain could	victoria would say ’ also 's different set products example data warehouse start data tech stack scratch would definitely use redshift cost lower end need probably would need migrate something like bigquery snowflake lot documentation 's pretty easy set 's around longer – think reason broader adoption mainly around things alexey seems aws popular well least berlin look companies use cloud maybe 70 people use aws 20 may use gcp remaining 10 use azure 's roughly see maybe 'm wrong course also got use gcp bit seems ui nicer go web console 's little bit nicer think comes tools 's bit difficult example aws pip install aws cli google cloud 's bit difficult maybe 's one reason aws popular – ’ mature maybe think gcp catching nice interface sometimes things cheaper bigquery also think big advantage maybe clouds n't really victoria plus aws around 10 years thesis using aws thing gcp yet 's lot people go 're familiar outside stability alexey think cloud services services quite similar ’ used aws many things gcp straightforward n't typical less think many concepts map one another	0
scenario use database like postgres instead something like google cloud storage data lake	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
scenario use database like postgres instead something like google cloud storage data lake	victoria course specifically think cover postgres mainly 's easier many people could n't set google cloud account 's nice alternative end still database query data load data still get concept get practice real life 've seen smaller companies since 's also easy set 's easy way start locally 'd probably instance something like ’ little bit setup would say least 've seen would scenario alexey would add postgres originally created still intended used transactional database analytical database means – let 's say ecommerce shop fill basket orders kinds queries like block something like – transactions transactions consistency would use different databases analytical workloads want analyze lot data databases actually based postgres example redshift greenplum quite based postgres n't usually use usually ’ use plain postgres kinds things local setup right easiest way work reasonably well quite lot databases – quite large databases n't think actually hit performance bottleneck postgres course need lot data notice	1
scenario use database like postgres instead something like google cloud storage data lake	alexey ideas sejal sejal yeah think updated ... maybe move section main readme page know part additional things could setup writing unit tests ci/cd framework forth think 's “ project ” section maybe move parent branch things cover bit would say make complete packaged environment including test cases automatic deployment pipelines alexey yeah think 's good idea move project like “ next steps ” right sejal exactly something based question see many things automated might cases run scheduled pipelines airflow whichever tool 're using order generate frequent reports maybe weekly reports results need daily basis let 's say forecasting pipeline 're estimating stock prices week maybe scheduled cron job based 's something dashboard reflect predictions real-time stock market prices also weekly basis daily basis also app create idea alexey 'm thinking things n't cover example n't cover serverless quite interesting concept also stream processing let 's say stream lambda function whatever n't remember called google function use consuming stream something putting another stream build quite complex pipelines streams without writing consumer code write lambda function applied stream something internally handles everything need handle consume quite cool concept – serverless another thing n't cover could useful data engineers kubernetes similar thing example using aws batch amazon using kubernetes jobs something like also could quite useful sejal yeah +1 kubernetes think companies using preferring use kubernetes choice cloud cluster instead aws settled hosted solutions kubernetes definitely plus definitely recommend learning alexey another thing would like recommend learning something n't cover serverless thing mentioned – kind covered little bit streaming week kubernetes kind covered little bit batch week least mentioned use kubernetes job batch jobs serverless streaming n't cover concept data monitoring data quality checks things like nice tools example tomorrow datatalks.club webinar workshop data monitoring whylogs whylogs tool data monitoring nice tools example maybe 've heard great expectations soda sql – quite something n't cover well maybe little bit dbt week covered testing bit think deserves attention something many companies looking definitely worth checking victoria maybe also ideas victoria definitely data quality agree one dbt lot bunch packages well course implementation observability 'm sure mentioned guess well alexey mentions monitoring observability also good tools alexey something also n't cover specifically mentioned times tools like fivetran think 're gaining popularity airbyte open source alternative fivetran also worth checking see many many many things n't make course look quite useful might think “ okay need learn five six topics. ” maybe n't try learn much possible goal let 's say work data engineer already start applying data engineering positions see ask – kind things need n't know yet thousands things potentially learn 's difficult select next one maybe base decision whatever hear potential employers	0
scenario use database like postgres instead something like google cloud storage data lake	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
scenario use database like postgres instead something like google cloud storage data lake	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	0
handle imbalance datasets	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
handle imbalance datasets	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
handle imbalance datasets	multiple ways first way train model need careful use accuracy evaluation metric use precision recall f1 score auc 's second option is… actually good talk youtube channel – machine learning design patterns sara talks one patterns handling imbalanced datasets think rebalancing data pattern take look sara explain exactly deal unbalanced datasets one thing note downsampling upsampling – training dataset validation dataset training dataset leave want evaluate model let draw think 's better let 's say dataset split usual 60/20/20 set training data aside training dataset validation dataset sorts tricks training datasets – oversampling undersampling check sara ’ talk details techniques like smote generating datasets training dataset validation dataset leave alone leave untouched touch validation dataset sorts things evaluate model validation dataset 's important use precision recall f1 score – n't use accuracy also experiment different things like “ okay oversample minority class undersample majority class way dealing works better ” apply validation dataset see one works better image 1 idea course find best approach test test dataset also leave test dataset alone – n't modify oversampling/undersampling full train dataset	1
handle imbalance datasets	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
handle imbalance datasets	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
sufficient complete final project get certificate course one also complete homework successfully get better scores	alexey add comment least workplace airflow instance set us data scientist would go instance airflow managed data engineers devops engineers would use never needed actually run docker compose moment multiple dags orchestration tool quite useful would personally maybe go luigi favorite one need think people well since people know luigi many like maybe airflow would actually better choice consider sejal would also like add question docker compose used docker compose airflow style use official setup seeing first time well 's unnecessarily complicated especially xcom variables forth generally docker compose acts wrapper docker containers production workflows docker containers used integration ci/cd pipelines wherever 're going deploy docker compose docker compose part helpful development environments case let 's say mock version airflow environment production want test dags local development environment use docker compose make things easier running simple commands like ‘ docker compose ’ would make fine instead running ‘ docker run container name etc whatever want ’ terms convenience terms usage ’ generally used development environments production alexey think first part question “ close usual data engineer role workflows ” workflows see work complex steps probably also case need start something already – would n't call complex pipeline – three four steps already something eventually might grow bigger bigger add steps ankush would also like add 're cloud maybe use airflow deploy docker maybe actually use service provided cloud provider one similar airflow think airflow would used internally like cloud composite google cloud platform would using terraform sejal yeah used aws step functions aws setup step functions combination lambda place airflow ankush said also something wanted say really depends production self-hosted cluster kubernetes wherever want use pure airflow version company could kind setup use native airflow cluster could cloud-based setup convenient option use managed services cloud composer aws also version called managed airflow	0
sufficient complete final project get certificate course one also complete homework successfully get better scores	alexey n't think actually time summarize everything course got inspired sejal – prepares notes 's recording video based notes share notes also take notes share notes fellow students way learn public – sort regurgitate content produce something new – useful useful others promise summaries encourage take notes share others jupyter books think something publish particular week jupyter notebook code snippets course share ankush definitely share code snippets code use particular week definitely shared github	0
sufficient complete final project get certificate course one also complete homework successfully get better scores	alexey yeah n't need complete homeworks get certificate homework make sure understand going get feedback homework try ask questions make sure understand lesson example one questions homework run terraform want make sure set environment able continue following week example already know things want maybe revise others n't homework need focus want learn project get certificate way want emphasize importance projects actually learn might question “ need bother homework ” homework give little bit incentive terms scores 's basically	1
sufficient complete final project get certificate course one also complete homework successfully get better scores	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	0
sufficient complete final project get certificate course one also complete homework successfully get better scores	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
train encoded features dictvectorizer onehotencoder using ‘ sparse=true ’ use cases	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
train encoded features dictvectorizer onehotencoder using ‘ sparse=true ’ use cases	building locally fine	0
train encoded features dictvectorizer onehotencoder using ‘ sparse=true ’ use cases	yes submit anywhere want say 20 pt n't remember yeah work good article “ good ” meaning 's copy/pasting code nothing else actually need explain 's going also article – closer time working article guidelines well	0
train encoded features dictvectorizer onehotencoder using ‘ sparse=true ’ use cases	flask app produce result actually return prediction 're good n't need fix answer question regarding many points deducted think whole project kind assumes managed deploy flask n't able put docker able deploy cloud lose like five six points something along lines think 's good idea make sure flask returns something problems please share code think know asking question talked already slack maybe error somewhere predict.py script need make sure fix error work perhaps take code lectures – think xgboost model logistic regression need put x variable t-matrix xgboost instead using predict_proba need use predict think 's – n't work please write slack let 's try figure 's working	0
train encoded features dictvectorizer onehotencoder using ‘ sparse=true ’ use cases	yes course train exactly way ‘ sparse=false ’ think code n't change example see even faster train sparse matrices advantages disadvantages n't want go sparse matrices course useful question “ 100 categorical variables ” case want use sparse matrices – lot categorical variables use dense – let maybe draw dense let 's say zero rest zeros column number 0 1 2 3 4 row number 0 1. sparse representation would – row number 0 4 1 's row number 1 3 1. n't write zeros since four five values zeros n't write use sparse matrices image 3 came think 's good rule thumb let 's say 50 values zeros go sparse matrix actually bit faster use case – lot categorical variables	1
's ranking different regression model	far week two week three follow simple approach simple approach described take entire dataset split three parts train validation test 'll use week two week three week four talk cross validation – explain right want say approach validation sufficient many cases think 's pretty safe say many many many applications split enough cross validation course nicer learn week four right go detail try explain wait video “ select approach cross validation ” would say multiple options k-fold… 'm sure many people understand n't covered yet basically dataset big n't need k-fold dataset smaller use k-fold “ leave p ” – 've never used personally	0
's ranking different regression model	ideally said take dataset – leave test dataset aside n't touch final model n't eda test dataset train full train – n't training dataset accidentally see pattern try use maybe build feature around pattern meanwhile pattern might true general case try – 's called data snooping look data might accidentally see something something may seem important accidentally overfit try avoid looking test dataset	0
's ranking different regression model	yes good references – course course called machine learning zoomcamp cover parts except conda git assume midterm project use git conda – use conda python interpreter install pipenv manage dependencies n't know good references honest maybe n't know answered question hope references course good enough ’ please let know think improved think missing	0
's ranking different regression model	think one videos talked three different subtypes supervised learning integration classification tracking ranking – image reference regression let ’ say car extract feature matrix car apply formula g function trained get prediction like price one object get one car prediction one car comes ranking n't one object course multiple cars apply multiple cars core difference ranking – ranking let 's say results google query results google could let 's say results 0 99. need apply function elements group need apply model model g element group produces ranked list let 's say apply g x0 apply g x99 x99 row – results query rerank output – rerank results using function 'm trying say look group try see good ranking within group case simple regression standalone objects sort ranking always group course comes ranking g also regression also classification always need think elements group hope answers question something go detail – course sure know 's little bit different maybe something want project explore article totally fine	1
's ranking different regression model	alexey one thing maybe n't clear enough – please always include code solution maybe people n't watch first said n't submit code homework get zero points saw submissions people put dot empty put link towards data science article example please n't please put code n't 'll get zero points entire week 'll make explicit form week three well please n't forget put code	0
expect submission project ’ assuming notebooks showing eda data pre-processing training script anything else	's actually think 's useful n't train.py produce pickle file let 's say want retrain model using different parameters – 's useful see changed score real life like projects work train.py 's always one file sometimes 's bunch files somehow log performance think 's useful n't want reason n't	0
expect submission project ’ assuming notebooks showing eda data pre-processing training script anything else	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
expect submission project ’ assuming notebooks showing eda data pre-processing training script anything else	answer – depends dataset – typically train/validation split first try scaling try without scaling see happens think something talked last week actually notebook prepared last week go classification notebook notebook scaling shows scale features use standard scaler use min max scaler – use two different scalars shows particular dataset turned little bit better experiment see scaling makes sense linear models sometimes 's actually good idea add extra complexity pipeline instead dictionary vectorizer whatnot 'll also need add next step scaling basically steps prediction pipeline becomes bit difficult maintain would say try scale see improvement get significant maybe 's worth extra complexity get use cross-validation use best judgment see makes sense	0
expect submission project ’ assuming notebooks showing eda data pre-processing training script anything else	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	1
expect submission project ’ assuming notebooks showing eda data pre-processing training script anything else	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
github page homework 2 mentioned ingest ny taxi data 2019-2021. homework 2 description however 's 2019-2020. 2021 2020	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
github page homework 2 mentioned ingest ny taxi data 2019-2021. homework 2 description however 's 2019-2020. 2021 2020	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
github page homework 2 mentioned ingest ny taxi data 2019-2021. homework 2 description however 's 2019-2020. 2021 2020	alexey week 2 yes – upload video solution also update leaderboard probably take time depending clean data – need lot data cleaning take lot time please n't put things n't look like emails email field fill please careful takes time clean later	0
github page homework 2 mentioned ingest ny taxi data 2019-2021. homework 2 description however 's 2019-2020. 2021 2020	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
github page homework 2 mentioned ingest ny taxi data 2019-2021. homework 2 description however 's 2019-2020. 2021 2020	alexey week 3 yellow taxi data use 2018-2020. frequently/high use think 2019. zones 's one file victoria week 4 use one month right victoria whatever load basically week 3. would understand one 2019-2020 right since 're going building model would independent – 'll use whatever table n't get homework something like least one month enough run models alexey 'll try update description 's less confusing yellow taxis ’ 2019-2020 for-hire vehicles 2019. n't use green taxis week 3	1
datasets allowed	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
datasets allowed	let 's say two dimensional thing 2d want turn 1d case need use pooling2d 'm sure right want quickly check reason remember showing usually go defining model build sort layer layer every time add one layer model predicts see output based see kind pooling layer example need let 's say want turn 2d 1d think need use 2d pooling 'm exactly sure whether 's 2d 1d think 1d pooling needed something one dimensional want turn one value use 1d pooling 3d pooling three dimensional thing want turn one-dimensional thing image 2 things always confuse honest 's follow step step try different poolings want make sure convert image vector presentation something one-dimensional usually size number images times something 's like 2d array image one-dimensional vector based try different poolings sometimes also flatten flatten takes whatever – let 's say kd – want turn 1d use flatten many different options think 's clear difference might remember exactly use 1d 2d difference kind input take kind output produce 's cube 's 2d 's hypercube three dimensions 's something else	0
datasets allowed	case everyone uses docker problem n't use docker n't get point using docker ’ run write “ sorry run this. ” hope n't happen 's actually whole idea behind docker – 's transferable different environments let 's say use gunicorn n't use docker able test case use scoring matrix evaluate accordingly gunicorn tick box would encourage still try learn submission even run reason think reason run something windows ubuntu gunicorn everything runnable	0
datasets allowed	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
datasets allowed	's actually good question datasets like titanic iris wine quality dataset boston housing also good example ethical issues datasets common n't think use learn much many tutorials list datasets suggest use really want predict somebody survive titanic catastrophe go it… 'm trying say ’ boring dataset besides that… people actually died anyways go kaggle pick dataset 'll fine	1
categorical target set distribution imbalanced example 90/10 approach used	alexey use something non-standard go usual things learned course hamed need test different strategies something noticed – many parse subclasses categorical inaudible careful using one-hot encoding might say use ordinal encoding data nature order useful particular data could n't domain knowledge n't know subclasses could n't decide strategy choose domain knowledge ’ key think	1
categorical target set distribution imbalanced example 90/10 approach used	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
categorical target set distribution imbalanced example 90/10 approach used	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
categorical target set distribution imbalanced example 90/10 approach used	yes good references – course course called machine learning zoomcamp cover parts except conda git assume midterm project use git conda – use conda python interpreter install pipenv manage dependencies n't know good references honest maybe n't know answered question hope references course good enough ’ please let know think improved think missing	0
categorical target set distribution imbalanced example 90/10 approach used	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
always good practice put data data lake putting data warehouse like done course exercises	alexey n't think really understand question ankush 're vpc matter alexey cluster – spark cluster cluster kafka cluster ankush 're running vpc able access 's ip right alexey yeah oh okay externally ip guess 're running inside network use internal ip 's inside kubernetes refer names deployments n't know ankush also n't know 's really specific particular use case need give us maybe slack explain bit – running kind machines exactly exposing terms external ip alexey usually kafka cluster kafka machines spark cluster spark cluster connects kafka reads data ankush depends vpc running 're running different vpcs need expose separately need expose ip running vpc matter able access internally might wrong 'm devops chuckles alexey usually take things granted maybe good thing work grateful data team makes tools work often n't ask “ okay actually configured ” works n't – n't go support channel ask “ hey please fix ” ankush think 's also data team 's also maybe devops team embedded inside data team 're working olx pretty big company maybe many smaller teams inside big teams guess 's devops topic	0
always good practice put data data lake putting data warehouse like done course exercises	ankush always good practice obviously 's never one way things cases definitely makes sense example let 's say 're using snowflake data warehouse 's expensive data warehouse honestly cases might want keep data lake layer machine learning solutions really help cut costs even maybe remove data data warehouse 's getting expensive something definitely use cases would definitely recommend everybody data lake data warehouse use cases small company small dataset want quick putting extra layer data lake might negative impact overall performance speed alexey thinking data company goes directly data warehouse n't think anything everything first goes data lake data might end data warehouse let 's say 10 20 rest stays lake ankush yep think 's company mature using data company also mature saving data generally lot companies maturity level great cases data lake might create data swamp chuckles	1
always good practice put data data lake putting data warehouse like done course exercises	alexey 's want look project 're behind right n't worry take time decide whether give certificate based projects homework 's – catch week 3 lighter week 1 maybe fast-forward directly docker compose run week 2 depending much time maybe go solution published today give everything need week 3 use transfer service video already watching video probably learn lot already like homework solution	0
always good practice put data data lake putting data warehouse like done course exercises	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
always good practice put data data lake putting data warehouse like done course exercises	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
's okay ask uploaded yellow taxi data supposedly according airflow n't upload completely need redo	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
's okay ask uploaded yellow taxi data supposedly according airflow n't upload completely need redo	alexey ankush way stopped started transfer using transfer service stopped transferred data right ankush automatically stop also use prefixes options use prefixes filters definitely try use	0
's okay ask uploaded yellow taxi data supposedly according airflow n't upload completely need redo	alexey n't airflow understand works understand create dag make dependencies – noticed overhead airflow experienced today sometimes break computer work expected really annoying run things locally script upload gcp using google cli web browser need data next week want follow along week 3 videos queries need data homework think important – need “ hire vehicles ” data shortcut use transfer service 's one click data google cloud storage need aws account little bit annoying n't need redo need somehow get data able week 3. figure something slack trouble running	1
's okay ask uploaded yellow taxi data supposedly according airflow n't upload completely need redo	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
's okay ask uploaded yellow taxi data supposedly according airflow n't upload completely need redo	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
many hours daily study recommended study continuously without long break	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
many hours daily study recommended study continuously without long break	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
many hours daily study recommended study continuously without long break	well honest n't know many hours daily study recommended 's added question form share results later many hours usually takes average answer depends background 're comfortable coding maybe n't need spend much time compared 're new python example would need put effort numbers least maybe want watch lectures first spend one two hours exercises maybe total – three four hours depends n't really know “ study continuously without long break ” maybe n't study continuously without breaks n't think 's good ’ better breaks	1
many hours daily study recommended study continuously without long break	building locally fine	0
many hours daily study recommended study continuously without long break	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
know awesome hosting course	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
know awesome hosting course	victoria yes weeks belong one bigger project taxi data new york shown beginning cover different stages idea also use afterwards build project final homework	0
know awesome hosting course	alexey know know ankush ankush n't definitely know thank much comment alexey yeah thanks also appreciate 're taking course – 're spending time taking course feedback give us thanks lot ankush yeah definitely think feedback really important please keep coming improve course maybe future even better videos courses	1
know awesome hosting course	victoria 'm sure answer mean yes definitely adding part project least workshop reason want people learn also consider people entirely new concept thought would much overhead try cover things already quite complex day-to-day even though work daily dbt work slow-changing dimensions daily basis example 's also think 's something look moment use definitely dbt especially deletions implementing load step done hooks think 's section advanced knowledge link hooks like incremental models mentioned changing dimensions n't link could link something could use snapshots think added concept updates guess could also pre-hook post-hook also something would incremental model incremental model loads new chunk data use merge depending workers use use could insert update 's something would work unique key project also final project 's understand complex go 's limit hope answers	0
know awesome hosting course	victoria n't used anaconda think 's also documentation install dbt locally general 're using anaconda 're using environment install use homebrew 're using mac also use pip install pip install dbt postgres example need adapter dbt snowflake dbt big query – whatever want use locally 's 're installed set profiles.yml show videos create project clone whatever 's 's need use locally never install using anaconda 'm guessing 's probably something similar probably conda forge install like 's possible otherwise would go pip install alexey anaconda usually pip victoria yeah 're environment install like 's	0
couple models deployment covered course preferred industry option kubernetes cloud	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
couple models deployment covered course preferred industry option kubernetes cloud	mostly see – usually companies prefer go kubernetes especially bigger companies people support kubernetes cluster quite popular approach decided covering thinking whether cover kubernetes fairly advanced topic 's difficult cover kubernetes one lecture similar neural networks easy think 's quite important get understanding kubernetes works afraid using kubectl would say yes preferred industry option options also quite popular example olx work use lambda n't use elastic beanstalk reason use lambda use kubernetes lot usually deploy things either fastapi flask mostly flask couple projects use fastapi go kubernetes although 're project alone n't people know kubernetes quite well would n't advise using kubernetes use something simpler like maybe lambda would better option	1
couple models deployment covered course preferred industry option kubernetes cloud	building locally fine	0
couple models deployment covered course preferred industry option kubernetes cloud	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
couple models deployment covered course preferred industry option kubernetes cloud	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
necessary data scientist workplace expertise end-to-end – data collection cleaning modeling deployment	n't think takes lot time first need record videos prepare homework imagine adding collaboration startups well maybe sometime future n't know somebody startup listening right want hire people graduate zoomcamp please reach let 's arrange something	0
necessary data scientist workplace expertise end-to-end – data collection cleaning modeling deployment	classification classification let 's say n't predicting price let 's say predicting 's average number hours image 1 case would need another layer – rather layer 's called activation n't activation 're still… ’ trying formulate way 's confusing think 's better go lecture rewatch ’ essentially retelling thing need from_logits price variable loss mean squared error logits loss categorical cross-entropy categorical cross entropy usually used multiple categories need use logits 's numerically stable n't use – stay activation softmax n't use legit hope 's clear regression n't need use homework need use from_logits either homework two classes 's binary classification problem multiple classes – let 's say three classes four classes five classes – need use logits loss categorical cross-entropy loss categorical cross-entropy 's recommended use from_logits=true 's numerically stable	0
necessary data scientist workplace expertise end-to-end – data collection cleaning modeling deployment	would say 's must – would say 's big plus get exposure areas also think able deploy models really crucial part 's much emphasis course deployment part able data scientist especially machine learning engineer 'm sure probably work kubernetes job – something similar kubernetes knowing quite beneficial learn experience companies get queue hire think 's useful experience	1
necessary data scientist workplace expertise end-to-end – data collection cleaning modeling deployment	one think 's good way checking feature importance mathematical/intuitive reason behind let 's say even train first model see features already give good separation positive negative classes remember attrition auc interpretation auc probability randomly selected positive example higher score randomly selected negative example score also mean feature ’ randomly selected positive example value feature higher randomly selected negative example think probably makes sense try let 's say seniority – negative correlation maybe 's little bit complex already see important features maybe features already good enough maybe n't even need model use feature prediction roll second iteration proper model 's think 's good idea 's added exercise	0
necessary data scientist workplace expertise end-to-end – data collection cleaning modeling deployment	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
deployment mean “ monitoring quality maintainability scalability model ”	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
deployment mean “ monitoring quality maintainability scalability model ”	building locally fine	0
deployment mean “ monitoring quality maintainability scalability model ”	deploy model – let 's say model deployed web service meaning phone old iphone button details listing car like title model make want predict price car right phone sends request web service idea eventually request ends model model replies gets information car like model make age – things – replies back predictions “ car costs much ” recommend user let 's say 50k model – say 's deployed production client talk model communicate model using web service let 's say use https communicate model deployment image reference monitoring quality means – let 's say deployed model one year car one year ago cost 50k probably costs 40k cars change new cars appear cars expensive time ago become less expensive maybe cars become expensive – things change need able detect – changes call model monitoring need see things call changes distributions “ drifts distributions ” things change need able detect let 's say retrain model scalability – let 's say ’ one phone millions one simple web service handle much load need deploy one instance web service let 's say 100 100 together deal 1 million requests time one single one mean scalability scale web service add instances model able process traffic maintainability means – let 's say bug model want fix bug easy us fix bug need debug model maintainability easy us move around code base web service usually follow best engineering practices 's easier something cover course probably need course software engineering practices actually talked model monitoring also simple monitoring like “ many requests per second model getting ” things like also need set monitoring	1
deployment mean “ monitoring quality maintainability scalability model ”	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
deployment mean “ monitoring quality maintainability scalability model ”	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
worth plus point share experience personal public tech blog instead social media	yeah guess course would like course otherwise n't see get point experience involves course share tech blog yeah – course n't use twitter linkedin – use anything think blog perfectly fine	1
worth plus point share experience personal public tech blog instead social media	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
worth plus point share experience personal public tech blog instead social media	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
worth plus point share experience personal public tech blog instead social media	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
worth plus point share experience personal public tech blog instead social media	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
hyperparameter tuning tuning using full train test dataset	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
hyperparameter tuning tuning using full train test dataset	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
hyperparameter tuning tuning using full train test dataset	's actually think 's useful n't train.py produce pickle file let 's say want retrain model using different parameters – 's useful see changed score real life like projects work train.py 's always one file sometimes 's bunch files somehow log performance think 's useful n't want reason n't	0
hyperparameter tuning tuning using full train test dataset	use validation dataset hyperparameter tuning tune model using validation dataset end join training validation dataset train final model tune parameters test dataset use dataset occasionally infrequently – double check n't accidentally overfit final model use preferably try look hyperparameter tuning use validation dataset	1
hyperparameter tuning tuning using full train test dataset	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
trying use conda environment manager able create environment container able activate directly startup container gunicorn build	really depends cloud showed elastic beanstalk n't think needed change much anything n't remember changing anything thing changed url service test.py script thing needed change think heroku 's similar actually couple tutorials using pythonanywhere heroku probably explain need change go see need change anything probably much	0
trying use conda environment manager able create environment container able activate directly startup container gunicorn build	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
trying use conda environment manager able create environment container able activate directly startup container gunicorn build	yeah tricky know saw link slack somebody posted best practices actually 's one reasons usually n't use conda docker like conda – love conda love anaconda installed laptops local development comes deploying things yeah… advantages disadvantages usually use something like pipenv manage environment use conda local development install anaconda libraries want local computer train model comes productionizing create environment file put scripts long answer short one go slack look link best practices putting conda docker	1
trying use conda environment manager able create environment container able activate directly startup container gunicorn build	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
trying use conda environment manager able create environment container able activate directly startup container gunicorn build	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
youtube playlist still available course	ankush avro parquet totally different things avro row-based parquet columnar something like batch processing data-analytical kind work parquet perfect solution use spark something 's super fast calculate something like sum account parquet file read particular column rather whole column whole row avro hand strong providing flexibility terms backward forward compatibility 're using avro kafka using something like protobuf kafka would really useful think like different technologies altogether use different use cases alexey avro guess use case would streaming parquet batch jobs ankush yeah think definitely beginning would basic use cases yes alexey also something called ocr ankush 's also columnar storage alexey oh 's different orc chuckles remember watching presentation berlin buzzwords format remember seeing cool presentation never actually tried ankush neither i. alexey 's optimized row columnar interesting never knew stood widely used people go parquet ankush seen parquet guess companies definitely using 's bad technology think 's parquet famous compatible solutions parquet people tend use often	0
youtube playlist still available course	ankush planning syllabus actually removed nosql databases wanted focus important tools data engineering 's covering spark data warehouse kafka dbt airflow nosql definitely important part data engineers know community really wants maybe develop couple videos nosql use – maybe give example cassandra something alexey also ran survey started course eight different topics concerning community wants hear analytics engineering top one terms preference nosql last one whether internals nosql n't remember based decisions survey well seemed like much interest nosql databases lot interest analytics engineering affected way decided come syllabus course maybe also let us know specifically mind nosql broad term talking redis talking mongo cassandra dynamo exactly	0
youtube playlist still available course	victoria 'm sure answer mean yes definitely adding part project least workshop reason want people learn also consider people entirely new concept thought would much overhead try cover things already quite complex day-to-day even though work daily dbt work slow-changing dimensions daily basis example 's also think 's something look moment use definitely dbt especially deletions implementing load step done hooks think 's section advanced knowledge link hooks like incremental models mentioned changing dimensions n't link could link something could use snapshots think added concept updates guess could also pre-hook post-hook also something would incremental model incremental model loads new chunk data use merge depending workers use use could insert update 's something would work unique key project also final project 's understand complex go 's limit hope answers	0
youtube playlist still available course	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
youtube playlist still available course	alexey yes youtube playlist still available	1
tips working really large datasets	building locally fine	0
tips working really large datasets	far week two week three follow simple approach simple approach described take entire dataset split three parts train validation test 'll use week two week three week four talk cross validation – explain right want say approach validation sufficient many cases think 's pretty safe say many many many applications split enough cross validation course nicer learn week four right go detail try explain wait video “ select approach cross validation ” would say multiple options k-fold… 'm sure many people understand n't covered yet basically dataset big n't need k-fold dataset smaller use k-fold “ leave p ” – 've never used personally	0
tips working really large datasets	yes use linear models ’ usually quite good large datasets – better let 's say tree-based models another tip using sgd classifier stochastic gradient descent classifier also linear models faster usual logistic regression look part data training – n't need entire dataset memory large datasets use go amazon rent bigger machine train xgboost probably better trying train linear model computer also sampling let 's say pretty large dataset take sample 10,000 rows work reasonably well 's case-dependent	1
tips working really large datasets	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
tips working really large datasets	n't think takes lot time first need record videos prepare homework imagine adding collaboration startups well maybe sometime future n't know somebody startup listening right want hire people graduate zoomcamp please reach let 's arrange something	0
need clean data homework values leading/trailing spaces	sejal think related orchestration since 've covered concepts orchestration alexey maybe take one putting two cents – airflow definitely meant orchestration multiple jobs sometimes even parallel asynchronously running jobs 's really meant sequential run small-scale usage definitely resource-intensive jobs alexey airflow quite heavy indeed benefits gives – mechanisms ui viewing history – useful moment three dags running parallel one airflow might overkill know 're going pipelines workflows 's time think kind workflow orchestration tool n't airflow airflow quite widespread let 's say team 're making decision workflow orchestration tool use airflow would quite good choice data engineers market know data scientists also know less usually like good tool course disadvantages – indeed resource-intensive needs run somewhere – typically n't set computer	0
need clean data homework values leading/trailing spaces	alexey 'm sure last part –how docker provided terraform ankush run stuff terraform think always ... set airflow cluster terraform n't think execute job alexey 's something like use terraform prepare environment prepare thing run need actually run something least two separate commands first “ terraform apply ” “ python run ” something like 50:02 1 upvotes allowed contact instructors directly discuss projects things unsure regarding project 'm using new tools alexey think 's best ask things slack channel others also interested learning answer scales better instead everyone writing us directly – answer everyone sees answer b others answer available right somebody knows tool maybe n't example pulsar one students might know able help 's best use channel contact us directly	0
need clean data homework values leading/trailing spaces	victoria homework counts end 's need ideally transformation happen week analytics engineering – right 're focusing loading data fair n't really put much effort cleaning part dbt focused building project could give concepts rather actual writing sql query already familiar one prerequisites something could add dbt project example alexey maybe also see spark think cleaning little bit easier sql also depends type problems want clean example leading/trailing spaces relatively simple handle sql victoria yeah like simpler query focused markers stuff like expect clean week 4 's would add	1
need clean data homework values leading/trailing spaces	alexey ankush like scala right chuckles ankush think scala definitely one best languages data engineers chuckles biggest part 's typesafe 's heavy verbose java scala perfect combination least coming java background love scala would say yes 's kind important language know data engineer especially spark written scala sometimes see errors spark- java-native might get confused knowledge like scala personally use developing pipelines also use write code spark sometimes also beam actually need really know python well enough proficient python get away minimum knowledge java scala would say unfortunately would say like scala personally like definitely go ahead learn 's still good language alexey think get performance benefits python extra layer abstraction top spark adds bit overhead cases want control things right ankush personal experience whenever written batch jobs – 'm talking real-time 'm talking batch jobs n't matter much time takes – always half hour 10-15 minutes minimum adding another 30 seconds one minute python performance n't really matter real-time streaming yes would huge loss n't think 's case pyspark real-time batch n't really matter alexey think serialization better use scala use “ case classes ” whatever called easily turn rdd case class think uses efficient serialization mechanism python sometimes difference minutes remember case difference like 20 minutes took pipeline rewrote scala difference big ankush would say spark 1.0-something data frames 's also really useful anymore alexey data frames yeah 1.2. remember used python every step pipeline found one job could optimize scala whole pipeline lot faster something needed scala year using spark chuckles think important – know java scala data engineer ankush good python would use already know java ’ need scala n't know either learn scala scala would much easier learn application especially spark alexey argue n't lot time chuckles cross-talk ankush preferences biased really like scala answers neutral definitely biased towards scala alexey victoria n't want add anything victoria n't know n't used scala several years reason people offer jobs java development linkedin chuckles alexey use victoria used university prove get title would use	0
need clean data homework values leading/trailing spaces	sejal maybe someone help answer really expert kubernetes side terraform lets build static code templates infrastructure iec-style manner would work cases want static-based infrastructure cases set infrastructure resources destroyed point time want restore certain image certain state – terraform useful case kubernetes 's actually like spin-up cluster deploy services onto kubernetes cluster use cases really different case maybe ankush alexey add kubernetes ankush yeah definitely think kubernetes terraform different think kubernetes thinking deploying microservices sort applications done terraform using aws fargate aws ecs solutions terraform much let ’ assume kubernetes use terraform set kubernetes cluster also use terraform set let 's say s3 buckets next week ’ course see use terraform set transfer service google cloud platform possible kubernetes yes argue certain services like spark flink coming new solutions run kubernetes still example say want run kafka clusters cases kubernetes good solution would like stable solution stable infrastructure case 's terraform would definitely help set kinds clusters alexey person quite far infrastructure would add usually people dedicated company terraform helps go web ui click things instead going clicking file say “ bunch resources set cloud. ” let 's say need move one account another whatever reason – happened multiple times work needed migrate different account terraform destroy one account actually would destroy later terraform “ plan apply ” one account go back old account terraform destroy course still takes time everything code know exactly kind services use example kind buckets use aws us would kind lambda functions basically resources kubernetes could one resource terraform file comment says “ terraform infrastructure code kubernetes infrastructure. ” think 's quite concise way summarizing	0
project review able contact project creator ask questions anonymous	alexey example spark google “ spark gcp bigquery connector ” good article shows need need jar need use jar submit spark job let 's say files stored google cloud storage something files multiple options first option save back google cloud storage create external table something like like saw week 3. another option would saving directly bigquery using say “ write ” say want write bigquery uses connector connect bigquery write n't know permissions actually work – maybe also need specify key somehow probably 're already dataproc already permissions need order write bigquery order read google cloud storage use dataproc theoretically easy – add extra jar write bigquery data data warehouse use materials dbt week analytics engineering week actually consume content data warehouse visualize	0
project review able contact project creator ask questions anonymous	victoria would say ’ also 's different set products example data warehouse start data tech stack scratch would definitely use redshift cost lower end need probably would need migrate something like bigquery snowflake lot documentation 's pretty easy set 's around longer – think reason broader adoption mainly around things alexey seems aws popular well least berlin look companies use cloud maybe 70 people use aws 20 may use gcp remaining 10 use azure 's roughly see maybe 'm wrong course also got use gcp bit seems ui nicer go web console 's little bit nicer think comes tools 's bit difficult example aws pip install aws cli google cloud 's bit difficult maybe 's one reason aws popular – ’ mature maybe think gcp catching nice interface sometimes things cheaper bigquery also think big advantage maybe clouds n't really victoria plus aws around 10 years thesis using aws thing gcp yet 's lot people go 're familiar outside stability alexey think cloud services services quite similar ’ used aws many things gcp straightforward n't typical less think many concepts map one another	0
project review able contact project creator ask questions anonymous	alexey well 's really anonymous asked submit link github n't know need ask questions previous course think ask contact information project description n't know ankush think encourage might unfair different people first maybe 're available answer questions let 's put like grading maybe 's unfair want learn 's definitely helpful think grading done based upon submitted 's also really important document steps alexey yeah guess need contact author means really give four points reproducibility ankush case maybe instructions clear report via contact alexey yeah exactly n't remember problem previous course maybe also create issue github n't know	1
project review able contact project creator ask questions anonymous	alexey week 3 yellow taxi data use 2018-2020. frequently/high use think 2019. zones 's one file victoria week 4 use one month right victoria whatever load basically week 3. would understand one 2019-2020 right since 're going building model would independent – 'll use whatever table n't get homework something like least one month enough run models alexey 'll try update description 's less confusing yellow taxis ’ 2019-2020 for-hire vehicles 2019. n't use green taxis week 3	0
project review able contact project creator ask questions anonymous	alexey 'm sure last part –how docker provided terraform ankush run stuff terraform think always ... set airflow cluster terraform n't think execute job alexey 's something like use terraform prepare environment prepare thing run need actually run something least two separate commands first “ terraform apply ” “ python run ” something like 50:02 1 upvotes allowed contact instructors directly discuss projects things unsure regarding project 'm using new tools alexey think 's best ask things slack channel others also interested learning answer scales better instead everyone writing us directly – answer everyone sees answer b others answer available right somebody knows tool maybe n't example pulsar one students might know able help 's best use channel contact us directly	0
'm starting week three feeling left behind dumb	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
'm starting week three feeling left behind dumb	alexey please n't feel dumb feel left behind dumb always ask questions chat channel also look things n't know state list frequently asked questions think abundant maybe put life question still 10k history slack question look slack find answer please put frequently asked questions way preserved course please n't feel dumb everyone different amount time dedicate course everyone different backgrounds somebody maybe 's first time ’ using docker others 've using docker last five years course second group people 's easy n't mean easy	1
'm starting week three feeling left behind dumb	alexey example spark google “ spark gcp bigquery connector ” good article shows need need jar need use jar submit spark job let 's say files stored google cloud storage something files multiple options first option save back google cloud storage create external table something like like saw week 3. another option would saving directly bigquery using say “ write ” say want write bigquery uses connector connect bigquery write n't know permissions actually work – maybe also need specify key somehow probably 're already dataproc already permissions need order write bigquery order read google cloud storage use dataproc theoretically easy – add extra jar write bigquery data data warehouse use materials dbt week analytics engineering week actually consume content data warehouse visualize	0
'm starting week three feeling left behind dumb	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
'm starting week three feeling left behind dumb	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
homeworks get scored final project also get score homework might leaderboard still get certificate correct	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
homeworks get scored final project also get score homework might leaderboard still get certificate correct	ankush avro parquet totally different things avro row-based parquet columnar something like batch processing data-analytical kind work parquet perfect solution use spark something 's super fast calculate something like sum account parquet file read particular column rather whole column whole row avro hand strong providing flexibility terms backward forward compatibility 're using avro kafka using something like protobuf kafka would really useful think like different technologies altogether use different use cases alexey avro guess use case would streaming parquet batch jobs ankush yeah think definitely beginning would basic use cases yes alexey also something called ocr ankush 's also columnar storage alexey oh 's different orc chuckles remember watching presentation berlin buzzwords format remember seeing cool presentation never actually tried ankush neither i. alexey 's optimized row columnar interesting never knew stood widely used people go parquet ankush seen parquet guess companies definitely using 's bad technology think 's parquet famous compatible solutions parquet people tend use often	0
homeworks get scored final project also get score homework might leaderboard still get certificate correct	victoria ’ think need 've berlin three years speak german work depend go startup course 're going safe try make sure ask happen end team lot germans experience feel like need speak german order integrate 's thing would recommend though outside learning part – live germany try learn german actually integrate least case feel like helped lot feel country able go doctor receive calls stuff like speak least bit german n't speak super-fluidly quite helpful work alexey maybe 's also helpful separate berlin rest germany berlin 's international many companies berlin go start speaking german people look say “ sorry n't understand you. ” munich german city speak german german quite bad ’ lived like five half six seven years lost count ’ long start feeling ashamed still n't speak german chuckles victoria everything said disclaimer – berlin 's true even sometimes go coffee shops reply english person n't speak german quite crazy german capital alexey couriers deliver things also n't speak german hear way speak n't speak german 's good n't speak chuckles switch english victoria yeah 's true get everything english also one small part question “ need visa ” depend italian citizenship n't need visa example every country would say yes 's european alexey n't need german visa already work contract use contract 's enough get blue card visa “ blue card ” special program want get job seeker visa think need show level proficiency german n't remember ’ a2 b1 one n't need spend lot learning get level victoria job seeker well someone like data engineer think requirements less strict alexey think maybe anymore five years ago think requirement get least a1 level victoria anymore know lot people job seeker one definitely n't speak german alexey okay ignore said look victoria yeah definitely look chuckles also depend country kind degree something like may also change things alexey right blue card need degree without degree get program check internet	0
homeworks get scored final project also get score homework might leaderboard still get certificate correct	alexey week 2 yes – upload video solution also update leaderboard probably take time depending clean data – need lot data cleaning take lot time please n't put things n't look like emails email field fill please careful takes time clean later	0
homeworks get scored final project also get score homework might leaderboard still get certificate correct	alexey yeah leaderboard open leaderboard see long people attempted projects 're still board end another tab saying passed project separate leaderboard	1
favorite non-common tools use data engineering/analytics	alexey add comment least workplace airflow instance set us data scientist would go instance airflow managed data engineers devops engineers would use never needed actually run docker compose moment multiple dags orchestration tool quite useful would personally maybe go luigi favorite one need think people well since people know luigi many like maybe airflow would actually better choice consider sejal would also like add question docker compose used docker compose airflow style use official setup seeing first time well 's unnecessarily complicated especially xcom variables forth generally docker compose acts wrapper docker containers production workflows docker containers used integration ci/cd pipelines wherever 're going deploy docker compose docker compose part helpful development environments case let 's say mock version airflow environment production want test dags local development environment use docker compose make things easier running simple commands like ‘ docker compose ’ would make fine instead running ‘ docker run container name etc whatever want ’ terms convenience terms usage ’ generally used development environments production alexey think first part question “ close usual data engineer role workflows ” workflows see work complex steps probably also case need start something already – would n't call complex pipeline – three four steps already something eventually might grow bigger bigger add steps ankush would also like add 're cloud maybe use airflow deploy docker maybe actually use service provided cloud provider one similar airflow think airflow would used internally like cloud composite google cloud platform would using terraform sejal yeah used aws step functions aws setup step functions combination lambda place airflow ankush said also something wanted say really depends production self-hosted cluster kubernetes wherever want use pure airflow version company could kind setup use native airflow cluster could cloud-based setup convenient option use managed services cloud composer aws also version called managed airflow	0
favorite non-common tools use data engineering/analytics	victoria use common tools guess chuckles also use etleap pipeline instead airflow load data basically snowflake s3 buckets etleap well-known 's small 's would n't say 's favorite like etl alexey think interviewed analytics engineering mentioned tool remember trying understand saying map tool think spent like five minutes chuckles victoria yeah end ’ integration 's well known would n't say 's favorite 's easy use needs five minutes pipeline works would say ’ non-common tool 've used really liked metabase get know inaudible provide local version really liked get know really liked idea select something like table gives lot graphs things show little bit videos easy use course limitations compare bigger ones like looker example 's clear 's newer works well least small experience – using work alexey ’ trying think would favorite non-common tools use data engineering analytics frankly ’ know chuckles tools use pretty common n't think favorite ones use airflow use spark pretty boring stuff victoria data quality example guess get team mature actually tool outside data catalogs good ones alexey use in-house data catalog n't really checked data catalog good idea purpose – data sources data flows data pipelines need sort recommendation need know responsible dataset/data source personal data in-house tool think many open source tools many non-open source tools victoria 're interested would recommend checking amundsen 's probably one first ones explore bunch conferences podcasts everything lot resources one open source leap remember correctly big ones linkedin data catalog think open source check well even build data catalogs built top alexey mean proprietary tools based open source tools victoria yeah chuckles	1
favorite non-common tools use data engineering/analytics	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
favorite non-common tools use data engineering/analytics	alexey 33 submissions think decision extend deadline good one hopefully end week see higher number understand project much difficult watching course following along 33 people – probably put lot effort thanks saw one person write put 100 hours already project outstanding sorry cruel 100 hours lot	0
favorite non-common tools use data engineering/analytics	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
airflow seems resource-intensive running couple scripts kind workflows benefit greatly	alexey well good job keep working almost finished first week finish start week two finish week three chuckles follow sequence videos playlist github sufficient think questions go slack ankush keep going alexey chuckles yes exactly ’ almost finished first week good sign first week year pretty tough many people lot problems docker whatnot 're almost finished good sign ’ good track keep working	0
airflow seems resource-intensive running couple scripts kind workflows benefit greatly	alexey well guess want dashboard aggregates hard time trying imagine dataset take raw form put dashboard maybe 's already grouped 's already aggregated 's probably small dataset 's suitable course think maybe select reasonably big dataset require transformation 're doubt ask slack really n't know kind dataset put dashboard without transformations ankush yeah minimum transformations would required alexey counts group-bys maybe joins ankush exactly think 's better choose dataset allow	0
airflow seems resource-intensive running couple scripts kind workflows benefit greatly	sejal think related orchestration since 've covered concepts orchestration alexey maybe take one putting two cents – airflow definitely meant orchestration multiple jobs sometimes even parallel asynchronously running jobs 's really meant sequential run small-scale usage definitely resource-intensive jobs alexey airflow quite heavy indeed benefits gives – mechanisms ui viewing history – useful moment three dags running parallel one airflow might overkill know 're going pipelines workflows 's time think kind workflow orchestration tool n't airflow airflow quite widespread let 's say team 're making decision workflow orchestration tool use airflow would quite good choice data engineers market know data scientists also know less usually like good tool course disadvantages – indeed resource-intensive needs run somewhere – typically n't set computer	1
airflow seems resource-intensive running couple scripts kind workflows benefit greatly	victoria n't used anaconda think 's also documentation install dbt locally general 're using anaconda 're using environment install use homebrew 're using mac also use pip install pip install dbt postgres example need adapter dbt snowflake dbt big query – whatever want use locally 's 're installed set profiles.yml show videos create project clone whatever 's 's need use locally never install using anaconda 'm guessing 's probably something similar probably conda forge install like 's possible otherwise would go pip install alexey anaconda usually pip victoria yeah 're environment install like 's	0
airflow seems resource-intensive running couple scripts kind workflows benefit greatly	alexey 's want look project 're behind right n't worry take time decide whether give certificate based projects homework 's – catch week 3 lighter week 1 maybe fast-forward directly docker compose run week 2 depending much time maybe go solution published today give everything need week 3 use transfer service video already watching video probably learn lot already like homework solution	0
driver node spark master node node dedicated in-memory processing	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
driver node spark master node node dedicated in-memory processing	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
driver node spark master node node dedicated in-memory processing	alexey week 2 yes – upload video solution also update leaderboard probably take time depending clean data – need lot data cleaning take lot time please n't put things n't look like emails email field fill please careful takes time clean later	0
driver node spark master node node dedicated in-memory processing	ankush master node alexey master node necessarily right master node something coordinates execution let 's say spark submit computer open jupyter computer connect master driver node becomes thing spark submit right ankush think 's spark context lies in-memory machine spark context would start would connect different workers give graph give data start executing right alexey open video anatomy spark cluster image thing driver node laptop whatever use submit job airflow whenever invoke spark submit connect spark master – ankush said wherever spark context lives let 's say lives jupyter notebook laptop laptop driver master thing coordinates execution job watches executors available one executors dies reassigns – coordinating role also watch progress jobs master node ankush right also looked basically run worker master node 's always running master node makes sense run master node 'd want spark context alive throughout process alexey spark submit say process live “ want keep master node want keep somewhere else ” ankush yeah specify config spark.driver.host might provide driver would run particular host machine alexey default think laptop computer connect spark master override say live somewhere else ankush 's see documentation ’ reading right	1
driver node spark master node node dedicated in-memory processing	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
similar solution like kubernetes deploying stateful applications example databases restart stateful application	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
similar solution like kubernetes deploying stateful applications example databases restart stateful application	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
similar solution like kubernetes deploying stateful applications example databases restart stateful application	alexey depends exactly want reddit data right ankush yeah exactly would focus final goal let 's say “ want calculate reddit data. ” might want say “ keywords used ” something like case transformations directly depend alexey something like “ active number posts per day. ” ankush exactly “ per hour distribution posts. ” 'm pretty sure us goes daytime goes night trying answer 's first question would focus know answer know transformations apply alexey yeah 's end goal project dashboard “ exactly want put dashboard ” go “ dashboard data use kind transformations need order go data dashboard ” become clear need group-by need join table	0
similar solution like kubernetes deploying stateful applications example databases restart stateful application	alexey solutions like kubernetes deploying stateful applications like ecs amazon elastic container service example like kubernetes 's kubernetes ecs n't need worry upgrading latest version kubernetes 's less popular maybe also well-documented compared kubernetes use aspects 's easier manage aspects 's difficult documentation maybe good 'm sure really want deploy database option use managed database maybe go go something simpler like dynamodb n't need manage think google also something similar 'm sure part means “ restart stateful application ” also thing called hashicorp nomad like kubernetes cooler hip try 're quite similar	1
similar solution like kubernetes deploying stateful applications example databases restart stateful application	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
’ really able understand week 2 homework please explain detail thank	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
’ really able understand week 2 homework please explain detail thank	alexey well use judgment want try force register account azure run want n't maybe look code see errors 's really ankush maybe matching ask criteria – like used – maybe match corresponding people alexey 's nice idea let 's see 'm afraid complicate things bit already script matching n't use criteria 's random ankush need modify script one column alexey yeah perhaps let 's see ankush easier enough group different things alexey create form submitting ask cloud used right ankush exactly right alexey makes sense machine learning zoomcamp – wanted re-execute whole thing re-executed 's really want learn project – execute grading also learning – want learn really want learn things work execute learn n't time since executing different cloud might take lot time – n't much time really ask invest something like five hours figuring azure works use judgment guess 's answer try matching minimize	0
’ really able understand week 2 homework please explain detail thank	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
’ really able understand week 2 homework please explain detail thank	alexey 'm afraid go words 'm sure else add want specific instructions let week 2 folder airflow data ingestion dag saw video actually need take modify order turn dag runs yellow taxi data puts data google cloud storage use base top work change modify code copy data google cloud storage another thing also quite helpful local dags folder data ingestion local dag code long video actually shows parameterize jobs solving homework take script gcp took things merged result got solution first two questions think figure exactly approach remaining ones hope n't give away much n't spoil fun figuring ankush think really good explanation still questions feel free use slack channel guess would best place ’ actually put specific examples 're struggling specific errors seeing 'm pretty sure community help	1
’ really able understand week 2 homework please explain detail thank	victoria would say yes go-to dbt yes dbt enforces go read viewpoint 'll explain also models kind structure mentioned modern concepts concept comes 80s kimball defined kitchen analogy find useful normally go course complex gets may need steps 's okay always try separated – presentation source need typecasting like example maybe duplication middle may models go data pipeline let ’ say flow less would look like alexey kitchen analogy victoria kitchen analogy way model data similar restaurants model food warehouse raw food source data everyone allowed fact 's actually dangerous 's security measures actually go warehouse stored stored things like kitchen people cook raw food make food 're going serve allowed would data engineers data analysts maybe analytics engineers would data warehouse trying process raw data end part restaurant people eat – dining hall presentation layer data warehouse fact tables look like data marts 're building data marts business stakeholders everyone allowed get foot let ’ say restrictions 's already presented 're presenting raw alexey think tableau self-service tool go cubes presentation layer right victoria yeah technically bi tools self-serve could everything could expose everything right external tables create tables inaudible create section model create factories could everything could even ones development exposed self-service bi tool like tableau looker whatever analogy says use presentation layer parts present raw data people would n't know use 's going like “ oh n't find phones ones maybe 's transformation ” things like alexey nice analogy	0
came across course best approach take course 'm late join course seems like submission deadlines	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
came across course best approach take course 'm late join course seems like submission deadlines	n't think takes lot time first need record videos prepare homework imagine adding collaboration startups well maybe sometime future n't know somebody startup listening right want hire people graduate zoomcamp please reach let 's arrange something	0
came across course best approach take course 'm late join course seems like submission deadlines	take pace follow beginning questions ask us slack guess 's best approach 're just-in-time learner go ahead start working projects next project 'll capstone project start working capstone project right figure need learn actually project ’ way imagined head – 's 're together videos stay youtube forever anyone join time follow videos homework ask questions	1
came across course best approach take course 'm late join course seems like submission deadlines	classification classification let 's say n't predicting price let 's say predicting 's average number hours image 1 case would need another layer – rather layer 's called activation n't activation 're still… ’ trying formulate way 's confusing think 's better go lecture rewatch ’ essentially retelling thing need from_logits price variable loss mean squared error logits loss categorical cross-entropy categorical cross entropy usually used multiple categories need use logits 's numerically stable n't use – stay activation softmax n't use legit hope 's clear regression n't need use homework need use from_logits either homework two classes 's binary classification problem multiple classes – let 's say three classes four classes five classes – need use logits loss categorical cross-entropy loss categorical cross-entropy 's recommended use from_logits=true 's numerically stable	0
came across course best approach take course 'm late join course seems like submission deadlines	two people want work dataset individually please feel free imagine way control go kaggle select competition even ’ accidentally happen long 's exactly code – long n't copy – ’ fine yeah please n't copy code	0
flask fastapi	's interesting one let 's say numpy array image reference numbers – 1 6 – compute mean mean 3.5. add 3.5 multiple times mean n't change happens nan pandas simply ignores – n't look pretends 's basically pandas ignores nans looks numbers replace nans mean case 3.5 get data effectively mean n't change try show formula let 's say bit math let 's say four cities couple n/as basically part usual values part n/as missing part let 's say want compute mean .mean pandas happens let 's say n numbers like k missing values total series k+n – total length compute mean part get 1/nσxi=x̄ mean case 3.5. let 's replace n/as x̄ happens instead looking 1/n look 1/n+k total number elements already filled sum another sum x̄ k n summed k times x̄ becomes kx̄ formula like multiply n/n n/n simply 1. 's multiplying thing one get n x̄ part mean image reference clue 'm talking bear 's almost result 1/ n+k n x̄+kx̄ leave x equation 1/ n+k n+k x̄=x̄ basically shows add many means like result always x̄ 'm sure everyone enjoyed little proof image reference prove saw question meeting got curious like “ show mathematically ” spent like 10 minutes trying figure nice exercise brain 'm sure would able show proof spot honest	0
flask fastapi	lisa kinds things missing data text data sometimes need cleaning like street written “ st ” “ street ” might want combine 's missing data might want take average median ’ outliers depending makes sense might want throw say elon musk 's salary dataset might good model go chuckles alexey think salary zero right wrote recently tweet n't salary shares lisa agrees laughs would outlier different way lisa direction actually laughs things like would want look might use inner core tiles cut tops bottoms lot different techniques use alexey data cleaning broad also abstract unfortunately 's manual magic button like datarobot would nice give bunch dirty data magic wand button click data becomes clean would nice datarobot maybe two buttons – “ make data nice `` “ make data clean. ” “ okay find best model data. ” maybe research automated data cleaning well must something lisa companies companies buying consolidating trend seems end-to-end also deploying models machine learning ops make sure n't data drift things like 's always robust might want particular situation try generalize build tools	0
flask fastapi	reason like fastapi go	1
flask fastapi	let 's say “ import numpy np ” let 's say “ np.random.exponential ” noticed press shift+tab press shift+tab shows docstring multiple times press shift+tab expands see funny thing n't even realize muscle memory 's n't saying loud know go shift+tab small thing open twice expand another thing put question mark instead parentheses “ np.random.exponential ” show docstring another thing think put “ help np.random.exponential ” print docstring	0
flask fastapi	yes think showed 'm sure mean yeah think 's good idea	0
course prepare us taking role data engineer	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	0
course prepare us taking role data engineer	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
course prepare us taking role data engineer	ankush absolutely think course meant give knowledge required job going talk analytical engineering going talk streaming 're going talk spark maybe useful job start eventually important progress career definitely help land first job definitely also help progress career currently already engineer	1
course prepare us taking role data engineer	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
course prepare us taking role data engineer	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
close usual data engineering role workforce demonstrated seemed like used docker compose airflow rarely used	victoria 's buzzwords right alexey yeah go repo see like practical week like five 10 minute video explaining use 's planned ankush maybe spend couple minutes today well data mesh basically concept idea – let 's go back say “ data pipelines data engineering teams built different companies right ” people generating data people consuming data one big team middle called data engineering team dataops team whatever want call different companies different names teams responsible ingesting data transforming putting making way 's consumable right way right fashion data mesh wants basically decentralize particular role big team wants give power let 's say responsibility team generating data teams consuming data therefore ’ layer instead team taking whole role 's kind better idea build services give responsibility give services software teams generating data consuming basically going higher level respect case happens team ’ generating data responsible quality data answering different questions data things 's really useful multiple teams handling data idea data knowledge data lost best person best team answer questions best team knowledge data team 's generating also responsible putting data data lake data warehouse also responsible queries attaches whole thing together 's kind rough idea data mesh alexey ’ abstract concept actually go youtube channel longer explanation data mesh “ longer ” mean one hour long – hour six minutes chuckles 're kind stuff… way dataops 101 also quite nice one check well one modern data stack analytics engineering also good basically check three – 're good 're interested data mesh specifically first one goes lot detail second one – bit detail ankush 's also nice blog post alexey zhamak actually writing book right think 's early release 's written five six chapters n't remember 's progress 's going quite big book right ankush blog post like today 's date read couple days impossible read one go 's huge victoria think also download first two chapters something book give quick overview well ankush 's also another book o'reilly called data mesh practice max schultze alexey one free right ankush think free 's free	0
close usual data engineering role workforce demonstrated seemed like used docker compose airflow rarely used	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
close usual data engineering role workforce demonstrated seemed like used docker compose airflow rarely used	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
close usual data engineering role workforce demonstrated seemed like used docker compose airflow rarely used	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
close usual data engineering role workforce demonstrated seemed like used docker compose airflow rarely used	alexey add comment least workplace airflow instance set us data scientist would go instance airflow managed data engineers devops engineers would use never needed actually run docker compose moment multiple dags orchestration tool quite useful would personally maybe go luigi favorite one need think people well since people know luigi many like maybe airflow would actually better choice consider sejal would also like add question docker compose used docker compose airflow style use official setup seeing first time well 's unnecessarily complicated especially xcom variables forth generally docker compose acts wrapper docker containers production workflows docker containers used integration ci/cd pipelines wherever 're going deploy docker compose docker compose part helpful development environments case let 's say mock version airflow environment production want test dags local development environment use docker compose make things easier running simple commands like ‘ docker compose ’ would make fine instead running ‘ docker run container name etc whatever want ’ terms convenience terms usage ’ generally used development environments production alexey think first part question “ close usual data engineer role workflows ” workflows see work complex steps probably also case need start something already – would n't call complex pipeline – three four steps already something eventually might grow bigger bigger add steps ankush would also like add 're cloud maybe use airflow deploy docker maybe actually use service provided cloud provider one similar airflow think airflow would used internally like cloud composite google cloud platform would using terraform sejal yeah used aws step functions aws setup step functions combination lambda place airflow ankush said also something wanted say really depends production self-hosted cluster kubernetes wherever want use pure airflow version company could kind setup use native airflow cluster could cloud-based setup convenient option use managed services cloud composer aws also version called managed airflow	1
points keep mind augmenting images ratio generated original images	dmitry usually testing different approaches n't know whether golden standard something exists alexey science maybe science sense validation dataset treat augmentation hyperparameter try augmentation without see works better annoying neural nets already lot parameters top also parameters augmentations n't know understand second part – ratio generated original images maybe comes way trained model homework first trained original images generated ones case ratio kind 50/50 arbitrary choice right science dmitry says “ ” okay	1
points keep mind augmenting images ratio generated original images	building locally fine	0
points keep mind augmenting images ratio generated original images	n't think takes lot time first need record videos prepare homework imagine adding collaboration startups well maybe sometime future n't know somebody startup listening right want hire people graduate zoomcamp please reach let 's arrange something	0
points keep mind augmenting images ratio generated original images	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
points keep mind augmenting images ratio generated original images	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
know solve particular problems using machine learning	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
know solve particular problems using machine learning	deploy model – let 's say model deployed web service meaning phone old iphone button details listing car like title model make want predict price car right phone sends request web service idea eventually request ends model model replies gets information car like model make age – things – replies back predictions “ car costs much ” recommend user let 's say 50k model – say 's deployed production client talk model communicate model using web service let 's say use https communicate model deployment image reference monitoring quality means – let 's say deployed model one year car one year ago cost 50k probably costs 40k cars change new cars appear cars expensive time ago become less expensive maybe cars become expensive – things change need able detect – changes call model monitoring need see things call changes distributions “ drifts distributions ” things change need able detect let 's say retrain model scalability – let 's say ’ one phone millions one simple web service handle much load need deploy one instance web service let 's say 100 100 together deal 1 million requests time one single one mean scalability scale web service add instances model able process traffic maintainability means – let 's say bug model want fix bug easy us fix bug need debug model maintainability easy us move around code base web service usually follow best engineering practices 's easier something cover course probably need course software engineering practices actually talked model monitoring also simple monitoring like “ many requests per second model getting ” things like also need set monitoring	0
know solve particular problems using machine learning	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
know solve particular problems using machine learning	answer – depends dataset – typically train/validation split first try scaling try without scaling see happens think something talked last week actually notebook prepared last week go classification notebook notebook scaling shows scale features use standard scaler use min max scaler – use two different scalars shows particular dataset turned little bit better experiment see scaling makes sense linear models sometimes 's actually good idea add extra complexity pipeline instead dictionary vectorizer whatnot 'll also need add next step scaling basically steps prediction pipeline becomes bit difficult maintain would say try scale see improvement get significant maybe 's worth extra complexity get use cross-validation use best judgment see makes sense	0
know solve particular problems using machine learning	's interesting question tricky part – machine learning formula saw many times g x ≈y first need thing y. target already think “ target 're predicting ” let 's say want understand picture picture want build model classifying dogs cats case target would dogs cats – present image need think target y. target important know want predict potentially solved machine learning basically answer question need think answer “ target variable 'm trying predict ” “ features ” latter represented x g x ≈y need need target – want predict need features x. express problem terms feature matrix target solved machine learning ’ probably need think	1
need upload docker image hub	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
need upload docker image hub	use cross-validation check validation get better scores – drop one drop one go sometimes makes sense drop even decrease score simply maybe one feature complex compute another one	0
need upload docker image hub	always let 's take case image 1 split data test dataset split data – validation dataset let show would data frame 'll split data frame use 20 25 validation okay data frame validation image 2 ’ still smote downsampling upsampling – data frame train touch data frame validation experiment need make sure validation distribution target variable stays distribution target variable see real-world data n't want change change able evaluate model real world scenario 's important change validation change train downsampling upsampling smote train model test validation dataset set check whatever target metric want train full model keep test dataset intact – n't touch n't apply downsampling apply techniques data frame train full	0
need upload docker image hub	yes	0
need upload docker image hub	building locally fine	1
terraform come picture n't used far since installed week 1	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
terraform come picture n't used far since installed week 1	maybe maybe see content available	0
terraform come picture n't used far since installed week 1	sejal 'm surprised n't able connect dots one infrastructure created gcp – gcs bucket well bigquery dataset – created terraform run gcs dag airflow dag upload data gcs view gcs data gcs external table infrastructure created previous week used n't know followed videos understood concept behind things 've skipped terraform gcp part went local installation local setup use airflow docker postgres also fine 're running time two journeys 've prepared one cloud-based journey one local setup users unable install gcp unable access gcp local setup 're going postgres-based operation also completely fine alexey would add terraform useful thing anyway regardless whether use week 2 knowing quite helpful sure data engineer use point ankush exactly think point added terraform basis learn eventually idea introduce level get comfortable extend knowledge top especially depending upon role company alexey use terraform weeks think right n't know spark – still need figure exactly works google cloud 's probably managed spark like aws could created terraform well sejal bigquery tables created terraform airflow operators disclose information next days ankush already using week 2 transfer service something manually manual steps make easy company production systems always use terraform set something like transfer service	1
terraform come picture n't used far since installed week 1	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
terraform come picture n't used far since installed week 1	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
impute missing data splitting data wanted impute need look distribution data snooping	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
impute missing data splitting data wanted impute need look distribution data snooping	far week two week three follow simple approach simple approach described take entire dataset split three parts train validation test 'll use week two week three week four talk cross validation – explain right want say approach validation sufficient many cases think 's pretty safe say many many many applications split enough cross validation course nicer learn week four right go detail try explain wait video “ select approach cross validation ” would say multiple options k-fold… 'm sure many people understand n't covered yet basically dataset big n't need k-fold dataset smaller use k-fold “ leave p ” – 've never used personally	0
impute missing data splitting data wanted impute need look distribution data snooping	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
impute missing data splitting data wanted impute need look distribution data snooping	alexey one thing maybe n't clear enough – please always include code solution maybe people n't watch first said n't submit code homework get zero points saw submissions people put dot empty put link towards data science article example please n't please put code n't 'll get zero points entire week 'll make explicit form week three well please n't forget put code	0
impute missing data splitting data wanted impute need look distribution data snooping	data snooping impute missing data splitting data real life go future see kind data available data need kind mimic scenario access future data test dataset n't access forget ready final model	1
suggestions get started cloud	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
suggestions get started cloud	yeah actually good tutorials github use aws follow materials – deploying aws n't use aws articles community “ deployment tutorials ” section use pythonanywhere use heroku – tutorials community quite good let 's say want use pythonanywhere see instruction 's relatively simple 's free start use going ahead using n't suggestions	1
suggestions get started cloud	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
suggestions get started cloud	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
suggestions get started cloud	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
free motivates put amount work dedication	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
free motivates put amount work dedication	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
free motivates put amount work dedication	victoria set warehouse end define connection difference would use adapter dbt several adapters would first want use dbt go check adapters make sure 's supported several official example redshift could use community-supported azure alexey yeah aws athena azure athena redshift presto well example presto cluster use aws – within aws presto cluster potentially similar setup connect dbt presto victoria general add little bit answer let 's say project right bigquery would change profile 's set connection may make changes 's something sql supports bigquery way around able run project minutes	0
free motivates put amount work dedication	victoria would say ’ also 's different set products example data warehouse start data tech stack scratch would definitely use redshift cost lower end need probably would need migrate something like bigquery snowflake lot documentation 's pretty easy set 's around longer – think reason broader adoption mainly around things alexey seems aws popular well least berlin look companies use cloud maybe 70 people use aws 20 may use gcp remaining 10 use azure 's roughly see maybe 'm wrong course also got use gcp bit seems ui nicer go web console 's little bit nicer think comes tools 's bit difficult example aws pip install aws cli google cloud 's bit difficult maybe 's one reason aws popular – ’ mature maybe think gcp catching nice interface sometimes things cheaper bigquery also think big advantage maybe clouds n't really victoria plus aws around 10 years thesis using aws thing gcp yet 's lot people go 're familiar outside stability alexey think cloud services services quite similar ’ used aws many things gcp straightforward n't typical less think many concepts map one another	0
free motivates put amount work dedication	ankush said alexey 's machine learning course said learning public really important personally also important teach public learn experience guys sejal think 's like says next question like “ altruism ” yeah kind 's form giving back community also learned community – looking open source projects looking lot publicly available material kind proved self-made way really receive theoretical course background computer science never data-related education learned experience colleagues public material outside yeah form giving back community victoria ’ feel like knowledge put time effort especially people get data result least get see 's something would interested especially course covers several parts topics 's also good opportunity people pay expensive course something like still get chance learn put effort actually shift career data engineering alexey nothing add chuckles victoria although 're biggest contributor chuckles alexey one feedback got previous course machine learning zoomcamp really awesome seeing feedback like “ coursera go directly course ” really motivating although n't think should… chuckles strange feeling people compare course courses actually taking say mine better motivating 's really like hope liked also please give us feedback think n't understand something feedback also helpful positive feedback well – like landed internship taking course found job things like helpful us motivating one reasons 'm really like feedback please reach us stories ankush think create separate page feedback put motivated make courses future	1
use recording videos	ankush use quicktime mac use imovie edit 's n't see face chuckles like tool actually alexey use called loom press button right see starts recording saying recorded press another button creates thing share 's really cool convenient really love tool ankush edit alexey 's download button loom download use kdenlive actually finished editing video different course – ml zoomcamp one video kept putting 's done used kdenlive kde name comes linux works surprisingly well windows well use linux moved windows also works actually works even better linux find surprising course 's free open source ankush oh really 's impressive alexey loom free costs like 5 per month something like n't use courses things well 's free tool ankush things alexey well ... recording documentation let 's say chuckles ankush chuckles oh 're recording documentation putting olx alexey olx datatalks.club documenting processes tool use recording also free obs virtual camera start example use start recording capture screen save file victoria also uses obs recording videos ankush think best thing like loom feedback right video talking face alexey yeah face 's cool 's shareable immediately record “ okay 's homework solution. ” next day sleeping download upload youtube n't need share file well course 's unedited also edit upload youtube 's shareable immediately pretty cool n't know sejal used know uses recording know couple videos recorded zoom call recorded n't know used videos	1
use recording videos	alexey 's want look project 're behind right n't worry take time decide whether give certificate based projects homework 's – catch week 3 lighter week 1 maybe fast-forward directly docker compose run week 2 depending much time maybe go solution published today give everything need week 3 use transfer service video already watching video probably learn lot already like homework solution	0
use recording videos	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
use recording videos	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
use recording videos	alexey 33 submissions think decision extend deadline good one hopefully end week see higher number understand project much difficult watching course following along 33 people – probably put lot effort thanks saw one person write put 100 hours already project outstanding sorry cruel 100 hours lot	0
project week 7 free choose dataset provided datatalks.club	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
project week 7 free choose dataset provided datatalks.club	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
project week 7 free choose dataset provided datatalks.club	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	0
project week 7 free choose dataset provided datatalks.club	alexey yes 're free choose dataset people already reached asking good dataset think everyone asked said “ yes good dataset. ” one thread question images – whether 's good idea use images project n't think 's good idea use images quite lot things need addition cover course like image processing trying extract information images outside scope course first time ’ working images could quite difficult n't think 's realistic 'll able data engineering project thing top two weeks might end spending much time maybe try select less ambitious dataset 's table csv file parquet file good 's images probably better use 's text – text also need quite work process maybe 's natural language ’ also good idea use example common crawl dataset lot text probably could great dataset depending exactly want 're sure maybe share idea – share want – tell 's doable within couple weeks also want share datatalks.club ’ slack dump also quite lot structured information could used 's also grouped days – day json file build nice pipeline 'll probably make dump share channel see 's something want use	1
project week 7 free choose dataset provided datatalks.club	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
wondering deadline extension many project submissions	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
wondering deadline extension many project submissions	ankush planning syllabus actually removed nosql databases wanted focus important tools data engineering 's covering spark data warehouse kafka dbt airflow nosql definitely important part data engineers know community really wants maybe develop couple videos nosql use – maybe give example cassandra something alexey also ran survey started course eight different topics concerning community wants hear analytics engineering top one terms preference nosql last one whether internals nosql n't remember based decisions survey well seemed like much interest nosql databases lot interest analytics engineering affected way decided come syllabus course maybe also let us know specifically mind nosql broad term talking redis talking mongo cassandra dynamo exactly	0
wondering deadline extension many project submissions	alexey 33 submissions think decision extend deadline good one hopefully end week see higher number understand project much difficult watching course following along 33 people – probably put lot effort thanks saw one person write put 100 hours already project outstanding sorry cruel 100 hours lot	1
wondering deadline extension many project submissions	alexey well good job keep working almost finished first week finish start week two finish week three chuckles follow sequence videos playlist github sufficient think questions go slack ankush keep going alexey chuckles yes exactly ’ almost finished first week good sign first week year pretty tough many people lot problems docker whatnot 're almost finished good sign ’ good track keep working	0
wondering deadline extension many project submissions	ankush yes absolutely aws redshift would quite similar bigquery – quite similar still similar think aws redshift postgres cluster – big postgres cluster s3 definitely one one comparison google cloud storage 'm sure want use ec2 want deploy spark jobs maybe consider emr 're using ec2 maybe pipelining airflow ec2 also pretty good 's machine alexey used virtual machines many things would equivalent indeed need remote computer running data center s3 buckets fast download something s3 upload something s3 's usually way faster ec2 another similar concept athena think 's thing top data lake gives “ data warehouse feeling ” ankush 's sql top data lake alexey would say 's something like ... feels like something bigquery redshift 's fast 's also cheaper redshift ankush yeah think 're maybe comparing presto might better fit 's presto basically internally outside 's presto cluster alexey yeah presto need google things – like need google syntax athena – usually add presto end works ankush chuckles yeah exactly aws also managed workflow apache airflow 're thinking using airflow also check deploy less stuff alexey think google cloud also aws create emr ankush mentioned already ankush emr aws something like google cloud dataflow alexey create spark cluster one click think shared document explains configure cluster maybe 'll share office hours want use	0
seems code reviewed copied case	n't think takes lot time first need record videos prepare homework imagine adding collaboration startups well maybe sometime future n't know somebody startup listening right want hire people graduate zoomcamp please reach let 's arrange something	0
seems code reviewed copied case	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
seems code reviewed copied case	flask – let draw let 's say want create web service image 5 web service gets requests web service responds something could predictions could something else – n't related machine learning implement web service use flask use fastapi use framework creating web services use flask use something else – flask allows use post example let 's say “ get ” usually parse parameters query let 's say go google put “ test ” see q=test https //www.google.com/search q=test oq=test aqs=chrome.0.69i59l2j69i65l3j69i60j69i61l2.1264j0j1 sourceid=chrome ie=utf-8 image 6 parameter pass “ get ” request “ get ” request sometimes let 's say want score customer n't want put gender=female contract=one year string send json post post also add body request generally use post general use case flask able create web service	0
seems code reviewed copied case	issue like – see code copied n't explicitly say 's allowed assumed 's kind clear 's okay copy code say 's see something like see part project copied example part training model productionizing give zero points part copied – model training example rest use criteria table next project explicitly saying copying tolerated result zero points see somebody copied work put zero everywhere one – next project also speaking next project think 's actually good idea start thinking right later already know projects look expected capstone project similar one already start thinking capstone project well	1
seems code reviewed copied case	eventually want analysis see much time people spent homework also want breakdown know people registered – sign course say 're data analyst data engineer student also want see breakdown per role – much time took let 's say software engineers think week n't difficult engineers probably difficult students perhaps maybe weeks way around want analytics point still finish editing videos week six first 'll need prepare guidelines regarding projects maybe 'll analytics	0
know homework accepted	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
know homework accepted	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
know homework accepted	alexey google form works fine n't worry clicked “ submit ” worked accepted guess question scores want see answers correct show solution wednesday 'll upload video solution also sort leaderboard maybe actually show already something similar course leaderboard shows many points get homework n't show email course 's hash email see many points got question result get leaderboard total amount points know soon answers correct	1
know homework accepted	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
know homework accepted	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	0
advice story friends shifted dba data engineer want shift career five years working dba data engineer	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
advice story friends shifted dba data engineer want shift career five years working dba data engineer	alexey ankush know anyone shift like ankush really n't know anyone like alexey actually 'm thinking people shifted data engineering mostly software engineers – java developers example also know python developer became data engineering know php developer became data engineer mostly come software engineering dbas usually see become platform engineers devops engineers focus ops part rather engineering part perhaps maybe natural transition maybe first get infra side things go data engineering maybe ankush always say “ data engineering ” broad term job profile dataops basically would good fit dba analytics focused towards analysis write sql scripts already sql dba shift would also pretty natural respect least working sql writing pipelines sql get comfortable start learning python think pick different technologies think appropriate interesting	1
advice story friends shifted dba data engineer want shift career five years working dba data engineer	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
advice story friends shifted dba data engineer want shift career five years working dba data engineer	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
advice story friends shifted dba data engineer want shift career five years working dba data engineer	victoria 'm sure answer mean yes definitely adding part project least workshop reason want people learn also consider people entirely new concept thought would much overhead try cover things already quite complex day-to-day even though work daily dbt work slow-changing dimensions daily basis example 's also think 's something look moment use definitely dbt especially deletions implementing load step done hooks think 's section advanced knowledge link hooks like incremental models mentioned changing dimensions n't link could link something could use snapshots think added concept updates guess could also pre-hook post-hook also something would incremental model incremental model loads new chunk data use merge depending workers use use could insert update 's something would work unique key project also final project 's understand complex go 's limit hope answers	0
deploy airflow kubernetes kubernetes operator airflow	sejal yes really kubernetes expert maybe alexey help alexey bit googling see kubernetes operator probably answers yeah operator use kubernetes internally think create kubernetes job execute typically use kubernetes operators use aws batch operators essentially thing except 're running kubernetes aws batch see usually pattern exactly use airflow use python operators bash operators – execute things workers – usually make workers pretty dumb n't lot resources instead delegate external compute environment like kubernetes aws batch maybe ecs alternatives google cloud azure state much resources need specific job want run docker image parameters executed somewhere – worker pattern see usually 's either sql operators execute bigquery athena presto something else kinds aws batch kubernetes jobs spark example	1
deploy airflow kubernetes kubernetes operator airflow	alexey think 's helpful 's necessary helpful happens dags run airflow dags tasks tasks often kubernetes jobs something like infra – many companies infrastructure managed kubernetes 's quite simple get new container execute new job kubernetes n't think 's necessary think 's helpful 'm data engineer – 'm data scientist data scientist knowledge kubernetes quite helpful could debug jobs 'm running airflow run kubernetes could see logs would say want learn hurt get hired without kubernetes knowledge materials course sufficient get hired pick kubernetes work example agree victoria victoria yeah think kubernetes getting popular ca n't hurt	0
deploy airflow kubernetes kubernetes operator airflow	victoria 'm sure answer mean yes definitely adding part project least workshop reason want people learn also consider people entirely new concept thought would much overhead try cover things already quite complex day-to-day even though work daily dbt work slow-changing dimensions daily basis example 's also think 's something look moment use definitely dbt especially deletions implementing load step done hooks think 's section advanced knowledge link hooks like incremental models mentioned changing dimensions n't link could link something could use snapshots think added concept updates guess could also pre-hook post-hook also something would incremental model incremental model loads new chunk data use merge depending workers use use could insert update 's something would work unique key project also final project 's understand complex go 's limit hope answers	0
deploy airflow kubernetes kubernetes operator airflow	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
deploy airflow kubernetes kubernetes operator airflow	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
use model2bin last question mentioned anywhere homework.md	two people want work dataset individually please feel free imagine way control go kaggle select competition even ’ accidentally happen long 's exactly code – long n't copy – ’ fine yeah please n't copy code	0
use model2bin last question mentioned anywhere homework.md	yes mentioned mentioned homework use docker image prepared actually done purpose 's possible run model1.bin without docker get question wanted execute docker model2.bin available anywhere docker image instruction asking base solution image	1
use model2bin last question mentioned anywhere homework.md	's interesting one let 's say numpy array image reference numbers – 1 6 – compute mean mean 3.5. add 3.5 multiple times mean n't change happens nan pandas simply ignores – n't look pretends 's basically pandas ignores nans looks numbers replace nans mean case 3.5 get data effectively mean n't change try show formula let 's say bit math let 's say four cities couple n/as basically part usual values part n/as missing part let 's say want compute mean .mean pandas happens let 's say n numbers like k missing values total series k+n – total length compute mean part get 1/nσxi=x̄ mean case 3.5. let 's replace n/as x̄ happens instead looking 1/n look 1/n+k total number elements already filled sum another sum x̄ k n summed k times x̄ becomes kx̄ formula like multiply n/n n/n simply 1. 's multiplying thing one get n x̄ part mean image reference clue 'm talking bear 's almost result 1/ n+k n x̄+kx̄ leave x equation 1/ n+k n+k x̄=x̄ basically shows add many means like result always x̄ 'm sure everyone enjoyed little proof image reference prove saw question meeting got curious like “ show mathematically ” spent like 10 minutes trying figure nice exercise brain 'm sure would able show proof spot honest	0
use model2bin last question mentioned anywhere homework.md	flask – let draw let 's say want create web service image 5 web service gets requests web service responds something could predictions could something else – n't related machine learning implement web service use flask use fastapi use framework creating web services use flask use something else – flask allows use post example let 's say “ get ” usually parse parameters query let 's say go google put “ test ” see q=test https //www.google.com/search q=test oq=test aqs=chrome.0.69i59l2j69i65l3j69i60j69i61l2.1264j0j1 sourceid=chrome ie=utf-8 image 6 parameter pass “ get ” request “ get ” request sometimes let 's say want score customer n't want put gender=female contract=one year string send json post post also add body request generally use post general use case flask able create web service	0
use model2bin last question mentioned anywhere homework.md	's broad question really depends background let 's say 're data analyst probably learn things differently software engineer ’ come background probably want take different approach really depends background helpful course coursera andrew ng ’ old – around 2012. one first courses took another thing helped kaggle course current course based book machine learning bookcamp hope point somebody say must-have resource beginner machine learning ai 'm bit biased course might say n't know 's necessarily true 'll leave decide whether 's case	0
'm using jupyter vscode directly plugins rather running jupyter server using web browser cons approach	really depends cloud showed elastic beanstalk n't think needed change much anything n't remember changing anything thing changed url service test.py script thing needed change think heroku 's similar actually couple tutorials using pythonanywhere heroku probably explain need change go see need change anything probably much	0
'm using jupyter vscode directly plugins rather running jupyter server using web browser cons approach	n't think works go ahead n't really tried know works n't experimented think 'm used screen image 5 n't even use jupyter lab used jupyter notebooks maybe 'll try n't think cons end 's jupyter n't matter exactly use	1
'm using jupyter vscode directly plugins rather running jupyter server using web browser cons approach	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
'm using jupyter vscode directly plugins rather running jupyter server using web browser cons approach	took course ago back useful think something use python immediately could helpful 'm sure would good recommendation current course course good would n't compare stanford course andrew ng simply different course 'll get lot theory one practical hands-on – 's mostly writing code together maybe take	0
'm using jupyter vscode directly plugins rather running jupyter server using web browser cons approach	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
would consider displaying question submitted slido based arrival time	victoria would say ’ also 's different set products example data warehouse start data tech stack scratch would definitely use redshift cost lower end need probably would need migrate something like bigquery snowflake lot documentation 's pretty easy set 's around longer – think reason broader adoption mainly around things alexey seems aws popular well least berlin look companies use cloud maybe 70 people use aws 20 may use gcp remaining 10 use azure 's roughly see maybe 'm wrong course also got use gcp bit seems ui nicer go web console 's little bit nicer think comes tools 's bit difficult example aws pip install aws cli google cloud 's bit difficult maybe 's one reason aws popular – ’ mature maybe think gcp catching nice interface sometimes things cheaper bigquery also think big advantage maybe clouds n't really victoria plus aws around 10 years thesis using aws thing gcp yet 's lot people go 're familiar outside stability alexey think cloud services services quite similar ’ used aws many things gcp straightforward n't typical less think many concepts map one another	0
would consider displaying question submitted slido based arrival time	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
would consider displaying question submitted slido based arrival time	ankush yes yes think transfer service right yes question data using aws want run test around data source feel free also leave comment maybe even know goes azure blob storage	0
would consider displaying question submitted slido based arrival time	alexey think question already asked last time 'm sure makes sense cases follow question question maybe think voting makes sense maybe question interesting 3 4 5 people voting natural n't know maybe ankush think voting beneficial bigger group question answered please ping slack 'm pretty sure somebody pick	1
would consider displaying question submitted slido based arrival time	victoria 'm sure answer mean yes definitely adding part project least workshop reason want people learn also consider people entirely new concept thought would much overhead try cover things already quite complex day-to-day even though work daily dbt work slow-changing dimensions daily basis example 's also think 's something look moment use definitely dbt especially deletions implementing load step done hooks think 's section advanced knowledge link hooks like incremental models mentioned changing dimensions n't link could link something could use snapshots think added concept updates guess could also pre-hook post-hook also something would incremental model incremental model loads new chunk data use merge depending workers use use could insert update 's something would work unique key project also final project 's understand complex go 's limit hope answers	0
format submissions week 2 homework	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
format submissions week 2 homework	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
format submissions week 2 homework	alexey n't created form yet week one google form choose questions trust actually homework code guess answers without work said able work week 3. better properly maybe 'll include checkbox form pinky-swear code course include link code time may randomly pick submissions check code	1
format submissions week 2 homework	alexey n't think really understand question ankush 're vpc matter alexey cluster – spark cluster cluster kafka cluster ankush 're running vpc able access 's ip right alexey yeah oh okay externally ip guess 're running inside network use internal ip 's inside kubernetes refer names deployments n't know ankush also n't know 's really specific particular use case need give us maybe slack explain bit – running kind machines exactly exposing terms external ip alexey usually kafka cluster kafka machines spark cluster spark cluster connects kafka reads data ankush depends vpc running 're running different vpcs need expose separately need expose ip running vpc matter able access internally might wrong 'm devops chuckles alexey usually take things granted maybe good thing work grateful data team makes tools work often n't ask “ okay actually configured ” works n't – n't go support channel ask “ hey please fix ” ankush think 's also data team 's also maybe devops team embedded inside data team 're working olx pretty big company maybe many smaller teams inside big teams guess 's devops topic	0
format submissions week 2 homework	ankush yes yes think transfer service right yes question data using aws want run test around data source feel free also leave comment maybe even know goes azure blob storage	0
propose designing total cloud-based automatic pipelines using gcp using gcp tools project address reproducibility case	alexey well documenting project make sure write everything show run thing creating instructions etc maybe one example could let 's say de zoomcamp – take one modules n't think code good let 's go docker example documentation actually shows need run thing something like maybe little bit descriptive first describe project run things – things needed accounts need create commands need run somebody 's looking n't see blank readme.md file information follow along information sequence execute things works mean reproducibility one tool like make even easier make create bunch make files cover course 're pretty useful maybe show example instead typing long commands “ make ” executes 's cool thing minimum documentation describes need code snippets – mean reproducibility requirements.txt file also important ideally pin versions write python_kafka==version also good ankush would also say using cloud-based assume person 's reviewing would also cloud google cloud platform would available person use terraform 're creating buckets use terraform maybe readme say “ first step would create bucket create bucket using terraform file 's run it. ” person would need google cloud authentication way google cloud authentication also highlight documentation “ first step would google cloud authentication ” maybe use criteria cloud provider proceed alexey yeah please n't forget steps like authentication assume authenticated run might error	1
propose designing total cloud-based automatic pipelines using gcp using gcp tools project address reproducibility case	ankush planning syllabus actually removed nosql databases wanted focus important tools data engineering 's covering spark data warehouse kafka dbt airflow nosql definitely important part data engineers know community really wants maybe develop couple videos nosql use – maybe give example cassandra something alexey also ran survey started course eight different topics concerning community wants hear analytics engineering top one terms preference nosql last one whether internals nosql n't remember based decisions survey well seemed like much interest nosql databases lot interest analytics engineering affected way decided come syllabus course maybe also let us know specifically mind nosql broad term talking redis talking mongo cassandra dynamo exactly	0
propose designing total cloud-based automatic pipelines using gcp using gcp tools project address reproducibility case	victoria yes weeks belong one bigger project taxi data new york shown beginning cover different stages idea also use afterwards build project final homework	0
propose designing total cloud-based automatic pipelines using gcp using gcp tools project address reproducibility case	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
propose designing total cloud-based automatic pipelines using gcp using gcp tools project address reproducibility case	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
similar tools aws azure dbt	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
similar tools aws azure dbt	ankush append parquet file sejal think probably meant – think alexey also talking – incremental loads since end result week 2 bigquery external table think default 's append-only mode right alexey alexey executing saw… executed bunch dags runs went bigquery see results table created first dag run basically n't append table reason n't know think saw week 3 videos way create statement create table needs use kind sort wildcard say “ 's 2019 minus 10 ” bigquery picks files pattern probably typically n't know – 'm bigquery expert least week 2 need upload data google cloud storage need create bigquery tables create bigquery table week 3 ankush show – saw video sql statement creating table ankush bigquery yes need use wildcards pick multiple files 's aim would still append parquet file parquet files meant purpose 're append-only kinds file 're like columnar storage basically append need rewrite whole thing case would suggest basically keep open-ended close file 're done writing probably let bigquery read side writing bigquery table internally think 's also covered later videos bigquery internally figures couple writes let 's say hour figures imbalance internal file structure reclusters automatically reshuffles data creates new file n't really worry cost side 's internally bigquery question alexey maybe add words way see usually happens folder data lake specific date let 's say first structure name whatever table year month day within folder bunch parquet files want add information add another parquet file folder way get data partition	0
similar tools aws azure dbt	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
similar tools aws azure dbt	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
similar tools aws azure dbt	victoria set warehouse end define connection difference would use adapter dbt several adapters would first want use dbt go check adapters make sure 's supported several official example redshift could use community-supported azure alexey yeah aws athena azure athena redshift presto well example presto cluster use aws – within aws presto cluster potentially similar setup connect dbt presto victoria general add little bit answer let 's say project right bigquery would change profile 's set connection may make changes 's something sql supports bigquery way around able run project minutes	1
data shuffling possible key spills another partition partition size limit another shuffle required	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
data shuffling possible key spills another partition partition size limit another shuffle required	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
data shuffling possible key spills another partition partition size limit another shuffle required	alexey ankush 're shaking head – means 's possible ankush depends upon operation operation key value pairs node happen node run memory either need upgrade machines high memory need find skewed keys basically unskew would case let 's assume something like account something like average average would also work average example intermediate nodes precalculation give precalculation ahead nodes cases imagine shuffling happening one node basically data getting one node something happening three steps happening cases might still able run limited memory something like join need think data skew maybe memory alexey data skew think ankush sent link deal data skew think article joins ankush agees maybe share link later quite useful understand joins work spark addition already explained scratched surface 'll try find maybe post ankush also look think slides	1
data shuffling possible key spills another partition partition size limit another shuffle required	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
data shuffling possible key spills another partition partition size limit another shuffle required	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
hash tag something similar find public learning contents others	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
hash tag something similar find public learning contents others	’ two weeks october 18 2021 meaning first november 2022	0
hash tag something similar find public learning contents others	yes hashtag hashtag mlzoomcamp use – put twitter linkedin think 'll find posts course use hashtag social posts well	1
hash tag something similar find public learning contents others	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
hash tag something similar find public learning contents others	think people good intentions n't think somebody intentionally give bad score somebody wants intentionally give bad scores please n't would seriously please n't 're learn somebody 's giving best see matrix use evaluating give score person deserves n't link coursera actually research peer reviewing found peer reviews actually work quite well people n't give bad scores sake another reason three people reviewing one project exactly avoid one person giving everyone bad scores 'll take median avoid 's three scores also think opportunity get feedback people write something saying “ hey problems running this. ” 's good somebody tries actually run project n't answer apart please bit trust people think good intentions	0
pickle load object methods associated object also loaded	kind actually pickle expects code let 's say pickle object particular class let 's say class could part package scikit learn linear class logistic regression pickle loads object expects class present python loads methods code – module expects code basically loads data behavior stored code code	1
pickle load object methods associated object also loaded	reason think flask popular framework 's relatively simple 's main reason	0
pickle load object methods associated object also loaded	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
pickle load object methods associated object also loaded	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
pickle load object methods associated object also loaded	alexey sure maybe answer thing talked regarding filters well use matplotlib take filter then… dmitry think pil interesting option 's images guess alexey yeah pil 's like want save jpeg example want plot see looks matplotlib probably sufficient homework also use matplotlib n't use pil 48:08 0 upvotes use dogs vs cats main capstone project better performance dmitry guess maybe datasets alexey yeah datasets dmitry mean lot open datasets guess alexey cats dogs yeah dmitry mean think cats dogs another time would n't interesting alexey yeah add data interesting dmitry maybe cats versus dogs versus something else alexey wombats cats versus wombats chuckles think image net must wombats right dmitry yeah alexey take part image net get wombats dmitry fun alexey laughs maybe wombats versus… similar wombats wombats cats quite different right different environments 's easy task dmitry maybe wildcats alexey yeah saw wildcat zoo looked like normal cat suspect maybe put normal cat said 's wildcat chuckles dmitry better marketing chuckles	0
way visualize images become passing filters	dmitry regarding question – “ filters ” mean regarding data augmentation alexey think talking convolutional filters apply filter image get feature map dmitry image anymore alexey yeah remember first big architecture alexnet – article maybe even original paper alexnet showed filters filters different like stripes color color shapes something like actually n't know ’ let 's say take filters exception try visualize principle filters available weights layer maybe use matplotlib try see ’ done remember seeing paper alexnet found really interesting stripes comment somebody saw alexnet paper	1
way visualize images become passing filters	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
way visualize images become passing filters	took course ago back useful think something use python immediately could helpful 'm sure would good recommendation current course course good would n't compare stanford course andrew ng simply different course 'll get lot theory one practical hands-on – 's mostly writing code together maybe take	0
way visualize images become passing filters	yes submit anywhere want say 20 pt n't remember yeah work good article “ good ” meaning 's copy/pasting code nothing else actually need explain 's going also article – closer time working article guidelines well	0
way visualize images become passing filters	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
use functional versus sequential	n't think takes lot time first need record videos prepare homework imagine adding collaboration startups well maybe sometime future n't know somebody startup listening right want hire people graduate zoomcamp please reach let 's arrange something	0
use functional versus sequential	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
use functional versus sequential	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
use functional versus sequential	actually time go functional 's habit reason found many tutorials example transfer learning use use functional style complex models also use functional style 's 'm kind used functional style 's thought would good idea cover functional style functional style easily go sequential go sequential think 's difficult understand functional style	1
use functional versus sequential	building locally fine	0
deploying machine learning expected predict also automate decision-making	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
deploying machine learning expected predict also automate decision-making	flask – let draw let 's say want create web service image 5 web service gets requests web service responds something could predictions could something else – n't related machine learning implement web service use flask use fastapi use framework creating web services use flask use something else – flask allows use post example let 's say “ get ” usually parse parameters query let 's say go google put “ test ” see q=test https //www.google.com/search q=test oq=test aqs=chrome.0.69i59l2j69i65l3j69i60j69i61l2.1264j0j1 sourceid=chrome ie=utf-8 image 6 parameter pass “ get ” request “ get ” request sometimes let 's say want score customer n't want put gender=female contract=one year string send json post post also add body request generally use post general use case flask able create web service	0
deploying machine learning expected predict also automate decision-making	classification classification let 's say n't predicting price let 's say predicting 's average number hours image 1 case would need another layer – rather layer 's called activation n't activation 're still… ’ trying formulate way 's confusing think 's better go lecture rewatch ’ essentially retelling thing need from_logits price variable loss mean squared error logits loss categorical cross-entropy categorical cross entropy usually used multiple categories need use logits 's numerically stable n't use – stay activation softmax n't use legit hope 's clear regression n't need use homework need use from_logits either homework two classes 's binary classification problem multiple classes – let 's say three classes four classes five classes – need use logits loss categorical cross-entropy loss categorical cross-entropy 's recommended use from_logits=true 's numerically stable	0
deploying machine learning expected predict also automate decision-making	ideally need think users application want predict want make decision think usually 's latter – users application interested raw predictions let 's take look example image 1 explicitly say decision say user churn “ okay send email campaign user. ” output probability maybe users service would like “ okay 's 72 probability churning ” 's think 's good idea actually automate decision-making basically make decision user	1
deploying machine learning expected predict also automate decision-making	building locally fine	0
late start project today	think one day half try actually want remind everyone third project optional 're behind midterm project keep working project pace without worrying deadlines time comes submit third project take project submit third project way able still submit something even skip week reason keep working project start today keep working submit third project fine	1
late start project today	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
late start project today	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
late start project today	answer – depends dataset – typically train/validation split first try scaling try without scaling see happens think something talked last week actually notebook prepared last week go classification notebook notebook scaling shows scale features use standard scaler use min max scaler – use two different scalars shows particular dataset turned little bit better experiment see scaling makes sense linear models sometimes 's actually good idea add extra complexity pipeline instead dictionary vectorizer whatnot 'll also need add next step scaling basically steps prediction pipeline becomes bit difficult maintain would say try scale see improvement get significant maybe 's worth extra complexity get use cross-validation use best judgment see makes sense	0
late start project today	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
plus come second last step	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	1
plus come second last step	example go kaggle see number notebooks available particular competition could good indicator 's good dataset n't really talk multi-class classification think 's okay want multi-class classification first dataset probably either binary classification see target clearly zero one think go already know deal numerical variables know deal categorical variables idea deal missing data see dataset things 's similar dataset good one 're sure ask slack help	0
plus come second last step	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
plus come second last step	flask app produce result actually return prediction 're good n't need fix answer question regarding many points deducted think whole project kind assumes managed deploy flask n't able put docker able deploy cloud lose like five six points something along lines think 's good idea make sure flask returns something problems please share code think know asking question talked already slack maybe error somewhere predict.py script need make sure fix error work perhaps take code lectures – think xgboost model logistic regression need put x variable t-matrix xgboost instead using predict_proba need use predict think 's – n't work please write slack let 's try figure 's working	0
plus come second last step	think showed multiple times lessons use validation dataset compare different models validation dataset one best performance final model good rule thumb 's always end story remember talked cross-validation let 's say cross-validation see model good performance also high standard deviation 's great model probably want model lower standard deviation even performance slightly worse right might much information stick rule whatever best performance validation dataset wins later work develop intuition actually pick best model	0
grade homework	use cross-validation check validation get better scores – drop one drop one go sometimes makes sense drop even decrease score simply maybe one feature complex compute another one	0
grade homework	honest n't used uvicorn used gunicorn n't know matters n't actually checked benchmarks use gunicorn also use others – n't remember ones reason basically	0
grade homework	like said 's always problem- data-specific datasets get good performance	0
grade homework	let 's say two dimensional thing 2d want turn 1d case need use pooling2d 'm sure right want quickly check reason remember showing usually go defining model build sort layer layer every time add one layer model predicts see output based see kind pooling layer example need let 's say want turn 2d 1d think need use 2d pooling 'm exactly sure whether 's 2d 1d think 1d pooling needed something one dimensional want turn one value use 1d pooling 3d pooling three dimensional thing want turn one-dimensional thing image 2 things always confuse honest 's follow step step try different poolings want make sure convert image vector presentation something one-dimensional usually size number images times something 's like 2d array image one-dimensional vector based try different poolings sometimes also flatten flatten takes whatever – let 's say kd – want turn 1d use flatten many different options think 's clear difference might remember exactly use 1d 2d difference kind input take kind output produce 's cube 's 2d 's hypercube three dimensions 's something else	0
grade homework	yes n't written script yet actually want – many people asked see results made mistake beginning letting people register n't add checkbox saying 's okay use data like first name last name public really public leaderboard everyone go check score simply n't ask permission want instead write web service ’ email field go put email get scores idea hope 'll able implement hopefully n't difficult able get scores way yes grade homework store scores course end remember first video talked public leaderboard end course 100 people leaderboard ask permission publish names 'll need figure exactly communicate results separate homework grading actually difficult want write script	1
model set binary options	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
model set binary options	yes submit anywhere want say 20 pt n't remember yeah work good article “ good ” meaning 's copy/pasting code nothing else actually need explain 's going also article – closer time working article guidelines well	0
model set binary options	yeah think options think shared link slack check options example save model json file save pmml standard predictive model markup language different way exporting models think also works scikit learn library converting scikit learn models format find easiest pickle also use joblib essentially almost thing pickle many advantages – model becomes smaller course disadvantages become dependent particular version code scikit learn gives warnings try load model saved one version want load different version gives warnings also think logistic regression ’ bunch coefficients bias term export json file load write code	1
model set binary options	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
model set binary options	really depends cloud showed elastic beanstalk n't think needed change much anything n't remember changing anything thing changed url service test.py script thing needed change think heroku 's similar actually couple tutorials using pythonanywhere heroku probably explain need change go see need change anything probably much	0
one move junior data engineer senior data engineer lead engineer	victoria 'm sure answer mean yes definitely adding part project least workshop reason want people learn also consider people entirely new concept thought would much overhead try cover things already quite complex day-to-day even though work daily dbt work slow-changing dimensions daily basis example 's also think 's something look moment use definitely dbt especially deletions implementing load step done hooks think 's section advanced knowledge link hooks like incremental models mentioned changing dimensions n't link could link something could use snapshots think added concept updates guess could also pre-hook post-hook also something would incremental model incremental model loads new chunk data use merge depending workers use use could insert update 's something would work unique key project also final project 's understand complex go 's limit hope answers	0
one move junior data engineer senior data engineer lead engineer	alexey week 2 yes – upload video solution also update leaderboard probably take time depending clean data – need lot data cleaning take lot time please n't put things n't look like emails email field fill please careful takes time clean later	0
one move junior data engineer senior data engineer lead engineer	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
one move junior data engineer senior data engineer lead engineer	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
one move junior data engineer senior data engineer lead engineer	ankush vic experience lead right feel free jump think one important things data engineer know lot things need know different stuff might want expert spark still would need knowledge kafka want expert analytical engineering go deep dbt sql still need knowledge data warehousing go towards senior data engineer feel need least knowledge areas also need core competency least one two areas mid-level wanted go senior realized company really depended upon kafka luckily also interesting area took lot interest developed competency time also interested data lakes spark gave opportunity work projects could lead senior data engineer also leading team later alexey would also add check datatalks podcast cover things like maybe specific data engineers think find talks interesting maybe another podcast episode n't think cover detail 10 minutes	1
data models use warehousing star schema data vault 2.0 ... 's “ relevant ” nowadays	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	0
data models use warehousing star schema data vault 2.0 ... 's “ relevant ” nowadays	ankush simple answer “ n't know. ” chuckles think save question victoria think would best person answer maybe put slack channel tag alexey also example taxi trip data – use star schemas anything well let 's kind star one ray right chuckles ankush kind like think common one star n't know done enough data modeling know data vault 2.0 would provide us particular dataset n't know 's relevant thing right honestly alexey look criteria 'll notice dimension called “ data modeling ” instead dbt ... n't use dbt things spark like spark n't like spark go dbt even though little bit different end 're sometimes interchangeable example maybe put data data lake transform data spark put data warehouse form ready queried dashboard decide put raw data data warehouse transformation dbt 's go either approach n't think right wrong ankush agrees	1
data models use warehousing star schema data vault 2.0 ... 's “ relevant ” nowadays	alexey one thing use images unless really know 're probably n't super small dataset one megabyte ideally victoria probably one file taxis four least two think also 's important understand course focused concepts everything 're going use focused building pipeline processing data actual presentation maybe 's super cool analysis images saying reality need practice processing data 's 's easier use something else alexey maybe come across dataset 're sure 's good dataset ask slack 'll help think list gave quite good 's good start course talk kaggle tons different datasets contain images tweets example way also use tweets structured information extract tweets well maybe actually good let 's say want practice streaming get stream tweets coming twitter process fly think could also good project sometimes also parse something example websites like ebay selling buying things build parser getting data thing could go every day take data something data put csv example put data warehouse something like n't spend much time scraping think already scrapers available maybe google “ ebay scraper ” find code n't spend lot time trying build scraper victoria also something like twitter suggested instagram well – apis makes part super easy want something like find something already api 's probably easiest option also get practice also good	0
data models use warehousing star schema data vault 2.0 ... 's “ relevant ” nowadays	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
data models use warehousing star schema data vault 2.0 ... 's “ relevant ” nowadays	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
minor side note reason display slido questions sorted popular recent	victoria set warehouse end define connection difference would use adapter dbt several adapters would first want use dbt go check adapters make sure 's supported several official example redshift could use community-supported azure alexey yeah aws athena azure athena redshift presto well example presto cluster use aws – within aws presto cluster potentially similar setup connect dbt presto victoria general add little bit answer let 's say project right bigquery would change profile 's set connection may make changes 's something sql supports bigquery way around able run project minutes	0
minor side note reason display slido questions sorted popular recent	alexey guess way question many people want see answered 'll cover first makes sense victoria yeah makes sense guess questions questions relating topics ’ answering get last row get see 's maybe comes makes sense popular way get vote	1
minor side note reason display slido questions sorted popular recent	alexey add comment least workplace airflow instance set us data scientist would go instance airflow managed data engineers devops engineers would use never needed actually run docker compose moment multiple dags orchestration tool quite useful would personally maybe go luigi favorite one need think people well since people know luigi many like maybe airflow would actually better choice consider sejal would also like add question docker compose used docker compose airflow style use official setup seeing first time well 's unnecessarily complicated especially xcom variables forth generally docker compose acts wrapper docker containers production workflows docker containers used integration ci/cd pipelines wherever 're going deploy docker compose docker compose part helpful development environments case let 's say mock version airflow environment production want test dags local development environment use docker compose make things easier running simple commands like ‘ docker compose ’ would make fine instead running ‘ docker run container name etc whatever want ’ terms convenience terms usage ’ generally used development environments production alexey think first part question “ close usual data engineer role workflows ” workflows see work complex steps probably also case need start something already – would n't call complex pipeline – three four steps already something eventually might grow bigger bigger add steps ankush would also like add 're cloud maybe use airflow deploy docker maybe actually use service provided cloud provider one similar airflow think airflow would used internally like cloud composite google cloud platform would using terraform sejal yeah used aws step functions aws setup step functions combination lambda place airflow ankush said also something wanted say really depends production self-hosted cluster kubernetes wherever want use pure airflow version company could kind setup use native airflow cluster could cloud-based setup convenient option use managed services cloud composer aws also version called managed airflow	0
minor side note reason display slido questions sorted popular recent	alexey week 2 yes – upload video solution also update leaderboard probably take time depending clean data – need lot data cleaning take lot time please n't put things n't look like emails email field fill please careful takes time clean later	0
minor side note reason display slido questions sorted popular recent	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
many hours recommend invest course	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
many hours recommend invest course	yes good references – course course called machine learning zoomcamp cover parts except conda git assume midterm project use git conda – use conda python interpreter install pipenv manage dependencies n't know good references honest maybe n't know answered question hope references course good enough ’ please let know think improved think missing	0
many hours recommend invest course	yes would say course machine learning engineers data scientists want work machine learning engineer ’ suggest skipping kubernetes part would actually say quite important part n't skip machine learning engineer data scientist 's okay skip still think beneficial help stand data scientist example olx open new position make job posting within couple days get several hundred applications 's difficult select candidates many think want stand kind volume – among hundreds people ’ likely know use kubernetes therefore ’ suggest skipping would suggest still course feel free – like enjoy process n't like kubernetes 's fine think 're gon na find job n't need worry	0
many hours recommend invest course	alexey one thing maybe n't clear enough – please always include code solution maybe people n't watch first said n't submit code homework get zero points saw submissions people put dot empty put link towards data science article example please n't please put code n't 'll get zero points entire week 'll make explicit form week three well please n't forget put code	0
many hours recommend invest course	n't know 's easy 's job think 's difficult invest time know minimum requirements work minimum requirements minimum requirement end web service deployed docker try multiple models also description project basically 're good want spend time – means satisfy requirements get good enough score ’ worry also ask many hours actually invested quite interesting see much time people spent see students spend like 20 hours materials iterations course already know much time people need invest submit homework please tell much time spent	1
threshold standards many features use build good model	's interesting one let 's say numpy array image reference numbers – 1 6 – compute mean mean 3.5. add 3.5 multiple times mean n't change happens nan pandas simply ignores – n't look pretends 's basically pandas ignores nans looks numbers replace nans mean case 3.5 get data effectively mean n't change try show formula let 's say bit math let 's say four cities couple n/as basically part usual values part n/as missing part let 's say want compute mean .mean pandas happens let 's say n numbers like k missing values total series k+n – total length compute mean part get 1/nσxi=x̄ mean case 3.5. let 's replace n/as x̄ happens instead looking 1/n look 1/n+k total number elements already filled sum another sum x̄ k n summed k times x̄ becomes kx̄ formula like multiply n/n n/n simply 1. 's multiplying thing one get n x̄ part mean image reference clue 'm talking bear 's almost result 1/ n+k n x̄+kx̄ leave x equation 1/ n+k n+k x̄=x̄ basically shows add many means like result always x̄ 'm sure everyone enjoyed little proof image reference prove saw question meeting got curious like “ show mathematically ” spent like 10 minutes trying figure nice exercise brain 'm sure would able show proof spot honest	0
threshold standards many features use build good model	n't think really problem-dependent sometimes adding lot features hurt model n't want add many features hope everything work need careful sometimes adding feature result model degradation standard really problem-dependent problems need features problems need lot features example talked online advertisements models use millions features like device type – use one-hot encoding use one-hot encoding get millions features 's really problem-dependent	1
threshold standards many features use build good model	yes think showed 'm sure mean yeah think 's good idea	0
threshold standards many features use build good model	case everyone uses docker problem n't use docker n't get point using docker ’ run write “ sorry run this. ” hope n't happen 's actually whole idea behind docker – 's transferable different environments let 's say use gunicorn n't use docker able test case use scoring matrix evaluate accordingly gunicorn tick box would encourage still try learn submission even run reason think reason run something windows ubuntu gunicorn everything runnable	0
threshold standards many features use build good model	use cross-validation check validation get better scores – drop one drop one go sometimes makes sense drop even decrease score simply maybe one feature complex compute another one	0
explain functional-style coding 's different object-oriented programming use sequential object building network	building locally fine	0
explain functional-style coding 's different object-oriented programming use sequential object building network	think covered less “ functional-style coding 's different object-oriented programming ” – 'm sure 's related object-oriented programming functional-style sense think terms functions let 's say function function input function produces output case function something like keras layer dense size 10 “ dense 10 ” function input function could array dimensionality 183 output would like something size 10. kind forces think “ input output ” time put together multiple functions sort chain transformations one thing another thing even smaller thing chain course chain sequential way building models 're quite similar n't think “ sequential ” anything object-oriented programming particular case also think sequential – take something first operation apply second operation apply first function – f1 f2 n't explicitly say input output 's happening hood sequential model image 3 n't need define 's defined internally	1
explain functional-style coding 's different object-oriented programming use sequential object building network	let 's say two dimensional thing 2d want turn 1d case need use pooling2d 'm sure right want quickly check reason remember showing usually go defining model build sort layer layer every time add one layer model predicts see output based see kind pooling layer example need let 's say want turn 2d 1d think need use 2d pooling 'm exactly sure whether 's 2d 1d think 1d pooling needed something one dimensional want turn one value use 1d pooling 3d pooling three dimensional thing want turn one-dimensional thing image 2 things always confuse honest 's follow step step try different poolings want make sure convert image vector presentation something one-dimensional usually size number images times something 's like 2d array image one-dimensional vector based try different poolings sometimes also flatten flatten takes whatever – let 's say kd – want turn 1d use flatten many different options think 's clear difference might remember exactly use 1d 2d difference kind input take kind output produce 's cube 's 2d 's hypercube three dimensions 's something else	0
explain functional-style coding 's different object-oriented programming use sequential object building network	yes mentioned mentioned homework use docker image prepared actually done purpose 's possible run model1.bin without docker get question wanted execute docker model2.bin available anywhere docker image instruction asking base solution image	0
explain functional-style coding 's different object-oriented programming use sequential object building network	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
tips learn linux	classification classification let 's say n't predicting price let 's say predicting 's average number hours image 1 case would need another layer – rather layer 's called activation n't activation 're still… ’ trying formulate way 's confusing think 's better go lecture rewatch ’ essentially retelling thing need from_logits price variable loss mean squared error logits loss categorical cross-entropy categorical cross entropy usually used multiple categories need use logits 's numerically stable n't use – stay activation softmax n't use legit hope 's clear regression n't need use homework need use from_logits either homework two classes 's binary classification problem multiple classes – let 's say three classes four classes five classes – need use logits loss categorical cross-entropy loss categorical cross-entropy 's recommended use from_logits=true 's numerically stable	0
tips learn linux	n't think takes lot time first need record videos prepare homework imagine adding collaboration startups well maybe sometime future n't know somebody startup listening right want hire people graduate zoomcamp please reach let 's arrange something	0
tips learn linux	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	1
tips learn linux	honest idea question means… oh flt pt floating pointers okay yes numpy floating point errors n't know – computers let 's say multiply two numbers operations take two numbers see instead 3 3 followed lot zeros 4 image reference floating point operations precise computers 's summing multiplying little floating point arithmetic error numpy 's running computer 's also prone guess question “ keep using logs data exploration ” 're probably referring week 2 think use logarithm price main reason numerical instability – ’ floating point errors – 'll probably learn video simply distributions long tails please refer eda video probably second third explain need 's keep using logs n't apply logarithm 's difficult machine learning models actually learn data	0
tips learn linux	start working n't inform imagine everyone started informing take lot time answer finish answering questions first november yeah n't inform please write slack 're sure think good dataset go ahead use	0
future iterations ml de zoomcamps pick videos difference homework	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
future iterations ml de zoomcamps pick videos difference homework	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
future iterations ml de zoomcamps pick videos difference homework	alexey 's early talk future iterations data engineering zoomcamp think chuckles ml camp yes – videos difference homework 's still discussion n't know 'll see ankush yeah think slight discussion met sure going repeat ml de zoomcamp let 's see response participation feel definitely success definitely feel free pass around course give us feedback maybe make better decision alexey re-recording videos… 's much effort n't think 'll want let 's say ml zoomcamp example much effort 'd rather reuse videos instead making new ones hope videos good decide rerun need re-record yeah 'll see	1
future iterations ml de zoomcamps pick videos difference homework	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
future iterations ml de zoomcamps pick videos difference homework	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	0
use kdenlive editing videos	answer – depends dataset – typically train/validation split first try scaling try without scaling see happens think something talked last week actually notebook prepared last week go classification notebook notebook scaling shows scale features use standard scaler use min max scaler – use two different scalars shows particular dataset turned little bit better experiment see scaling makes sense linear models sometimes 's actually good idea add extra complexity pipeline instead dictionary vectorizer whatnot 'll also need add next step scaling basically steps prediction pipeline becomes bit difficult maintain would say try scale see improvement get significant maybe 's worth extra complexity get use cross-validation use best judgment see makes sense	0
use kdenlive editing videos	reason think flask popular framework 's relatively simple 's main reason	0
use kdenlive editing videos	matter fact soon editing three four right n't remember think four use editing videos frankly n't find anything else linux works surprisingly well	1
use kdenlive editing videos	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
use kdenlive editing videos	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
project involve deployment using flask django	yes think showed 'm sure mean yeah think 's good idea	0
project involve deployment using flask django	one think 's good way checking feature importance mathematical/intuitive reason behind let 's say even train first model see features already give good separation positive negative classes remember attrition auc interpretation auc probability randomly selected positive example higher score randomly selected negative example score also mean feature ’ randomly selected positive example value feature higher randomly selected negative example think probably makes sense try let 's say seniority – negative correlation maybe 's little bit complex already see important features maybe features already good enough maybe n't even need model use feature prediction roll second iteration proper model 's think 's good idea 's added exercise	0
project involve deployment using flask django	's really case-dependent saw slack remember correctly carlos asked needs drop features – seemed okay drop highly correlated feature assistant name target variable churn basically people work one particular person tend turn less people work somebody else think case seems like relevant feature include n't know – 's really case-dependent usually need sort domain expertise decide feature introduces leakage carlos ’ case think 's safe leave drop see performance validation dataset changes gives idea important feature	0
project involve deployment using flask django	please use flask want use django use well clear explaining exactly 're others use test ideally said everything docker image end people run test way n't really know django fastapi ’ flask n't docker image runnable	1
project involve deployment using flask django	classification classification let 's say n't predicting price let 's say predicting 's average number hours image 1 case would need another layer – rather layer 's called activation n't activation 're still… ’ trying formulate way 's confusing think 's better go lecture rewatch ’ essentially retelling thing need from_logits price variable loss mean squared error logits loss categorical cross-entropy categorical cross entropy usually used multiple categories need use logits 's numerically stable n't use – stay activation softmax n't use legit hope 's clear regression n't need use homework need use from_logits either homework two classes 's binary classification problem multiple classes – let 's say three classes four classes five classes – need use logits loss categorical cross-entropy loss categorical cross-entropy 's recommended use from_logits=true 's numerically stable	0
eda initial data frame df_full_train splitting correct strategy	correct strategy would look testing dataset usually let 's say eda work say use sql query get data computer initial analysis sql query select rows need training n't look test 's generally good idea look test ’ called data snooping bad might influence decision n't want n't want influence decision maybe look think “ okay things become better. ” maybe real pattern see new dataset pattern accidentally observed true try avoid snooping much possible use testing test models avoid looking tests costs basically	1
eda initial data frame df_full_train splitting correct strategy	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
eda initial data frame df_full_train splitting correct strategy	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
eda initial data frame df_full_train splitting correct strategy	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
eda initial data frame df_full_train splitting correct strategy	yes	0
'm still confused use data frame use spark sql advice simple case	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
'm still confused use data frame use spark sql advice simple case	alexey well use judgment want try force register account azure run want n't maybe look code see errors 's really ankush maybe matching ask criteria – like used – maybe match corresponding people alexey 's nice idea let 's see 'm afraid complicate things bit already script matching n't use criteria 's random ankush need modify script one column alexey yeah perhaps let 's see ankush easier enough group different things alexey create form submitting ask cloud used right ankush exactly right alexey makes sense machine learning zoomcamp – wanted re-execute whole thing re-executed 's really want learn project – execute grading also learning – want learn really want learn things work execute learn n't time since executing different cloud might take lot time – n't much time really ask invest something like five hours figuring azure works use judgment guess 's answer try matching minimize	0
'm still confused use data frame use spark sql advice simple case	alexey ask help 's difficult us prepare materials also create curated list frequently asked questions help us – 're already slack answering questions – help us 'll much appreciate sure 's physically possible us also invest time right least 'm sure 'll able please want happy support create pull requests example repo frequently asked questions section example readme – place many errors maybe create pull request saying “ error like please that. ” helpful many people slack think 's bit overwhelming right amount questions many repetitive kind wish list frequently asked questions well could easier direct 's putting together requires attention sejal another thing actually thinking maybe create github readme page faqs people create pull requests add questions cases pull requests repeated questions mark stale already go look end alexey said since big time crunch resource crunch – swamped preparing material well managing full-time jobs 're trying much someone volunteer prepare maybe notion doc example organize questions asked slack channel put onto notion doc google doc whatever like helpful us	0
'm still confused use data frame use spark sql advice simple case	victoria n't used anaconda think 's also documentation install dbt locally general 're using anaconda 're using environment install use homebrew 're using mac also use pip install pip install dbt postgres example need adapter dbt snowflake dbt big query – whatever want use locally 's 're installed set profiles.yml show videos create project clone whatever 's 's need use locally never install using anaconda 'm guessing 's probably something similar probably conda forge install like 's possible otherwise would go pip install alexey anaconda usually pip victoria yeah 're environment install like 's	0
'm still confused use data frame use spark sql advice simple case	ankush pretty similar spark sql offer everything limitations comes spark sql cases might want go towards “ raw ” level data let 's say – basically data frame rdd comfortable sql start sql start using spark sql probably quite cases would covered 're coming data science background data frame might fit naturally also choice make simple use cases spark sql works fine maybe n't work complicated optimized jobs alexey way often use try best worlds things writing sql query faster example need join group – simple cases would go sql need use udf something like would initial preprocessing sql sql execution sql returns another data frame execute things data frame well combine	1
review expected us try install/run code following instructions fine review documentation	alexey well depends much time ideally yes try run try learn much possible code 'm pretty sure try run learn lot projects know three projects might time actually know – life 's check documentation see things make sense n't see big mistakes jump run everything 's time please run please try learn much possible peers based heard students machine learning zoomcamp course learned lot trying run projects please way saw one projects run yandex cloud n't think would possible actually try reproduce run cases even able 's case thing review documentation	1
review expected us try install/run code following instructions fine review documentation	alexey think 's helpful 's necessary helpful happens dags run airflow dags tasks tasks often kubernetes jobs something like infra – many companies infrastructure managed kubernetes 's quite simple get new container execute new job kubernetes n't think 's necessary think 's helpful 'm data engineer – 'm data scientist data scientist knowledge kubernetes quite helpful could debug jobs 'm running airflow run kubernetes could see logs would say want learn hurt get hired without kubernetes knowledge materials course sufficient get hired pick kubernetes work example agree victoria victoria yeah think kubernetes getting popular ca n't hurt	0
review expected us try install/run code following instructions fine review documentation	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
review expected us try install/run code following instructions fine review documentation	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
review expected us try install/run code following instructions fine review documentation	victoria 'm sure answer mean yes definitely adding part project least workshop reason want people learn also consider people entirely new concept thought would much overhead try cover things already quite complex day-to-day even though work daily dbt work slow-changing dimensions daily basis example 's also think 's something look moment use definitely dbt especially deletions implementing load step done hooks think 's section advanced knowledge link hooks like incremental models mentioned changing dimensions n't link could link something could use snapshots think added concept updates guess could also pre-hook post-hook also something would incremental model incremental model loads new chunk data use merge depending workers use use could insert update 's something would work unique key project also final project 's understand complex go 's limit hope answers	0
quickly summarize upload data google cloud platform without using airflow way transfer data gcp cloud storage bigquery without airflow	alexey try first transfer service use aws account reason need aws account asks enter secret access keys basically even though data publicly available still need also alternative think actually something sejal shared script use run preferably run cloud virtual machine quite fast uncomment need let 's say need green data 2019 run need yellow data uncomment script downloads file saves csv also saves parquet think 's uploaded parquet file need upload csv probably replace somehow yeah probably comment parquet part without airflow probably easiest transfer service victoria also something similar script sejal chose maybe best way drag drop csv backup backup like ankush explained first video three videos also alexey download data script select drag drop google cloud storage 's 's smart victoria best way n't data engineer got ta get thing somehow laughs yeah guess question much targeted 're general many tools could use well believe mentioned fivetran stitch general guess tool like could used alexey without air flow show videos right ankush insert something select start external table inserts data ankush also way directly upload bigquery chose external table 's easy	1
quickly summarize upload data google cloud platform without using airflow way transfer data gcp cloud storage bigquery without airflow	victoria would say yes go-to dbt yes dbt enforces go read viewpoint 'll explain also models kind structure mentioned modern concepts concept comes 80s kimball defined kitchen analogy find useful normally go course complex gets may need steps 's okay always try separated – presentation source need typecasting like example maybe duplication middle may models go data pipeline let ’ say flow less would look like alexey kitchen analogy victoria kitchen analogy way model data similar restaurants model food warehouse raw food source data everyone allowed fact 's actually dangerous 's security measures actually go warehouse stored stored things like kitchen people cook raw food make food 're going serve allowed would data engineers data analysts maybe analytics engineers would data warehouse trying process raw data end part restaurant people eat – dining hall presentation layer data warehouse fact tables look like data marts 're building data marts business stakeholders everyone allowed get foot let ’ say restrictions 's already presented 're presenting raw alexey think tableau self-service tool go cubes presentation layer right victoria yeah technically bi tools self-serve could everything could expose everything right external tables create tables inaudible create section model create factories could everything could even ones development exposed self-service bi tool like tableau looker whatever analogy says use presentation layer parts present raw data people would n't know use 's going like “ oh n't find phones ones maybe 's transformation ” things like alexey nice analogy	0
quickly summarize upload data google cloud platform without using airflow way transfer data gcp cloud storage bigquery without airflow	alexey 's early talk future iterations data engineering zoomcamp think chuckles ml camp yes – videos difference homework 's still discussion n't know 'll see ankush yeah think slight discussion met sure going repeat ml de zoomcamp let 's see response participation feel definitely success definitely feel free pass around course give us feedback maybe make better decision alexey re-recording videos… 's much effort n't think 'll want let 's say ml zoomcamp example much effort 'd rather reuse videos instead making new ones hope videos good decide rerun need re-record yeah 'll see	0
quickly summarize upload data google cloud platform without using airflow way transfer data gcp cloud storage bigquery without airflow	victoria 'm sure answer mean yes definitely adding part project least workshop reason want people learn also consider people entirely new concept thought would much overhead try cover things already quite complex day-to-day even though work daily dbt work slow-changing dimensions daily basis example 's also think 's something look moment use definitely dbt especially deletions implementing load step done hooks think 's section advanced knowledge link hooks like incremental models mentioned changing dimensions n't link could link something could use snapshots think added concept updates guess could also pre-hook post-hook also something would incremental model incremental model loads new chunk data use merge depending workers use use could insert update 's something would work unique key project also final project 's understand complex go 's limit hope answers	0
quickly summarize upload data google cloud platform without using airflow way transfer data gcp cloud storage bigquery without airflow	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
need speak german work de german companies	alexey well good job keep working almost finished first week finish start week two finish week three chuckles follow sequence videos playlist github sufficient think questions go slack ankush keep going alexey chuckles yes exactly ’ almost finished first week good sign first week year pretty tough many people lot problems docker whatnot 're almost finished good sign ’ good track keep working	0
need speak german work de german companies	alexey well 's really anonymous asked submit link github n't know need ask questions previous course think ask contact information project description n't know ankush think encourage might unfair different people first maybe 're available answer questions let 's put like grading maybe 's unfair want learn 's definitely helpful think grading done based upon submitted 's also really important document steps alexey yeah guess need contact author means really give four points reproducibility ankush case maybe instructions clear report via contact alexey yeah exactly n't remember problem previous course maybe also create issue github n't know	0
need speak german work de german companies	victoria ’ think need 've berlin three years speak german work depend go startup course 're going safe try make sure ask happen end team lot germans experience feel like need speak german order integrate 's thing would recommend though outside learning part – live germany try learn german actually integrate least case feel like helped lot feel country able go doctor receive calls stuff like speak least bit german n't speak super-fluidly quite helpful work alexey maybe 's also helpful separate berlin rest germany berlin 's international many companies berlin go start speaking german people look say “ sorry n't understand you. ” munich german city speak german german quite bad ’ lived like five half six seven years lost count ’ long start feeling ashamed still n't speak german chuckles victoria everything said disclaimer – berlin 's true even sometimes go coffee shops reply english person n't speak german quite crazy german capital alexey couriers deliver things also n't speak german hear way speak n't speak german 's good n't speak chuckles switch english victoria yeah 's true get everything english also one small part question “ need visa ” depend italian citizenship n't need visa example every country would say yes 's european alexey n't need german visa already work contract use contract 's enough get blue card visa “ blue card ” special program want get job seeker visa think need show level proficiency german n't remember ’ a2 b1 one n't need spend lot learning get level victoria job seeker well someone like data engineer think requirements less strict alexey think maybe anymore five years ago think requirement get least a1 level victoria anymore know lot people job seeker one definitely n't speak german alexey okay ignore said look victoria yeah definitely look chuckles also depend country kind degree something like may also change things alexey right blue card need degree without degree get program check internet	1
need speak german work de german companies	alexey well use judgment want try force register account azure run want n't maybe look code see errors 's really ankush maybe matching ask criteria – like used – maybe match corresponding people alexey 's nice idea let 's see 'm afraid complicate things bit already script matching n't use criteria 's random ankush need modify script one column alexey yeah perhaps let 's see ankush easier enough group different things alexey create form submitting ask cloud used right ankush exactly right alexey makes sense machine learning zoomcamp – wanted re-execute whole thing re-executed 's really want learn project – execute grading also learning – want learn really want learn things work execute learn n't time since executing different cloud might take lot time – n't much time really ask invest something like five hours figuring azure works use judgment guess 's answer try matching minimize	0
need speak german work de german companies	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
table bigquery called external table	ankush reason ’ called way data lying externally another storage like google cloud storage use case data imported inside bigquery bigquery saves format whatever format basically internal table bigquery normally called table data lying outside bigquery like csv format case calling external tables data source external alexey external tables slower 's also expensive query general would use external tables temporary place would keep parquet files go data warehouse right ankush exactly generally 's definitely one use cases also use load internal tables also use quick analysis see data correct things like also build external tables partitioning clustering big problem future victoria case bucket could technically bucket someone shares something like drop lose lost table entirely storage n't technically ankush agrees think would also something consider ankush yeah external tables metadata information metadata information would still exist create try query table – basically select star particular table longer exists data source deleted – query run issues alexey ran issues says “ source exist ” something like refuses execute query ankush exactly	1
table bigquery called external table	alexey 's early talk future iterations data engineering zoomcamp think chuckles ml camp yes – videos difference homework 's still discussion n't know 'll see ankush yeah think slight discussion met sure going repeat ml de zoomcamp let 's see response participation feel definitely success definitely feel free pass around course give us feedback maybe make better decision alexey re-recording videos… 's much effort n't think 'll want let 's say ml zoomcamp example much effort 'd rather reuse videos instead making new ones hope videos good decide rerun need re-record yeah 'll see	0
table bigquery called external table	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
table bigquery called external table	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
table bigquery called external table	alexey yes 're free choose dataset people already reached asking good dataset think everyone asked said “ yes good dataset. ” one thread question images – whether 's good idea use images project n't think 's good idea use images quite lot things need addition cover course like image processing trying extract information images outside scope course first time ’ working images could quite difficult n't think 's realistic 'll able data engineering project thing top two weeks might end spending much time maybe try select less ambitious dataset 's table csv file parquet file good 's images probably better use 's text – text also need quite work process maybe 's natural language ’ also good idea use example common crawl dataset lot text probably could great dataset depending exactly want 're sure maybe share idea – share want – tell 's doable within couple weeks also want share datatalks.club ’ slack dump also quite lot structured information could used 's also grouped days – day json file build nice pipeline 'll probably make dump share channel see 's something want use	0
think 's necessary basic knowledge kubernetes data engineering	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
think 's necessary basic knowledge kubernetes data engineering	alexey think 's helpful 's necessary helpful happens dags run airflow dags tasks tasks often kubernetes jobs something like infra – many companies infrastructure managed kubernetes 's quite simple get new container execute new job kubernetes n't think 's necessary think 's helpful 'm data engineer – 'm data scientist data scientist knowledge kubernetes quite helpful could debug jobs 'm running airflow run kubernetes could see logs would say want learn hurt get hired without kubernetes knowledge materials course sufficient get hired pick kubernetes work example agree victoria victoria yeah think kubernetes getting popular ca n't hurt	1
think 's necessary basic knowledge kubernetes data engineering	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
think 's necessary basic knowledge kubernetes data engineering	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
think 's necessary basic knowledge kubernetes data engineering	alexey depends exactly want reddit data right ankush yeah exactly would focus final goal let 's say “ want calculate reddit data. ” might want say “ keywords used ” something like case transformations directly depend alexey something like “ active number posts per day. ” ankush exactly “ per hour distribution posts. ” 'm pretty sure us goes daytime goes night trying answer 's first question would focus know answer know transformations apply alexey yeah 's end goal project dashboard “ exactly want put dashboard ” go “ dashboard data use kind transformations need order go data dashboard ” become clear need group-by need join table	0
linux mint works way better ubuntu based ubuntu user-friendly better many ways	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
linux mint works way better ubuntu based ubuntu user-friendly better many ways	yeah want experiment linux try mint 'm using ubuntu see mint probably good well	1
linux mint works way better ubuntu based ubuntu user-friendly better many ways	alexey one thing maybe n't clear enough – please always include code solution maybe people n't watch first said n't submit code homework get zero points saw submissions people put dot empty put link towards data science article example please n't please put code n't 'll get zero points entire week 'll make explicit form week three well please n't forget put code	0
linux mint works way better ubuntu based ubuntu user-friendly better many ways	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
linux mint works way better ubuntu based ubuntu user-friendly better many ways	yes n't written script yet actually want – many people asked see results made mistake beginning letting people register n't add checkbox saying 's okay use data like first name last name public really public leaderboard everyone go check score simply n't ask permission want instead write web service ’ email field go put email get scores idea hope 'll able implement hopefully n't difficult able get scores way yes grade homework store scores course end remember first video talked public leaderboard end course 100 people leaderboard ask permission publish names 'll need figure exactly communicate results separate homework grading actually difficult want write script	0
click tab parentheses ipython code n't show much info different versions	mean another iteration course yes go course repository see answer first take course self-paced mode videos available except last videos kserve start watching 's important first attempt homework without looking answer check solutions least one project 's also important n't follow videos sure learn something 's better solidify learning also practicing least one project 're taking course self-paced mode want get feedback project also share link slack happy give feedback project answer actual question regarding next cohort – start september cohort started september probably start bit earlier started middle september time probably start maybe one week earlier want notified informed link form click put email send email course starts	0
click tab parentheses ipython code n't show much info different versions	let 's say “ import numpy np ” let 's say “ np.random.exponential ” noticed press shift+tab press shift+tab shows docstring multiple times press shift+tab expands see funny thing n't even realize muscle memory 's n't saying loud know go shift+tab small thing open twice expand another thing put question mark instead parentheses “ np.random.exponential ” show docstring another thing think put “ help np.random.exponential ” print docstring	1
click tab parentheses ipython code n't show much info different versions	like said 's always problem- data-specific datasets get good performance	0
click tab parentheses ipython code n't show much info different versions	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
click tab parentheses ipython code n't show much info different versions	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
meet work company	alexey 'm sure last part –how docker provided terraform ankush run stuff terraform think always ... set airflow cluster terraform n't think execute job alexey 's something like use terraform prepare environment prepare thing run need actually run something least two separate commands first “ terraform apply ” “ python run ” something like 50:02 1 upvotes allowed contact instructors directly discuss projects things unsure regarding project 'm using new tools alexey think 's best ask things slack channel others also interested learning answer scales better instead everyone writing us directly – answer everyone sees answer b others answer available right somebody knows tool maybe n't example pulsar one students might know able help 's best use channel contact us directly	0
meet work company	alexey yes think n't think draw anywhere let 's say one machine spark another machine airflow machine airflow needs able send request spark spark submit need live network case sparks submit save url ip address spark computer 's enough one thing might need though probably need install spark airflow order able maybe sejal correct think need need java need spark submit script order able actually send jar network spark master need modify airflow container – used docker images course need modify need install java need install spark spark submit say “ -- master ” specify master virtual machine spark would actually instead running spark virtual machine somewhere would use dataproc relatively simple run need click buttons wait five minutes creates cluster cluster 's pretty convenient	0
meet work company	victoria 'm sure answer mean yes definitely adding part project least workshop reason want people learn also consider people entirely new concept thought would much overhead try cover things already quite complex day-to-day even though work daily dbt work slow-changing dimensions daily basis example 's also think 's something look moment use definitely dbt especially deletions implementing load step done hooks think 's section advanced knowledge link hooks like incremental models mentioned changing dimensions n't link could link something could use snapshots think added concept updates guess could also pre-hook post-hook also something would incremental model incremental model loads new chunk data use merge depending workers use use could insert update 's something would work unique key project also final project 's understand complex go 's limit hope answers	0
meet work company	ankush work company seen work different companies think sejal moving berlin whereas living munich even city met datatalks.club think alexey ’ motivation videos ml zoomcamp putting gave us encouragement contribute work towards creating new course data engineering victoria sejal also participated podcast 's met	1
meet work company	alexey well ’ aws account go transfer service 's probably easiest need click buttons want set gcp virtual machine need spend time setting everything 's worth weeks – spark probably kafka well – useful probably worth time investment want quickly copy files transfer service probably easiest way	0
'm data scientist would like learn data engineering right place	alexey 's want look project 're behind right n't worry take time decide whether give certificate based projects homework 's – catch week 3 lighter week 1 maybe fast-forward directly docker compose run week 2 depending much time maybe go solution published today give everything need week 3 use transfer service video already watching video probably learn lot already like homework solution	0
'm data scientist would like learn data engineering right place	alexey think 's helpful 's necessary helpful happens dags run airflow dags tasks tasks often kubernetes jobs something like infra – many companies infrastructure managed kubernetes 's quite simple get new container execute new job kubernetes n't think 's necessary think 's helpful 'm data engineer – 'm data scientist data scientist knowledge kubernetes quite helpful could debug jobs 'm running airflow run kubernetes could see logs would say want learn hurt get hired without kubernetes knowledge materials course sufficient get hired pick kubernetes work example agree victoria victoria yeah think kubernetes getting popular ca n't hurt	0
'm data scientist would like learn data engineering right place	alexey think actually talking course course – one profiles considered think popular among data scientists think “ anything else besides data science ” things like preparing data machine learning jobs machine learning models happens built model 's definitely one profiles considered indeed right place 's data scientists 's also software engineers know program also right place analysts 're comfortable programming command line know python also right place general discussed prerequisites – know program know use command line know bit sql course	1
'm data scientist would like learn data engineering right place	alexey 33 submissions think decision extend deadline good one hopefully end week see higher number understand project much difficult watching course following along 33 people – probably put lot effort thanks saw one person write put 100 hours already project outstanding sorry cruel 100 hours lot	0
'm data scientist would like learn data engineering right place	ankush yes yes think transfer service right yes question data using aws want run test around data source feel free also leave comment maybe even know goes azure blob storage	0
dbt local one visualize dags	victoria yes actually visualize dags whole dbt doc service cloud way terminal thought shown video basically run dbt doc spinner generate manifest relationships everything dbt docs serve host dbt docs local host 's repo 'm pretty sure see use something like – 'll find comments go project readme see generate dbt docs go dbt docs generate make sure always latest version everything depending run could post somewhere else want look locally alexey show picture right victoria yeah show dbt docs show videos kind like data catalog everything every table go check dag well	1
dbt local one visualize dags	victoria every table materialized table ’ ankush exactly answered previous question external tables think 's meaning question basically external tables versus internal tables alexey okay also concept materialized view view view nothing sql query executed every time want something table view 're kind creating query way speed – materialize view meaning create table data instead querying query query actual data get right ankush victoria agree	0
dbt local one visualize dags	alexey 's want look project 're behind right n't worry take time decide whether give certificate based projects homework 's – catch week 3 lighter week 1 maybe fast-forward directly docker compose run week 2 depending much time maybe go solution published today give everything need week 3 use transfer service video already watching video probably learn lot already like homework solution	0
dbt local one visualize dags	alexey 33 submissions think decision extend deadline good one hopefully end week see higher number understand project much difficult watching course following along 33 people – probably put lot effort thanks saw one person write put 100 hours already project outstanding sorry cruel 100 hours lot	0
dbt local one visualize dags	alexey add comment least workplace airflow instance set us data scientist would go instance airflow managed data engineers devops engineers would use never needed actually run docker compose moment multiple dags orchestration tool quite useful would personally maybe go luigi favorite one need think people well since people know luigi many like maybe airflow would actually better choice consider sejal would also like add question docker compose used docker compose airflow style use official setup seeing first time well 's unnecessarily complicated especially xcom variables forth generally docker compose acts wrapper docker containers production workflows docker containers used integration ci/cd pipelines wherever 're going deploy docker compose docker compose part helpful development environments case let 's say mock version airflow environment production want test dags local development environment use docker compose make things easier running simple commands like ‘ docker compose ’ would make fine instead running ‘ docker run container name etc whatever want ’ terms convenience terms usage ’ generally used development environments production alexey think first part question “ close usual data engineer role workflows ” workflows see work complex steps probably also case need start something already – would n't call complex pipeline – three four steps already something eventually might grow bigger bigger add steps ankush would also like add 're cloud maybe use airflow deploy docker maybe actually use service provided cloud provider one similar airflow think airflow would used internally like cloud composite google cloud platform would using terraform sejal yeah used aws step functions aws setup step functions combination lambda place airflow ankush said also something wanted say really depends production self-hosted cluster kubernetes wherever want use pure airflow version company could kind setup use native airflow cluster could cloud-based setup convenient option use managed services cloud composer aws also version called managed airflow	0
tensorflow always used keras heard keras absorbed tensorflow	example go kaggle see number notebooks available particular competition could good indicator 's good dataset n't really talk multi-class classification think 's okay want multi-class classification first dataset probably either binary classification see target clearly zero one think go already know deal numerical variables know deal categorical variables idea deal missing data see dataset things 's similar dataset good one 're sure ask slack help	0
tensorflow always used keras heard keras absorbed tensorflow	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
tensorflow always used keras heard keras absorbed tensorflow	dmitry kind different things keras recently kind united right 's one thing sure still use keras cross-talk alexey work standalone thing right dmitry yeah n't know plan finish supporting n't know plans right still use alexey initially keras n't work tensorflow first backend keras was… n't remember name library remember dmitry tensorflow appeared everyone forgot library guess something people used tensorflow theano name exactly thing used backend keras tensorflow appeared keras added support tensorflow backend main author keras joined google maybe 's main reason keras part tensorflow move google get users tensorflow guess chuckles dmitry n't know answer think ’ speculating	1
tensorflow always used keras heard keras absorbed tensorflow	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
tensorflow always used keras heard keras absorbed tensorflow	building locally fine	0
mean using powers two batch size easy parallelization	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
mean using powers two batch size easy parallelization	's really case-dependent saw slack remember correctly carlos asked needs drop features – seemed okay drop highly correlated feature assistant name target variable churn basically people work one particular person tend turn less people work somebody else think case seems like relevant feature include n't know – 's really case-dependent usually need sort domain expertise decide feature introduces leakage carlos ’ case think 's safe leave drop see performance validation dataset changes gives idea important feature	0
mean using powers two batch size easy parallelization	n't think takes lot time first need record videos prepare homework imagine adding collaboration startups well maybe sometime future n't know somebody startup listening right want hire people graduate zoomcamp please reach let 's arrange something	0
mean using powers two batch size easy parallelization	honest n't used uvicorn used gunicorn n't know matters n't actually checked benchmarks use gunicorn also use others – n't remember ones reason basically	0
mean using powers two batch size easy parallelization	alexey idea idea dmitry said split data two parts right dmitry yeah far understand used basically separate set run different machines example quite big dataset split machines run	1
auc 93 f1 63 good bad	know honest first thing probably need realize especially comes matrices things matrix multiplication example everything expressed python code code see take formula try decompose one one try see translates n't know actually answers question better way try translate also helps reference implementation let 's say implementing matrix multiplication plain python could compare results implementation numpy see results existing implementation compare implementation reference implementation probably helps	0
auc 93 f1 63 good bad	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
auc 93 f1 63 good bad	alexey one thing maybe n't clear enough – please always include code solution maybe people n't watch first said n't submit code homework get zero points saw submissions people put dot empty put link towards data science article example please n't please put code n't 'll get zero points entire week 'll make explicit form week three well please n't forget put code	0
auc 93 f1 63 good bad	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
auc 93 f1 63 good bad	's suspicious auc looks quite good f1 score compared auc general f1 score 63 good n't forget auc evaluates model thresholds f1 score one threshold maybe threshold used evaluating f1 best one play bit thresholds see actually f1 better actually numbers look good say bad also auc around 90 slightly less sometimes bit suspicious – yeah 's good maybe 's bit good think 's fine	1
hard time differentiating data warehouse data lake could please quickly recap	alexey n't think actually time summarize everything course got inspired sejal – prepares notes 's recording video based notes share notes also take notes share notes fellow students way learn public – sort regurgitate content produce something new – useful useful others promise summaries encourage take notes share others jupyter books think something publish particular week jupyter notebook code snippets course share ankush definitely share code snippets code use particular week definitely shared github	0
hard time differentiating data warehouse data lake could please quickly recap	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
hard time differentiating data warehouse data lake could please quickly recap	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
hard time differentiating data warehouse data lake could please quickly recap	victoria would say video ankush nice recap chuckles ankush actually recap right data warehouse vs data lake let 's go differences quickly data warehouse basically meant structured data already know schema data already know different tables interact less sure future data look like 're changing regular basis cases data warehouse perfect solution put data particular schema nicely tables data warehouse basically query across huge amount data respect joins different sql queries top basically concept data warehouse schema already know everything data size generally terabytes petabytes let ’ say long-running sql queries working top comes data lake basic idea know kind schema know data used future really want moment save data soon possible place used future used different use cases maybe covered right cases basically take data whatever schema – n't matter json parquet – whatever 's good standards even ’ still dump data cloud storage s3 bucket hdfs instance basically dump whole data whatever format want idea data recoverable sort metadata attached information searchable know column names mean another team wants interact data data lake idea would “ hey n't know use case right dump tomorrow one year data scientists data analysts come use data see information available maybe transform data put data warehouse put different bucket use it. ” use case typically machine learning model collects lot data years let 's say data recommendations data people buying different products last five years build recommendation system top lose data basically generate kinds models 's data lake really plays vital role alexey 's pretty comprehensive recap ankush think good one chuckles victoria end also trick silly trick think word store things warehouse store things lake n't everything available image least helps understand concepts well normal warehouse would shelves things structure also data looks data warehouse kind alexey way understand maybe best way thinking data lake like file storage write parquet files could s3 google cloud storage hdfs – us ’ hdfs – bunch buckets buckets structure bunch files files documented follow schema less bunch files want access files something needs go read files – could spark thing could airflow thing could simple python script – 's files comes data warehouses 's files thing 's properly structured tables know schema use cases etc 's also faster data lake usually need download files something put somewhere data warehouse optimized specific analytical queries let 's say want create dashboard data warehouse model data way 's relatively fast n't need go download files run aggregations – 's lot easier understand difference quick recap would – files dumped s3 data lake nice schema database 's data warehouse much simplification victoria think explains big difference data warehouse data lake bigger concept always go deeper	1
hard time differentiating data warehouse data lake could please quickly recap	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
recommend terms building projects jupyter python docker leave jupyter notebooks jupyter ever used day-to-day work	alexey one thing maybe n't clear enough – please always include code solution maybe people n't watch first said n't submit code homework get zero points saw submissions people put dot empty put link towards data science article example please n't please put code n't 'll get zero points entire week 'll make explicit form week three well please n't forget put code	0
recommend terms building projects jupyter python docker leave jupyter notebooks jupyter ever used day-to-day work	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
recommend terms building projects jupyter python docker leave jupyter notebooks jupyter ever used day-to-day work	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
recommend terms building projects jupyter python docker leave jupyter notebooks jupyter ever used day-to-day work	use jupyter extensively – use jupyter lot work initial experiments like playing different models exploratory data analysis cleaning dataset – use jupyter lot reason interactive environment type something immediately get answer script write something need run script see changes think feedback loop lot longer use scripts n't use jupyter 's really love jupyter come software development background idea writing code web browser alien got used imagine life work least without using jupyter think super convenient	1
recommend terms building projects jupyter python docker leave jupyter notebooks jupyter ever used day-to-day work	yes could soon post message slack need find time couple tweaks script	0
final conversion rate previous course terms many people registered actually finished	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
final conversion rate previous course terms many people registered actually finished	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
final conversion rate previous course terms many people registered actually finished	alexey chuckles yeah conversion terrible 6000 people registered 400 attempted first homework around 50 submitted last homework 's like 1 – little bit less think happened – life happened people many left email came time actually course many realized something else 's beauty openly maybe many people watch submit homework 's fine well take best course tell us need help – get stuck n't stop get stuck ask help 'll happy help ankush definitely personally also registered ml course watched lot videos time homework assignments finish project learned lot videos totally worth	1
final conversion rate previous course terms many people registered actually finished	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
final conversion rate previous course terms many people registered actually finished	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
reviewers assigned randomly	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
reviewers assigned randomly	’ two weeks october 18 2021 meaning first november 2022	0
reviewers assigned randomly	alexey one thing maybe n't clear enough – please always include code solution maybe people n't watch first said n't submit code homework get zero points saw submissions people put dot empty put link towards data science article example please n't please put code n't 'll get zero points entire week 'll make explicit form week three well please n't forget put code	0
reviewers assigned randomly	yes randomly think 's easiest way	1
reviewers assigned randomly	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
looked quite simple examples using dbt complex business logic	ankush yes definitely explore aws azure comfortable using gcp course using gcp tools like bigquery might available platforms might similarities alternatives might touch course course based upon google cloud platform one biggest reasons create new account get 300 credit 's really useful everybody 's familiar cloud create free account course ideally spend money platform services alexey think another reason decided go gcp connection dbt bigquery athena think 's bit tricky want go snowflake dbt 's super expensive something want pay course 's thinking whether go aws gcp – gcp gives free credits b dbt works bigquery ’ led decision gcp sejal mentioned able run everything locally except bigquery part course bigquery lives cloud rest stuff runnable locally	0
looked quite simple examples using dbt complex business logic	alexey 'm sure last part –how docker provided terraform ankush run stuff terraform think always ... set airflow cluster terraform n't think execute job alexey 's something like use terraform prepare environment prepare thing run need actually run something least two separate commands first “ terraform apply ” “ python run ” something like 50:02 1 upvotes allowed contact instructors directly discuss projects things unsure regarding project 'm using new tools alexey think 's best ask things slack channel others also interested learning answer scales better instead everyone writing us directly – answer everyone sees answer b others answer available right somebody knows tool maybe n't example pulsar one students might know able help 's best use channel contact us directly	0
looked quite simple examples using dbt complex business logic	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
looked quite simple examples using dbt complex business logic	alexey well guess want dashboard aggregates hard time trying imagine dataset take raw form put dashboard maybe 's already grouped 's already aggregated 's probably small dataset 's suitable course think maybe select reasonably big dataset require transformation 're doubt ask slack really n't know kind dataset put dashboard without transformations ankush yeah minimum transformations would required alexey counts group-bys maybe joins ankush exactly think 's better choose dataset allow	0
looked quite simple examples using dbt complex business logic	victoria 'm sure answer mean yes definitely adding part project least workshop reason want people learn also consider people entirely new concept thought would much overhead try cover things already quite complex day-to-day even though work daily dbt work slow-changing dimensions daily basis example 's also think 's something look moment use definitely dbt especially deletions implementing load step done hooks think 's section advanced knowledge link hooks like incremental models mentioned changing dimensions n't link could link something could use snapshots think added concept updates guess could also pre-hook post-hook also something would incremental model incremental model loads new chunk data use merge depending workers use use could insert update 's something would work unique key project also final project 's understand complex go 's limit hope answers	1
applied min-max normalization data set ’ sure inference made single input transformation	alexey one thing maybe n't clear enough – please always include code solution maybe people n't watch first said n't submit code homework get zero points saw submissions people put dot empty put link towards data science article example please n't please put code n't 'll get zero points entire week 'll make explicit form week three well please n't forget put code	0
applied min-max normalization data set ’ sure inference made single input transformation	use cross-validation check validation get better scores – drop one drop one go sometimes makes sense drop even decrease score simply maybe one feature complex compute another one	0
applied min-max normalization data set ’ sure inference made single input transformation	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
applied min-max normalization data set ’ sure inference made single input transformation	lectures used dictionary vectorizer also transformer like min-max normalization 're talking min-max normalization transformer scikit learn 's called min max scaler basically method transform first method called fit use learn x min x max step called transform invoke way invoke dictionary vectorizer 're using scaler suggest use makes things simpler fit transformer fit model transform data apply model	1
applied min-max normalization data set ’ sure inference made single input transformation	building locally fine	0
already submitted project deadline able begin peer reviewing right away need wait deadline	alexey 'm sure last part –how docker provided terraform ankush run stuff terraform think always ... set airflow cluster terraform n't think execute job alexey 's something like use terraform prepare environment prepare thing run need actually run something least two separate commands first “ terraform apply ” “ python run ” something like 50:02 1 upvotes allowed contact instructors directly discuss projects things unsure regarding project 'm using new tools alexey think 's best ask things slack channel others also interested learning answer scales better instead everyone writing us directly – answer everyone sees answer b others answer available right somebody knows tool maybe n't example pulsar one students might know able help 's best use channel contact us directly	0
already submitted project deadline able begin peer reviewing right away need wait deadline	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	0
already submitted project deadline able begin peer reviewing right away need wait deadline	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
already submitted project deadline able begin peer reviewing right away need wait deadline	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
already submitted project deadline able begin peer reviewing right away need wait deadline	alexey unfortunately need wait process quite manual need create things deadline	1
mean proof screen – get plus	n't think takes lot time first need record videos prepare homework imagine adding collaboration startups well maybe sometime future n't know somebody startup listening right want hire people graduate zoomcamp please reach let 's arrange something	0
mean proof screen – get plus	maybe 're referring plus image reference let maybe say first compute mean without missing values use part array number values formula σxi let 's say include missing values fill missing values mean x̄ instead n elements k elements total k+n 's k+n 's plus first need sum elements need sum elements 's sum hope understood question	1
mean proof screen – get plus	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
mean proof screen – get plus	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
mean proof screen – get plus	means published one publish medium example blog whatever linkedin posts – n't think put nice long article linkedin post think linkedin articles n't code – embed code would n't recommend using linkedin articles medium good blog use blog	0
research spark dbt use spark dbt etlt use cases one preferable	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
research spark dbt use spark dbt etlt use cases one preferable	alexey 's want look project 're behind right n't worry take time decide whether give certificate based projects homework 's – catch week 3 lighter week 1 maybe fast-forward directly docker compose run week 2 depending much time maybe go solution published today give everything need week 3 use transfer service video already watching video probably learn lot already like homework solution	0
research spark dbt use spark dbt etlt use cases one preferable	victoria 'm sure answer mean yes definitely adding part project least workshop reason want people learn also consider people entirely new concept thought would much overhead try cover things already quite complex day-to-day even though work daily dbt work slow-changing dimensions daily basis example 's also think 's something look moment use definitely dbt especially deletions implementing load step done hooks think 's section advanced knowledge link hooks like incremental models mentioned changing dimensions n't link could link something could use snapshots think added concept updates guess could also pre-hook post-hook also something would incremental model incremental model loads new chunk data use merge depending workers use use could insert update 's something would work unique key project also final project 's understand complex go 's limit hope answers	0
research spark dbt use spark dbt etlt use cases one preferable	alexey add comment least workplace airflow instance set us data scientist would go instance airflow managed data engineers devops engineers would use never needed actually run docker compose moment multiple dags orchestration tool quite useful would personally maybe go luigi favorite one need think people well since people know luigi many like maybe airflow would actually better choice consider sejal would also like add question docker compose used docker compose airflow style use official setup seeing first time well 's unnecessarily complicated especially xcom variables forth generally docker compose acts wrapper docker containers production workflows docker containers used integration ci/cd pipelines wherever 're going deploy docker compose docker compose part helpful development environments case let 's say mock version airflow environment production want test dags local development environment use docker compose make things easier running simple commands like ‘ docker compose ’ would make fine instead running ‘ docker run container name etc whatever want ’ terms convenience terms usage ’ generally used development environments production alexey think first part question “ close usual data engineer role workflows ” workflows see work complex steps probably also case need start something already – would n't call complex pipeline – three four steps already something eventually might grow bigger bigger add steps ankush would also like add 're cloud maybe use airflow deploy docker maybe actually use service provided cloud provider one similar airflow think airflow would used internally like cloud composite google cloud platform would using terraform sejal yeah used aws step functions aws setup step functions combination lambda place airflow ankush said also something wanted say really depends production self-hosted cluster kubernetes wherever want use pure airflow version company could kind setup use native airflow cluster could cloud-based setup convenient option use managed services cloud composer aws also version called managed airflow	0
research spark dbt use spark dbt etlt use cases one preferable	alexey n't know etlt maybe summarize something discussed slack recently would say write something sql already things data warehouse 're afraid costs – think ankush wants mention probably talk – sql data warehouse something like hive presto something managed like athena aws n't need worry spark cluster needs maintenance use managed spark cluster aws something like gives opportunity write sql query put sql query airflow dbt runs things sometimes need things difficult express sql sometimes need control things go spark ankush sometimes want unit test code write spark think dbt completely different spark 's one one comparison dbt something sequential 's sequential sql queries many features spark would offer box write let 's say 20 steps dbt 's understandable using dbt graph dbt cloud think thing spark would get hard debug time think use case think one one comparison fair either dbt spark case alexey end used batch processing right similarities sejal think also depends upon larger use case dbt particularly tool used analytics engineers standpoint creating data marts transformations could generate views analytical dashboards meanwhile spark multi-purpose technology used ml well distributed processing namely core value distributed processing like ankush said 's really comparable alexey also flexibility using whatever want python java whatever environment use dbt dbt use sql bigquery user-defined functions 're pretty limited hard test turn nightmare end right ankush absolutely think maybe also see something like company using something like dbt airflow – go solution company focused spark familiar spark feel free explore spark sql area depend upon maturity team company well – basically moment alexey use case n't yet spark cluster maybe 's good idea try avoid maintain spark cluster express things sql use serverless things like bigquery athena something like manage spark cluster yarn whatever think 's better beginning eventually probably need look direction get spark cluster ankush said depends maturity beginning probably n't want deal ankush yeah also depends upon clients analytical engineers company familiar python familiar spark also n't want debug every use case every issue maybe dbt run sql much better maintenance-wise might also want compare things respect obviously cost	1
n't use streamlit instead flask	let 's say two dimensional thing 2d want turn 1d case need use pooling2d 'm sure right want quickly check reason remember showing usually go defining model build sort layer layer every time add one layer model predicts see output based see kind pooling layer example need let 's say want turn 2d 1d think need use 2d pooling 'm exactly sure whether 's 2d 1d think 1d pooling needed something one dimensional want turn one value use 1d pooling 3d pooling three dimensional thing want turn one-dimensional thing image 2 things always confuse honest 's follow step step try different poolings want make sure convert image vector presentation something one-dimensional usually size number images times something 's like 2d array image one-dimensional vector based try different poolings sometimes also flatten flatten takes whatever – let 's say kd – want turn 1d use flatten many different options think 's clear difference might remember exactly use 1d 2d difference kind input take kind output produce 's cube 's 2d 's hypercube three dimensions 's something else	0
n't use streamlit instead flask	's really case-dependent saw slack remember correctly carlos asked needs drop features – seemed okay drop highly correlated feature assistant name target variable churn basically people work one particular person tend turn less people work somebody else think case seems like relevant feature include n't know – 's really case-dependent usually need sort domain expertise decide feature introduces leakage carlos ’ case think 's safe leave drop see performance validation dataset changes gives idea important feature	0
n't use streamlit instead flask	flask – let draw let 's say want create web service image 5 web service gets requests web service responds something could predictions could something else – n't related machine learning implement web service use flask use fastapi use framework creating web services use flask use something else – flask allows use post example let 's say “ get ” usually parse parameters query let 's say go google put “ test ” see q=test https //www.google.com/search q=test oq=test aqs=chrome.0.69i59l2j69i65l3j69i60j69i61l2.1264j0j1 sourceid=chrome ie=utf-8 image 6 parameter pass “ get ” request “ get ” request sometimes let 's say want score customer n't want put gender=female contract=one year string send json post post also add body request generally use post general use case flask able create web service	0
n't use streamlit instead flask	reason think flask popular framework 's relatively simple 's main reason	1
n't use streamlit instead flask	honest idea question means… oh flt pt floating pointers okay yes numpy floating point errors n't know – computers let 's say multiply two numbers operations take two numbers see instead 3 3 followed lot zeros 4 image reference floating point operations precise computers 's summing multiplying little floating point arithmetic error numpy 's running computer 's also prone guess question “ keep using logs data exploration ” 're probably referring week 2 think use logarithm price main reason numerical instability – ’ floating point errors – 'll probably learn video simply distributions long tails please refer eda video probably second third explain need 's keep using logs n't apply logarithm 's difficult machine learning models actually learn data	0
load data s3 bucket ny data website	alexey 'm sure last part –how docker provided terraform ankush run stuff terraform think always ... set airflow cluster terraform n't think execute job alexey 's something like use terraform prepare environment prepare thing run need actually run something least two separate commands first “ terraform apply ” “ python run ” something like 50:02 1 upvotes allowed contact instructors directly discuss projects things unsure regarding project 'm using new tools alexey think 's best ask things slack channel others also interested learning answer scales better instead everyone writing us directly – answer everyone sees answer b others answer available right somebody knows tool maybe n't example pulsar one students might know able help 's best use channel contact us directly	0
load data s3 bucket ny data website	alexey sure answer question actually something think spoiler homework solution chuckles using airflow helps use airflow 's 've whole week week 2 using airflow first download data ny taxi website parquetize upload google cloud storage n't s3 oh see question coming guess go ny taxi data website – transfer service – one videos transfer service transfer service moving data s3 google cloud storage understand question data already s3 – look urls see url contains “ s3 amazon aws ” name packet data already s3 n't need probably long answer short data already s3 n't need	1
load data s3 bucket ny data website	alexey 33 submissions think decision extend deadline good one hopefully end week see higher number understand project much difficult watching course following along 33 people – probably put lot effort thanks saw one person write put 100 hours already project outstanding sorry cruel 100 hours lot	0
load data s3 bucket ny data website	alexey 's want look project 're behind right n't worry take time decide whether give certificate based projects homework 's – catch week 3 lighter week 1 maybe fast-forward directly docker compose run week 2 depending much time maybe go solution published today give everything need week 3 use transfer service video already watching video probably learn lot already like homework solution	0
load data s3 bucket ny data website	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
feels difficult follow along approach course	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	0
feels difficult follow along approach course	alexey 'm curious know exactly difficulties knowing next knowing start maybe ’ take stab first go github repo github repo things structured weeks click week 1 example want learn materials week 1. week 2 're preparing right look different finish look like week 1 – concise know details need go everything need starting introduction code docker gcp terraform setting environment actually environment setup n't problems installing docker installing terraform installing google cloud ignore thing reason example trouble running postgres docker check video 's step-by-step video shows set environment using virtual machine google cloud video like bonus watch point follow order see page playlist actually google cloud intro first n't think actually matters introduction google cloud platform short video n't matter watch docker try follow format weeks 'll multiple sections week example week two three sections like data lake orchestration first one video data lakes basics orchestration bunch videos airflow answer question – go github repo follow approach suggested difficulties feel free go slack ask difficulties something particular going slack channels looking kind problems people 's likely somebody else problem solution one threads since 're talking 's amazing see helpful 's big heart thanks lot n't know personally 's overwhelming open slack channel see like 50 new messages 'm happy see already helped basically resolve issues big big thank 's helpful us 'm sure would able answer questions thanks lot sejal yeah plus one alexey said thank much helping pressing questions definitely answer definitely looking threads ’ wo n't constantly available answer possible alexey also wanted say need refresher github maybe google “ github tutorial ” “ getting started github ” 'll find lot amazing resources 'm sure able better available sources ankush using git continuously videos obviously would using commands hope might help refreshing github knowledge best way would small youtube search github 'm pretty sure really good videos alexey anyone comes across nice github tutorial helpful please share rest students	1
feels difficult follow along approach course	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
feels difficult follow along approach course	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
feels difficult follow along approach course	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
cases imbalanced datasets apply smote/downsampling techniques – splitting data	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
cases imbalanced datasets apply smote/downsampling techniques – splitting data	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
cases imbalanced datasets apply smote/downsampling techniques – splitting data	always let 's take case image 1 split data test dataset split data – validation dataset let show would data frame 'll split data frame use 20 25 validation okay data frame validation image 2 ’ still smote downsampling upsampling – data frame train touch data frame validation experiment need make sure validation distribution target variable stays distribution target variable see real-world data n't want change change able evaluate model real world scenario 's important change validation change train downsampling upsampling smote train model test validation dataset set check whatever target metric want train full model keep test dataset intact – n't touch n't apply downsampling apply techniques data frame train full	1
cases imbalanced datasets apply smote/downsampling techniques – splitting data	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
cases imbalanced datasets apply smote/downsampling techniques – splitting data	mae – talk great detail mae rmse metrics evaluating quality regression models course use rmse mae alternative one rmse formula mae formula little bit different instead looking squared error look absolute error absolute means use bars formula 're little bit different rmse penalizes big errors let 's say prediction 0.1 actual prediction 100. rmse get higher penalty 's 99.9 squared huge huge penalty mae 99.9. pros cons – use n't choose one always report numbers	0
please elaborate bit project review part etc	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
please elaborate bit project review part etc	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
please elaborate bit project review part etc	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
please elaborate bit project review part etc	alexey week 2 yes – upload video solution also update leaderboard probably take time depending clean data – need lot data cleaning take lot time please n't put things n't look like emails email field fill please careful takes time clean later	0
please elaborate bit project review part etc	alexey 'll start “ ” next week extended deadline one week means many still working project deadline next monday april 4th 2022. 's deadline review starts review start next monday monday comes “ ” guidelines criteria need look google form need enter id project 're reviewing section forum say “ problem description 0.1 point 2 points ” something like criteria previous course problem description would need select one points dimension would need select option given end script takes sort average points get final score see projects assigned review table hash email hash project need review	1
use spark files saved google cloud storage output spark queries saved could visualize spark output	sejal yes really kubernetes expert maybe alexey help alexey bit googling see kubernetes operator probably answers yeah operator use kubernetes internally think create kubernetes job execute typically use kubernetes operators use aws batch operators essentially thing except 're running kubernetes aws batch see usually pattern exactly use airflow use python operators bash operators – execute things workers – usually make workers pretty dumb n't lot resources instead delegate external compute environment like kubernetes aws batch maybe ecs alternatives google cloud azure state much resources need specific job want run docker image parameters executed somewhere – worker pattern see usually 's either sql operators execute bigquery athena presto something else kinds aws batch kubernetes jobs spark example	0
use spark files saved google cloud storage output spark queries saved could visualize spark output	alexey 's early talk future iterations data engineering zoomcamp think chuckles ml camp yes – videos difference homework 's still discussion n't know 'll see ankush yeah think slight discussion met sure going repeat ml de zoomcamp let 's see response participation feel definitely success definitely feel free pass around course give us feedback maybe make better decision alexey re-recording videos… 's much effort n't think 'll want let 's say ml zoomcamp example much effort 'd rather reuse videos instead making new ones hope videos good decide rerun need re-record yeah 'll see	0
use spark files saved google cloud storage output spark queries saved could visualize spark output	ankush n't think 's focus course think focus course learn data engineering basics learn develop software kafka spark might cover test cases yes extensive writing good unit test cases course	0
use spark files saved google cloud storage output spark queries saved could visualize spark output	alexey example spark google “ spark gcp bigquery connector ” good article shows need need jar need use jar submit spark job let 's say files stored google cloud storage something files multiple options first option save back google cloud storage create external table something like like saw week 3. another option would saving directly bigquery using say “ write ” say want write bigquery uses connector connect bigquery write n't know permissions actually work – maybe also need specify key somehow probably 're already dataproc already permissions need order write bigquery order read google cloud storage use dataproc theoretically easy – add extra jar write bigquery data data warehouse use materials dbt week analytics engineering week actually consume content data warehouse visualize	1
use spark files saved google cloud storage output spark queries saved could visualize spark output	victoria would say ’ also 's different set products example data warehouse start data tech stack scratch would definitely use redshift cost lower end need probably would need migrate something like bigquery snowflake lot documentation 's pretty easy set 's around longer – think reason broader adoption mainly around things alexey seems aws popular well least berlin look companies use cloud maybe 70 people use aws 20 may use gcp remaining 10 use azure 's roughly see maybe 'm wrong course also got use gcp bit seems ui nicer go web console 's little bit nicer think comes tools 's bit difficult example aws pip install aws cli google cloud 's bit difficult maybe 's one reason aws popular – ’ mature maybe think gcp catching nice interface sometimes things cheaper bigquery also think big advantage maybe clouds n't really victoria plus aws around 10 years thesis using aws thing gcp yet 's lot people go 're familiar outside stability alexey think cloud services services quite similar ’ used aws many things gcp straightforward n't typical less think many concepts map one another	0
landing zone	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
landing zone	alexey 's want look project 're behind right n't worry take time decide whether give certificate based projects homework 's – catch week 3 lighter week 1 maybe fast-forward directly docker compose run week 2 depending much time maybe go solution published today give everything need week 3 use transfer service video already watching video probably learn lot already like homework solution	0
landing zone	ankush example 're data pipeline first time might want consider something like data lake data warehouse solution would landing zone lot cases see people already start think wide range say “ okay raw landing zone first transformation landing zone main transformation landing zone. ” also name “ bronze ” “ gold ” yes keep maybe couple n't overcomplicate n't keep like seven overkill first pipeline	1
landing zone	alexey week 2 yes – upload video solution also update leaderboard probably take time depending clean data – need lot data cleaning take lot time please n't put things n't look like emails email field fill please careful takes time clean later	0
landing zone	alexey think 's helpful 's necessary helpful happens dags run airflow dags tasks tasks often kubernetes jobs something like infra – many companies infrastructure managed kubernetes 's quite simple get new container execute new job kubernetes n't think 's necessary think 's helpful 'm data engineer – 'm data scientist data scientist knowledge kubernetes quite helpful could debug jobs 'm running airflow run kubernetes could see logs would say want learn hurt get hired without kubernetes knowledge materials course sufficient get hired pick kubernetes work example agree victoria victoria yeah think kubernetes getting popular ca n't hurt	0
okay follow videos without fully understanding behind sometimes n't understand line line anything supplement	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
okay follow videos without fully understanding behind sometimes n't understand line line anything supplement	alexey think question asking model building features bigquery good think good n't done understand ’ pretty basic “ ab initio ” something latin right victoria “ starting based first principles. ” sounds like spanish 's latin wanted check chuckles 's usually used legal terminology alexey maybe 's coming lawyer wants become data engineer anyway model building features pretty basic 's good start try switch jupyter notebook advanced things let ’ say – need flexibility think n't really experience bigquery aws 's little bit different aware similar method aws athena similar bigquery n't know create model athena maybe aws would use sagemaker sagemaker jupyter notebooks works 's good need complexity switch jupyter notebooks	0
okay follow videos without fully understanding behind sometimes n't understand line line anything supplement	alexey well use judgment want try force register account azure run want n't maybe look code see errors 's really ankush maybe matching ask criteria – like used – maybe match corresponding people alexey 's nice idea let 's see 'm afraid complicate things bit already script matching n't use criteria 's random ankush need modify script one column alexey yeah perhaps let 's see ankush easier enough group different things alexey create form submitting ask cloud used right ankush exactly right alexey makes sense machine learning zoomcamp – wanted re-execute whole thing re-executed 's really want learn project – execute grading also learning – want learn really want learn things work execute learn n't time since executing different cloud might take lot time – n't much time really ask invest something like five hours figuring azure works use judgment guess 's answer try matching minimize	0
okay follow videos without fully understanding behind sometimes n't understand line line anything supplement	alexey well 's really anonymous asked submit link github n't know need ask questions previous course think ask contact information project description n't know ankush think encourage might unfair different people first maybe 're available answer questions let 's put like grading maybe 's unfair want learn 's definitely helpful think grading done based upon submitted 's also really important document steps alexey yeah guess need contact author means really give four points reproducibility ankush case maybe instructions clear report via contact alexey yeah exactly n't remember problem previous course maybe also create issue github n't know	0
okay follow videos without fully understanding behind sometimes n't understand line line anything supplement	alexey think 's okay particular problems maybe ask slack first exposure tools might overwhelming lot information 's understandable things n't clear actually rewatch videos editing example realize “ way people understand meant here. ” fine things unclear particular doubt particular line n't understand want understand ask slack anything supplement would say go projects get stuck google ask help learn fully understand things	1
much time need dedicate every week successfully complete course	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
much time need dedicate every week successfully complete course	alexey add comment least workplace airflow instance set us data scientist would go instance airflow managed data engineers devops engineers would use never needed actually run docker compose moment multiple dags orchestration tool quite useful would personally maybe go luigi favorite one need think people well since people know luigi many like maybe airflow would actually better choice consider sejal would also like add question docker compose used docker compose airflow style use official setup seeing first time well 's unnecessarily complicated especially xcom variables forth generally docker compose acts wrapper docker containers production workflows docker containers used integration ci/cd pipelines wherever 're going deploy docker compose docker compose part helpful development environments case let 's say mock version airflow environment production want test dags local development environment use docker compose make things easier running simple commands like ‘ docker compose ’ would make fine instead running ‘ docker run container name etc whatever want ’ terms convenience terms usage ’ generally used development environments production alexey think first part question “ close usual data engineer role workflows ” workflows see work complex steps probably also case need start something already – would n't call complex pipeline – three four steps already something eventually might grow bigger bigger add steps ankush would also like add 're cloud maybe use airflow deploy docker maybe actually use service provided cloud provider one similar airflow think airflow would used internally like cloud composite google cloud platform would using terraform sejal yeah used aws step functions aws setup step functions combination lambda place airflow ankush said also something wanted say really depends production self-hosted cluster kubernetes wherever want use pure airflow version company could kind setup use native airflow cluster could cloud-based setup convenient option use managed services cloud composer aws also version called managed airflow	0
much time need dedicate every week successfully complete course	ankush think dedicating around two three hours would enough going videos less might want homework might want experiment bit technologies familiar might take bit time average two three hours per week enough finish course	1
much time need dedicate every week successfully complete course	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
much time need dedicate every week successfully complete course	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
data vault differ “ classic ” data warehouse concepts like cubes star/snowflake schemas	alexey think 's helpful 's necessary helpful happens dags run airflow dags tasks tasks often kubernetes jobs something like infra – many companies infrastructure managed kubernetes 's quite simple get new container execute new job kubernetes n't think 's necessary think 's helpful 'm data engineer – 'm data scientist data scientist knowledge kubernetes quite helpful could debug jobs 'm running airflow run kubernetes could see logs would say want learn hurt get hired without kubernetes knowledge materials course sufficient get hired pick kubernetes work example agree victoria victoria yeah think kubernetes getting popular ca n't hurt	0
data vault differ “ classic ” data warehouse concepts like cubes star/snowflake schemas	victoria bought book data vaults 's called elephant fridge found quite nice read n't read reason saw data vault practical use point view work understanding generates extra layer something else like data mart right warehouse load data transform build something like data marts star schema something like depending want build data modeling concepts want apply lies data warehouse understand data vault – sorry 'm understanding wrong way – generates extra layer something else like another database something like data marts tries denormalize data similar happens star schema reason solution 're looking star schema difficult understand first define business need accommodate similar concepts star schema accommodate generate extra layer main difference understood probably go google little bit also n't know companies use honest 've never met anyone could conversation looks like real life alexey exactly vault mean place keep things right victoria yeah ’ technically data warehouse get chuckles bolted extra layer data n't expose data – n't get vault – except marts similar concept data marts business process group ’ process share book read much alexey something elephants right victoria elephant fridge ’ called reason fridge also called vault 's quite nice 's creators data vault ’ someone actually explains creator found useful ’ clearly written 's	1
data vault differ “ classic ” data warehouse concepts like cubes star/snowflake schemas	alexey add comment least workplace airflow instance set us data scientist would go instance airflow managed data engineers devops engineers would use never needed actually run docker compose moment multiple dags orchestration tool quite useful would personally maybe go luigi favorite one need think people well since people know luigi many like maybe airflow would actually better choice consider sejal would also like add question docker compose used docker compose airflow style use official setup seeing first time well 's unnecessarily complicated especially xcom variables forth generally docker compose acts wrapper docker containers production workflows docker containers used integration ci/cd pipelines wherever 're going deploy docker compose docker compose part helpful development environments case let 's say mock version airflow environment production want test dags local development environment use docker compose make things easier running simple commands like ‘ docker compose ’ would make fine instead running ‘ docker run container name etc whatever want ’ terms convenience terms usage ’ generally used development environments production alexey think first part question “ close usual data engineer role workflows ” workflows see work complex steps probably also case need start something already – would n't call complex pipeline – three four steps already something eventually might grow bigger bigger add steps ankush would also like add 're cloud maybe use airflow deploy docker maybe actually use service provided cloud provider one similar airflow think airflow would used internally like cloud composite google cloud platform would using terraform sejal yeah used aws step functions aws setup step functions combination lambda place airflow ankush said also something wanted say really depends production self-hosted cluster kubernetes wherever want use pure airflow version company could kind setup use native airflow cluster could cloud-based setup convenient option use managed services cloud composer aws also version called managed airflow	0
data vault differ “ classic ” data warehouse concepts like cubes star/snowflake schemas	alexey 's want look project 're behind right n't worry take time decide whether give certificate based projects homework 's – catch week 3 lighter week 1 maybe fast-forward directly docker compose run week 2 depending much time maybe go solution published today give everything need week 3 use transfer service video already watching video probably learn lot already like homework solution	0
data vault differ “ classic ” data warehouse concepts like cubes star/snowflake schemas	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
first inform dataset 're using	really depends cloud showed elastic beanstalk n't think needed change much anything n't remember changing anything thing changed url service test.py script thing needed change think heroku 's similar actually couple tutorials using pythonanywhere heroku probably explain need change go see need change anything probably much	0
first inform dataset 're using	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
first inform dataset 're using	start working n't inform imagine everyone started informing take lot time answer finish answering questions first november yeah n't inform please write slack 're sure think good dataset go ahead use	1
first inform dataset 're using	far week two week three follow simple approach simple approach described take entire dataset split three parts train validation test 'll use week two week three week four talk cross validation – explain right want say approach validation sufficient many cases think 's pretty safe say many many many applications split enough cross validation course nicer learn week four right go detail try explain wait video “ select approach cross validation ” would say multiple options k-fold… 'm sure many people understand n't covered yet basically dataset big n't need k-fold dataset smaller use k-fold “ leave p ” – 've never used personally	0
first inform dataset 're using	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
collaborate startups get chance internship bootcamp	n't think takes lot time first need record videos prepare homework imagine adding collaboration startups well maybe sometime future n't know somebody startup listening right want hire people graduate zoomcamp please reach let 's arrange something	1
collaborate startups get chance internship bootcamp	classification classification let 's say n't predicting price let 's say predicting 's average number hours image 1 case would need another layer – rather layer 's called activation n't activation 're still… ’ trying formulate way 's confusing think 's better go lecture rewatch ’ essentially retelling thing need from_logits price variable loss mean squared error logits loss categorical cross-entropy categorical cross entropy usually used multiple categories need use logits 's numerically stable n't use – stay activation softmax n't use legit hope 's clear regression n't need use homework need use from_logits either homework two classes 's binary classification problem multiple classes – let 's say three classes four classes five classes – need use logits loss categorical cross-entropy loss categorical cross-entropy 's recommended use from_logits=true 's numerically stable	0
collaborate startups get chance internship bootcamp	case everyone uses docker problem n't use docker n't get point using docker ’ run write “ sorry run this. ” hope n't happen 's actually whole idea behind docker – 's transferable different environments let 's say use gunicorn n't use docker able test case use scoring matrix evaluate accordingly gunicorn tick box would encourage still try learn submission even run reason think reason run something windows ubuntu gunicorn everything runnable	0
collaborate startups get chance internship bootcamp	eventually want analysis see much time people spent homework also want breakdown know people registered – sign course say 're data analyst data engineer student also want see breakdown per role – much time took let 's say software engineers think week n't difficult engineers probably difficult students perhaps maybe weeks way around want analytics point still finish editing videos week six first 'll need prepare guidelines regarding projects maybe 'll analytics	0
collaborate startups get chance internship bootcamp	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
need use spark project size data big ca n't whole orchestration tables data warehouse airflow	alexey n't think actually time summarize everything course got inspired sejal – prepares notes 's recording video based notes share notes also take notes share notes fellow students way learn public – sort regurgitate content produce something new – useful useful others promise summaries encourage take notes share others jupyter books think something publish particular week jupyter notebook code snippets course share ankush definitely share code snippets code use particular week definitely shared github	0
need use spark project size data big ca n't whole orchestration tables data warehouse airflow	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
need use spark project size data big ca n't whole orchestration tables data warehouse airflow	alexey n't use spark depending exactly 're maybe tell us exactly dataset maybe give better recommendations n't use spark 's optional course use want n't use n't want tools think using airflow also fine maybe consider using dpt use case may easier whole thing airflow	1
need use spark project size data big ca n't whole orchestration tables data warehouse airflow	alexey 's early talk future iterations data engineering zoomcamp think chuckles ml camp yes – videos difference homework 's still discussion n't know 'll see ankush yeah think slight discussion met sure going repeat ml de zoomcamp let 's see response participation feel definitely success definitely feel free pass around course give us feedback maybe make better decision alexey re-recording videos… 's much effort n't think 'll want let 's say ml zoomcamp example much effort 'd rather reuse videos instead making new ones hope videos good decide rerun need re-record yeah 'll see	0
need use spark project size data big ca n't whole orchestration tables data warehouse airflow	victoria would say ’ also 's different set products example data warehouse start data tech stack scratch would definitely use redshift cost lower end need probably would need migrate something like bigquery snowflake lot documentation 's pretty easy set 's around longer – think reason broader adoption mainly around things alexey seems aws popular well least berlin look companies use cloud maybe 70 people use aws 20 may use gcp remaining 10 use azure 's roughly see maybe 'm wrong course also got use gcp bit seems ui nicer go web console 's little bit nicer think comes tools 's bit difficult example aws pip install aws cli google cloud 's bit difficult maybe 's one reason aws popular – ’ mature maybe think gcp catching nice interface sometimes things cheaper bigquery also think big advantage maybe clouds n't really victoria plus aws around 10 years thesis using aws thing gcp yet 's lot people go 're familiar outside stability alexey think cloud services services quite similar ’ used aws many things gcp straightforward n't typical less think many concepts map one another	0
explain different flask modules use flask learn	's really case-dependent saw slack remember correctly carlos asked needs drop features – seemed okay drop highly correlated feature assistant name target variable churn basically people work one particular person tend turn less people work somebody else think case seems like relevant feature include n't know – 's really case-dependent usually need sort domain expertise decide feature introduces leakage carlos ’ case think 's safe leave drop see performance validation dataset changes gives idea important feature	0
explain different flask modules use flask learn	n't know much different flask modules machine learning probably n't need know need play around flask need machine learning engineer course many sufficient opinion maybe 'm wrong 'm happy talk slack	1
explain different flask modules use flask learn	yes submit anywhere want say 20 pt n't remember yeah work good article “ good ” meaning 's copy/pasting code nothing else actually need explain 's going also article – closer time working article guidelines well	0
explain different flask modules use flask learn	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
explain different flask modules use flask learn	alexey question lecture use exception output exception n't include top 5x5x2048 long long long tube – three-dimensional thing dmitry know 's 5x5 think convolutional layers 1x1 – quite end 2000-something filters result applying filters still lot feature maps 'm completely sure know 's going dmitry happens exception last layer dmitry think 's filters alexey yeah bunch of… dmitry convolutional nature alexey yeah something specified coming pretrained network	0
would advantage using airflow azure datafactory etl tool	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
would advantage using airflow azure datafactory etl tool	victoria 'm sure answer mean yes definitely adding part project least workshop reason want people learn also consider people entirely new concept thought would much overhead try cover things already quite complex day-to-day even though work daily dbt work slow-changing dimensions daily basis example 's also think 's something look moment use definitely dbt especially deletions implementing load step done hooks think 's section advanced knowledge link hooks like incremental models mentioned changing dimensions n't link could link something could use snapshots think added concept updates guess could also pre-hook post-hook also something would incremental model incremental model loads new chunk data use merge depending workers use use could insert update 's something would work unique key project also final project 's understand complex go 's limit hope answers	0
would advantage using airflow azure datafactory etl tool	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
would advantage using airflow azure datafactory etl tool	alexey add comment least workplace airflow instance set us data scientist would go instance airflow managed data engineers devops engineers would use never needed actually run docker compose moment multiple dags orchestration tool quite useful would personally maybe go luigi favorite one need think people well since people know luigi many like maybe airflow would actually better choice consider sejal would also like add question docker compose used docker compose airflow style use official setup seeing first time well 's unnecessarily complicated especially xcom variables forth generally docker compose acts wrapper docker containers production workflows docker containers used integration ci/cd pipelines wherever 're going deploy docker compose docker compose part helpful development environments case let 's say mock version airflow environment production want test dags local development environment use docker compose make things easier running simple commands like ‘ docker compose ’ would make fine instead running ‘ docker run container name etc whatever want ’ terms convenience terms usage ’ generally used development environments production alexey think first part question “ close usual data engineer role workflows ” workflows see work complex steps probably also case need start something already – would n't call complex pipeline – three four steps already something eventually might grow bigger bigger add steps ankush would also like add 're cloud maybe use airflow deploy docker maybe actually use service provided cloud provider one similar airflow think airflow would used internally like cloud composite google cloud platform would using terraform sejal yeah used aws step functions aws setup step functions combination lambda place airflow ankush said also something wanted say really depends production self-hosted cluster kubernetes wherever want use pure airflow version company could kind setup use native airflow cluster could cloud-based setup convenient option use managed services cloud composer aws also version called managed airflow	0
would advantage using airflow azure datafactory etl tool	alexey ’ guessing sort managed solution etl advantage n't need manage airflow airflow need host somewhere somebody needs look dags n't run somebody needs go check n't run could want 's managed solution headaches go away n't know flexible 's advantage ’ also disadvantage airflow lot azure datafactory might quite specific azure maybe things outside azure 'm speculating 've never used victoria actually turns 's cloud version information service think sql 've used past would say 'm also super-expert airflow 's definitely flexible 'm guessing 's better azure problem need sql server 're limited sorts things use 're limited things n't super easy n't dag – 're gon na generate generate steps 's clear least experience airflow using ssis alexey azure datafactory something similar ssis right victoria yeah 's like cloud version kind says 's looking modernize ssis alexey remember first time saw ssis like “ spend time coding things java could drag drop things connect arrows works ” answer n't always work sometimes need flexibility 's need java developers would go fix	1
project use spark transformation can/should embed airflow	alexey 33 submissions think decision extend deadline good one hopefully end week see higher number understand project much difficult watching course following along 33 people – probably put lot effort thanks saw one person write put 100 hours already project outstanding sorry cruel 100 hours lot	0
project use spark transformation can/should embed airflow	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
project use spark transformation can/should embed airflow	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
project use spark transformation can/should embed airflow	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
project use spark transformation can/should embed airflow	alexey something n't show week five ideally yes one steps workflow yeah use definitely ankush yeah absolutely decide reason please document – reproduce rather person evaluating reproduce alexey spark operators see 's operator ankush think sparksubmitoperator one would important one us think one would one submit spark job right maybe one might helpful alexey honest n't really know use work use wrapper around airflow write yaml file saying “ want execute job cluster. ” data engineers made simple us use	1
'm master 's computer science aiming machine learning engineer want prepare job market course right overkill	sejal recently podcast episode alexey coincidentally published spotify anchor day transitioning data engineering machine learning engineering covers lot overlapping skills would recommend listen nutshell would say particular course would really foundational build basics see really overlaps building production skills – productionize ml workflows example lot technologies data engineering use ml engineering mlops side think definitely useful overkill alexey another zoomcamp course specifically training machine learning engineer wife nice illustration course learn basics machine learning know basics n't need take modules like deploying machine learning models serverless deep learning kubernetes tensorflow serving – kinds things course course n't cover data preparation data preparation course de zoomcamp quite useful	1
'm master 's computer science aiming machine learning engineer want prepare job market course right overkill	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	0
'm master 's computer science aiming machine learning engineer want prepare job market course right overkill	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
'm master 's computer science aiming machine learning engineer want prepare job market course right overkill	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
'm master 's computer science aiming machine learning engineer want prepare job market course right overkill	alexey 's early talk future iterations data engineering zoomcamp think chuckles ml camp yes – videos difference homework 's still discussion n't know 'll see ankush yeah think slight discussion met sure going repeat ml de zoomcamp let 's see response participation feel definitely success definitely feel free pass around course give us feedback maybe make better decision alexey re-recording videos… 's much effort n't think 'll want let 's say ml zoomcamp example much effort 'd rather reuse videos instead making new ones hope videos good decide rerun need re-record yeah 'll see	0
many points get article project	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
many points get article project	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
many points get article project	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
many points get article project	need check exactly promised first lesson 5-10 maybe – something like write n't need write article yet	1
many points get article project	flask app produce result actually return prediction 're good n't need fix answer question regarding many points deducted think whole project kind assumes managed deploy flask n't able put docker able deploy cloud lose like five six points something along lines think 's good idea make sure flask returns something problems please share code think know asking question talked already slack maybe error somewhere predict.py script need make sure fix error work perhaps take code lectures – think xgboost model logistic regression need put x variable t-matrix xgboost instead using predict_proba need use predict think 's – n't work please write slack let 's try figure 's working	0
features correlate highly labels recommended drop order avoid leakage dropped risk losing relevant features	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
features correlate highly labels recommended drop order avoid leakage dropped risk losing relevant features	’ two weeks october 18 2021 meaning first november 2022	0
features correlate highly labels recommended drop order avoid leakage dropped risk losing relevant features	flask – let draw let 's say want create web service image 5 web service gets requests web service responds something could predictions could something else – n't related machine learning implement web service use flask use fastapi use framework creating web services use flask use something else – flask allows use post example let 's say “ get ” usually parse parameters query let 's say go google put “ test ” see q=test https //www.google.com/search q=test oq=test aqs=chrome.0.69i59l2j69i65l3j69i60j69i61l2.1264j0j1 sourceid=chrome ie=utf-8 image 6 parameter pass “ get ” request “ get ” request sometimes let 's say want score customer n't want put gender=female contract=one year string send json post post also add body request generally use post general use case flask able create web service	0
features correlate highly labels recommended drop order avoid leakage dropped risk losing relevant features	's really case-dependent saw slack remember correctly carlos asked needs drop features – seemed okay drop highly correlated feature assistant name target variable churn basically people work one particular person tend turn less people work somebody else think case seems like relevant feature include n't know – 's really case-dependent usually need sort domain expertise decide feature introduces leakage carlos ’ case think 's safe leave drop see performance validation dataset changes gives idea important feature	1
features correlate highly labels recommended drop order avoid leakage dropped risk losing relevant features	took course ago back useful think something use python immediately could helpful 'm sure would good recommendation current course course good would n't compare stanford course andrew ng simply different course 'll get lot theory one practical hands-on – 's mostly writing code together maybe take	0
would productionize dbt airflow	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
would productionize dbt airflow	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	0
would productionize dbt airflow	victoria 's dbt package two ways use dbt cloud could actually put airflow trigger shell dbt cloud already something think example created shells run everything airflow job id could trigger send credentials dbt cloud would one way 'm guessing targeted shell using airflow scheduler dbt cloud completely valid 's package could use 's another thing says “ invoking dbt bash operator. ” page official documentation run dbt production general one options via airflow check 's link package also link explanation use via bash operator also check ways guess example suggested using chrome 're working locally also something 's ’ fancy airflow setup chrome never fails alexey well airflow nothing fancy chrome right victoria yeah exactly chuckles	1
would productionize dbt airflow	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
would productionize dbt airflow	victoria 'm sure answer mean yes definitely adding part project least workshop reason want people learn also consider people entirely new concept thought would much overhead try cover things already quite complex day-to-day even though work daily dbt work slow-changing dimensions daily basis example 's also think 's something look moment use definitely dbt especially deletions implementing load step done hooks think 's section advanced knowledge link hooks like incremental models mentioned changing dimensions n't link could link something could use snapshots think added concept updates guess could also pre-hook post-hook also something would incremental model incremental model loads new chunk data use merge depending workers use use could insert update 's something would work unique key project also final project 's understand complex go 's limit hope answers	0
certificate end require end project validated	ankush avro parquet totally different things avro row-based parquet columnar something like batch processing data-analytical kind work parquet perfect solution use spark something 's super fast calculate something like sum account parquet file read particular column rather whole column whole row avro hand strong providing flexibility terms backward forward compatibility 're using avro kafka using something like protobuf kafka would really useful think like different technologies altogether use different use cases alexey avro guess use case would streaming parquet batch jobs ankush yeah think definitely beginning would basic use cases yes alexey also something called ocr ankush 's also columnar storage alexey oh 's different orc chuckles remember watching presentation berlin buzzwords format remember seeing cool presentation never actually tried ankush neither i. alexey 's optimized row columnar interesting never knew stood widely used people go parquet ankush seen parquet guess companies definitely using 's bad technology think 's parquet famous compatible solutions parquet people tend use often	0
certificate end require end project validated	alexey n't think really understand question ankush 're vpc matter alexey cluster – spark cluster cluster kafka cluster ankush 're running vpc able access 's ip right alexey yeah oh okay externally ip guess 're running inside network use internal ip 's inside kubernetes refer names deployments n't know ankush also n't know 's really specific particular use case need give us maybe slack explain bit – running kind machines exactly exposing terms external ip alexey usually kafka cluster kafka machines spark cluster spark cluster connects kafka reads data ankush depends vpc running 're running different vpcs need expose separately need expose ip running vpc matter able access internally might wrong 'm devops chuckles alexey usually take things granted maybe good thing work grateful data team makes tools work often n't ask “ okay actually configured ” works n't – n't go support channel ask “ hey please fix ” ankush think 's also data team 's also maybe devops team embedded inside data team 're working olx pretty big company maybe many smaller teams inside big teams guess 's devops topic	0
certificate end require end project validated	alexey example spark google “ spark gcp bigquery connector ” good article shows need need jar need use jar submit spark job let 's say files stored google cloud storage something files multiple options first option save back google cloud storage create external table something like like saw week 3. another option would saving directly bigquery using say “ write ” say want write bigquery uses connector connect bigquery write n't know permissions actually work – maybe also need specify key somehow probably 're already dataproc already permissions need order write bigquery order read google cloud storage use dataproc theoretically easy – add extra jar write bigquery data data warehouse use materials dbt week analytics engineering week actually consume content data warehouse visualize	0
certificate end require end project validated	victoria n't used anaconda think 's also documentation install dbt locally general 're using anaconda 're using environment install use homebrew 're using mac also use pip install pip install dbt postgres example need adapter dbt snowflake dbt big query – whatever want use locally 's 're installed set profiles.yml show videos create project clone whatever 's 's need use locally never install using anaconda 'm guessing 's probably something similar probably conda forge install like 's possible otherwise would go pip install alexey anaconda usually pip victoria yeah 're environment install like 's	0
certificate end require end project validated	alexey yes mentioned certificate require end project validated way peer reviews get review three projects peers 'll formal criteria need use evaluate peers score based might wonder “ okay fair somebody else judges work ” tested way evaluating projects different course running machine learning zoomcamp worked quite well good approach also chance learn peers every time review project learn something new people already know something maybe peers use something else n't covered course course case ask document everything feel lost get idea exactly project also see way learning extra things going validate projects	1
able review others ’ projects work ubuntu use windows wo n't able run code case	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
able review others ’ projects work ubuntu use windows wo n't able run code case	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
able review others ’ projects work ubuntu use windows wo n't able run code case	multi-classification – yes clustering – 'm sure probably good idea n't cover hard – know evaluate peers know better clustering	0
able review others ’ projects work ubuntu use windows wo n't able run code case	case everyone uses docker problem n't use docker n't get point using docker ’ run write “ sorry run this. ” hope n't happen 's actually whole idea behind docker – 's transferable different environments let 's say use gunicorn n't use docker able test case use scoring matrix evaluate accordingly gunicorn tick box would encourage still try learn submission even run reason think reason run something windows ubuntu gunicorn everything runnable	1
able review others ’ projects work ubuntu use windows wo n't able run code case	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
expected document model performance case use competition dataset since test labels available submit predictions score	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
expected document model performance case use competition dataset since test labels available submit predictions score	’ two weeks october 18 2021 meaning first november 2022	0
expected document model performance case use competition dataset since test labels available submit predictions score	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
expected document model performance case use competition dataset since test labels available submit predictions score	case take dataset available kaggle pretend data split usual thing – test validation train – test full train decide want validate models using cross-validation holding one part dataset usual thing n't rely test dataset want submit model show maybe take screenshot final performance 's also fine 's required	1
expected document model performance case use competition dataset since test labels available submit predictions score	yes submit anywhere want say 20 pt n't remember yeah work good article “ good ” meaning 's copy/pasting code nothing else actually need explain 's going also article – closer time working article guidelines well	0
deployed models work apis	building locally fine	0
deployed models work apis	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
deployed models work apis	think around 200 204 think something like	0
deployed models work apis	yes	1
deployed models work apis	’ two weeks october 18 2021 meaning first november 2022	0
help us coordinate peer reviews decide review review	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
help us coordinate peer reviews decide review review	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
help us coordinate peer reviews decide review review	honest n't used uvicorn used gunicorn n't know matters n't actually checked benchmarks use gunicorn also use others – n't remember ones reason basically	0
help us coordinate peer reviews decide review review	flask – let draw let 's say want create web service image 5 web service gets requests web service responds something could predictions could something else – n't related machine learning implement web service use flask use fastapi use framework creating web services use flask use something else – flask allows use post example let 's say “ get ” usually parse parameters query let 's say go google put “ test ” see q=test https //www.google.com/search q=test oq=test aqs=chrome.0.69i59l2j69i65l3j69i60j69i61l2.1264j0j1 sourceid=chrome ie=utf-8 image 6 parameter pass “ get ” request “ get ” request sometimes let 's say want score customer n't want put gender=female contract=one year string send json post post also add body request generally use post general use case flask able create web service	0
help us coordinate peer reviews decide review review	way want organize everyone working project submit first november first november see submitted project people projects randomly select three people review means one get review three projects send email hope work still n't written script sending emails saying “ hey need review three projects 's form need put hash need put assessment there. ” one week	1
easier way one-hot encoding without using dictvectorizer	flask app produce result actually return prediction 're good n't need fix answer question regarding many points deducted think whole project kind assumes managed deploy flask n't able put docker able deploy cloud lose like five six points something along lines think 's good idea make sure flask returns something problems please share code think know asking question talked already slack maybe error somewhere predict.py script need make sure fix error work perhaps take code lectures – think xgboost model logistic regression need put x variable t-matrix xgboost instead using predict_proba need use predict think 's – n't work please write slack let 's try figure 's working	0
easier way one-hot encoding without using dictvectorizer	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
easier way one-hot encoding without using dictvectorizer	think dictvectorizer easiest one reason thought would good use course let show use one-hot encoder also showed need “ column stack ” makes bit difficult using dictvectorizer dictvectorizer throw everything get matrix find easier one-hot encoder 'll share code well take look decide easier – one-hot encoder dictvectorizer	1
easier way one-hot encoding without using dictvectorizer	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
easier way one-hot encoding without using dictvectorizer	reason think flask popular framework 's relatively simple 's main reason	0
project long timeframe complete	alexey right mind two weeks depends goes first time ’ running course see much time takes n't want spend two weeks want get something small n't get ambitious please n't process common crawl data project ideally two weeks let 's see	1
project long timeframe complete	alexey think 's helpful 's necessary helpful happens dags run airflow dags tasks tasks often kubernetes jobs something like infra – many companies infrastructure managed kubernetes 's quite simple get new container execute new job kubernetes n't think 's necessary think 's helpful 'm data engineer – 'm data scientist data scientist knowledge kubernetes quite helpful could debug jobs 'm running airflow run kubernetes could see logs would say want learn hurt get hired without kubernetes knowledge materials course sufficient get hired pick kubernetes work example agree victoria victoria yeah think kubernetes getting popular ca n't hurt	0
project long timeframe complete	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
project long timeframe complete	alexey individual groups think becomes trickier victoria yes guess always discuss slack someone else could using dataset alexey yeah think becomes group project case right dataset different use cases end	0
project long timeframe complete	victoria course specifically think cover postgres mainly 's easier many people could n't set google cloud account 's nice alternative end still database query data load data still get concept get practice real life 've seen smaller companies since 's also easy set 's easy way start locally 'd probably instance something like ’ little bit setup would say least 've seen would scenario alexey would add postgres originally created still intended used transactional database analytical database means – let 's say ecommerce shop fill basket orders kinds queries like block something like – transactions transactions consistency would use different databases analytical workloads want analyze lot data databases actually based postgres example redshift greenplum quite based postgres n't usually use usually ’ use plain postgres kinds things local setup right easiest way work reasonably well quite lot databases – quite large databases n't think actually hit performance bottleneck postgres course need lot data notice	0
see leaderboard understand 'm right answers	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
see leaderboard understand 'm right answers	alexey think saw thing back talking dataproc bigquery spark connector probably unfortunately n't used tool use dataproc n't know reason could 're using vm reason could vm maybe permissions write bigquery need check configure connector order able use key suspect could problem also make sure role use accessing google cloud storage actually write bigquery think 's “ bigquery admin ” something like make sure permission 's called “ service account ” think service account right permissions case created separate service account make sure permission unfortunately working example share right something want check weekend maybe record video time please feel free share slack think students help well think managed make work used spark gcp please share problem slack perhaps somebody solved problem already help	0
see leaderboard understand 'm right answers	alexey one thing use images unless really know 're probably n't super small dataset one megabyte ideally victoria probably one file taxis four least two think also 's important understand course focused concepts everything 're going use focused building pipeline processing data actual presentation maybe 's super cool analysis images saying reality need practice processing data 's 's easier use something else alexey maybe come across dataset 're sure 's good dataset ask slack 'll help think list gave quite good 's good start course talk kaggle tons different datasets contain images tweets example way also use tweets structured information extract tweets well maybe actually good let 's say want practice streaming get stream tweets coming twitter process fly think could also good project sometimes also parse something example websites like ebay selling buying things build parser getting data thing could go every day take data something data put csv example put data warehouse something like n't spend much time scraping think already scrapers available maybe google “ ebay scraper ” find code n't spend lot time trying build scraper victoria also something like twitter suggested instagram well – apis makes part super easy want something like find something already api 's probably easiest option also get practice also good	0
see leaderboard understand 'm right answers	alexey week 2 yes – upload video solution also update leaderboard probably take time depending clean data – need lot data cleaning take lot time please n't put things n't look like emails email field fill please careful takes time clean later	1
see leaderboard understand 'm right answers	victoria set warehouse end define connection difference would use adapter dbt several adapters would first want use dbt go check adapters make sure 's supported several official example redshift could use community-supported azure alexey yeah aws athena azure athena redshift presto well example presto cluster use aws – within aws presto cluster potentially similar setup connect dbt presto victoria general add little bit answer let 's say project right bigquery would change profile 's set connection may make changes 's something sql supports bigquery way around able run project minutes	0
know examples/code object-oriented programming applied data engineering	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
know examples/code object-oriented programming applied data engineering	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
know examples/code object-oriented programming applied data engineering	alexey one thing use images unless really know 're probably n't super small dataset one megabyte ideally victoria probably one file taxis four least two think also 's important understand course focused concepts everything 're going use focused building pipeline processing data actual presentation maybe 's super cool analysis images saying reality need practice processing data 's 's easier use something else alexey maybe come across dataset 're sure 's good dataset ask slack 'll help think list gave quite good 's good start course talk kaggle tons different datasets contain images tweets example way also use tweets structured information extract tweets well maybe actually good let 's say want practice streaming get stream tweets coming twitter process fly think could also good project sometimes also parse something example websites like ebay selling buying things build parser getting data thing could go every day take data something data put csv example put data warehouse something like n't spend much time scraping think already scrapers available maybe google “ ebay scraper ” find code n't spend lot time trying build scraper victoria also something like twitter suggested instagram well – apis makes part super easy want something like find something already api 's probably easiest option also get practice also good	0
know examples/code object-oriented programming applied data engineering	ankush 're using java scala might somehow using object-oriented programming hood let 's say scala 're using spark would creating user classes – 're using dataframes 're using rdd create 's already object-oriented programming ways obviously extensive might libraries working libraries definitely however would still stick data frames maybe stuff rather using core object-oriented programming technology data engineering say alexey alexey think streaming like producers consumers – objects usually right ankush bytes convert python convert dictionary scala also convert dictionary want ... cross-talk alexey think consumes – let 's say kafka consumer ... usually ... 'm trying ... remember kinesis kind used thing class class method like “ consume ” define extend base class say “ consumer run method consume method one. ” define logic processing message logic pulling message committing lives elsewhere ankush yeah 's style programming regarding object-oriented programming understand like interfaces inheritance polymorphism 's say – n't overcomplicate 's data alexey think complicate example ankush laughs see use oop might overcomplicate things alexey yeah think inheritance – base object pull message commit extend define consume method 's useful maybe would easier function “ consume ” would thing right think cases better modularity modular nicely-documented code use oop object-oriented programming cases 's mess n't know 's going ankush exactly simple stuff generally wo n't need	1
know examples/code object-oriented programming applied data engineering	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
explain json use post general use cases flask	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
explain json use post general use cases flask	like said 's always problem- data-specific datasets get good performance	0
explain json use post general use cases flask	flask – let draw let 's say want create web service image 5 web service gets requests web service responds something could predictions could something else – n't related machine learning implement web service use flask use fastapi use framework creating web services use flask use something else – flask allows use post example let 's say “ get ” usually parse parameters query let 's say go google put “ test ” see q=test https //www.google.com/search q=test oq=test aqs=chrome.0.69i59l2j69i65l3j69i60j69i61l2.1264j0j1 sourceid=chrome ie=utf-8 image 6 parameter pass “ get ” request “ get ” request sometimes let 's say want score customer n't want put gender=female contract=one year string send json post post also add body request generally use post general use case flask able create web service	1
explain json use post general use cases flask	alexey one thing maybe n't clear enough – please always include code solution maybe people n't watch first said n't submit code homework get zero points saw submissions people put dot empty put link towards data science article example please n't please put code n't 'll get zero points entire week 'll make explicit form week three well please n't forget put code	0
explain json use post general use cases flask	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
way transform variable apart log1p expm1 transform	yes good references – course course called machine learning zoomcamp cover parts except conda git assume midterm project use git conda – use conda python interpreter install pipenv manage dependencies n't know good references honest maybe n't know answered question hope references course good enough ’ please let know think improved think missing	0
way transform variable apart log1p expm1 transform	really depends cloud showed elastic beanstalk n't think needed change much anything n't remember changing anything thing changed url service test.py script thing needed change think heroku 's similar actually couple tutorials using pythonanywhere heroku probably explain need change go see need change anything probably much	0
way transform variable apart log1p expm1 transform	flask – let draw let 's say want create web service image 5 web service gets requests web service responds something could predictions could something else – n't related machine learning implement web service use flask use fastapi use framework creating web services use flask use something else – flask allows use post example let 's say “ get ” usually parse parameters query let 's say go google put “ test ” see q=test https //www.google.com/search q=test oq=test aqs=chrome.0.69i59l2j69i65l3j69i60j69i61l2.1264j0j1 sourceid=chrome ie=utf-8 image 6 parameter pass “ get ” request “ get ” request sometimes let 's say want score customer n't want put gender=female contract=one year string send json post post also add body request generally use post general use case flask able create web service	0
way transform variable apart log1p expm1 transform	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	1
way transform variable apart log1p expm1 transform	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
preferred use mae metric instead rmse	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
preferred use mae metric instead rmse	mae – talk great detail mae rmse metrics evaluating quality regression models course use rmse mae alternative one rmse formula mae formula little bit different instead looking squared error look absolute error absolute means use bars formula 're little bit different rmse penalizes big errors let 's say prediction 0.1 actual prediction 100. rmse get higher penalty 's 99.9 squared huge huge penalty mae 99.9. pros cons – use n't choose one always report numbers	1
preferred use mae metric instead rmse	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
preferred use mae metric instead rmse	far week two week three follow simple approach simple approach described take entire dataset split three parts train validation test 'll use week two week three week four talk cross validation – explain right want say approach validation sufficient many cases think 's pretty safe say many many many applications split enough cross validation course nicer learn week four right go detail try explain wait video “ select approach cross validation ” would say multiple options k-fold… 'm sure many people understand n't covered yet basically dataset big n't need k-fold dataset smaller use k-fold “ leave p ” – 've never used personally	0
preferred use mae metric instead rmse	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
main challenge data pipelines advise companies want build first-time data architecture/infrastructure	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	0
main challenge data pipelines advise companies want build first-time data architecture/infrastructure	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
main challenge data pipelines advise companies want build first-time data architecture/infrastructure	alexey discussion/thread announcement channel week ago – lot datasets shared n't know many cool websites datasets check victoria put announcements alexey also want slack dump datatalks.club also use play around dataset see 's bunch json files build data pipelines well	0
main challenge data pipelines advise companies want build first-time data architecture/infrastructure	ankush 're starting would suggest keep basics correct try keep simple possible complicate data pipeline harder let 's rephrase maybe talk basics first use terraform use docker use spark example technology scale n't use hadoop – use s3 google cloud storage 're already cloud services try use services already provided cloud provider example aws emr elastic mapreduce deploy spark jobs google cloud services dataproc dataflow also airflow cloud composer called cloud composer google cloud bigquery try use much possible try keep everything whole data pipeline simple possible less steps 're going easier 's going introduce another step another landing zone something like really need simple keep – 's easier maintain 's easier change obviously 'll change obviously get complicated grow use cases try keep basics right beginning start simple use cases sejal would say crux data pipeline end automatic level etl pipeline basic steps extract transform load 're using modern version elt extract load transform focus building core components add complexity around areas automate things like like ankush said	1
main challenge data pipelines advise companies want build first-time data architecture/infrastructure	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
could give us breakdown left course 're expected finish	ninad presenting quick sketch still left today 're starting capstone project peer reviewing capstone project two weeks capstone project starts today two weeks one week review project sure time able extend many countries christmas break starts week finish peer reviewing 'm sure flexible capstone project please try finish deadline 'm sure 'll able extend reason shuffled things around bit know many people vacation holidays christmas break 's focus capstone project right probably idea expected already midterm project hope got inspired two presentations right terms things kubernetes kubeflow still kubernetes try record everything week make available somebody wants use kubernetes capstone project able hopefully homework kubernetes 'll publish videos lot time finish actually let 's say christmas break end december first week january spend solving kubernetes homework however prefer know countries break actually starts 31st december finishes like 10th january russia example ’ like 're free choose exactly want spend time kubernetes homework kubeflow – setting kubeflow really difficult 're really motivated follow videos homework required homework – optional want practice kubeflow something graded homework also promised third project bit behind midterm projects n't time finish various reasons third project like projects also 's like two three three well third project officially start 10th january course start earlier exactly midterm capstone project different dataset 'll one week peer-reviewing finish entire course 31st january last thing – articles article take topic n't covered course cover – write short article code n't really figured logistics yet something want prepare time – december january – work article want course optional n't – want ’ two months basically remind – get certificate course need finish two three projects 's main requirement article optional – get extra points well 's roughly plan	1
could give us breakdown left course 're expected finish	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
could give us breakdown left course 're expected finish	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
could give us breakdown left course 're expected finish	building locally fine	0
could give us breakdown left course 're expected finish	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
use test dataset	's really case-dependent saw slack remember correctly carlos asked needs drop features – seemed okay drop highly correlated feature assistant name target variable churn basically people work one particular person tend turn less people work somebody else think case seems like relevant feature include n't know – 's really case-dependent usually need sort domain expertise decide feature introduces leakage carlos ’ case think 's safe leave drop see performance validation dataset changes gives idea important feature	0
use test dataset	soft skills resources career questions would recommend actually go datatalks.club slack channel called career questions go ask questions use slido link course way separate two sessions talk course career slack talk careers	0
use test dataset	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
use test dataset	building locally fine	0
use test dataset	n't use test dataset homework actually used trained model validation found best parameters found best set features want use found best model compared multiple models perhaps combine train validation datasets train one bigger model check test dataset make sure everything worked check lectures – think every week starting first third week covered make sure check	1
use google colab submit homework 1. use rest program recommended set local environment linux	n't think takes lot time first need record videos prepare homework imagine adding collaboration startups well maybe sometime future n't know somebody startup listening right want hire people graduate zoomcamp please reach let 's arrange something	0
use google colab submit homework 1. use rest program recommended set local environment linux	yes n't written script yet actually want – many people asked see results made mistake beginning letting people register n't add checkbox saying 's okay use data like first name last name public really public leaderboard everyone go check score simply n't ask permission want instead write web service ’ email field go put email get scores idea hope 'll able implement hopefully n't difficult able get scores way yes grade homework store scores course end remember first video talked public leaderboard end course 100 people leaderboard ask permission publish names 'll need figure exactly communicate results separate homework grading actually difficult want write script	0
use google colab submit homework 1. use rest program recommended set local environment linux	use google colab yes definitely n't see problem – caveat deployment sessions possible use colab likely n't know use need sort local environment something cloud access command line execute things like docker access python interpreter windows work – linux mac os probably best option use windows 10 – ’ actually windows right show image reference windows subsystem linux 'm basically running linux right usual linux commands like git whatnot recommend try otherwise use plain windows well	1
use google colab submit homework 1. use rest program recommended set local environment linux	reason like fastapi go	0
use google colab submit homework 1. use rest program recommended set local environment linux	building locally fine	0
'm thinking using aws redshift ec2 s3 project concepts similar gcp	ankush yes absolutely aws redshift would quite similar bigquery – quite similar still similar think aws redshift postgres cluster – big postgres cluster s3 definitely one one comparison google cloud storage 'm sure want use ec2 want deploy spark jobs maybe consider emr 're using ec2 maybe pipelining airflow ec2 also pretty good 's machine alexey used virtual machines many things would equivalent indeed need remote computer running data center s3 buckets fast download something s3 upload something s3 's usually way faster ec2 another similar concept athena think 's thing top data lake gives “ data warehouse feeling ” ankush 's sql top data lake alexey would say 's something like ... feels like something bigquery redshift 's fast 's also cheaper redshift ankush yeah think 're maybe comparing presto might better fit 's presto basically internally outside 's presto cluster alexey yeah presto need google things – like need google syntax athena – usually add presto end works ankush chuckles yeah exactly aws also managed workflow apache airflow 're thinking using airflow also check deploy less stuff alexey think google cloud also aws create emr ankush mentioned already ankush emr aws something like google cloud dataflow alexey create spark cluster one click think shared document explains configure cluster maybe 'll share office hours want use	1
'm thinking using aws redshift ec2 s3 project concepts similar gcp	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
'm thinking using aws redshift ec2 s3 project concepts similar gcp	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
'm thinking using aws redshift ec2 s3 project concepts similar gcp	alexey well good job keep working almost finished first week finish start week two finish week three chuckles follow sequence videos playlist github sufficient think questions go slack ankush keep going alexey chuckles yes exactly ’ almost finished first week good sign first week year pretty tough many people lot problems docker whatnot 're almost finished good sign ’ good track keep working	0
'm thinking using aws redshift ec2 s3 project concepts similar gcp	alexey well guess want dashboard aggregates hard time trying imagine dataset take raw form put dashboard maybe 's already grouped 's already aggregated 's probably small dataset 's suitable course think maybe select reasonably big dataset require transformation 're doubt ask slack really n't know kind dataset put dashboard without transformations ankush yeah minimum transformations would required alexey counts group-bys maybe joins ankush exactly think 's better choose dataset allow	0
know equivalent keras pytorch	classification classification let 's say n't predicting price let 's say predicting 's average number hours image 1 case would need another layer – rather layer 's called activation n't activation 're still… ’ trying formulate way 's confusing think 's better go lecture rewatch ’ essentially retelling thing need from_logits price variable loss mean squared error logits loss categorical cross-entropy categorical cross entropy usually used multiple categories need use logits 's numerically stable n't use – stay activation softmax n't use legit hope 's clear regression n't need use homework need use from_logits either homework two classes 's binary classification problem multiple classes – let 's say three classes four classes five classes – need use logits loss categorical cross-entropy loss categorical cross-entropy 's recommended use from_logits=true 's numerically stable	0
know equivalent keras pytorch	n't think takes lot time first need record videos prepare homework imagine adding collaboration startups well maybe sometime future n't know somebody startup listening right want hire people graduate zoomcamp please reach let 's arrange something	0
know equivalent keras pytorch	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
know equivalent keras pytorch	dmitry ’ quite strange question think interpret way 're frameworks keras high-level framework meaning create network let 's say 10 rows code blocks like model app model compile create something lego style 's easy pytorch 's low-level framework meaning things example layer definitions functions need write right far heard checked pytorch going direction predefined functions people n't need rewrite use minor tweaks think equivalent understand question correctly alexey think library n't really use pytorch well ran things pytorch never trained model n't write code pytorch heard thing called pytorch lightning also thing called catalyst 're higher level abstractions top pytorch probably similar keras way dmitry think high-level abstraction yeah sure – use catalyst 're basically two frameworks main goal sure add something pytorch like wrapper basically alexey keras historically also separate library move tensorflow get users	1
know equivalent keras pytorch	multi-classification – yes clustering – 'm sure probably good idea n't cover hard – know evaluate peers know better clustering	0
provide standard solution final project like 're weekly homeworks	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
provide standard solution final project like 're weekly homeworks	alexey 's want look project 're behind right n't worry take time decide whether give certificate based projects homework 's – catch week 3 lighter week 1 maybe fast-forward directly docker compose run week 2 depending much time maybe go solution published today give everything need week 3 use transfer service video already watching video probably learn lot already like homework solution	0
provide standard solution final project like 're weekly homeworks	alexey n't think projects work become standard examples future generations honor first one making easier future generations still n't decided want one time decide simpler ankush even n't decide videos	1
provide standard solution final project like 're weekly homeworks	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
provide standard solution final project like 're weekly homeworks	victoria next week difficult one right chuckles alexey think dbt difficult one victoria 'm already dbt week meant week five alexey week five n't know think kafka difficult one terms setup give problems every time need set kafka 's nightmare n't really needed set airflow locally think 's similar let 's see hope easier week bigquery relatively easy 's managed n't need infra setup machine dbt part also relatively simple maybe end course 'll survey 'll see difficult one hope airflow hope kafka spark simpler	0
unbalanced dataset need add parameter class_weight logistic regression model add properly	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
unbalanced dataset need add parameter class_weight logistic regression model add properly	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
unbalanced dataset need add parameter class_weight logistic regression model add properly	answer – depends dataset – typically train/validation split first try scaling try without scaling see happens think something talked last week actually notebook prepared last week go classification notebook notebook scaling shows scale features use standard scaler use min max scaler – use two different scalars shows particular dataset turned little bit better experiment see scaling makes sense linear models sometimes 's actually good idea add extra complexity pipeline instead dictionary vectorizer whatnot 'll also need add next step scaling basically steps prediction pipeline becomes bit difficult maintain would say try scale see improvement get significant maybe 's worth extra complexity get use cross-validation use best judgment see makes sense	0
unbalanced dataset need add parameter class_weight logistic regression model add properly	's actually think 's useful n't train.py produce pickle file let 's say want retrain model using different parameters – 's useful see changed score real life like projects work train.py 's always one file sometimes 's bunch files somehow log performance think 's useful n't want reason n't	0
unbalanced dataset need add parameter class_weight logistic regression model add properly	answer – use cross-validation find try different weights see makes sense particular dataset	1
homework 3 going require students submit public code url	alexey discussion/thread announcement channel week ago – lot datasets shared n't know many cool websites datasets check victoria put announcements alexey also want slack dump datatalks.club also use play around dataset see 's bunch json files build data pipelines well	0
homework 3 going require students submit public code url	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	0
homework 3 going require students submit public code url	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
homework 3 going require students submit public code url	alexey created form forgot include field found form located also edit permissions form go add url field make announcement please resubmit work reason already submitted chance resubmit maybe 'll tweak grading script little bit accept work people already submitted without url yeah change url	1
homework 3 going require students submit public code url	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
different thresholds also distribution target class churn data churn non-churn 50/50 think non-churn churn	building locally fine	0
different thresholds also distribution target class churn data churn non-churn 50/50 think non-churn churn	n't think takes lot time first need record videos prepare homework imagine adding collaboration startups well maybe sometime future n't know somebody startup listening right want hire people graduate zoomcamp please reach let 's arrange something	0
different thresholds also distribution target class churn data churn non-churn 50/50 think non-churn churn	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
different thresholds also distribution target class churn data churn non-churn 50/50 think non-churn churn	yeah indeed 's case move threshold like looked precision recall curve recall precision precision high high threshold could 0.7 0.8. precision high recall low distribution would – tiny amount customers predicted default rest okay – default image 4 course use best judgment model 's really important precise n't care identifying defaulting churning users set higher threshold course changes distribution	1
different thresholds also distribution target class churn data churn non-churn 50/50 think non-churn churn	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
please share code model2.bin tried unpickle could n't	n't think takes lot time first need record videos prepare homework imagine adding collaboration startups well maybe sometime future n't know somebody startup listening right want hire people graduate zoomcamp please reach let 's arrange something	0
please share code model2.bin tried unpickle could n't	multi-classification – yes clustering – 'm sure probably good idea n't cover hard – know evaluate peers know better clustering	0
please share code model2.bin tried unpickle could n't	yes 's really dataset dependent datasets good score datasets 0.6 good score example kaggle competition click prediction best models 0.65 auc really depends dataset already even core models used rule every model yeah 0.70 reasonably good auc value go ahead use final model	0
please share code model2.bin tried unpickle could n't	building locally fine	0
please share code model2.bin tried unpickle could n't	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	1
converting dataframes numpy arrays better use df.values df.to_numpy	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
converting dataframes numpy arrays better use df.values df.to_numpy	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
converting dataframes numpy arrays better use df.values df.to_numpy	n't think difference honest	1
converting dataframes numpy arrays better use df.values df.to_numpy	n't think takes lot time first need record videos prepare homework imagine adding collaboration startups well maybe sometime future n't know somebody startup listening right want hire people graduate zoomcamp please reach let 's arrange something	0
converting dataframes numpy arrays better use df.values df.to_numpy	took course ago back useful think something use python immediately could helpful 'm sure would good recommendation current course course good would n't compare stanford course andrew ng simply different course 'll get lot theory one practical hands-on – 's mostly writing code together maybe take	0
use unsupervised learning daily work much difficult traditional supervised machine learning	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
use unsupervised learning daily work much difficult traditional supervised machine learning	far week two week three follow simple approach simple approach described take entire dataset split three parts train validation test 'll use week two week three week four talk cross validation – explain right want say approach validation sufficient many cases think 's pretty safe say many many many applications split enough cross validation course nicer learn week four right go detail try explain wait video “ select approach cross validation ” would say multiple options k-fold… 'm sure many people understand n't covered yet basically dataset big n't need k-fold dataset smaller use k-fold “ leave p ” – 've never used personally	0
use unsupervised learning daily work much difficult traditional supervised machine learning	n't remember last time needed use unsupervised learning daily work need use master 's thesis think used kaggle competitions well work something similar clustering called “ locality-sensitive hashing ” allows group similar items together think clustering olx actually project needs clustering ’ one projects ’ tend happen often 's actually one reasons decided include unsupervised learning course n't think 's widely used supervised learning difficult traditional supervised machine learning sense 's easy actually evaluate performance clustering results let 's say group clients five groups know groups good difficult know 's probably main challenge unsupervised learning – evaluate quality	1
use unsupervised learning daily work much difficult traditional supervised machine learning	answer – depends dataset – typically train/validation split first try scaling try without scaling see happens think something talked last week actually notebook prepared last week go classification notebook notebook scaling shows scale features use standard scaler use min max scaler – use two different scalars shows particular dataset turned little bit better experiment see scaling makes sense linear models sometimes 's actually good idea add extra complexity pipeline instead dictionary vectorizer whatnot 'll also need add next step scaling basically steps prediction pipeline becomes bit difficult maintain would say try scale see improvement get significant maybe 's worth extra complexity get use cross-validation use best judgment see makes sense	0
use unsupervised learning daily work much difficult traditional supervised machine learning	yes n't written script yet actually want – many people asked see results made mistake beginning letting people register n't add checkbox saying 's okay use data like first name last name public really public leaderboard everyone go check score simply n't ask permission want instead write web service ’ email field go put email get scores idea hope 'll able implement hopefully n't difficult able get scores way yes grade homework store scores course end remember first video talked public leaderboard end course 100 people leaderboard ask permission publish names 'll need figure exactly communicate results separate homework grading actually difficult want write script	0
first remove target variable ‘ price ’ applied log transformation price variable	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
first remove target variable ‘ price ’ applied log transformation price variable	dmitry actually applied transformation removed data frame ’ understand question correctly alexey n't think matters whether first remove apply way around dmitry yeah important part future set end corrupted results alexey think somebody wrote slack forgot remove variable spent time figuring model good 's probably reason 's good chuckles right 's try remove data frame ’ accidentally use	1
first remove target variable ‘ price ’ applied log transformation price variable	flask app produce result actually return prediction 're good n't need fix answer question regarding many points deducted think whole project kind assumes managed deploy flask n't able put docker able deploy cloud lose like five six points something along lines think 's good idea make sure flask returns something problems please share code think know asking question talked already slack maybe error somewhere predict.py script need make sure fix error work perhaps take code lectures – think xgboost model logistic regression need put x variable t-matrix xgboost instead using predict_proba need use predict think 's – n't work please write slack let 's try figure 's working	0
first remove target variable ‘ price ’ applied log transformation price variable	yes submit anywhere want say 20 pt n't remember yeah work good article “ good ” meaning 's copy/pasting code nothing else actually need explain 's going also article – closer time working article guidelines well	0
first remove target variable ‘ price ’ applied log transformation price variable	answer – depends dataset – typically train/validation split first try scaling try without scaling see happens think something talked last week actually notebook prepared last week go classification notebook notebook scaling shows scale features use standard scaler use min max scaler – use two different scalars shows particular dataset turned little bit better experiment see scaling makes sense linear models sometimes 's actually good idea add extra complexity pipeline instead dictionary vectorizer whatnot 'll also need add next step scaling basically steps prediction pipeline becomes bit difficult maintain would say try scale see improvement get significant maybe 's worth extra complexity get use cross-validation use best judgment see makes sense	0
scripts process google forms	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
scripts process google forms	mean another iteration course yes go course repository see answer first take course self-paced mode videos available except last videos kserve start watching 's important first attempt homework without looking answer check solutions least one project 's also important n't follow videos sure learn something 's better solidify learning also practicing least one project 're taking course self-paced mode want get feedback project also share link slack happy give feedback project answer actual question regarding next cohort – start september cohort started september probably start bit earlier started middle september time probably start maybe one week earlier want notified informed link form click put email send email course starts	0
scripts process google forms	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
scripts process google forms	building locally fine	0
scripts process google forms	yes script share maybe course finishes	1
regularization strategy adding small values main diagonal feature matrix related ridge lasso regularization techniques	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
regularization strategy adding small values main diagonal feature matrix related ridge lasso regularization techniques	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
regularization strategy adding small values main diagonal feature matrix related ridge lasso regularization techniques	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
regularization strategy adding small values main diagonal feature matrix related ridge lasso regularization techniques	yes submit anywhere want say 20 pt n't remember yeah work good article “ good ” meaning 's copy/pasting code nothing else actually need explain 's going also article – closer time working article guidelines well	0
regularization strategy adding small values main diagonal feature matrix related ridge lasso regularization techniques	yes 's related ridge regression mathematical proof shows 's exactly 's bit hairy lot derivations need go show equivalent yeah 're lasso bit different n't know go detail 's different way regularizing one key features lasso forces weights model zero feature important makes zero ridge regression makes small short related	1
plan hands-on nosql database	ankush planning syllabus actually removed nosql databases wanted focus important tools data engineering 's covering spark data warehouse kafka dbt airflow nosql definitely important part data engineers know community really wants maybe develop couple videos nosql use – maybe give example cassandra something alexey also ran survey started course eight different topics concerning community wants hear analytics engineering top one terms preference nosql last one whether internals nosql n't remember based decisions survey well seemed like much interest nosql databases lot interest analytics engineering affected way decided come syllabus course maybe also let us know specifically mind nosql broad term talking redis talking mongo cassandra dynamo exactly	1
plan hands-on nosql database	alexey maybe like every time heard new language new tool new something wanted try realized new tools usually pretty raw look shiny start using lot problems come sometimes 's better use time-proven tools bit conservative something happened experience trying tools seeing even though look shiny still bugs especially 's new tool mitigate mindset workplace n't know guess depends use case ask team “ much new tool actually bring pros cons much work add integrate new tool potential benefits ” benefits outweigh headaches go usually analysis see maybe n't actually need new tool old one fine victoria critical mindset go something 's hype always go read everything kind see “ something 'm already using add much value much headaches add ” also 's another tool maintain consider critical mindset definitely	0
plan hands-on nosql database	victoria would say ’ also 's different set products example data warehouse start data tech stack scratch would definitely use redshift cost lower end need probably would need migrate something like bigquery snowflake lot documentation 's pretty easy set 's around longer – think reason broader adoption mainly around things alexey seems aws popular well least berlin look companies use cloud maybe 70 people use aws 20 may use gcp remaining 10 use azure 's roughly see maybe 'm wrong course also got use gcp bit seems ui nicer go web console 's little bit nicer think comes tools 's bit difficult example aws pip install aws cli google cloud 's bit difficult maybe 's one reason aws popular – ’ mature maybe think gcp catching nice interface sometimes things cheaper bigquery also think big advantage maybe clouds n't really victoria plus aws around 10 years thesis using aws thing gcp yet 's lot people go 're familiar outside stability alexey think cloud services services quite similar ’ used aws many things gcp straightforward n't typical less think many concepts map one another	0
plan hands-on nosql database	victoria think two parts talk part data already bigquery dbt part would loading data incrementally dbt 's third materialization briefly mentioned n't go details project 's advanced 's called incremental exactly – models table materialized table 'll find table data warehouse allows use incremental block would define new data every time run model first time run “ create table ” “ create table ” “ select everything ” selecting entirely everything source next time run insert new data depending incremental clause tell new data let 's say loaded timestamp “ select everything ” timeframe greater max timestamp already table way load incrementally also 's useful likely n't need reprocess let 's say case 're processing trip data let 's imagine processing trip data every day work company taxi new york n't want reprocess trip data past years want process new data came would make sense also incrementally instead applying whole business time whole amount years data think covers part ankush restartability mean mean loading data victoria yeah understand least incrementally 's point want refresh maybe 's understand restartability would load everything source ankush yeah exactly would way basically choose different time ranges say “ filtering give data. ” case table created fresh data basically make automatic operation remember create replace table since 's automatic operation would issues queries already running n't know question think 's meant restartability	0
plan hands-on nosql database	alexey 'm sure last part –how docker provided terraform ankush run stuff terraform think always ... set airflow cluster terraform n't think execute job alexey 's something like use terraform prepare environment prepare thing run need actually run something least two separate commands first “ terraform apply ” “ python run ” something like 50:02 1 upvotes allowed contact instructors directly discuss projects things unsure regarding project 'm using new tools alexey think 's best ask things slack channel others also interested learning answer scales better instead everyone writing us directly – answer everyone sees answer b others answer available right somebody knows tool maybe n't example pulsar one students might know able help 's best use channel contact us directly	0
data engineers expected know redshift snowflake bigquery able integrate multiple data warehouses sufficient know one	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
data engineers expected know redshift snowflake bigquery able integrate multiple data warehouses sufficient know one	alexey 'm sure last part –how docker provided terraform ankush run stuff terraform think always ... set airflow cluster terraform n't think execute job alexey 's something like use terraform prepare environment prepare thing run need actually run something least two separate commands first “ terraform apply ” “ python run ” something like 50:02 1 upvotes allowed contact instructors directly discuss projects things unsure regarding project 'm using new tools alexey think 's best ask things slack channel others also interested learning answer scales better instead everyone writing us directly – answer everyone sees answer b others answer available right somebody knows tool maybe n't example pulsar one students might know able help 's best use channel contact us directly	0
data engineers expected know redshift snowflake bigquery able integrate multiple data warehouses sufficient know one	alexey add comment least workplace airflow instance set us data scientist would go instance airflow managed data engineers devops engineers would use never needed actually run docker compose moment multiple dags orchestration tool quite useful would personally maybe go luigi favorite one need think people well since people know luigi many like maybe airflow would actually better choice consider sejal would also like add question docker compose used docker compose airflow style use official setup seeing first time well 's unnecessarily complicated especially xcom variables forth generally docker compose acts wrapper docker containers production workflows docker containers used integration ci/cd pipelines wherever 're going deploy docker compose docker compose part helpful development environments case let 's say mock version airflow environment production want test dags local development environment use docker compose make things easier running simple commands like ‘ docker compose ’ would make fine instead running ‘ docker run container name etc whatever want ’ terms convenience terms usage ’ generally used development environments production alexey think first part question “ close usual data engineer role workflows ” workflows see work complex steps probably also case need start something already – would n't call complex pipeline – three four steps already something eventually might grow bigger bigger add steps ankush would also like add 're cloud maybe use airflow deploy docker maybe actually use service provided cloud provider one similar airflow think airflow would used internally like cloud composite google cloud platform would using terraform sejal yeah used aws step functions aws setup step functions combination lambda place airflow ankush said also something wanted say really depends production self-hosted cluster kubernetes wherever want use pure airflow version company could kind setup use native airflow cluster could cloud-based setup convenient option use managed services cloud composer aws also version called managed airflow	0
data engineers expected know redshift snowflake bigquery able integrate multiple data warehouses sufficient know one	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
data engineers expected know redshift snowflake bigquery able integrate multiple data warehouses sufficient know one	victoria 've already said n't need know everything one knows everything 's point beginning everything bit difficult everyone lot problems docker bigquery stuff like also get point learning everything starts getting bit easier example worked several years microsoft sql server changed jobs used redshift learned quickly knew basics migrated snowflake easy basics always – data warehouse data warehouse sql sql python python etl processes etl processes n't focus much tool long know one well understand concept behind everything works things like 'll able work 're able integrate know etl process example also okay integrate regardless tool 'll use top also n't believe least would n't – hiring someone person n't happen know exact data warehouse example use snowflake wo n't ask specific things snowflake know snowflake 've certified whatever snowflake – 's big plus 's something would reject someone n't understand data warehousing concepts 're good alexey yeah 's like technology let 's say use airflow somebody n't used airflow used luigi instead comes us – 's like 're going reject n't experience airflow concepts think transferable cloud let 's say somebody experience gcp use aws person experience cloud 's enough think many things transferable one cloud another course specifics – particular things specific cloud concepts pretty transferable take technology covered course many alternatives know one alternatives good ideally long build data pipeline know concepts covered know least one technology concepts 's already quite good	1
compilation useful interesting links discussed slack used personally would insanely helpful	alexey wonder maybe start google document help us actually come list would really helpful maybe turn google document wiki page github something like guess start google document see gets traction – people start putting links happens	1
compilation useful interesting links discussed slack used personally would insanely helpful	alexey 's early talk future iterations data engineering zoomcamp think chuckles ml camp yes – videos difference homework 's still discussion n't know 'll see ankush yeah think slight discussion met sure going repeat ml de zoomcamp let 's see response participation feel definitely success definitely feel free pass around course give us feedback maybe make better decision alexey re-recording videos… 's much effort n't think 'll want let 's say ml zoomcamp example much effort 'd rather reuse videos instead making new ones hope videos good decide rerun need re-record yeah 'll see	0
compilation useful interesting links discussed slack used personally would insanely helpful	alexey add comment least workplace airflow instance set us data scientist would go instance airflow managed data engineers devops engineers would use never needed actually run docker compose moment multiple dags orchestration tool quite useful would personally maybe go luigi favorite one need think people well since people know luigi many like maybe airflow would actually better choice consider sejal would also like add question docker compose used docker compose airflow style use official setup seeing first time well 's unnecessarily complicated especially xcom variables forth generally docker compose acts wrapper docker containers production workflows docker containers used integration ci/cd pipelines wherever 're going deploy docker compose docker compose part helpful development environments case let 's say mock version airflow environment production want test dags local development environment use docker compose make things easier running simple commands like ‘ docker compose ’ would make fine instead running ‘ docker run container name etc whatever want ’ terms convenience terms usage ’ generally used development environments production alexey think first part question “ close usual data engineer role workflows ” workflows see work complex steps probably also case need start something already – would n't call complex pipeline – three four steps already something eventually might grow bigger bigger add steps ankush would also like add 're cloud maybe use airflow deploy docker maybe actually use service provided cloud provider one similar airflow think airflow would used internally like cloud composite google cloud platform would using terraform sejal yeah used aws step functions aws setup step functions combination lambda place airflow ankush said also something wanted say really depends production self-hosted cluster kubernetes wherever want use pure airflow version company could kind setup use native airflow cluster could cloud-based setup convenient option use managed services cloud composer aws also version called managed airflow	0
compilation useful interesting links discussed slack used personally would insanely helpful	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
compilation useful interesting links discussed slack used personally would insanely helpful	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
many questions correct session order get certificate	yes good references – course course called machine learning zoomcamp cover parts except conda git assume midterm project use git conda – use conda python interpreter install pipenv manage dependencies n't know good references honest maybe n't know answered question hope references course good enough ’ please let know think improved think missing	0
many questions correct session order get certificate	multi-classification – yes clustering – 'm sure probably good idea n't cover hard – know evaluate peers know better clustering	0
many questions correct session order get certificate	building locally fine	0
many questions correct session order get certificate	zero n't actually submit homework get certificate certificate need finish two projects – midterm project capstone project need get certificate rest optional give points something happens n't find time complete one homework assignment world ’ stop catch perhaps another homework assignment get virtual points certificate need finish two projects	1
many questions correct session order get certificate	's interesting one let 's say numpy array image reference numbers – 1 6 – compute mean mean 3.5. add 3.5 multiple times mean n't change happens nan pandas simply ignores – n't look pretends 's basically pandas ignores nans looks numbers replace nans mean case 3.5 get data effectively mean n't change try show formula let 's say bit math let 's say four cities couple n/as basically part usual values part n/as missing part let 's say want compute mean .mean pandas happens let 's say n numbers like k missing values total series k+n – total length compute mean part get 1/nσxi=x̄ mean case 3.5. let 's replace n/as x̄ happens instead looking 1/n look 1/n+k total number elements already filled sum another sum x̄ k n summed k times x̄ becomes kx̄ formula like multiply n/n n/n simply 1. 's multiplying thing one get n x̄ part mean image reference clue 'm talking bear 's almost result 1/ n+k n x̄+kx̄ leave x equation 1/ n+k n+k x̄=x̄ basically shows add many means like result always x̄ 'm sure everyone enjoyed little proof image reference prove saw question meeting got curious like “ show mathematically ” spent like 10 minutes trying figure nice exercise brain 'm sure would able show proof spot honest	0
test-driven development applied machine learning	really depends cloud showed elastic beanstalk n't think needed change much anything n't remember changing anything thing changed url service test.py script thing needed change think heroku 's similar actually couple tutorials using pythonanywhere heroku probably explain need change go see need change anything probably much	0
test-driven development applied machine learning	's actually think 's useful n't train.py produce pickle file let 's say want retrain model using different parameters – 's useful see changed score real life like projects work train.py 's always one file sometimes 's bunch files somehow log performance think 's useful n't want reason n't	0
test-driven development applied machine learning	answer – depends dataset – typically train/validation split first try scaling try without scaling see happens think something talked last week actually notebook prepared last week go classification notebook notebook scaling shows scale features use standard scaler use min max scaler – use two different scalars shows particular dataset turned little bit better experiment see scaling makes sense linear models sometimes 's actually good idea add extra complexity pipeline instead dictionary vectorizer whatnot 'll also need add next step scaling basically steps prediction pipeline becomes bit difficult maintain would say try scale see improvement get significant maybe 's worth extra complexity get use cross-validation use best judgment see makes sense	0
test-driven development applied machine learning	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
test-driven development applied machine learning	probably honest 'm big fan test-driven development way mean tests important way tdd works first come test fails implement things fix make test green iteration n't find super useful like 's personal preference usually implement something test like general software engineering practices ’ talking test driven development machine learning way first write test implement something – ’ know ’ talking model say “ okay model kind accuracy ” train model 's way expected maybe n't make much sense hand lot code around model needs testing example things showed today – may fail behave way expect tests around think quite important want make sure things control example could actually good test example transform want make sure neighborhood fordham n't appear categories could good test maybe actually makes sense write tests – actual machine learning thing model – things around model data preparation pipeline probably want test get results post-processing results want test testing model could tricky	1
include code homework solution	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
include code homework solution	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
include code homework solution	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
include code homework solution	alexey one thing maybe n't clear enough – please always include code solution maybe people n't watch first said n't submit code homework get zero points saw submissions people put dot empty put link towards data science article example please n't please put code n't 'll get zero points entire week 'll make explicit form week three well please n't forget put code	1
include code homework solution	far week two week three follow simple approach simple approach described take entire dataset split three parts train validation test 'll use week two week three week four talk cross validation – explain right want say approach validation sufficient many cases think 's pretty safe say many many many applications split enough cross validation course nicer learn week four right go detail try explain wait video “ select approach cross validation ” would say multiple options k-fold… 'm sure many people understand n't covered yet basically dataset big n't need k-fold dataset smaller use k-fold “ leave p ” – 've never used personally	0
earn internships full-time offers data engineering role fresher	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
earn internships full-time offers data engineering role fresher	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
earn internships full-time offers data engineering role fresher	sejal tricky question actually multiple ways really either brush multiple projects would say start taking opportunities within current company current team see overlap data engineering side 're data analyst data scientist maybe take automation tasks start could also learn team already experts think starting aspect gives understanding align standards current company technologies current company uses business domain knowledge required respect company many different companies different ways things example everyone uses gcp aws recommendation would start ankush think 's definitely one paths go fresher also learn plenty resources available already recommended books ’ course complete course go videos homework maybe even project strong foundation develop data engineer probably would able take tasks grow knowledge videos actually help land full-time job internship 're interviewing right internship position data engineering position would say best way would via showing work showing progress linkedin sharing updates linkedin publicly available channels show eventual employer interested domain making gradual progress towards becoming good data engineer would definitely one recommended ways go forward alexey 's encourage learning public thing 's give extra points know 's easy actually tell world 're learning 's want give little bit push little bit encouragement giving extra points hope successful finding internship soon need spend lot time good luck	1
earn internships full-time offers data engineering role fresher	victoria 's dbt package two ways use dbt cloud could actually put airflow trigger shell dbt cloud already something think example created shells run everything airflow job id could trigger send credentials dbt cloud would one way 'm guessing targeted shell using airflow scheduler dbt cloud completely valid 's package could use 's another thing says “ invoking dbt bash operator. ” page official documentation run dbt production general one options via airflow check 's link package also link explanation use via bash operator also check ways guess example suggested using chrome 're working locally also something 's ’ fancy airflow setup chrome never fails alexey well airflow nothing fancy chrome right victoria yeah exactly chuckles	0
earn internships full-time offers data engineering role fresher	alexey feel sorry need deal work chuckles n't really experience sejal used working oracle like long time ago ’ assuming person wrote question still working legacy databases like oracle really n't answer either chuckles guess 'll talk generic terms ci/cd pipelines wherever 're deploying infrastructure basically wherever 're deploying schemas think would better place keep things ankush maybe using git alexey think issue code stored database easily version control somebody point thought good idea let databases run arbitrary code like triggers sejal possibly victoria would able answer case coupling ci/cd pipelines dbt pipelines n't know alexey think usually people days try extract logic put backend – move database put backend version controlled	0
use something else besides gcp recommendations	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
use something else besides gcp recommendations	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
use something else besides gcp recommendations	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
use something else besides gcp recommendations	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
use something else besides gcp recommendations	ankush yes definitely explore aws azure comfortable using gcp course using gcp tools like bigquery might available platforms might similarities alternatives might touch course course based upon google cloud platform one biggest reasons create new account get 300 credit 's really useful everybody 's familiar cloud create free account course ideally spend money platform services alexey think another reason decided go gcp connection dbt bigquery athena think 's bit tricky want go snowflake dbt 's super expensive something want pay course 's thinking whether go aws gcp – gcp gives free credits b dbt works bigquery ’ led decision gcp sejal mentioned able run everything locally except bigquery part course bigquery lives cloud rest stuff runnable locally	1
get better coding understand lectures comes implementing python takes quite long time	took course ago back useful think something use python immediately could helpful 'm sure would good recommendation current course course good would n't compare stanford course andrew ng simply different course 'll get lot theory one practical hands-on – 's mostly writing code together maybe take	0
get better coding understand lectures comes implementing python takes quite long time	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
get better coding understand lectures comes implementing python takes quite long time	need practice n't think better way	1
get better coding understand lectures comes implementing python takes quite long time	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
get better coding understand lectures comes implementing python takes quite long time	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
function library recode categorical variables automatically instead map method imagining 100 variables	maybe n't know still need names think question referring part image 1 need know 1 means “ ok ” 2 means default need know 1 means “ rent ” 2 means “ owner ” “ private ” information somewhere json csv file whatever information somewhere load could stored json file use need know 1 stands “ rent ” 2 stands “ owner ” etc n't care machine learning models n't care – n't care 's “ rent ” “ owner ” 's 1 2 – case turn home variable string using something like image 2 stay encoded number string use dictionary vectorizer treat string number hope answers question	1
function library recode categorical variables automatically instead map method imagining 100 variables	flask – let draw let 's say want create web service image 5 web service gets requests web service responds something could predictions could something else – n't related machine learning implement web service use flask use fastapi use framework creating web services use flask use something else – flask allows use post example let 's say “ get ” usually parse parameters query let 's say go google put “ test ” see q=test https //www.google.com/search q=test oq=test aqs=chrome.0.69i59l2j69i65l3j69i60j69i61l2.1264j0j1 sourceid=chrome ie=utf-8 image 6 parameter pass “ get ” request “ get ” request sometimes let 's say want score customer n't want put gender=female contract=one year string send json post post also add body request generally use post general use case flask able create web service	0
function library recode categorical variables automatically instead map method imagining 100 variables	start working n't inform imagine everyone started informing take lot time answer finish answering questions first november yeah n't inform please write slack 're sure think good dataset go ahead use	0
function library recode categorical variables automatically instead map method imagining 100 variables	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
function library recode categorical variables automatically instead map method imagining 100 variables	classification classification let 's say n't predicting price let 's say predicting 's average number hours image 1 case would need another layer – rather layer 's called activation n't activation 're still… ’ trying formulate way 's confusing think 's better go lecture rewatch ’ essentially retelling thing need from_logits price variable loss mean squared error logits loss categorical cross-entropy categorical cross entropy usually used multiple categories need use logits 's numerically stable n't use – stay activation softmax n't use legit hope 's clear regression n't need use homework need use from_logits either homework two classes 's binary classification problem multiple classes – let 's say three classes four classes five classes – need use logits loss categorical cross-entropy loss categorical cross-entropy 's recommended use from_logits=true 's numerically stable	0
've really enjoying learning throughout course would like continue would suggest formal education self-learning	alexey right mind two weeks depends goes first time ’ running course see much time takes n't want spend two weeks want get something small n't get ambitious please n't process common crawl data project ideally two weeks let 's see	0
've really enjoying learning throughout course would like continue would suggest formal education self-learning	sejal answer would opinionated generally learned via formal education trying different projects trying open source projects github trying little things automate work even 're data analyst data scientist take automation opportunities wherever see engineer pipelines proceed see help current use cases company self-learning well try things based whatever tech stack example want learn particular core area want learn example recommendations engine data engineering pipelines within recommendations domain search engine forth thoughts top head maybe ankush alexey add ankush say basically depends end goal formal education get opportunity meet people get opportunity learn phds professors really proficient domain learn theory behind practical right focused let 's say particular technology even 're talking theory talking theory technology going deep theory computers maths behind example distributed system would mathematics behind actually even starting want learn things go formal education learn good university aim socialize phd master 's afterwards formal education also right way build relationships aim learn especially learn domain want work specific technologies become data engineer let 's say nothing better self-taught never learn things use work formal education anyway formal education things learn bit different learn self learning focus particular subject alexey remember education – master ’ n't think things learned useful mean directly applicable work useful example studied database internals actually needed implement database fun 'm sure useful work 's also two years maybe half year realize data engineering chuckles self-learning 's kind safer right say “ okay let try something else. ” 're halfway master ’ “ okay finish finish 's one year becomes difficult ’ thing called master ’ thesis sighs remember much trouble actually wanted phd master ’ thought “ okay 'll master ’ ’ phd. ” master 's thesis realized hate writing papers basically phd students time fun something super useful work ankush 're thanking master ’ two years right alexey yeah exactly germany take longer easily	1
've really enjoying learning throughout course would like continue would suggest formal education self-learning	alexey 33 submissions think decision extend deadline good one hopefully end week see higher number understand project much difficult watching course following along 33 people – probably put lot effort thanks saw one person write put 100 hours already project outstanding sorry cruel 100 hours lot	0
've really enjoying learning throughout course would like continue would suggest formal education self-learning	alexey 's want look project 're behind right n't worry take time decide whether give certificate based projects homework 's – catch week 3 lighter week 1 maybe fast-forward directly docker compose run week 2 depending much time maybe go solution published today give everything need week 3 use transfer service video already watching video probably learn lot already like homework solution	0
've really enjoying learning throughout course would like continue would suggest formal education self-learning	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
rating data number stars use linear regression model	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
rating data number stars use linear regression model	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
rating data number stars use linear regression model	answer – depends dataset – typically train/validation split first try scaling try without scaling see happens think something talked last week actually notebook prepared last week go classification notebook notebook scaling shows scale features use standard scaler use min max scaler – use two different scalars shows particular dataset turned little bit better experiment see scaling makes sense linear models sometimes 's actually good idea add extra complexity pipeline instead dictionary vectorizer whatnot 'll also need add next step scaling basically steps prediction pipeline becomes bit difficult maintain would say try scale see improvement get significant maybe 's worth extra complexity get use cross-validation use best judgment see makes sense	0
rating data number stars use linear regression model	yes	1
rating data number stars use linear regression model	far week two week three follow simple approach simple approach described take entire dataset split three parts train validation test 'll use week two week three week four talk cross validation – explain right want say approach validation sufficient many cases think 's pretty safe say many many many applications split enough cross validation course nicer learn week four right go detail try explain wait video “ select approach cross validation ” would say multiple options k-fold… 'm sure many people understand n't covered yet basically dataset big n't need k-fold dataset smaller use k-fold “ leave p ” – 've never used personally	0
project used dataset batch processing transform data comes batch used dbt “ bad ” spark part project	alexey well good job keep working almost finished first week finish start week two finish week three chuckles follow sequence videos playlist github sufficient think questions go slack ankush keep going alexey chuckles yes exactly ’ almost finished first week good sign first week year pretty tough many people lot problems docker whatnot 're almost finished good sign ’ good track keep working	0
project used dataset batch processing transform data comes batch used dbt “ bad ” spark part project	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
project used dataset batch processing transform data comes batch used dbt “ bad ” spark part project	alexey well 's really anonymous asked submit link github n't know need ask questions previous course think ask contact information project description n't know ankush think encourage might unfair different people first maybe 're available answer questions let 's put like grading maybe 's unfair want learn 's definitely helpful think grading done based upon submitted 's also really important document steps alexey yeah guess need contact author means really give four points reproducibility ankush case maybe instructions clear report via contact alexey yeah exactly n't remember problem previous course maybe also create issue github n't know	0
project used dataset batch processing transform data comes batch used dbt “ bad ” spark part project	victoria 's bad normally would n't say 're working depend lot use cases actually thought project way could use technology want use okay 's okay use dbt also would okay would used spark dbt future want always spark part 's kind like equivalent transformation 've done dbt even use spark dbt well want get think 's okay alexey think 's quite common pre-processing data still data lake raw data pre-process spark put back data lake processed/prepared format load data warehouse data warehouse find final grouping aggregations things order prepare data reports dashboards – dbt understanding companies often victoria yes would say maybe use kind data spark kind data dbt combination data goes directly dbt directly spark would always depend always use case company alexey reminds talk recently rahul video rahul talks modern data warehouse design mentions emr – use spark processing streaming things consume kafka dbt generic etls principle something like spark puts staging data dbt gets data staging area domain area check video n't judging number views probably already watched victoria chuckles n't watched yet alexey yeah good one victoria yeah wanted downloaded client alexey chuckles cool rahul shared link linkedin lot people saw post quite popular 's lot people saw video 's cool video check	1
project used dataset batch processing transform data comes batch used dbt “ bad ” spark part project	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
would really helpful make one video per week per comfort summarizes week 's concepts better understanding	alexey maybe tell us problems concepts problems maybe ask help us perhaps could written n't video concepts trouble understanding ask others answer 'll sort document use make sure really understand could one suggestion sejal think 've also updated summarized notes repository week 1 week 2 's already possibly based instructors ’ convenience releasing weeks also understanding would summarization wrap-up ever want go back reference recall whatever learned would also good place go alexey please let us know concepts hard understand 's also useful feedback us next time record something maybe 'll try explain clearer least know maybe thing needs better explanation	1
would really helpful make one video per week per comfort summarizes week 's concepts better understanding	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
would really helpful make one video per week per comfort summarizes week 's concepts better understanding	alexey example spark google “ spark gcp bigquery connector ” good article shows need need jar need use jar submit spark job let 's say files stored google cloud storage something files multiple options first option save back google cloud storage create external table something like like saw week 3. another option would saving directly bigquery using say “ write ” say want write bigquery uses connector connect bigquery write n't know permissions actually work – maybe also need specify key somehow probably 're already dataproc already permissions need order write bigquery order read google cloud storage use dataproc theoretically easy – add extra jar write bigquery data data warehouse use materials dbt week analytics engineering week actually consume content data warehouse visualize	0
would really helpful make one video per week per comfort summarizes week 's concepts better understanding	alexey n't think really understand question ankush 're vpc matter alexey cluster – spark cluster cluster kafka cluster ankush 're running vpc able access 's ip right alexey yeah oh okay externally ip guess 're running inside network use internal ip 's inside kubernetes refer names deployments n't know ankush also n't know 's really specific particular use case need give us maybe slack explain bit – running kind machines exactly exposing terms external ip alexey usually kafka cluster kafka machines spark cluster spark cluster connects kafka reads data ankush depends vpc running 're running different vpcs need expose separately need expose ip running vpc matter able access internally might wrong 'm devops chuckles alexey usually take things granted maybe good thing work grateful data team makes tools work often n't ask “ okay actually configured ” works n't – n't go support channel ask “ hey please fix ” ankush think 's also data team 's also maybe devops team embedded inside data team 're working olx pretty big company maybe many smaller teams inside big teams guess 's devops topic	0
would really helpful make one video per week per comfort summarizes week 's concepts better understanding	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	0
use wsgi server like gunicorn waitress still required deployment deploy flask app directly cloud	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
use wsgi server like gunicorn waitress still required deployment deploy flask app directly cloud	think required depends cloud want deploy docker image cloud need put gunicorn waitress inside docker saw warning flask says use production use production flask developers cry	1
use wsgi server like gunicorn waitress still required deployment deploy flask app directly cloud	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
use wsgi server like gunicorn waitress still required deployment deploy flask app directly cloud	n't think 's rule thumb notebook multiple things deploy… basically python file ideally one single thing one python file training example one python file testing model – something like n't think particular rule thumb sometimes makes sense larger files sometimes smaller files	0
use wsgi server like gunicorn waitress still required deployment deploy flask app directly cloud	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
restartability incremental loads usually made bigquery redshift stored procedures approach bigquery	ankush simple answer “ n't know. ” chuckles think save question victoria think would best person answer maybe put slack channel tag alexey also example taxi trip data – use star schemas anything well let 's kind star one ray right chuckles ankush kind like think common one star n't know done enough data modeling know data vault 2.0 would provide us particular dataset n't know 's relevant thing right honestly alexey look criteria 'll notice dimension called “ data modeling ” instead dbt ... n't use dbt things spark like spark n't like spark go dbt even though little bit different end 're sometimes interchangeable example maybe put data data lake transform data spark put data warehouse form ready queried dashboard decide put raw data data warehouse transformation dbt 's go either approach n't think right wrong ankush agrees	0
restartability incremental loads usually made bigquery redshift stored procedures approach bigquery	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
restartability incremental loads usually made bigquery redshift stored procedures approach bigquery	victoria think two parts talk part data already bigquery dbt part would loading data incrementally dbt 's third materialization briefly mentioned n't go details project 's advanced 's called incremental exactly – models table materialized table 'll find table data warehouse allows use incremental block would define new data every time run model first time run “ create table ” “ create table ” “ select everything ” selecting entirely everything source next time run insert new data depending incremental clause tell new data let 's say loaded timestamp “ select everything ” timeframe greater max timestamp already table way load incrementally also 's useful likely n't need reprocess let 's say case 're processing trip data let 's imagine processing trip data every day work company taxi new york n't want reprocess trip data past years want process new data came would make sense also incrementally instead applying whole business time whole amount years data think covers part ankush restartability mean mean loading data victoria yeah understand least incrementally 's point want refresh maybe 's understand restartability would load everything source ankush yeah exactly would way basically choose different time ranges say “ filtering give data. ” case table created fresh data basically make automatic operation remember create replace table since 's automatic operation would issues queries already running n't know question think 's meant restartability	1
restartability incremental loads usually made bigquery redshift stored procedures approach bigquery	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
restartability incremental loads usually made bigquery redshift stored procedures approach bigquery	alexey 's early talk future iterations data engineering zoomcamp think chuckles ml camp yes – videos difference homework 's still discussion n't know 'll see ankush yeah think slight discussion met sure going repeat ml de zoomcamp let 's see response participation feel definitely success definitely feel free pass around course give us feedback maybe make better decision alexey re-recording videos… 's much effort n't think 'll want let 's say ml zoomcamp example much effort 'd rather reuse videos instead making new ones hope videos good decide rerun need re-record yeah 'll see	0
aws step functions compare airflow infrastructure fully aws use airflow would good case	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
aws step functions compare airflow infrastructure fully aws use airflow would good case	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
aws step functions compare airflow infrastructure fully aws use airflow would good case	sejal previous setup like said bunch lambda functions execution coordinated step functions would work case airflow ankush used step functions honestly 've also dug deep cloud composite google cloud platform certain criteria need choose one biggest criteria price think price would super less comparison price pay maintenance developing deploying probably scaling future go ahead use aws step function price high requirements use case maybe digging deeper deployment airflow would better would really depend upon use case calculate price alexey think aws step functions relatively cheap convenient ui airflow things complex impression using personally seeing demo aws step functions work tried one project decided use n't easy configure workflows guess ’ also used airflow	1
aws step functions compare airflow infrastructure fully aws use airflow would good case	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
aws step functions compare airflow infrastructure fully aws use airflow would good case	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
would next biggest step dbt opinion terms deployment	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
would next biggest step dbt opinion terms deployment	alexey 33 submissions think decision extend deadline good one hopefully end week see higher number understand project much difficult watching course following along 33 people – probably put lot effort thanks saw one person write put 100 hours already project outstanding sorry cruel 100 hours lot	0
would next biggest step dbt opinion terms deployment	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
would next biggest step dbt opinion terms deployment	victoria around tool specifically dbt going or… read question alexey think 's tool company right victoria yep roadmap something like would expect see us project alexey probably – tool going next tool victoria 's interesting dbt one first videos two parts basically one dbt core part open source 's part whole magic dbt cloud think next steps going around developing dbt cloud 's make money thing would dbt core open source literally provide information competition would try dbt cloud stronger compete better would say specifically things recently introduced would really like see would around data cataloging data governance recently introduced something called metrics could actually define something like utilization rate net revenue stuff like – calculated comes 's big problem companies 's common everyone spreadsheet 're like “ calculation revenues this. ” numbers change treating data product general would say would next step well said opinion	1
would next biggest step dbt opinion terms deployment	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
model roc auc score around 0.70 validation used final model	took course ago back useful think something use python immediately could helpful 'm sure would good recommendation current course course good would n't compare stanford course andrew ng simply different course 'll get lot theory one practical hands-on – 's mostly writing code together maybe take	0
model roc auc score around 0.70 validation used final model	yes 's really dataset dependent datasets good score datasets 0.6 good score example kaggle competition click prediction best models 0.65 auc really depends dataset already even core models used rule every model yeah 0.70 reasonably good auc value go ahead use final model	1
model roc auc score around 0.70 validation used final model	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
model roc auc score around 0.70 validation used final model	n't think takes lot time first need record videos prepare homework imagine adding collaboration startups well maybe sometime future n't know somebody startup listening right want hire people graduate zoomcamp please reach let 's arrange something	0
model roc auc score around 0.70 validation used final model	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
set execution_date airflow past example january 2019	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
set execution_date airflow past example january 2019	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
set execution_date airflow past example january 2019	alexey week 2 yes – upload video solution also update leaderboard probably take time depending clean data – need lot data cleaning take lot time please n't put things n't look like emails email field fill please careful takes time clean later	0
set execution_date airflow past example january 2019	alexey well ’ aws account go transfer service 's probably easiest need click buttons want set gcp virtual machine need spend time setting everything 's worth weeks – spark probably kafka well – useful probably worth time investment want quickly copy files transfer service probably easiest way	0
set execution_date airflow past example january 2019	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	1
enjoy outside work data	alexey depends exactly want reddit data right ankush yeah exactly would focus final goal let 's say “ want calculate reddit data. ” might want say “ keywords used ” something like case transformations directly depend alexey something like “ active number posts per day. ” ankush exactly “ per hour distribution posts. ” 'm pretty sure us goes daytime goes night trying answer 's first question would focus know answer know transformations apply alexey yeah 's end goal project dashboard “ exactly want put dashboard ” go “ dashboard data use kind transformations need order go data dashboard ” become clear need group-by need join table	0
enjoy outside work data	alexey think 's helpful 's necessary helpful happens dags run airflow dags tasks tasks often kubernetes jobs something like infra – many companies infrastructure managed kubernetes 's quite simple get new container execute new job kubernetes n't think 's necessary think 's helpful 'm data engineer – 'm data scientist data scientist knowledge kubernetes quite helpful could debug jobs 'm running airflow run kubernetes could see logs would say want learn hurt get hired without kubernetes knowledge materials course sufficient get hired pick kubernetes work example agree victoria victoria yeah think kubernetes getting popular ca n't hurt	0
enjoy outside work data	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
enjoy outside work data	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
enjoy outside work data	sejal like enjoying sun work 's consistently sunny last couple weeks 's really nice berlin least per berlin weather victoria yeah definitely enjoying sun going park tourist also – even berlin argentina reading friends think 'm pretty normal chuckles bit boring alexey like lego son make things lego 's something quite enjoy n't know – meditative effect like 3000 pieces need build something pretty fun chuckles sejal 'm inclined get jigsaw puzzle reasons times feel like n't know would nice something 'm building – code technical projects something fun change chuckles alexey jigsaw puzzle victoria 's puzzle several well lend 've done second time kind boring alexey yeah saw one one saw white thing need figure build chuckles 's probably cruel people already good solving usual ones yes guess 's well guess 's thing ... sejal yeah know like building things – building communities building projects alexey n't know counts us “ outside data work ” mean outside work yes outside data sure victoria think counts counts hobby alexey okay chuckles	1
catching homeworks tuning able turn final project	alexey yes submit project promised beginning give certificates based project completion based homework 're catching right n't homework 's fine caught know take part project	1
catching homeworks tuning able turn final project	ankush n't know 's example project alexey well sure good example projects project right chuckles create great projects example projects right think good project open recommend must projects n't know “ ideal pipeline ” vague thing ankush yeah really depends use case ideal pipeline would one least manual steps requires least intervention even adding new data sources let 's say similar kinds data consume producing data scale accordingly minimum intervention side possible data engineer would good pipeline would say alexey definition ideal pipeline stitch together open source tools like airflow spark ankush chuckles 'm pretty sure product manager come new use case ideal pipeline would longer ideal laughs alexey think tools like fivetran never used closer ideal one case fivetran works said product manager comes use case longer fits closed-source platform therefore need figure actually make things work experience working things n't know olx love stitch together open source products hope best ankush going open source think tools discussed used ones alexey ideal would n't say work ankush think enterprise ones much better – much easier – less flexible well basic use cases 's perfectly fine want something specific would stuck might need pay ask improvements might take months come kinds things always really depend upon kind majority company kind use cases	0
catching homeworks tuning able turn final project	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
catching homeworks tuning able turn final project	alexey think 's helpful 's necessary helpful happens dags run airflow dags tasks tasks often kubernetes jobs something like infra – many companies infrastructure managed kubernetes 's quite simple get new container execute new job kubernetes n't think 's necessary think 's helpful 'm data engineer – 'm data scientist data scientist knowledge kubernetes quite helpful could debug jobs 'm running airflow run kubernetes could see logs would say want learn hurt get hired without kubernetes knowledge materials course sufficient get hired pick kubernetes work example agree victoria victoria yeah think kubernetes getting popular ca n't hurt	0
catching homeworks tuning able turn final project	victoria 's buzzwords right alexey yeah go repo see like practical week like five 10 minute video explaining use 's planned ankush maybe spend couple minutes today well data mesh basically concept idea – let 's go back say “ data pipelines data engineering teams built different companies right ” people generating data people consuming data one big team middle called data engineering team dataops team whatever want call different companies different names teams responsible ingesting data transforming putting making way 's consumable right way right fashion data mesh wants basically decentralize particular role big team wants give power let 's say responsibility team generating data teams consuming data therefore ’ layer instead team taking whole role 's kind better idea build services give responsibility give services software teams generating data consuming basically going higher level respect case happens team ’ generating data responsible quality data answering different questions data things 's really useful multiple teams handling data idea data knowledge data lost best person best team answer questions best team knowledge data team 's generating also responsible putting data data lake data warehouse also responsible queries attaches whole thing together 's kind rough idea data mesh alexey ’ abstract concept actually go youtube channel longer explanation data mesh “ longer ” mean one hour long – hour six minutes chuckles 're kind stuff… way dataops 101 also quite nice one check well one modern data stack analytics engineering also good basically check three – 're good 're interested data mesh specifically first one goes lot detail second one – bit detail ankush 's also nice blog post alexey zhamak actually writing book right think 's early release 's written five six chapters n't remember 's progress 's going quite big book right ankush blog post like today 's date read couple days impossible read one go 's huge victoria think also download first two chapters something book give quick overview well ankush 's also another book o'reilly called data mesh practice max schultze alexey one free right ankush think free 's free	0
say junior senior data engineers scale know spark hadoop certifications	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	0
say junior senior data engineers scale know spark hadoop certifications	alexey 's certificates 's whether know hadoop 's lead project technical leader project make decisions drive decisions ’ senior say “ okay want use technology technology xyz reasons. ” call senior course varies company company companies like amazon senior already leadership role smaller companies maybe senior less technical leader still answer question scale – google “ public career progression matrix ” think dropbox public one companies public check would answer question ankush yeah absolutely really depends talking terms companies really depends upon company talking terms personal consideration set scale based upon think specifically alexey certificates personally n't care much ’ hiring example ankush zero certificates	1
say junior senior data engineers scale know spark hadoop certifications	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
say junior senior data engineers scale know spark hadoop certifications	alexey right mind two weeks depends goes first time ’ running course see much time takes n't want spend two weeks want get something small n't get ambitious please n't process common crawl data project ideally two weeks let 's see	0
say junior senior data engineers scale know spark hadoop certifications	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
practice writing codes formulas better	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
practice writing codes formulas better	alexey sure maybe answer thing talked regarding filters well use matplotlib take filter then… dmitry think pil interesting option 's images guess alexey yeah pil 's like want save jpeg example want plot see looks matplotlib probably sufficient homework also use matplotlib n't use pil 48:08 0 upvotes use dogs vs cats main capstone project better performance dmitry guess maybe datasets alexey yeah datasets dmitry mean lot open datasets guess alexey cats dogs yeah dmitry mean think cats dogs another time would n't interesting alexey yeah add data interesting dmitry maybe cats versus dogs versus something else alexey wombats cats versus wombats chuckles think image net must wombats right dmitry yeah alexey take part image net get wombats dmitry fun alexey laughs maybe wombats versus… similar wombats wombats cats quite different right different environments 's easy task dmitry maybe wildcats alexey yeah saw wildcat zoo looked like normal cat suspect maybe put normal cat said 's wildcat chuckles dmitry better marketing chuckles	0
practice writing codes formulas better	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
practice writing codes formulas better	know honest first thing probably need realize especially comes matrices things matrix multiplication example everything expressed python code code see take formula try decompose one one try see translates n't know actually answers question better way try translate also helps reference implementation let 's say implementing matrix multiplication plain python could compare results implementation numpy see results existing implementation compare implementation reference implementation probably helps	1
practice writing codes formulas better	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
use gridsearchcv hyperparameter tuning still use validation dataset use full train dataset	answer – depends dataset – typically train/validation split first try scaling try without scaling see happens think something talked last week actually notebook prepared last week go classification notebook notebook scaling shows scale features use standard scaler use min max scaler – use two different scalars shows particular dataset turned little bit better experiment see scaling makes sense linear models sometimes 's actually good idea add extra complexity pipeline instead dictionary vectorizer whatnot 'll also need add next step scaling basically steps prediction pipeline becomes bit difficult maintain would say try scale see improvement get significant maybe 's worth extra complexity get use cross-validation use best judgment see makes sense	0
use gridsearchcv hyperparameter tuning still use validation dataset use full train dataset	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
use gridsearchcv hyperparameter tuning still use validation dataset use full train dataset	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
use gridsearchcv hyperparameter tuning still use validation dataset use full train dataset	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
use gridsearchcv hyperparameter tuning still use validation dataset use full train dataset	think use full train dataset let look gridsearchcv ’ know gridsearchcv way parameter tuning selecting best parameters internally cross-validation find best parameters think must example somewhere ’ good example – parameter estimating using cross-validation check example tuning support vector machine classifier ’ tuning parameters trying different values c. 're trying different values gamma linear kernel – internal details svm n't matter 're trying different sets parameters – train/test dataset separation call full train fit full train dataset n't split train/validation 're using full train dataset gridsearchcv split internally think cool thing	1
quickly run difference external table materialized table	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
quickly run difference external table materialized table	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
quickly run difference external table materialized table	victoria every table materialized table ’ ankush exactly answered previous question external tables think 's meaning question basically external tables versus internal tables alexey okay also concept materialized view view view nothing sql query executed every time want something table view 're kind creating query way speed – materialize view meaning create table data instead querying query query actual data get right ankush victoria agree	1
quickly run difference external table materialized table	victoria set warehouse end define connection difference would use adapter dbt several adapters would first want use dbt go check adapters make sure 's supported several official example redshift could use community-supported azure alexey yeah aws athena azure athena redshift presto well example presto cluster use aws – within aws presto cluster potentially similar setup connect dbt presto victoria general add little bit answer let 's say project right bigquery would change profile 's set connection may make changes 's something sql supports bigquery way around able run project minutes	0
quickly run difference external table materialized table	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
program used whiteboard course find really useful practice thoughts interviews	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
program used whiteboard course find really useful practice thoughts interviews	case everyone uses docker problem n't use docker n't get point using docker ’ run write “ sorry run this. ” hope n't happen 's actually whole idea behind docker – 's transferable different environments let 's say use gunicorn n't use docker able test case use scoring matrix evaluate accordingly gunicorn tick box would encourage still try learn submission even run reason think reason run something windows ubuntu gunicorn everything runnable	0
program used whiteboard course find really useful practice thoughts interviews	flask – let draw let 's say want create web service image 5 web service gets requests web service responds something could predictions could something else – n't related machine learning implement web service use flask use fastapi use framework creating web services use flask use something else – flask allows use post example let 's say “ get ” usually parse parameters query let 's say go google put “ test ” see q=test https //www.google.com/search q=test oq=test aqs=chrome.0.69i59l2j69i65l3j69i60j69i61l2.1264j0j1 sourceid=chrome ie=utf-8 image 6 parameter pass “ get ” request “ get ” request sometimes let 's say want score customer n't want put gender=female contract=one year string send json post post also add body request generally use post general use case flask able create web service	0
program used whiteboard course find really useful practice thoughts interviews	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
program used whiteboard course find really useful practice thoughts interviews	use windows surface like windows tablet 's called drawboard put favorite search engine find 's also available windows store microsoft store sure ’ called 'm pretty new windows app store windows put 's actually free lot cool stuff example use two colors mostly use colors 's cool tool special pen ’ microsoft let share part screen maybe n't want people know windows 's 'm trading part screen also n't want people see taskbar example 's useful	1
good streaming datasets apart twitter	alexey victoria know good streaming datasets victoria 've used twitter alexey think get data stock exchanges real time n't know whether 's free pay maybe exchanges give data free victoria yeah give data free mention think ’ also one us sports news could n't remember name also api could also good streaming source 's news might something structured but… alexey yeah think google public data streaming datasets could look promising coinbase twitter ’ good list	1
good streaming datasets apart twitter	victoria data mesh one topics going cover going smaller video 's practice around anything like like weeks 's around explaining concept familiarity deepen concept need start working let 's say data mesh specifically 'll recording end week 'm back berlin 'm sure plan upload soon enough – deadline alexey actually want actually record one videos spark something planned like month ago still n't done want week probably see problems reading google cloud running things dataproc want first take care mlops video least two videos – data mesh mlops n't know rest	0
good streaming datasets apart twitter	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
good streaming datasets apart twitter	victoria 's buzzwords right alexey yeah go repo see like practical week like five 10 minute video explaining use 's planned ankush maybe spend couple minutes today well data mesh basically concept idea – let 's go back say “ data pipelines data engineering teams built different companies right ” people generating data people consuming data one big team middle called data engineering team dataops team whatever want call different companies different names teams responsible ingesting data transforming putting making way 's consumable right way right fashion data mesh wants basically decentralize particular role big team wants give power let 's say responsibility team generating data teams consuming data therefore ’ layer instead team taking whole role 's kind better idea build services give responsibility give services software teams generating data consuming basically going higher level respect case happens team ’ generating data responsible quality data answering different questions data things 's really useful multiple teams handling data idea data knowledge data lost best person best team answer questions best team knowledge data team 's generating also responsible putting data data lake data warehouse also responsible queries attaches whole thing together 's kind rough idea data mesh alexey ’ abstract concept actually go youtube channel longer explanation data mesh “ longer ” mean one hour long – hour six minutes chuckles 're kind stuff… way dataops 101 also quite nice one check well one modern data stack analytics engineering also good basically check three – 're good 're interested data mesh specifically first one goes lot detail second one – bit detail ankush 's also nice blog post alexey zhamak actually writing book right think 's early release 's written five six chapters n't remember 's progress 's going quite big book right ankush blog post like today 's date read couple days impossible read one go 's huge victoria think also download first two chapters something book give quick overview well ankush 's also another book o'reilly called data mesh practice max schultze alexey one free right ankush think free 's free	0
good streaming datasets apart twitter	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
n't mean change filling nans	answer – depends dataset – typically train/validation split first try scaling try without scaling see happens think something talked last week actually notebook prepared last week go classification notebook notebook scaling shows scale features use standard scaler use min max scaler – use two different scalars shows particular dataset turned little bit better experiment see scaling makes sense linear models sometimes 's actually good idea add extra complexity pipeline instead dictionary vectorizer whatnot 'll also need add next step scaling basically steps prediction pipeline becomes bit difficult maintain would say try scale see improvement get significant maybe 's worth extra complexity get use cross-validation use best judgment see makes sense	0
n't mean change filling nans	's interesting one let 's say numpy array image reference numbers – 1 6 – compute mean mean 3.5. add 3.5 multiple times mean n't change happens nan pandas simply ignores – n't look pretends 's basically pandas ignores nans looks numbers replace nans mean case 3.5 get data effectively mean n't change try show formula let 's say bit math let 's say four cities couple n/as basically part usual values part n/as missing part let 's say want compute mean .mean pandas happens let 's say n numbers like k missing values total series k+n – total length compute mean part get 1/nσxi=x̄ mean case 3.5. let 's replace n/as x̄ happens instead looking 1/n look 1/n+k total number elements already filled sum another sum x̄ k n summed k times x̄ becomes kx̄ formula like multiply n/n n/n simply 1. 's multiplying thing one get n x̄ part mean image reference clue 'm talking bear 's almost result 1/ n+k n x̄+kx̄ leave x equation 1/ n+k n+k x̄=x̄ basically shows add many means like result always x̄ 'm sure everyone enjoyed little proof image reference prove saw question meeting got curious like “ show mathematically ” spent like 10 minutes trying figure nice exercise brain 'm sure would able show proof spot honest	1
n't mean change filling nans	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
n't mean change filling nans	flask app produce result actually return prediction 're good n't need fix answer question regarding many points deducted think whole project kind assumes managed deploy flask n't able put docker able deploy cloud lose like five six points something along lines think 's good idea make sure flask returns something problems please share code think know asking question talked already slack maybe error somewhere predict.py script need make sure fix error work perhaps take code lectures – think xgboost model logistic regression need put x variable t-matrix xgboost instead using predict_proba need use predict think 's – n't work please write slack let 's try figure 's working	0
n't mean change filling nans	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
check correlation/feature importance categorical variables numeric target	eventually want analysis see much time people spent homework also want breakdown know people registered – sign course say 're data analyst data engineer student also want see breakdown per role – much time took let 's say software engineers think week n't difficult engineers probably difficult students perhaps maybe weeks way around want analytics point still finish editing videos week six first 'll need prepare guidelines regarding projects maybe 'll analytics	0
check correlation/feature importance categorical variables numeric target	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
check correlation/feature importance categorical variables numeric target	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
check correlation/feature importance categorical variables numeric target	n't think takes lot time first need record videos prepare homework imagine adding collaboration startups well maybe sometime future n't know somebody startup listening right want hire people graduate zoomcamp please reach let 's arrange something	0
check correlation/feature importance categorical variables numeric target	yes think showed 'm sure mean yeah think 's good idea	1
help docker work windows via wsl2 backend always get error exit code 1	's hard say ’ windows right remember problems eventually sorted problems n't remember exactly tutorials saw links slack show always use google cloud shell rent ec2 machine docker 's maybe also option sorry wish could help 's enough information help windows main system use usually use linux remember tutorial followed please share people slack managed make work maybe help	1
help docker work windows via wsl2 backend always get error exit code 1	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
help docker work windows via wsl2 backend always get error exit code 1	let 's go notes course repo course let 's take say linear regression notes – says “ add notes video prs welcome ” mean – let 's say watch video took notes want share links notes fellow learners taking notes putting somewhere notion medium github whatever prefer – online page put create pull request link notes somebody maybe n't like watching videos go notes read 's idea even better instead adding links add notes directly people need go external site – everything repo ’ idea n't time make notes 's added notes time want contribute notes contribute notes help everyone	0
help docker work windows via wsl2 backend always get error exit code 1	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
help docker work windows via wsl2 backend always get error exit code 1	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
avro versus parquet	alexey discussion/thread announcement channel week ago – lot datasets shared n't know many cool websites datasets check victoria put announcements alexey also want slack dump datatalks.club also use play around dataset see 's bunch json files build data pipelines well	0
avro versus parquet	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
avro versus parquet	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
avro versus parquet	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
avro versus parquet	ankush avro parquet totally different things avro row-based parquet columnar something like batch processing data-analytical kind work parquet perfect solution use spark something 's super fast calculate something like sum account parquet file read particular column rather whole column whole row avro hand strong providing flexibility terms backward forward compatibility 're using avro kafka using something like protobuf kafka would really useful think like different technologies altogether use different use cases alexey avro guess use case would streaming parquet batch jobs ankush yeah think definitely beginning would basic use cases yes alexey also something called ocr ankush 's also columnar storage alexey oh 's different orc chuckles remember watching presentation berlin buzzwords format remember seeing cool presentation never actually tried ankush neither i. alexey 's optimized row columnar interesting never knew stood widely used people go parquet ankush seen parquet guess companies definitely using 's bad technology think 's parquet famous compatible solutions parquet people tend use often	1
read book tran-dev set partition know purpose additional partition dataset applied	honest n't used uvicorn used gunicorn n't know matters n't actually checked benchmarks use gunicorn also use others – n't remember ones reason basically	0
read book tran-dev set partition know purpose additional partition dataset applied	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
read book tran-dev set partition know purpose additional partition dataset applied	alexey one thing maybe n't clear enough – please always include code solution maybe people n't watch first said n't submit code homework get zero points saw submissions people put dot empty put link towards data science article example please n't please put code n't 'll get zero points entire week 'll make explicit form week three well please n't forget put code	0
read book tran-dev set partition know purpose additional partition dataset applied	's really case-dependent saw slack remember correctly carlos asked needs drop features – seemed okay drop highly correlated feature assistant name target variable churn basically people work one particular person tend turn less people work somebody else think case seems like relevant feature include n't know – 's really case-dependent usually need sort domain expertise decide feature introduces leakage carlos ’ case think 's safe leave drop see performance validation dataset changes gives idea important feature	0
read book tran-dev set partition know purpose additional partition dataset applied	think explained video check 'll find answers 's called model selection n't think spend time explaining watch video questions actually said tran-dev set partition sorry dev case validation dev testing parameters think usually refer dev way validation	1
building project course could give us advice regarding	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
building project course could give us advice regarding	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
building project course could give us advice regarding	ankush yes yes think transfer service right yes question data using aws want run test around data source feel free also leave comment maybe even know goes azure blob storage	0
building project course could give us advice regarding	victoria yes weeks belong one bigger project taxi data new york shown beginning cover different stages idea also use afterwards build project final homework	1
building project course could give us advice regarding	alexey 'm sure last part –how docker provided terraform ankush run stuff terraform think always ... set airflow cluster terraform n't think execute job alexey 's something like use terraform prepare environment prepare thing run need actually run something least two separate commands first “ terraform apply ” “ python run ” something like 50:02 1 upvotes allowed contact instructors directly discuss projects things unsure regarding project 'm using new tools alexey think 's best ask things slack channel others also interested learning answer scales better instead everyone writing us directly – answer everyone sees answer b others answer available right somebody knows tool maybe n't example pulsar one students might know able help 's best use channel contact us directly	0
ever seen data engineering position required spark kafka	alexey example spark google “ spark gcp bigquery connector ” good article shows need need jar need use jar submit spark job let 's say files stored google cloud storage something files multiple options first option save back google cloud storage create external table something like like saw week 3. another option would saving directly bigquery using say “ write ” say want write bigquery uses connector connect bigquery write n't know permissions actually work – maybe also need specify key somehow probably 're already dataproc already permissions need order write bigquery order read google cloud storage use dataproc theoretically easy – add extra jar write bigquery data data warehouse use materials dbt week analytics engineering week actually consume content data warehouse visualize	0
ever seen data engineering position required spark kafka	victoria yeah one software uses spark n't use spark still got job n't take going back said – one going reject n't use exactly use 's important guess learn regarding kafka 's important understand 's going 'll build pipeline top kafka analytics engineer depends much data engineers work 'll still consume data analytics engineer going bridge data engineers analysts 's important maybe 're expert kafka understand kafka would least would know would n't recommend anyone go learn spark mostly focused analytics engineering said need understand concept 's important understand model things	1
ever seen data engineering position required spark kafka	alexey think question asking model building features bigquery good think good n't done understand ’ pretty basic “ ab initio ” something latin right victoria “ starting based first principles. ” sounds like spanish 's latin wanted check chuckles 's usually used legal terminology alexey maybe 's coming lawyer wants become data engineer anyway model building features pretty basic 's good start try switch jupyter notebook advanced things let ’ say – need flexibility think n't really experience bigquery aws 's little bit different aware similar method aws athena similar bigquery n't know create model athena maybe aws would use sagemaker sagemaker jupyter notebooks works 's good need complexity switch jupyter notebooks	0
ever seen data engineering position required spark kafka	victoria n't used anaconda think 's also documentation install dbt locally general 're using anaconda 're using environment install use homebrew 're using mac also use pip install pip install dbt postgres example need adapter dbt snowflake dbt big query – whatever want use locally 's 're installed set profiles.yml show videos create project clone whatever 's 's need use locally never install using anaconda 'm guessing 's probably something similar probably conda forge install like 's possible otherwise would go pip install alexey anaconda usually pip victoria yeah 're environment install like 's	0
ever seen data engineering position required spark kafka	alexey well good job keep working almost finished first week finish start week two finish week three chuckles follow sequence videos playlist github sufficient think questions go slack ankush keep going alexey chuckles yes exactly ’ almost finished first week good sign first week year pretty tough many people lot problems docker whatnot 're almost finished good sign ’ good track keep working	0
part deliverable expect peer evaluator download/clone git repo build docker image run docker container local/host cloud	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
part deliverable expect peer evaluator download/clone git repo build docker image run docker container local/host cloud	honest idea question means… oh flt pt floating pointers okay yes numpy floating point errors n't know – computers let 's say multiply two numbers operations take two numbers see instead 3 3 followed lot zeros 4 image reference floating point operations precise computers 's summing multiplying little floating point arithmetic error numpy 's running computer 's also prone guess question “ keep using logs data exploration ” 're probably referring week 2 think use logarithm price main reason numerical instability – ’ floating point errors – 'll probably learn video simply distributions long tails please refer eda video probably second third explain need 's keep using logs n't apply logarithm 's difficult machine learning models actually learn data	0
part deliverable expect peer evaluator download/clone git repo build docker image run docker container local/host cloud	alexey one thing maybe n't clear enough – please always include code solution maybe people n't watch first said n't submit code homework get zero points saw submissions people put dot empty put link towards data science article example please n't please put code n't 'll get zero points entire week 'll make explicit form week three well please n't forget put code	0
part deliverable expect peer evaluator download/clone git repo build docker image run docker container local/host cloud	n't know 's reasonable expect really want run learn look evaluation criteria n't say “ okay runnable. ” ideally – able git clone docker build docker run test works evaluation criteria ask see example criteria say code deploying docker file provided readme clearly describe actually build container run want clone errors actually maybe also add section errors 's actually want run advise run also recognize maybe require much time – need run three different projects run might difficult leave decision time want learn others highly encourage actually run would say 's optional	1
part deliverable expect peer evaluator download/clone git repo build docker image run docker container local/host cloud	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
ports host container different using flask docker could please give hint exactly port changes would reflected	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
ports host container different using flask docker could please give hint exactly port changes would reflected	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
ports host container different using flask docker could please give hint exactly port changes would reflected	yes run image 3 always forget whether first one host machine container let open docker file image 4 specify port container docker file need map port host machine port changes would reflected 's decide port want use	1
ports host container different using flask docker could please give hint exactly port changes would reflected	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
ports host container different using flask docker could please give hint exactly port changes would reflected	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
2019 2020 files transferred using transfer service entire bucket transferred	sejal maybe someone help answer really expert kubernetes side terraform lets build static code templates infrastructure iec-style manner would work cases want static-based infrastructure cases set infrastructure resources destroyed point time want restore certain image certain state – terraform useful case kubernetes 's actually like spin-up cluster deploy services onto kubernetes cluster use cases really different case maybe ankush alexey add kubernetes ankush yeah definitely think kubernetes terraform different think kubernetes thinking deploying microservices sort applications done terraform using aws fargate aws ecs solutions terraform much let ’ assume kubernetes use terraform set kubernetes cluster also use terraform set let 's say s3 buckets next week ’ course see use terraform set transfer service google cloud platform possible kubernetes yes argue certain services like spark flink coming new solutions run kubernetes still example say want run kafka clusters cases kubernetes good solution would like stable solution stable infrastructure case 's terraform would definitely help set kinds clusters alexey person quite far infrastructure would add usually people dedicated company terraform helps go web ui click things instead going clicking file say “ bunch resources set cloud. ” let 's say need move one account another whatever reason – happened multiple times work needed migrate different account terraform destroy one account actually would destroy later terraform “ plan apply ” one account go back old account terraform destroy course still takes time everything code know exactly kind services use example kind buckets use aws us would kind lambda functions basically resources kubernetes could one resource terraform file comment says “ terraform infrastructure code kubernetes infrastructure. ” think 's quite concise way summarizing	0
2019 2020 files transferred using transfer service entire bucket transferred	ankush vic experience lead right feel free jump think one important things data engineer know lot things need know different stuff might want expert spark still would need knowledge kafka want expert analytical engineering go deep dbt sql still need knowledge data warehousing go towards senior data engineer feel need least knowledge areas also need core competency least one two areas mid-level wanted go senior realized company really depended upon kafka luckily also interesting area took lot interest developed competency time also interested data lakes spark gave opportunity work projects could lead senior data engineer also leading team later alexey would also add check datatalks podcast cover things like maybe specific data engineers think find talks interesting maybe another podcast episode n't think cover detail 10 minutes	0
2019 2020 files transferred using transfer service entire bucket transferred	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
2019 2020 files transferred using transfer service entire bucket transferred	alexey well good job keep working almost finished first week finish start week two finish week three chuckles follow sequence videos playlist github sufficient think questions go slack ankush keep going alexey chuckles yes exactly ’ almost finished first week good sign first week year pretty tough many people lot problems docker whatnot 're almost finished good sign ’ good track keep working	0
2019 2020 files transferred using transfer service entire bucket transferred	alexey ankush way stopped started transfer using transfer service stopped transferred data right ankush automatically stop also use prefixes options use prefixes filters definitely try use	1
's feasible could provide insights hypothesis testing use case could referred order understand implementation	n't think takes lot time first need record videos prepare homework imagine adding collaboration startups well maybe sometime future n't know somebody startup listening right want hire people graduate zoomcamp please reach let 's arrange something	0
's feasible could provide insights hypothesis testing use case could referred order understand implementation	multi-classification – yes clustering – 'm sure probably good idea n't cover hard – know evaluate peers know better clustering	0
's feasible could provide insights hypothesis testing use case could referred order understand implementation	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
's feasible could provide insights hypothesis testing use case could referred order understand implementation	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
's feasible could provide insights hypothesis testing use case could referred order understand implementation	's scope course good video channel called a/b testing former colleague agnes agnes talked use a/b testing olx also goes hypothesis testing little bit cover course	1
somewhat unrelated lecture videos would pros cons aws gcp seems like aws popular	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
somewhat unrelated lecture videos would pros cons aws gcp seems like aws popular	alexey add comment least workplace airflow instance set us data scientist would go instance airflow managed data engineers devops engineers would use never needed actually run docker compose moment multiple dags orchestration tool quite useful would personally maybe go luigi favorite one need think people well since people know luigi many like maybe airflow would actually better choice consider sejal would also like add question docker compose used docker compose airflow style use official setup seeing first time well 's unnecessarily complicated especially xcom variables forth generally docker compose acts wrapper docker containers production workflows docker containers used integration ci/cd pipelines wherever 're going deploy docker compose docker compose part helpful development environments case let 's say mock version airflow environment production want test dags local development environment use docker compose make things easier running simple commands like ‘ docker compose ’ would make fine instead running ‘ docker run container name etc whatever want ’ terms convenience terms usage ’ generally used development environments production alexey think first part question “ close usual data engineer role workflows ” workflows see work complex steps probably also case need start something already – would n't call complex pipeline – three four steps already something eventually might grow bigger bigger add steps ankush would also like add 're cloud maybe use airflow deploy docker maybe actually use service provided cloud provider one similar airflow think airflow would used internally like cloud composite google cloud platform would using terraform sejal yeah used aws step functions aws setup step functions combination lambda place airflow ankush said also something wanted say really depends production self-hosted cluster kubernetes wherever want use pure airflow version company could kind setup use native airflow cluster could cloud-based setup convenient option use managed services cloud composer aws also version called managed airflow	0
somewhat unrelated lecture videos would pros cons aws gcp seems like aws popular	alexey example spark google “ spark gcp bigquery connector ” good article shows need need jar need use jar submit spark job let 's say files stored google cloud storage something files multiple options first option save back google cloud storage create external table something like like saw week 3. another option would saving directly bigquery using say “ write ” say want write bigquery uses connector connect bigquery write n't know permissions actually work – maybe also need specify key somehow probably 're already dataproc already permissions need order write bigquery order read google cloud storage use dataproc theoretically easy – add extra jar write bigquery data data warehouse use materials dbt week analytics engineering week actually consume content data warehouse visualize	0
somewhat unrelated lecture videos would pros cons aws gcp seems like aws popular	victoria would say ’ also 's different set products example data warehouse start data tech stack scratch would definitely use redshift cost lower end need probably would need migrate something like bigquery snowflake lot documentation 's pretty easy set 's around longer – think reason broader adoption mainly around things alexey seems aws popular well least berlin look companies use cloud maybe 70 people use aws 20 may use gcp remaining 10 use azure 's roughly see maybe 'm wrong course also got use gcp bit seems ui nicer go web console 's little bit nicer think comes tools 's bit difficult example aws pip install aws cli google cloud 's bit difficult maybe 's one reason aws popular – ’ mature maybe think gcp catching nice interface sometimes things cheaper bigquery also think big advantage maybe clouds n't really victoria plus aws around 10 years thesis using aws thing gcp yet 's lot people go 're familiar outside stability alexey think cloud services services quite similar ’ used aws many things gcp straightforward n't typical less think many concepts map one another	1
somewhat unrelated lecture videos would pros cons aws gcp seems like aws popular	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
okay two guys work dataset individually	answer – depends dataset – typically train/validation split first try scaling try without scaling see happens think something talked last week actually notebook prepared last week go classification notebook notebook scaling shows scale features use standard scaler use min max scaler – use two different scalars shows particular dataset turned little bit better experiment see scaling makes sense linear models sometimes 's actually good idea add extra complexity pipeline instead dictionary vectorizer whatnot 'll also need add next step scaling basically steps prediction pipeline becomes bit difficult maintain would say try scale see improvement get significant maybe 's worth extra complexity get use cross-validation use best judgment see makes sense	0
okay two guys work dataset individually	yes could soon post message slack need find time couple tweaks script	0
okay two guys work dataset individually	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
okay two guys work dataset individually	two people want work dataset individually please feel free imagine way control go kaggle select competition even ’ accidentally happen long 's exactly code – long n't copy – ’ fine yeah please n't copy code	1
okay two guys work dataset individually	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
time categorical variable 11 values	yes n't written script yet actually want – many people asked see results made mistake beginning letting people register n't add checkbox saying 's okay use data like first name last name public really public leaderboard everyone go check score simply n't ask permission want instead write web service ’ email field go put email get scores idea hope 'll able implement hopefully n't difficult able get scores way yes grade homework store scores course end remember first video talked public leaderboard end course 100 people leaderboard ask permission publish names 'll need figure exactly communicate results separate homework grading actually difficult want write script	0
time categorical variable 11 values	would say 's must – would say 's big plus get exposure areas also think able deploy models really crucial part 's much emphasis course deployment part able data scientist especially machine learning engineer 'm sure probably work kubernetes job – something similar kubernetes knowing quite beneficial learn experience companies get queue hire think 's useful experience	0
time categorical variable 11 values	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
time categorical variable 11 values	n't know n't checked honest took – took data preparation step – n't see data preparation step time maybe categorical variable	1
time categorical variable 11 values	alexey one thing maybe n't clear enough – please always include code solution maybe people n't watch first said n't submit code homework get zero points saw submissions people put dot empty put link towards data science article example please n't please put code n't 'll get zero points entire week 'll make explicit form week three well please n't forget put code	0
consider train data validation needs separated validation eda done entire train splitting validation	alexey sure maybe answer thing talked regarding filters well use matplotlib take filter then… dmitry think pil interesting option 's images guess alexey yeah pil 's like want save jpeg example want plot see looks matplotlib probably sufficient homework also use matplotlib n't use pil 48:08 0 upvotes use dogs vs cats main capstone project better performance dmitry guess maybe datasets alexey yeah datasets dmitry mean lot open datasets guess alexey cats dogs yeah dmitry mean think cats dogs another time would n't interesting alexey yeah add data interesting dmitry maybe cats versus dogs versus something else alexey wombats cats versus wombats chuckles think image net must wombats right dmitry yeah alexey take part image net get wombats dmitry fun alexey laughs maybe wombats versus… similar wombats wombats cats quite different right different environments 's easy task dmitry maybe wildcats alexey yeah saw wildcat zoo looked like normal cat suspect maybe put normal cat said 's wildcat chuckles dmitry better marketing chuckles	0
consider train data validation needs separated validation eda done entire train splitting validation	n't think understand question validation needs separated n't know mean maybe ask slack think 're sure eda training dataset – first train dataset full full train dataset nothing bad happen	1
consider train data validation needs separated validation eda done entire train splitting validation	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
consider train data validation needs separated validation eda done entire train splitting validation	eventually want analysis see much time people spent homework also want breakdown know people registered – sign course say 're data analyst data engineer student also want see breakdown per role – much time took let 's say software engineers think week n't difficult engineers probably difficult students perhaps maybe weeks way around want analytics point still finish editing videos week six first 'll need prepare guidelines regarding projects maybe 'll analytics	0
consider train data validation needs separated validation eda done entire train splitting validation	's really case-dependent saw slack remember correctly carlos asked needs drop features – seemed okay drop highly correlated feature assistant name target variable churn basically people work one particular person tend turn less people work somebody else think case seems like relevant feature include n't know – 's really case-dependent usually need sort domain expertise decide feature introduces leakage carlos ’ case think 's safe leave drop see performance validation dataset changes gives idea important feature	0
separate materials videos setting environment wsl vsc	anything like feel free ask anything slack n't think anything special lot links already shared slack like set environment vcl maybe also search – go slack look wsl 'll see links	1
separate materials videos setting environment wsl vsc	's really case-dependent saw slack remember correctly carlos asked needs drop features – seemed okay drop highly correlated feature assistant name target variable churn basically people work one particular person tend turn less people work somebody else think case seems like relevant feature include n't know – 's really case-dependent usually need sort domain expertise decide feature introduces leakage carlos ’ case think 's safe leave drop see performance validation dataset changes gives idea important feature	0
separate materials videos setting environment wsl vsc	soft skills resources career questions would recommend actually go datatalks.club slack channel called career questions go ask questions use slido link course way separate two sessions talk course career slack talk careers	0
separate materials videos setting environment wsl vsc	classification classification let 's say n't predicting price let 's say predicting 's average number hours image 1 case would need another layer – rather layer 's called activation n't activation 're still… ’ trying formulate way 's confusing think 's better go lecture rewatch ’ essentially retelling thing need from_logits price variable loss mean squared error logits loss categorical cross-entropy categorical cross entropy usually used multiple categories need use logits 's numerically stable n't use – stay activation softmax n't use legit hope 's clear regression n't need use homework need use from_logits either homework two classes 's binary classification problem multiple classes – let 's say three classes four classes five classes – need use logits loss categorical cross-entropy loss categorical cross-entropy 's recommended use from_logits=true 's numerically stable	0
separate materials videos setting environment wsl vsc	maybe n't know actually started like windows editing videos linux terrible windows 's much easier bunch stuff comes development n't think 's anything better linux comes simple day-to-day stuff like editing video editing image n't think linux best environment	0
use azure blob storage instead aws transfer gcp	alexey discussion/thread announcement channel week ago – lot datasets shared n't know many cool websites datasets check victoria put announcements alexey also want slack dump datatalks.club also use play around dataset see 's bunch json files build data pipelines well	0
use azure blob storage instead aws transfer gcp	alexey example spark google “ spark gcp bigquery connector ” good article shows need need jar need use jar submit spark job let 's say files stored google cloud storage something files multiple options first option save back google cloud storage create external table something like like saw week 3. another option would saving directly bigquery using say “ write ” say want write bigquery uses connector connect bigquery write n't know permissions actually work – maybe also need specify key somehow probably 're already dataproc already permissions need order write bigquery order read google cloud storage use dataproc theoretically easy – add extra jar write bigquery data data warehouse use materials dbt week analytics engineering week actually consume content data warehouse visualize	0
use azure blob storage instead aws transfer gcp	alexey 's early talk future iterations data engineering zoomcamp think chuckles ml camp yes – videos difference homework 's still discussion n't know 'll see ankush yeah think slight discussion met sure going repeat ml de zoomcamp let 's see response participation feel definitely success definitely feel free pass around course give us feedback maybe make better decision alexey re-recording videos… 's much effort n't think 'll want let 's say ml zoomcamp example much effort 'd rather reuse videos instead making new ones hope videos good decide rerun need re-record yeah 'll see	0
use azure blob storage instead aws transfer gcp	ankush yes yes think transfer service right yes question data using aws want run test around data source feel free also leave comment maybe even know goes azure blob storage	1
use azure blob storage instead aws transfer gcp	alexey sure answer question actually something think spoiler homework solution chuckles using airflow helps use airflow 's 've whole week week 2 using airflow first download data ny taxi website parquetize upload google cloud storage n't s3 oh see question coming guess go ny taxi data website – transfer service – one videos transfer service transfer service moving data s3 google cloud storage understand question data already s3 – look urls see url contains “ s3 amazon aws ” name packet data already s3 n't need probably long answer short data already s3 n't need	0
want know theory behind machine learning could recommend book course covers part	eventually want analysis see much time people spent homework also want breakdown know people registered – sign course say 're data analyst data engineer student also want see breakdown per role – much time took let 's say software engineers think week n't difficult engineers probably difficult students perhaps maybe weeks way around want analytics point still finish editing videos week six first 'll need prepare guidelines regarding projects maybe 'll analytics	0
want know theory behind machine learning could recommend book course covers part	yes submit anywhere want say 20 pt n't remember yeah work good article “ good ” meaning 's copy/pasting code nothing else actually need explain 's going also article – closer time working article guidelines well	0
want know theory behind machine learning could recommend book course covers part	really depends cloud showed elastic beanstalk n't think needed change much anything n't remember changing anything thing changed url service test.py script thing needed change think heroku 's similar actually couple tutorials using pythonanywhere heroku probably explain need change go see need change anything probably much	0
want know theory behind machine learning could recommend book course covers part	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
want know theory behind machine learning could recommend book course covers part	think recommend course watch course coursera andrew ng 's called machine learning covers theory quite well theory also intuitive level – math comparatively less difficult compared textbooks course get textbooks good textbook elements statistical learning ’ really good – lot math think book called pattern recognition machine learning probabilistic approach – 's another book one bishop one author murphy 're mathematical lot math really need couple years calculus background able understand n't background course andrew ng good	1
n't trust others give good scores project	alexey one thing maybe n't clear enough – please always include code solution maybe people n't watch first said n't submit code homework get zero points saw submissions people put dot empty put link towards data science article example please n't please put code n't 'll get zero points entire week 'll make explicit form week three well please n't forget put code	0
n't trust others give good scores project	's actually think 's useful n't train.py produce pickle file let 's say want retrain model using different parameters – 's useful see changed score real life like projects work train.py 's always one file sometimes 's bunch files somehow log performance think 's useful n't want reason n't	0
n't trust others give good scores project	think people good intentions n't think somebody intentionally give bad score somebody wants intentionally give bad scores please n't would seriously please n't 're learn somebody 's giving best see matrix use evaluating give score person deserves n't link coursera actually research peer reviewing found peer reviews actually work quite well people n't give bad scores sake another reason three people reviewing one project exactly avoid one person giving everyone bad scores 'll take median avoid 's three scores also think opportunity get feedback people write something saying “ hey problems running this. ” 's good somebody tries actually run project n't answer apart please bit trust people think good intentions	1
n't trust others give good scores project	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
n't trust others give good scores project	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
n't feature engineering modeling	alexey think question related homework – n't create extra features n't know dmitry ideas n't dmitry actually thought simplicity say perspective go step step regarding feature engineering percentage covered week week 3 next week therefore think sets good tempo go step step sure 's good question sure usually need	1
n't feature engineering modeling	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
n't feature engineering modeling	yes could soon post message slack need find time couple tweaks script	0
n't feature engineering modeling	alexey one thing maybe n't clear enough – please always include code solution maybe people n't watch first said n't submit code homework get zero points saw submissions people put dot empty put link towards data science article example please n't please put code n't 'll get zero points entire week 'll make explicit form week three well please n't forget put code	0
n't feature engineering modeling	's actually think 's useful n't train.py produce pickle file let 's say want retrain model using different parameters – 's useful see changed score real life like projects work train.py 's always one file sometimes 's bunch files somehow log performance think 's useful n't want reason n't	0
important skills new analytics engineer master sql	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	1
important skills new analytics engineer master sql	ankush yes yes think transfer service right yes question data using aws want run test around data source feel free also leave comment maybe even know goes azure blob storage	0
important skills new analytics engineer master sql	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
important skills new analytics engineer master sql	victoria 's buzzwords right alexey yeah go repo see like practical week like five 10 minute video explaining use 's planned ankush maybe spend couple minutes today well data mesh basically concept idea – let 's go back say “ data pipelines data engineering teams built different companies right ” people generating data people consuming data one big team middle called data engineering team dataops team whatever want call different companies different names teams responsible ingesting data transforming putting making way 's consumable right way right fashion data mesh wants basically decentralize particular role big team wants give power let 's say responsibility team generating data teams consuming data therefore ’ layer instead team taking whole role 's kind better idea build services give responsibility give services software teams generating data consuming basically going higher level respect case happens team ’ generating data responsible quality data answering different questions data things 's really useful multiple teams handling data idea data knowledge data lost best person best team answer questions best team knowledge data team 's generating also responsible putting data data lake data warehouse also responsible queries attaches whole thing together 's kind rough idea data mesh alexey ’ abstract concept actually go youtube channel longer explanation data mesh “ longer ” mean one hour long – hour six minutes chuckles 're kind stuff… way dataops 101 also quite nice one check well one modern data stack analytics engineering also good basically check three – 're good 're interested data mesh specifically first one goes lot detail second one – bit detail ankush 's also nice blog post alexey zhamak actually writing book right think 's early release 's written five six chapters n't remember 's progress 's going quite big book right ankush blog post like today 's date read couple days impossible read one go 's huge victoria think also download first two chapters something book give quick overview well ankush 's also another book o'reilly called data mesh practice max schultze alexey one free right ankush think free 's free	0
important skills new analytics engineer master sql	alexey 's want look project 're behind right n't worry take time decide whether give certificate based projects homework 's – catch week 3 lighter week 1 maybe fast-forward directly docker compose run week 2 depending much time maybe go solution published today give everything need week 3 use transfer service video already watching video probably learn lot already like homework solution	0
many homework submissions week	flask – let draw let 's say want create web service image 5 web service gets requests web service responds something could predictions could something else – n't related machine learning implement web service use flask use fastapi use framework creating web services use flask use something else – flask allows use post example let 's say “ get ” usually parse parameters query let 's say go google put “ test ” see q=test https //www.google.com/search q=test oq=test aqs=chrome.0.69i59l2j69i65l3j69i60j69i61l2.1264j0j1 sourceid=chrome ie=utf-8 image 6 parameter pass “ get ” request “ get ” request sometimes let 's say want score customer n't want put gender=female contract=one year string send json post post also add body request generally use post general use case flask able create web service	0
many homework submissions week	took course ago back useful think something use python immediately could helpful 'm sure would good recommendation current course course good would n't compare stanford course andrew ng simply different course 'll get lot theory one practical hands-on – 's mostly writing code together maybe take	0
many homework submissions week	yes submit anywhere want say 20 pt n't remember yeah work good article “ good ” meaning 's copy/pasting code nothing else actually need explain 's going also article – closer time working article guidelines well	0
many homework submissions week	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
many homework submissions week	's 404. somebody managed right stopped accepting responses somebody got lucky	1
data mesh covered course	alexey think 's helpful 's necessary helpful happens dags run airflow dags tasks tasks often kubernetes jobs something like infra – many companies infrastructure managed kubernetes 's quite simple get new container execute new job kubernetes n't think 's necessary think 's helpful 'm data engineer – 'm data scientist data scientist knowledge kubernetes quite helpful could debug jobs 'm running airflow run kubernetes could see logs would say want learn hurt get hired without kubernetes knowledge materials course sufficient get hired pick kubernetes work example agree victoria victoria yeah think kubernetes getting popular ca n't hurt	0
data mesh covered course	alexey week 2 yes – upload video solution also update leaderboard probably take time depending clean data – need lot data cleaning take lot time please n't put things n't look like emails email field fill please careful takes time clean later	0
data mesh covered course	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
data mesh covered course	victoria 's buzzwords right alexey yeah go repo see like practical week like five 10 minute video explaining use 's planned ankush maybe spend couple minutes today well data mesh basically concept idea – let 's go back say “ data pipelines data engineering teams built different companies right ” people generating data people consuming data one big team middle called data engineering team dataops team whatever want call different companies different names teams responsible ingesting data transforming putting making way 's consumable right way right fashion data mesh wants basically decentralize particular role big team wants give power let 's say responsibility team generating data teams consuming data therefore ’ layer instead team taking whole role 's kind better idea build services give responsibility give services software teams generating data consuming basically going higher level respect case happens team ’ generating data responsible quality data answering different questions data things 's really useful multiple teams handling data idea data knowledge data lost best person best team answer questions best team knowledge data team 's generating also responsible putting data data lake data warehouse also responsible queries attaches whole thing together 's kind rough idea data mesh alexey ’ abstract concept actually go youtube channel longer explanation data mesh “ longer ” mean one hour long – hour six minutes chuckles 're kind stuff… way dataops 101 also quite nice one check well one modern data stack analytics engineering also good basically check three – 're good 're interested data mesh specifically first one goes lot detail second one – bit detail ankush 's also nice blog post alexey zhamak actually writing book right think 's early release 's written five six chapters n't remember 's progress 's going quite big book right ankush blog post like today 's date read couple days impossible read one go 's huge victoria think also download first two chapters something book give quick overview well ankush 's also another book o'reilly called data mesh practice max schultze alexey one free right ankush think free 's free	1
data mesh covered course	alexey 's want look project 're behind right n't worry take time decide whether give certificate based projects homework 's – catch week 3 lighter week 1 maybe fast-forward directly docker compose run week 2 depending much time maybe go solution published today give everything need week 3 use transfer service video already watching video probably learn lot already like homework solution	0
olx work data science team also leverage swe experience time time	yes time time use software engineering skills mostly spend day meetings n't lot hands-on work years ago would code day long lot meetings well days n't leverage past software engineering experience much yeah always cases let 's say need deploy something useful opinion data science still engineering thing – need write lot code background software engineering programming helpful	1
olx work data science team also leverage swe experience time time	's really case-dependent saw slack remember correctly carlos asked needs drop features – seemed okay drop highly correlated feature assistant name target variable churn basically people work one particular person tend turn less people work somebody else think case seems like relevant feature include n't know – 's really case-dependent usually need sort domain expertise decide feature introduces leakage carlos ’ case think 's safe leave drop see performance validation dataset changes gives idea important feature	0
olx work data science team also leverage swe experience time time	honest n't used uvicorn used gunicorn n't know matters n't actually checked benchmarks use gunicorn also use others – n't remember ones reason basically	0
olx work data science team also leverage swe experience time time	let 's say two dimensional thing 2d want turn 1d case need use pooling2d 'm sure right want quickly check reason remember showing usually go defining model build sort layer layer every time add one layer model predicts see output based see kind pooling layer example need let 's say want turn 2d 1d think need use 2d pooling 'm exactly sure whether 's 2d 1d think 1d pooling needed something one dimensional want turn one value use 1d pooling 3d pooling three dimensional thing want turn one-dimensional thing image 2 things always confuse honest 's follow step step try different poolings want make sure convert image vector presentation something one-dimensional usually size number images times something 's like 2d array image one-dimensional vector based try different poolings sometimes also flatten flatten takes whatever – let 's say kd – want turn 1d use flatten many different options think 's clear difference might remember exactly use 1d 2d difference kind input take kind output produce 's cube 's 2d 's hypercube three dimensions 's something else	0
olx work data science team also leverage swe experience time time	's interesting one let 's say numpy array image reference numbers – 1 6 – compute mean mean 3.5. add 3.5 multiple times mean n't change happens nan pandas simply ignores – n't look pretends 's basically pandas ignores nans looks numbers replace nans mean case 3.5 get data effectively mean n't change try show formula let 's say bit math let 's say four cities couple n/as basically part usual values part n/as missing part let 's say want compute mean .mean pandas happens let 's say n numbers like k missing values total series k+n – total length compute mean part get 1/nσxi=x̄ mean case 3.5. let 's replace n/as x̄ happens instead looking 1/n look 1/n+k total number elements already filled sum another sum x̄ k n summed k times x̄ becomes kx̄ formula like multiply n/n n/n simply 1. 's multiplying thing one get n x̄ part mean image reference clue 'm talking bear 's almost result 1/ n+k n x̄+kx̄ leave x equation 1/ n+k n+k x̄=x̄ basically shows add many means like result always x̄ 'm sure everyone enjoyed little proof image reference prove saw question meeting got curious like “ show mathematically ” spent like 10 minutes trying figure nice exercise brain 'm sure would able show proof spot honest	0
override schema parquet files bigquery stored cloud storage ’ working csv easy	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
override schema parquet files bigquery stored cloud storage ’ working csv easy	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
override schema parquet files bigquery stored cloud storage ’ working csv easy	ankush n't know answer tried csv n't try parquet thing csv automatically detects date time parquet date fields converted byte arrays think 've seen issue multiple times slack sorted right suggest try maybe put solution slack alexey think problem one already schema embedded parquet files right bigquery sees one parquet file one schema one different type n't know right ankush yeah definitely case	1
override schema parquet files bigquery stored cloud storage ’ working csv easy	victoria set warehouse end define connection difference would use adapter dbt several adapters would first want use dbt go check adapters make sure 's supported several official example redshift could use community-supported azure alexey yeah aws athena azure athena redshift presto well example presto cluster use aws – within aws presto cluster potentially similar setup connect dbt presto victoria general add little bit answer let 's say project right bigquery would change profile 's set connection may make changes 's something sql supports bigquery way around able run project minutes	0
override schema parquet files bigquery stored cloud storage ’ working csv easy	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	0
hw hand sql files put .md file	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
hw hand sql files put .md file	ankush n't know think aws possible without restrictions alexey probably public open bucket like taxi trips theoretically thing would private s3 buckets ankush yeah think problem would open source data google cloud storage definitely need move s3 bucket	0
hw hand sql files put .md file	alexey 's want look project 're behind right n't worry take time decide whether give certificate based projects homework 's – catch week 3 lighter week 1 maybe fast-forward directly docker compose run week 2 depending much time maybe go solution published today give everything need week 3 use transfer service video already watching video probably learn lot already like homework solution	0
hw hand sql files put .md file	victoria 's buzzwords right alexey yeah go repo see like practical week like five 10 minute video explaining use 's planned ankush maybe spend couple minutes today well data mesh basically concept idea – let 's go back say “ data pipelines data engineering teams built different companies right ” people generating data people consuming data one big team middle called data engineering team dataops team whatever want call different companies different names teams responsible ingesting data transforming putting making way 's consumable right way right fashion data mesh wants basically decentralize particular role big team wants give power let 's say responsibility team generating data teams consuming data therefore ’ layer instead team taking whole role 's kind better idea build services give responsibility give services software teams generating data consuming basically going higher level respect case happens team ’ generating data responsible quality data answering different questions data things 's really useful multiple teams handling data idea data knowledge data lost best person best team answer questions best team knowledge data team 's generating also responsible putting data data lake data warehouse also responsible queries attaches whole thing together 's kind rough idea data mesh alexey ’ abstract concept actually go youtube channel longer explanation data mesh “ longer ” mean one hour long – hour six minutes chuckles 're kind stuff… way dataops 101 also quite nice one check well one modern data stack analytics engineering also good basically check three – 're good 're interested data mesh specifically first one goes lot detail second one – bit detail ankush 's also nice blog post alexey zhamak actually writing book right think 's early release 's written five six chapters n't remember 's progress 's going quite big book right ankush blog post like today 's date read couple days impossible read one go 's huge victoria think also download first two chapters something book give quick overview well ankush 's also another book o'reilly called data mesh practice max schultze alexey one free right ankush think free 's free	0
hw hand sql files put .md file	alexey terms exactly hand code – n't matter put somehow format add link github ankush actually solution learn – maybe want something learning purposes 's perfectly fine .md file whatever file thinking project would like highlight future employer want attract future employers putting linkedin open source would actually also focus putting right file format everything 's easier github read 's easier download locally open intelligence color coding think 's kind nice n't stress much put .md 's fine maybe future put .sql file	1
opinion learning scala spark	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
opinion learning scala spark	victoria n't used anaconda think 's also documentation install dbt locally general 're using anaconda 're using environment install use homebrew 're using mac also use pip install pip install dbt postgres example need adapter dbt snowflake dbt big query – whatever want use locally 's 're installed set profiles.yml show videos create project clone whatever 's 's need use locally never install using anaconda 'm guessing 's probably something similar probably conda forge install like 's possible otherwise would go pip install alexey anaconda usually pip victoria yeah 're environment install like 's	0
opinion learning scala spark	alexey well 's really anonymous asked submit link github n't know need ask questions previous course think ask contact information project description n't know ankush think encourage might unfair different people first maybe 're available answer questions let 's put like grading maybe 's unfair want learn 's definitely helpful think grading done based upon submitted 's also really important document steps alexey yeah guess need contact author means really give four points reproducibility ankush case maybe instructions clear report via contact alexey yeah exactly n't remember problem previous course maybe also create issue github n't know	0
opinion learning scala spark	alexey yeah learn scala spark useful think 've discussion ankush said ’ definitely going useful remember one case previous company needed rewrite something python scala improved speed step significantly – rewriting knowing scala helpful think get away python stuff depends deep want go spark maybe learn personal opinion better ways learn scala chuckles tastes differ victoria suggestions ideas learn scala something else victoria think 's data engineering focus definitely 'm – learn python stuff also think come computer science background depends right already know program n't scared pick scala 'll probably able anyway course like everything 'll learn something n't scared able know something people wo n't hire alexey yeah get hired python sufficient also talking scala – understood kafka packages java world jvm java scala jvm languages – advanced python things example streaming also libraries like flink – python integration still java api scala api better use cases might end learning scala java would say – n't learn need programming languages pick victoria think 's always better know things well – like python sql stuff like – pick things opposed knowing bunch things well 's strategy would say	1
opinion learning scala spark	sejal think related orchestration since 've covered concepts orchestration alexey maybe take one putting two cents – airflow definitely meant orchestration multiple jobs sometimes even parallel asynchronously running jobs 's really meant sequential run small-scale usage definitely resource-intensive jobs alexey airflow quite heavy indeed benefits gives – mechanisms ui viewing history – useful moment three dags running parallel one airflow might overkill know 're going pipelines workflows 's time think kind workflow orchestration tool n't airflow airflow quite widespread let 's say team 're making decision workflow orchestration tool use airflow would quite good choice data engineers market know data scientists also know less usually like good tool course disadvantages – indeed resource-intensive needs run somewhere – typically n't set computer	0
certificate issued individually right project peer reviewed issued single day specific deadline	alexey issued single day process quite manual course type name every certificate save 's semi-automated still need come csv file names emails even need come design script run upload actually website things published certificate script creating certificate automatic still manual work 's single day specific deadline specific deadline case means csv file design certificate hit enter command line	1
certificate issued individually right project peer reviewed issued single day specific deadline	sejal think related orchestration since 've covered concepts orchestration alexey maybe take one putting two cents – airflow definitely meant orchestration multiple jobs sometimes even parallel asynchronously running jobs 's really meant sequential run small-scale usage definitely resource-intensive jobs alexey airflow quite heavy indeed benefits gives – mechanisms ui viewing history – useful moment three dags running parallel one airflow might overkill know 're going pipelines workflows 's time think kind workflow orchestration tool n't airflow airflow quite widespread let 's say team 're making decision workflow orchestration tool use airflow would quite good choice data engineers market know data scientists also know less usually like good tool course disadvantages – indeed resource-intensive needs run somewhere – typically n't set computer	0
certificate issued individually right project peer reviewed issued single day specific deadline	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
certificate issued individually right project peer reviewed issued single day specific deadline	victoria n't used anaconda think 's also documentation install dbt locally general 're using anaconda 're using environment install use homebrew 're using mac also use pip install pip install dbt postgres example need adapter dbt snowflake dbt big query – whatever want use locally 's 're installed set profiles.yml show videos create project clone whatever 's 's need use locally never install using anaconda 'm guessing 's probably something similar probably conda forge install like 's possible otherwise would go pip install alexey anaconda usually pip victoria yeah 're environment install like 's	0
certificate issued individually right project peer reviewed issued single day specific deadline	alexey ankush like scala right chuckles ankush think scala definitely one best languages data engineers chuckles biggest part 's typesafe 's heavy verbose java scala perfect combination least coming java background love scala would say yes 's kind important language know data engineer especially spark written scala sometimes see errors spark- java-native might get confused knowledge like scala personally use developing pipelines also use write code spark sometimes also beam actually need really know python well enough proficient python get away minimum knowledge java scala would say unfortunately would say like scala personally like definitely go ahead learn 's still good language alexey think get performance benefits python extra layer abstraction top spark adds bit overhead cases want control things right ankush personal experience whenever written batch jobs – 'm talking real-time 'm talking batch jobs n't matter much time takes – always half hour 10-15 minutes minimum adding another 30 seconds one minute python performance n't really matter real-time streaming yes would huge loss n't think 's case pyspark real-time batch n't really matter alexey think serialization better use scala use “ case classes ” whatever called easily turn rdd case class think uses efficient serialization mechanism python sometimes difference minutes remember case difference like 20 minutes took pipeline rewrote scala difference big ankush would say spark 1.0-something data frames 's also really useful anymore alexey data frames yeah 1.2. remember used python every step pipeline found one job could optimize scala whole pipeline lot faster something needed scala year using spark chuckles think important – know java scala data engineer ankush good python would use already know java ’ need scala n't know either learn scala scala would much easier learn application especially spark alexey argue n't lot time chuckles cross-talk ankush preferences biased really like scala answers neutral definitely biased towards scala alexey victoria n't want add anything victoria n't know n't used scala several years reason people offer jobs java development linkedin chuckles alexey use victoria used university prove get title would use	0
missed current zoomcamp 2022 another one near future	maybe maybe see content available	1
missed current zoomcamp 2022 another one near future	alexey yes let tell “ ” first use peer reviewing project deadline already peers review project let 's say “ support ” course forever 's want get project reviewed need need submit within deadline peers review give feedback “ ” also said “ yes ” yes might remember weeks ago made announcement another project cohort right first one finish project another project immediately war ukraine people work project right everyone else work project right reason 're giving chance work let 's say joined course catch everything submit project second “ trial/iteration/cohort ” – basically one month three weeks another project cohort submit project way submit project	0
missed current zoomcamp 2022 another one near future	victoria sure understand 're taking csv really database right alexey yeah think question general related homework understand correctly let 's say database ecommerce could orders something like data constantly changing example today price – tomorrow price changes question put data warehouse change know victoria n't know question targeting schema evolution example data changes right case would primary key would kind deduplication assume 'll original record modified record 're modeling data kind deduplication always take last record primary key let 's say “ order number ” case schema evolution 's question going little bit complex 'll somehow adapt pipeline 're using etl service tools like fivetran questions well tools also adapt schema evolution alexey another thing add – let 's say database records item id item name price let 's say price changes capture change new version know record database updated save change data warehouse therefore every time record changes save data warehouse bunch rows change basically capture every time something changes save data warehouse sort travel back time – see state record particular date 'm sure though 's specific data warehouse – least every time record changes capture change somehow sql query say “ want see date timestamp. ” see latest price example implement actually idea maybe fivetran databases – let 's say use mysql – something send data source changes something like postgres probably something similar victoria yeah 'll implement pipeline basically logging 's going even though original database maybe 's order like case update generate second entry like alexey mentioned 're logging fivetran could well course alexey fivetran low-code thing say “ okay source destination ” thing right moves data one location another works victoria yeah less chuckles set connection s3 bucket whatever want – several adapters set run rule scripts set destination	0
missed current zoomcamp 2022 another one near future	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
missed current zoomcamp 2022 another one near future	victoria 'm sure answer mean yes definitely adding part project least workshop reason want people learn also consider people entirely new concept thought would much overhead try cover things already quite complex day-to-day even though work daily dbt work slow-changing dimensions daily basis example 's also think 's something look moment use definitely dbt especially deletions implementing load step done hooks think 's section advanced knowledge link hooks like incremental models mentioned changing dimensions n't link could link something could use snapshots think added concept updates guess could also pre-hook post-hook also something would incremental model incremental model loads new chunk data use merge depending workers use use could insert update 's something would work unique key project also final project 's understand complex go 's limit hope answers	0
think data engineering position started etl tools long time ago say de started	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
think data engineering position started etl tools long time ago say de started	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
think data engineering position started etl tools long time ago say de started	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
think data engineering position started etl tools long time ago say de started	victoria 'm sure answer mean yes definitely adding part project least workshop reason want people learn also consider people entirely new concept thought would much overhead try cover things already quite complex day-to-day even though work daily dbt work slow-changing dimensions daily basis example 's also think 's something look moment use definitely dbt especially deletions implementing load step done hooks think 's section advanced knowledge link hooks like incremental models mentioned changing dimensions n't link could link something could use snapshots think added concept updates guess could also pre-hook post-hook also something would incremental model incremental model loads new chunk data use merge depending workers use use could insert update 's something would work unique key project also final project 's understand complex go 's limit hope answers	0
think data engineering position started etl tools long time ago say de started	victoria would say ’ probably similar happened data analytics engineering suddenly whole stack became complex could n't make scripts unintelligible 's probably started would assume n't know whole story data engineer role alexey maybe give maybe correct answer one data scientist ’ perspective data science started 10 years ago maybe people expected much data science thought would hire bunch people phds mathematics would magic data would clean models would perfect companies would earn lot money things n't work thought “ okay problem ” turned required lot engineering – data pipelines need think around time data science getting traction people realized 's also important think also together analytics – maybe beginning 2010s somewhere around victoria want right answer 2011. airbnb facebook apparently started 's indeed related things talking – complexity etl process suddenly needed role oriented building pipelines transport transform data alexey imagine company hires analyst analyst says “ okay n't know somebody needs come set infra. ” analysts could n't data scientists could n't – somebody needed etl developers kind rebranded data engineers guess nice history lesson	1
see data engineering outsourced like coding example tend see people sourced russia ukraine poland data engineering also affected similar moves 2022	victoria set warehouse end define connection difference would use adapter dbt several adapters would first want use dbt go check adapters make sure 's supported several official example redshift could use community-supported azure alexey yeah aws athena azure athena redshift presto well example presto cluster use aws – within aws presto cluster potentially similar setup connect dbt presto victoria general add little bit answer let 's say project right bigquery would change profile 's set connection may make changes 's something sql supports bigquery way around able run project minutes	0
see data engineering outsourced like coding example tend see people sourced russia ukraine poland data engineering also affected similar moves 2022	alexey 's early talk future iterations data engineering zoomcamp think chuckles ml camp yes – videos difference homework 's still discussion n't know 'll see ankush yeah think slight discussion met sure going repeat ml de zoomcamp let 's see response participation feel definitely success definitely feel free pass around course give us feedback maybe make better decision alexey re-recording videos… 's much effort n't think 'll want let 's say ml zoomcamp example much effort 'd rather reuse videos instead making new ones hope videos good decide rerun need re-record yeah 'll see	0
see data engineering outsourced like coding example tend see people sourced russia ukraine poland data engineering also affected similar moves 2022	victoria yes n't think received bunch linkedin messages people offering outsource engineers n't know though alexey approach n't mean companies actually agree victoria oh yeah n't know agrees 've never agreed far ’ never even talked must someone agency providing service right alexey think comes data 's little bit trickier basic coding data sensitive need right things place first need lawyer would put proper data regulation contract also need infrastructure allows maybe send sensitive data outsourcing partners think becomes bit trickier let 's say traditional software engineering hand traditional software engineering also get see things like phone numbers emails etc maybe see something like see example data science often kept in-house rather outsourced something similar probably still happens data engineering n't know maybe 'll see one year changes people also reach saying “ great team minsk belarus want start working projects. ” answer saying “ sorry n't need services right now. ” victoria yeah alexey probably assuming much distinction data engineers software engineers remember poland worked bank actually outsourced sort team n't see personal data – sort masked data – still needed write code would deal sensitive data aspect think ways ensure data leaked maybe need wait bit area gets mature 'll see data engineer outsourced work victoria ’ also seen company came ukraine ukraine used come office year something like complete access sign something alexey also needed come office right victoria yeah year week guess meetings	1
see data engineering outsourced like coding example tend see people sourced russia ukraine poland data engineering also affected similar moves 2022	victoria would say yes go-to dbt yes dbt enforces go read viewpoint 'll explain also models kind structure mentioned modern concepts concept comes 80s kimball defined kitchen analogy find useful normally go course complex gets may need steps 's okay always try separated – presentation source need typecasting like example maybe duplication middle may models go data pipeline let ’ say flow less would look like alexey kitchen analogy victoria kitchen analogy way model data similar restaurants model food warehouse raw food source data everyone allowed fact 's actually dangerous 's security measures actually go warehouse stored stored things like kitchen people cook raw food make food 're going serve allowed would data engineers data analysts maybe analytics engineers would data warehouse trying process raw data end part restaurant people eat – dining hall presentation layer data warehouse fact tables look like data marts 're building data marts business stakeholders everyone allowed get foot let ’ say restrictions 's already presented 're presenting raw alexey think tableau self-service tool go cubes presentation layer right victoria yeah technically bi tools self-serve could everything could expose everything right external tables create tables inaudible create section model create factories could everything could even ones development exposed self-service bi tool like tableau looker whatever analogy says use presentation layer parts present raw data people would n't know use 's going like “ oh n't find phones ones maybe 's transformation ” things like alexey nice analogy	0
see data engineering outsourced like coding example tend see people sourced russia ukraine poland data engineering also affected similar moves 2022	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
evaluate models find final one	's really case-dependent saw slack remember correctly carlos asked needs drop features – seemed okay drop highly correlated feature assistant name target variable churn basically people work one particular person tend turn less people work somebody else think case seems like relevant feature include n't know – 's really case-dependent usually need sort domain expertise decide feature introduces leakage carlos ’ case think 's safe leave drop see performance validation dataset changes gives idea important feature	0
evaluate models find final one	n't think takes lot time first need record videos prepare homework imagine adding collaboration startups well maybe sometime future n't know somebody startup listening right want hire people graduate zoomcamp please reach let 's arrange something	0
evaluate models find final one	think showed multiple times lessons use validation dataset compare different models validation dataset one best performance final model good rule thumb 's always end story remember talked cross-validation let 's say cross-validation see model good performance also high standard deviation 's great model probably want model lower standard deviation even performance slightly worse right might much information stick rule whatever best performance validation dataset wins later work develop intuition actually pick best model	1
evaluate models find final one	need practice n't think better way	0
evaluate models find final one	's interesting one let 's say numpy array image reference numbers – 1 6 – compute mean mean 3.5. add 3.5 multiple times mean n't change happens nan pandas simply ignores – n't look pretends 's basically pandas ignores nans looks numbers replace nans mean case 3.5 get data effectively mean n't change try show formula let 's say bit math let 's say four cities couple n/as basically part usual values part n/as missing part let 's say want compute mean .mean pandas happens let 's say n numbers like k missing values total series k+n – total length compute mean part get 1/nσxi=x̄ mean case 3.5. let 's replace n/as x̄ happens instead looking 1/n look 1/n+k total number elements already filled sum another sum x̄ k n summed k times x̄ becomes kx̄ formula like multiply n/n n/n simply 1. 's multiplying thing one get n x̄ part mean image reference clue 'm talking bear 's almost result 1/ n+k n x̄+kx̄ leave x equation 1/ n+k n+k x̄=x̄ basically shows add many means like result always x̄ 'm sure everyone enjoyed little proof image reference prove saw question meeting got curious like “ show mathematically ” spent like 10 minutes trying figure nice exercise brain 'm sure would able show proof spot honest	0
think scala important data engineers know language	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
think scala important data engineers know language	ankush append parquet file sejal think probably meant – think alexey also talking – incremental loads since end result week 2 bigquery external table think default 's append-only mode right alexey alexey executing saw… executed bunch dags runs went bigquery see results table created first dag run basically n't append table reason n't know think saw week 3 videos way create statement create table needs use kind sort wildcard say “ 's 2019 minus 10 ” bigquery picks files pattern probably typically n't know – 'm bigquery expert least week 2 need upload data google cloud storage need create bigquery tables create bigquery table week 3 ankush show – saw video sql statement creating table ankush bigquery yes need use wildcards pick multiple files 's aim would still append parquet file parquet files meant purpose 're append-only kinds file 're like columnar storage basically append need rewrite whole thing case would suggest basically keep open-ended close file 're done writing probably let bigquery read side writing bigquery table internally think 's also covered later videos bigquery internally figures couple writes let 's say hour figures imbalance internal file structure reclusters automatically reshuffles data creates new file n't really worry cost side 's internally bigquery question alexey maybe add words way see usually happens folder data lake specific date let 's say first structure name whatever table year month day within folder bunch parquet files want add information add another parquet file folder way get data partition	0
think scala important data engineers know language	alexey ankush like scala right chuckles ankush think scala definitely one best languages data engineers chuckles biggest part 's typesafe 's heavy verbose java scala perfect combination least coming java background love scala would say yes 's kind important language know data engineer especially spark written scala sometimes see errors spark- java-native might get confused knowledge like scala personally use developing pipelines also use write code spark sometimes also beam actually need really know python well enough proficient python get away minimum knowledge java scala would say unfortunately would say like scala personally like definitely go ahead learn 's still good language alexey think get performance benefits python extra layer abstraction top spark adds bit overhead cases want control things right ankush personal experience whenever written batch jobs – 'm talking real-time 'm talking batch jobs n't matter much time takes – always half hour 10-15 minutes minimum adding another 30 seconds one minute python performance n't really matter real-time streaming yes would huge loss n't think 's case pyspark real-time batch n't really matter alexey think serialization better use scala use “ case classes ” whatever called easily turn rdd case class think uses efficient serialization mechanism python sometimes difference minutes remember case difference like 20 minutes took pipeline rewrote scala difference big ankush would say spark 1.0-something data frames 's also really useful anymore alexey data frames yeah 1.2. remember used python every step pipeline found one job could optimize scala whole pipeline lot faster something needed scala year using spark chuckles think important – know java scala data engineer ankush good python would use already know java ’ need scala n't know either learn scala scala would much easier learn application especially spark alexey argue n't lot time chuckles cross-talk ankush preferences biased really like scala answers neutral definitely biased towards scala alexey victoria n't want add anything victoria n't know n't used scala several years reason people offer jobs java development linkedin chuckles alexey use victoria used university prove get title would use	1
think scala important data engineers know language	alexey 's early talk future iterations data engineering zoomcamp think chuckles ml camp yes – videos difference homework 's still discussion n't know 'll see ankush yeah think slight discussion met sure going repeat ml de zoomcamp let 's see response participation feel definitely success definitely feel free pass around course give us feedback maybe make better decision alexey re-recording videos… 's much effort n't think 'll want let 's say ml zoomcamp example much effort 'd rather reuse videos instead making new ones hope videos good decide rerun need re-record yeah 'll see	0
think scala important data engineers know language	alexey well good job keep working almost finished first week finish start week two finish week three chuckles follow sequence videos playlist github sufficient think questions go slack ankush keep going alexey chuckles yes exactly ’ almost finished first week good sign first week year pretty tough many people lot problems docker whatnot 're almost finished good sign ’ good track keep working	0
scenario data engineering jobs terms backend development knowledge much backend development knowledge data engineers need	ankush would really depend upon kind job data engineer 're analytical data engineering job mlops machine learning data engineering job backend development might interesting topic knowledge backend development might really help 're data engineering terms data products maybe lot real-time streaming working lot microservices becomes really important really interesting use combine knowledge backend development maybe use nosql technology build data product use kafka stream streaming data product cases backend developer knowledge becomes really helpful really excel job background hand would also say backend development knowledge learning terms sql connecting sources microservices help future anyway 's future going towards software development group think 's saying “ knowledge lost. ” never case use backend development knowledge would use terms building data product alexey maybe ask “ actually backend development knowledge ” talking general software engineering principles talking use specific microservice framework maybe need microservice framework data engineering maybe – knows never know general software engineering principles practices apply engineering job useful sure think skills backend engineers translate data engineers machine learning engineers – sort engineers	1
scenario data engineering jobs terms backend development knowledge much backend development knowledge data engineers need	ankush yes absolutely aws redshift would quite similar bigquery – quite similar still similar think aws redshift postgres cluster – big postgres cluster s3 definitely one one comparison google cloud storage 'm sure want use ec2 want deploy spark jobs maybe consider emr 're using ec2 maybe pipelining airflow ec2 also pretty good 's machine alexey used virtual machines many things would equivalent indeed need remote computer running data center s3 buckets fast download something s3 upload something s3 's usually way faster ec2 another similar concept athena think 's thing top data lake gives “ data warehouse feeling ” ankush 's sql top data lake alexey would say 's something like ... feels like something bigquery redshift 's fast 's also cheaper redshift ankush yeah think 're maybe comparing presto might better fit 's presto basically internally outside 's presto cluster alexey yeah presto need google things – like need google syntax athena – usually add presto end works ankush chuckles yeah exactly aws also managed workflow apache airflow 're thinking using airflow also check deploy less stuff alexey think google cloud also aws create emr ankush mentioned already ankush emr aws something like google cloud dataflow alexey create spark cluster one click think shared document explains configure cluster maybe 'll share office hours want use	0
scenario data engineering jobs terms backend development knowledge much backend development knowledge data engineers need	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	0
scenario data engineering jobs terms backend development knowledge much backend development knowledge data engineers need	alexey 's early talk future iterations data engineering zoomcamp think chuckles ml camp yes – videos difference homework 's still discussion n't know 'll see ankush yeah think slight discussion met sure going repeat ml de zoomcamp let 's see response participation feel definitely success definitely feel free pass around course give us feedback maybe make better decision alexey re-recording videos… 's much effort n't think 'll want let 's say ml zoomcamp example much effort 'd rather reuse videos instead making new ones hope videos good decide rerun need re-record yeah 'll see	0
scenario data engineering jobs terms backend development knowledge much backend development knowledge data engineers need	victoria set warehouse end define connection difference would use adapter dbt several adapters would first want use dbt go check adapters make sure 's supported several official example redshift could use community-supported azure alexey yeah aws athena azure athena redshift presto well example presto cluster use aws – within aws presto cluster potentially similar setup connect dbt presto victoria general add little bit answer let 's say project right bigquery would change profile 's set connection may make changes 's something sql supports bigquery way around able run project minutes	0
've learned many amazing things data engineering zoomcamp may ask areas topics left scope also important	alexey well documenting project make sure write everything show run thing creating instructions etc maybe one example could let 's say de zoomcamp – take one modules n't think code good let 's go docker example documentation actually shows need run thing something like maybe little bit descriptive first describe project run things – things needed accounts need create commands need run somebody 's looking n't see blank readme.md file information follow along information sequence execute things works mean reproducibility one tool like make even easier make create bunch make files cover course 're pretty useful maybe show example instead typing long commands “ make ” executes 's cool thing minimum documentation describes need code snippets – mean reproducibility requirements.txt file also important ideally pin versions write python_kafka==version also good ankush would also say using cloud-based assume person 's reviewing would also cloud google cloud platform would available person use terraform 're creating buckets use terraform maybe readme say “ first step would create bucket create bucket using terraform file 's run it. ” person would need google cloud authentication way google cloud authentication also highlight documentation “ first step would google cloud authentication ” maybe use criteria cloud provider proceed alexey yeah please n't forget steps like authentication assume authenticated run might error	0
've learned many amazing things data engineering zoomcamp may ask areas topics left scope also important	alexey 's early talk future iterations data engineering zoomcamp think chuckles ml camp yes – videos difference homework 's still discussion n't know 'll see ankush yeah think slight discussion met sure going repeat ml de zoomcamp let 's see response participation feel definitely success definitely feel free pass around course give us feedback maybe make better decision alexey re-recording videos… 's much effort n't think 'll want let 's say ml zoomcamp example much effort 'd rather reuse videos instead making new ones hope videos good decide rerun need re-record yeah 'll see	0
've learned many amazing things data engineering zoomcamp may ask areas topics left scope also important	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
've learned many amazing things data engineering zoomcamp may ask areas topics left scope also important	alexey ideas sejal sejal yeah think updated ... maybe move section main readme page know part additional things could setup writing unit tests ci/cd framework forth think 's “ project ” section maybe move parent branch things cover bit would say make complete packaged environment including test cases automatic deployment pipelines alexey yeah think 's good idea move project like “ next steps ” right sejal exactly something based question see many things automated might cases run scheduled pipelines airflow whichever tool 're using order generate frequent reports maybe weekly reports results need daily basis let 's say forecasting pipeline 're estimating stock prices week maybe scheduled cron job based 's something dashboard reflect predictions real-time stock market prices also weekly basis daily basis also app create idea alexey 'm thinking things n't cover example n't cover serverless quite interesting concept also stream processing let 's say stream lambda function whatever n't remember called google function use consuming stream something putting another stream build quite complex pipelines streams without writing consumer code write lambda function applied stream something internally handles everything need handle consume quite cool concept – serverless another thing n't cover could useful data engineers kubernetes similar thing example using aws batch amazon using kubernetes jobs something like also could quite useful sejal yeah +1 kubernetes think companies using preferring use kubernetes choice cloud cluster instead aws settled hosted solutions kubernetes definitely plus definitely recommend learning alexey another thing would like recommend learning something n't cover serverless thing mentioned – kind covered little bit streaming week kubernetes kind covered little bit batch week least mentioned use kubernetes job batch jobs serverless streaming n't cover concept data monitoring data quality checks things like nice tools example tomorrow datatalks.club webinar workshop data monitoring whylogs whylogs tool data monitoring nice tools example maybe 've heard great expectations soda sql – quite something n't cover well maybe little bit dbt week covered testing bit think deserves attention something many companies looking definitely worth checking victoria maybe also ideas victoria definitely data quality agree one dbt lot bunch packages well course implementation observability 'm sure mentioned guess well alexey mentions monitoring observability also good tools alexey something also n't cover specifically mentioned times tools like fivetran think 're gaining popularity airbyte open source alternative fivetran also worth checking see many many many things n't make course look quite useful might think “ okay need learn five six topics. ” maybe n't try learn much possible goal let 's say work data engineer already start applying data engineering positions see ask – kind things need n't know yet thousands things potentially learn 's difficult select next one maybe base decision whatever hear potential employers	1
've learned many amazing things data engineering zoomcamp may ask areas topics left scope also important	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
please upload live session recording homework answers github	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
please upload live session recording homework answers github	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
please upload live session recording homework answers github	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
please upload live session recording homework answers github	yes course 's 'll today	1
please upload live session recording homework answers github	multi-classification – yes clustering – 'm sure probably good idea n't cover hard – know evaluate peers know better clustering	0
clarify difference using gunicorn waitress plain flask understand related dev stuff 'm sure	building locally fine	0
clarify difference using gunicorn waitress plain flask understand related dev stuff 'm sure	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
clarify difference using gunicorn waitress plain flask understand related dev stuff 'm sure	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
clarify difference using gunicorn waitress plain flask understand related dev stuff 'm sure	idea flask comes web service service simple send multiple requests n't think able handle multiple requests time able multi-processing set multiple processes handle multiple requests things like 's limited sense use testing run things machine want deploy something production environment need use gunicorn optimized real production traffic multiple requests coming time – 50 requests per second 5000 requests per second flask able handle understanding honest n't really look much	1
clarify difference using gunicorn waitress plain flask understand related dev stuff 'm sure	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
important certificates unable complete course course due work pressure pace	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
important certificates unable complete course course due work pressure pace	alexey think 's helpful 's necessary helpful happens dags run airflow dags tasks tasks often kubernetes jobs something like infra – many companies infrastructure managed kubernetes 's quite simple get new container execute new job kubernetes n't think 's necessary think 's helpful 'm data engineer – 'm data scientist data scientist knowledge kubernetes quite helpful could debug jobs 'm running airflow run kubernetes could see logs would say want learn hurt get hired without kubernetes knowledge materials course sufficient get hired pick kubernetes work example agree victoria victoria yeah think kubernetes getting popular ca n't hurt	0
important certificates unable complete course course due work pressure pace	alexey important hire people never asked anyone show certificates n't care usually interview see person 're interviewing knows something know something n't care certificates would n't say 's important maybe 's also nice-to-have thing gives sort achievement n't think stress much take course pace important thing learning whether get certificate end ankush would like add whatever alexey said absolutely correct 's learning 's certificate case fresher starting first job n't know would helpful might helpful think like get certificate put linkedin people review profile actually see alexey also serve advertisement course right see “ okay data engineering zoomcamp ” click thing find amazing course – know course amazing immediately trust learned something maybe hire ankush direction going yeah – chuckles	1
important certificates unable complete course course due work pressure pace	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
important certificates unable complete course course due work pressure pace	alexey purpose method let 's say this… well 'm going say rdd n't released videos rdd 'll try explain briefly rdd distributed dataset let 's say usual python list usual python – list distributed across many machines data frames data frames also sort distributed collection consists many partitions operations data frame rdd invoke .collect get everything data frame rdd contains memory object kind move think distributed collection turn collection python driver node maybe long explanation maybe shorter one ankush ankush want count elements need put one node count 's use .collect remember use .collect large datasets also run memory careful using aware 're 's really helpful want collect write one file helpful careful using alexey transformation talking actions transformations operations spark transformations actions example actions talked “ show ” talked “ take ” talked “ head ” also talked “ write ” “ collect ” also action triggers entire execution instead saving somewhere get object driver node	0
could please explain code load github	alexey 's want look project 're behind right n't worry take time decide whether give certificate based projects homework 's – catch week 3 lighter week 1 maybe fast-forward directly docker compose run week 2 depending much time maybe go solution published today give everything need week 3 use transfer service video already watching video probably learn lot already like homework solution	0
could please explain code load github	ankush yes absolutely aws redshift would quite similar bigquery – quite similar still similar think aws redshift postgres cluster – big postgres cluster s3 definitely one one comparison google cloud storage 'm sure want use ec2 want deploy spark jobs maybe consider emr 're using ec2 maybe pipelining airflow ec2 also pretty good 's machine alexey used virtual machines many things would equivalent indeed need remote computer running data center s3 buckets fast download something s3 upload something s3 's usually way faster ec2 another similar concept athena think 's thing top data lake gives “ data warehouse feeling ” ankush 's sql top data lake alexey would say 's something like ... feels like something bigquery redshift 's fast 's also cheaper redshift ankush yeah think 're maybe comparing presto might better fit 's presto basically internally outside 's presto cluster alexey yeah presto need google things – like need google syntax athena – usually add presto end works ankush chuckles yeah exactly aws also managed workflow apache airflow 're thinking using airflow also check deploy less stuff alexey think google cloud also aws create emr ankush mentioned already ankush emr aws something like google cloud dataflow alexey create spark cluster one click think shared document explains configure cluster maybe 'll share office hours want use	0
could please explain code load github	sejal maybe someone help answer really expert kubernetes side terraform lets build static code templates infrastructure iec-style manner would work cases want static-based infrastructure cases set infrastructure resources destroyed point time want restore certain image certain state – terraform useful case kubernetes 's actually like spin-up cluster deploy services onto kubernetes cluster use cases really different case maybe ankush alexey add kubernetes ankush yeah definitely think kubernetes terraform different think kubernetes thinking deploying microservices sort applications done terraform using aws fargate aws ecs solutions terraform much let ’ assume kubernetes use terraform set kubernetes cluster also use terraform set let 's say s3 buckets next week ’ course see use terraform set transfer service google cloud platform possible kubernetes yes argue certain services like spark flink coming new solutions run kubernetes still example say want run kafka clusters cases kubernetes good solution would like stable solution stable infrastructure case 's terraform would definitely help set kinds clusters alexey person quite far infrastructure would add usually people dedicated company terraform helps go web ui click things instead going clicking file say “ bunch resources set cloud. ” let 's say need move one account another whatever reason – happened multiple times work needed migrate different account terraform destroy one account actually would destroy later terraform “ plan apply ” one account go back old account terraform destroy course still takes time everything code know exactly kind services use example kind buckets use aws us would kind lambda functions basically resources kubernetes could one resource terraform file comment says “ terraform infrastructure code kubernetes infrastructure. ” think 's quite concise way summarizing	0
could please explain code load github	alexey think question refers homework form asked submit code create repo github create folder homework 1 put sql files leave link file submit anything else gitlab kind person create report gitlab something publicly accessible put sql queries submit form	1
could please explain code load github	victoria n't used anaconda think 's also documentation install dbt locally general 're using anaconda 're using environment install use homebrew 're using mac also use pip install pip install dbt postgres example need adapter dbt snowflake dbt big query – whatever want use locally 's 're installed set profiles.yml show videos create project clone whatever 's 's need use locally never install using anaconda 'm guessing 's probably something similar probably conda forge install like 's possible otherwise would go pip install alexey anaconda usually pip victoria yeah 're environment install like 's	0
need deploy model week five country n't support aws payment	yes said provide matrix multiple dimensions like parameter tuning data prep etc dimensions criteria saying example “ 0 parameter tuning whatsoever ” “ 1 model tuned ” “ 2 person tuned multiple models selected best one. ” parameter tuning tried multiple models selected best parameter get score 2. whoever reviewing three options form say “ parameter tuning ” option one option two option three relevant points attached click correct option get score	0
need deploy model week five country n't support aws payment	let 's say “ import numpy np ” let 's say “ np.random.exponential ” noticed press shift+tab press shift+tab shows docstring multiple times press shift+tab expands see funny thing n't even realize muscle memory 's n't saying loud know go shift+tab small thing open twice expand another thing put question mark instead parentheses “ np.random.exponential ” show docstring another thing think put “ help np.random.exponential ” print docstring	0
need deploy model week five country n't support aws payment	yes think writing article github good basically article would suggest trying medium 's difficult seems medium also start recommending article people see 's github fine want actually turn github repo website 's called github pages take look maybe instead putting article git repo 's possible turn git page blog page need follow instruction 's difficult ’ come engineering background n't done 're comfortable using command line take time actually figure think 's worth investing time seeing works easily website free – entirely free n't need anything else actually secret 's secret actually datatalks.club site actually page run github pages everything see website github use github pages host 's free need pay domain – datatalks.club – rest github provides free hope convinced try	0
need deploy model week five country n't support aws payment	n't n't use aws use cloud provider use heroku use pythonanywhere – actually give free access deploy one thing free experiments homework need deploy anything anywhere – locally last question deploy model locally score customer homework need however highly encourage actually try deploying model cloud n't aws try find something else example google cloud give couple hundred dollars credit spend platform give free play deploy models highly encourage	1
need deploy model week five country n't support aws payment	two people want work dataset individually please feel free imagine way control go kaggle select competition even ’ accidentally happen long 's exactly code – long n't copy – ’ fine yeah please n't copy code	0
advice could apply learned course 3-week project end course could suggest project ideas	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
advice could apply learned course 3-week project end course could suggest project ideas	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
advice could apply learned course 3-week project end course could suggest project ideas	alexey take look thread datasets slack ’ pretty sure find something interesting go would start dataset first would find dataset try think exactly dataset ankush say finish one project basically using batch processing maybe next project real-time processing similarly 're less airflow airflow something like try explore different areas broaden knowledge sejal nothing else add end really basically guys said think really depends needs situation based feel like learning think important learn order implement respect 're currently working think 'll figure addition 're learning course maybe could also try learning standard software engineering skills applied data engineering well orchestrating ci/cd pipelines workflows gitops automated test cases especially unit tests maybe pytest use pytest combination airflow make code production-friendly basically n't really covered 'm sure covering modules next iterations find time definitely add bonus lectures also something try	1
advice could apply learned course 3-week project end course could suggest project ideas	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
advice could apply learned course 3-week project end course could suggest project ideas	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
deadline week 2 7th february 2022	alexey let see 31st january right think released homework friday deadline 4th february 'm sure 7th comes 's next monday think beginning talked making monday maybe something 'll talk internally see 're right deadline friday stick deadline promise friday	1
deadline week 2 7th february 2022	alexey n't think really understand question ankush 're vpc matter alexey cluster – spark cluster cluster kafka cluster ankush 're running vpc able access 's ip right alexey yeah oh okay externally ip guess 're running inside network use internal ip 's inside kubernetes refer names deployments n't know ankush also n't know 's really specific particular use case need give us maybe slack explain bit – running kind machines exactly exposing terms external ip alexey usually kafka cluster kafka machines spark cluster spark cluster connects kafka reads data ankush depends vpc running 're running different vpcs need expose separately need expose ip running vpc matter able access internally might wrong 'm devops chuckles alexey usually take things granted maybe good thing work grateful data team makes tools work often n't ask “ okay actually configured ” works n't – n't go support channel ask “ hey please fix ” ankush think 's also data team 's also maybe devops team embedded inside data team 're working olx pretty big company maybe many smaller teams inside big teams guess 's devops topic	0
deadline week 2 7th february 2022	ankush n't know think aws possible without restrictions alexey probably public open bucket like taxi trips theoretically thing would private s3 buckets ankush yeah think problem would open source data google cloud storage definitely need move s3 bucket	0
deadline week 2 7th february 2022	alexey 's want look project 're behind right n't worry take time decide whether give certificate based projects homework 's – catch week 3 lighter week 1 maybe fast-forward directly docker compose run week 2 depending much time maybe go solution published today give everything need week 3 use transfer service video already watching video probably learn lot already like homework solution	0
deadline week 2 7th february 2022	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
base.model shape 32x5x5x2048 average pooling need know 5x5 specify something somewhere	classification classification let 's say n't predicting price let 's say predicting 's average number hours image 1 case would need another layer – rather layer 's called activation n't activation 're still… ’ trying formulate way 's confusing think 's better go lecture rewatch ’ essentially retelling thing need from_logits price variable loss mean squared error logits loss categorical cross-entropy categorical cross entropy usually used multiple categories need use logits 's numerically stable n't use – stay activation softmax n't use legit hope 's clear regression n't need use homework need use from_logits either homework two classes 's binary classification problem multiple classes – let 's say three classes four classes five classes – need use logits loss categorical cross-entropy loss categorical cross-entropy 's recommended use from_logits=true 's numerically stable	0
base.model shape 32x5x5x2048 average pooling need know 5x5 specify something somewhere	alexey question lecture use exception output exception n't include top 5x5x2048 long long long tube – three-dimensional thing dmitry know 's 5x5 think convolutional layers 1x1 – quite end 2000-something filters result applying filters still lot feature maps 'm completely sure know 's going dmitry happens exception last layer dmitry think 's filters alexey yeah bunch of… dmitry convolutional nature alexey yeah something specified coming pretrained network	1
base.model shape 32x5x5x2048 average pooling need know 5x5 specify something somewhere	one thing immediately comes mind click prediction let 's say go website banner happens behind banner model predicts “ probability particular user comes webpage meaning click banner ” usually sort linear logistic regression hood pretty fast advertisements 're used quite lot especially talk real-time advertisements enter website immediately get ad linear models logistic regression used	0
base.model shape 32x5x5x2048 average pooling need know 5x5 specify something somewhere	yes said provide matrix multiple dimensions like parameter tuning data prep etc dimensions criteria saying example “ 0 parameter tuning whatsoever ” “ 1 model tuned ” “ 2 person tuned multiple models selected best one. ” parameter tuning tried multiple models selected best parameter get score 2. whoever reviewing three options form say “ parameter tuning ” option one option two option three relevant points attached click correct option get score	0
base.model shape 32x5x5x2048 average pooling need know 5x5 specify something somewhere	yes good references – course course called machine learning zoomcamp cover parts except conda git assume midterm project use git conda – use conda python interpreter install pipenv manage dependencies n't know good references honest maybe n't know answered question hope references course good enough ’ please let know think improved think missing	0
data n't require transformation 's data warehouse still need use dbt	alexey 'm sure last part –how docker provided terraform ankush run stuff terraform think always ... set airflow cluster terraform n't think execute job alexey 's something like use terraform prepare environment prepare thing run need actually run something least two separate commands first “ terraform apply ” “ python run ” something like 50:02 1 upvotes allowed contact instructors directly discuss projects things unsure regarding project 'm using new tools alexey think 's best ask things slack channel others also interested learning answer scales better instead everyone writing us directly – answer everyone sees answer b others answer available right somebody knows tool maybe n't example pulsar one students might know able help 's best use channel contact us directly	0
data n't require transformation 's data warehouse still need use dbt	victoria n't used anaconda think 's also documentation install dbt locally general 're using anaconda 're using environment install use homebrew 're using mac also use pip install pip install dbt postgres example need adapter dbt snowflake dbt big query – whatever want use locally 's 're installed set profiles.yml show videos create project clone whatever 's 's need use locally never install using anaconda 'm guessing 's probably something similar probably conda forge install like 's possible otherwise would go pip install alexey anaconda usually pip victoria yeah 're environment install like 's	0
data n't require transformation 's data warehouse still need use dbt	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
data n't require transformation 's data warehouse still need use dbt	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
data n't require transformation 's data warehouse still need use dbt	alexey well guess want dashboard aggregates hard time trying imagine dataset take raw form put dashboard maybe 's already grouped 's already aggregated 's probably small dataset 's suitable course think maybe select reasonably big dataset require transformation 're doubt ask slack really n't know kind dataset put dashboard without transformations ankush yeah minimum transformations would required alexey counts group-bys maybe joins ankush exactly think 's better choose dataset allow	1
go choosing regression models corresponding parameters feed arguments c alpha etc. learn	flask – let draw let 's say want create web service image 5 web service gets requests web service responds something could predictions could something else – n't related machine learning implement web service use flask use fastapi use framework creating web services use flask use something else – flask allows use post example let 's say “ get ” usually parse parameters query let 's say go google put “ test ” see q=test https //www.google.com/search q=test oq=test aqs=chrome.0.69i59l2j69i65l3j69i60j69i61l2.1264j0j1 sourceid=chrome ie=utf-8 image 6 parameter pass “ get ” request “ get ” request sometimes let 's say want score customer n't want put gender=female contract=one year string send json post post also add body request generally use post general use case flask able create web service	0
go choosing regression models corresponding parameters feed arguments c alpha etc. learn	n't think takes lot time first need record videos prepare homework imagine adding collaboration startups well maybe sometime future n't know somebody startup listening right want hire people graduate zoomcamp please reach let 's arrange something	0
go choosing regression models corresponding parameters feed arguments c alpha etc. learn	need validation set use validation set exactly use validation set try different ways like let 's say want try different c 's want try different alphas – use validation set want get rid features try see score improves – use validation set also week talk evaluating models binary classification models particular talk cross validation k fold cross-validation also useful general use validation set	1
go choosing regression models corresponding parameters feed arguments c alpha etc. learn	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
go choosing regression models corresponding parameters feed arguments c alpha etc. learn	's interesting one let 's say numpy array image reference numbers – 1 6 – compute mean mean 3.5. add 3.5 multiple times mean n't change happens nan pandas simply ignores – n't look pretends 's basically pandas ignores nans looks numbers replace nans mean case 3.5 get data effectively mean n't change try show formula let 's say bit math let 's say four cities couple n/as basically part usual values part n/as missing part let 's say want compute mean .mean pandas happens let 's say n numbers like k missing values total series k+n – total length compute mean part get 1/nσxi=x̄ mean case 3.5. let 's replace n/as x̄ happens instead looking 1/n look 1/n+k total number elements already filled sum another sum x̄ k n summed k times x̄ becomes kx̄ formula like multiply n/n n/n simply 1. 's multiplying thing one get n x̄ part mean image reference clue 'm talking bear 's almost result 1/ n+k n x̄+kx̄ leave x equation 1/ n+k n+k x̄=x̄ basically shows add many means like result always x̄ 'm sure everyone enjoyed little proof image reference prove saw question meeting got curious like “ show mathematically ” spent like 10 minutes trying figure nice exercise brain 'm sure would able show proof spot honest	0
use pca discriminate important less important features	n't really cover pca n't know explain pca words dmitry maybe know explain words think lost dmitry okay 'll try basically feature – pca finds way create new features existing ones usually use pca features get less easy interpret anyways n't want go much detail pca look using pca feature importance ask questions slack	1
use pca discriminate important less important features	far week two week three follow simple approach simple approach described take entire dataset split three parts train validation test 'll use week two week three week four talk cross validation – explain right want say approach validation sufficient many cases think 's pretty safe say many many many applications split enough cross validation course nicer learn week four right go detail try explain wait video “ select approach cross validation ” would say multiple options k-fold… 'm sure many people understand n't covered yet basically dataset big n't need k-fold dataset smaller use k-fold “ leave p ” – 've never used personally	0
use pca discriminate important less important features	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
use pca discriminate important less important features	really depends cloud showed elastic beanstalk n't think needed change much anything n't remember changing anything thing changed url service test.py script thing needed change think heroku 's similar actually couple tutorials using pythonanywhere heroku probably explain need change go see need change anything probably much	0
use pca discriminate important less important features	yes good references – course course called machine learning zoomcamp cover parts except conda git assume midterm project use git conda – use conda python interpreter install pipenv manage dependencies n't know good references honest maybe n't know answered question hope references course good enough ’ please let know think improved think missing	0
wondering spark could also handle streaming data like kafka see spark also spark streaming could also use substitute kafka	victoria n't used anaconda think 's also documentation install dbt locally general 're using anaconda 're using environment install use homebrew 're using mac also use pip install pip install dbt postgres example need adapter dbt snowflake dbt big query – whatever want use locally 's 're installed set profiles.yml show videos create project clone whatever 's 's need use locally never install using anaconda 'm guessing 's probably something similar probably conda forge install like 's possible otherwise would go pip install alexey anaconda usually pip victoria yeah 're environment install like 's	0
wondering spark could also handle streaming data like kafka see spark also spark streaming could also use substitute kafka	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
wondering spark could also handle streaming data like kafka see spark also spark streaming could also use substitute kafka	ankush avro parquet totally different things avro row-based parquet columnar something like batch processing data-analytical kind work parquet perfect solution use spark something 's super fast calculate something like sum account parquet file read particular column rather whole column whole row avro hand strong providing flexibility terms backward forward compatibility 're using avro kafka using something like protobuf kafka would really useful think like different technologies altogether use different use cases alexey avro guess use case would streaming parquet batch jobs ankush yeah think definitely beginning would basic use cases yes alexey also something called ocr ankush 's also columnar storage alexey oh 's different orc chuckles remember watching presentation berlin buzzwords format remember seeing cool presentation never actually tried ankush neither i. alexey 's optimized row columnar interesting never knew stood widely used people go parquet ankush seen parquet guess companies definitely using 's bad technology think 's parquet famous compatible solutions parquet people tend use often	0
wondering spark could also handle streaming data like kafka see spark also spark streaming could also use substitute kafka	alexey n't think really understand question ankush 're vpc matter alexey cluster – spark cluster cluster kafka cluster ankush 're running vpc able access 's ip right alexey yeah oh okay externally ip guess 're running inside network use internal ip 's inside kubernetes refer names deployments n't know ankush also n't know 's really specific particular use case need give us maybe slack explain bit – running kind machines exactly exposing terms external ip alexey usually kafka cluster kafka machines spark cluster spark cluster connects kafka reads data ankush depends vpc running 're running different vpcs need expose separately need expose ip running vpc matter able access internally might wrong 'm devops chuckles alexey usually take things granted maybe good thing work grateful data team makes tools work often n't ask “ okay actually configured ” works n't – n't go support channel ask “ hey please fix ” ankush think 's also data team 's also maybe devops team embedded inside data team 're working olx pretty big company maybe many smaller teams inside big teams guess 's devops topic	0
wondering spark could also handle streaming data like kafka see spark also spark streaming could also use substitute kafka	ankush use substitute kafka streams yes 's mean yes definitely use spark streaming consume data kafka transformations joins windowing whatever want put another kafka topic file system whatever want course need kafka cluster need broker kafka actually publish messages kafka n't use kafka streaming thing think lot people realized python library us limited 're python person really want use streaming spark streaming might option alexey know spark connect kinesis aws ankush yes spark connect kinesis also alexey multiple options right different brokers call tools streaming engine sort ankush brokers yeah alexey brokers kafka kinesis pulsar – others ankush basically connect google cloud storage connect bigquery connect anything want anything want whatever connectors available lots connectors available spark basically easily alexey okay spark streaming thing right ankush yeah 's biggest limitation kafka streams well kafka streams kafka kafka want something else need either export data kafka use something else like spark streaming alexey flink pretty much thing connect kafka pulsar kinesis right ankush exactly	1
way collate frequently asked questions knowledge repository 's easier self-help troubleshooting issues	alexey 's want look project 're behind right n't worry take time decide whether give certificate based projects homework 's – catch week 3 lighter week 1 maybe fast-forward directly docker compose run week 2 depending much time maybe go solution published today give everything need week 3 use transfer service video already watching video probably learn lot already like homework solution	0
way collate frequently asked questions knowledge repository 's easier self-help troubleshooting issues	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
way collate frequently asked questions knowledge repository 's easier self-help troubleshooting issues	alexey ask help 's difficult us prepare materials also create curated list frequently asked questions help us – 're already slack answering questions – help us 'll much appreciate sure 's physically possible us also invest time right least 'm sure 'll able please want happy support create pull requests example repo frequently asked questions section example readme – place many errors maybe create pull request saying “ error like please that. ” helpful many people slack think 's bit overwhelming right amount questions many repetitive kind wish list frequently asked questions well could easier direct 's putting together requires attention sejal another thing actually thinking maybe create github readme page faqs people create pull requests add questions cases pull requests repeated questions mark stale already go look end alexey said since big time crunch resource crunch – swamped preparing material well managing full-time jobs 're trying much someone volunteer prepare maybe notion doc example organize questions asked slack channel put onto notion doc google doc whatever like helpful us	1
way collate frequently asked questions knowledge repository 's easier self-help troubleshooting issues	alexey add comment least workplace airflow instance set us data scientist would go instance airflow managed data engineers devops engineers would use never needed actually run docker compose moment multiple dags orchestration tool quite useful would personally maybe go luigi favorite one need think people well since people know luigi many like maybe airflow would actually better choice consider sejal would also like add question docker compose used docker compose airflow style use official setup seeing first time well 's unnecessarily complicated especially xcom variables forth generally docker compose acts wrapper docker containers production workflows docker containers used integration ci/cd pipelines wherever 're going deploy docker compose docker compose part helpful development environments case let 's say mock version airflow environment production want test dags local development environment use docker compose make things easier running simple commands like ‘ docker compose ’ would make fine instead running ‘ docker run container name etc whatever want ’ terms convenience terms usage ’ generally used development environments production alexey think first part question “ close usual data engineer role workflows ” workflows see work complex steps probably also case need start something already – would n't call complex pipeline – three four steps already something eventually might grow bigger bigger add steps ankush would also like add 're cloud maybe use airflow deploy docker maybe actually use service provided cloud provider one similar airflow think airflow would used internally like cloud composite google cloud platform would using terraform sejal yeah used aws step functions aws setup step functions combination lambda place airflow ankush said also something wanted say really depends production self-hosted cluster kubernetes wherever want use pure airflow version company could kind setup use native airflow cluster could cloud-based setup convenient option use managed services cloud composer aws also version called managed airflow	0
way collate frequently asked questions knowledge repository 's easier self-help troubleshooting issues	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
good resource website learn logistics implementing pipeline someone good server use cost etc	alexey well good job keep working almost finished first week finish start week two finish week three chuckles follow sequence videos playlist github sufficient think questions go slack ankush keep going alexey chuckles yes exactly ’ almost finished first week good sign first week year pretty tough many people lot problems docker whatnot 're almost finished good sign ’ good track keep working	0
good resource website learn logistics implementing pipeline someone good server use cost etc	alexey ankush like scala right chuckles ankush think scala definitely one best languages data engineers chuckles biggest part 's typesafe 's heavy verbose java scala perfect combination least coming java background love scala would say yes 's kind important language know data engineer especially spark written scala sometimes see errors spark- java-native might get confused knowledge like scala personally use developing pipelines also use write code spark sometimes also beam actually need really know python well enough proficient python get away minimum knowledge java scala would say unfortunately would say like scala personally like definitely go ahead learn 's still good language alexey think get performance benefits python extra layer abstraction top spark adds bit overhead cases want control things right ankush personal experience whenever written batch jobs – 'm talking real-time 'm talking batch jobs n't matter much time takes – always half hour 10-15 minutes minimum adding another 30 seconds one minute python performance n't really matter real-time streaming yes would huge loss n't think 's case pyspark real-time batch n't really matter alexey think serialization better use scala use “ case classes ” whatever called easily turn rdd case class think uses efficient serialization mechanism python sometimes difference minutes remember case difference like 20 minutes took pipeline rewrote scala difference big ankush would say spark 1.0-something data frames 's also really useful anymore alexey data frames yeah 1.2. remember used python every step pipeline found one job could optimize scala whole pipeline lot faster something needed scala year using spark chuckles think important – know java scala data engineer ankush good python would use already know java ’ need scala n't know either learn scala scala would much easier learn application especially spark alexey argue n't lot time chuckles cross-talk ankush preferences biased really like scala answers neutral definitely biased towards scala alexey victoria n't want add anything victoria n't know n't used scala several years reason people offer jobs java development linkedin chuckles alexey use victoria used university prove get title would use	0
good resource website learn logistics implementing pipeline someone good server use cost etc	sejal pipeline someone… “ someone ” alexey maybe 're data engineer freelancer somebody comes pile money says “ hey implement pipeline me. ” would approach sejal would start gathering requirements based automated solution want make much want invest really simple straightforward task 'm giving example data ingestion – custom pipelines built airflow could use something like fivetran automated etl solutions provide input output integrate targets snowflake minimal standardized level etl job providing schema playing around sql custom ingestion pipelines may need use airflow example add additional steps preprocessing really depends use case would say alexey ankush ankush would let ’ say pipeline mind already know kind steps google provides nice calculator called google cloud pricing calculator basically put resources want put kind compute engines want dataflow also gcp bigquery example cloud storage bigquery data prog data flow know kind tools want use know basically on-demand flat rate good estimation price obviously always overprice overestimate maybe cut resources based also help decide – maybe n't want deploy kafka cluster want use pub/sub pub/sub much cheaper use case cases basically kinds estimations fly gets tricky obviously estimate everything make signs sort comparison think similar ones also offered aws alexey ’ maybe convenient one think aws least knowledge n't everything one place example want calculate pricing lambda separate page lambda maybe something convenient one 'm aware ankush found googling 'm gcp netapp aws calculator alexey like third-party thing right ankush yeah think 's third-party thing 'm 100 sure aws offer also kind estimate costs thing google cloud google cloud one way better sejal may also want think about… 's always good option begin 're building someone begin cloud-based solution managed services cases let 's say security reasons want cluster manage stuff already devops team future also consider kubernetes cluster deploy self-hosted solution open source stack lot trade-offs need make order satisfy need ankush aws calculator calculator.aws alexey wanted share link scling one datatalks.club podcast guests podcast dataops 101 episode collected quite lot resources related data engineering like batch processing stream processing maybe something probably find something useful ankush last link saw transitioning scala see live without scala alexey need learn alexey pleasure use scala long chuckles ankush laughs long okay alexey like three years something like ankush also see 's also moving team scala golang alexey think article read star ship operator come across thing code maybe try google get “ matching documents ” chuckles funny article figured starship operator functional programming library scala z. think bunch right cats yeah 's funny article think scala set standards make sure people get carried away sejal yeah rule – learn python unless 're joining company like spotify requires scala chuckles ankush true even start scala n't start functional libraries stuff try keep simple beginning	1
good resource website learn logistics implementing pipeline someone good server use cost etc	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
good resource website learn logistics implementing pipeline someone good server use cost etc	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
general architecture shown last week could go bit details technologies learning – docker terraform first	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
general architecture shown last week could go bit details technologies learning – docker terraform first	victoria 'm sure answer mean yes definitely adding part project least workshop reason want people learn also consider people entirely new concept thought would much overhead try cover things already quite complex day-to-day even though work daily dbt work slow-changing dimensions daily basis example 's also think 's something look moment use definitely dbt especially deletions implementing load step done hooks think 's section advanced knowledge link hooks like incremental models mentioned changing dimensions n't link could link something could use snapshots think added concept updates guess could also pre-hook post-hook also something would incremental model incremental model loads new chunk data use merge depending workers use use could insert update 's something would work unique key project also final project 's understand complex go 's limit hope answers	0
general architecture shown last week could go bit details technologies learning – docker terraform first	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
general architecture shown last week could go bit details technologies learning – docker terraform first	alexey add comment least workplace airflow instance set us data scientist would go instance airflow managed data engineers devops engineers would use never needed actually run docker compose moment multiple dags orchestration tool quite useful would personally maybe go luigi favorite one need think people well since people know luigi many like maybe airflow would actually better choice consider sejal would also like add question docker compose used docker compose airflow style use official setup seeing first time well 's unnecessarily complicated especially xcom variables forth generally docker compose acts wrapper docker containers production workflows docker containers used integration ci/cd pipelines wherever 're going deploy docker compose docker compose part helpful development environments case let 's say mock version airflow environment production want test dags local development environment use docker compose make things easier running simple commands like ‘ docker compose ’ would make fine instead running ‘ docker run container name etc whatever want ’ terms convenience terms usage ’ generally used development environments production alexey think first part question “ close usual data engineer role workflows ” workflows see work complex steps probably also case need start something already – would n't call complex pipeline – three four steps already something eventually might grow bigger bigger add steps ankush would also like add 're cloud maybe use airflow deploy docker maybe actually use service provided cloud provider one similar airflow think airflow would used internally like cloud composite google cloud platform would using terraform sejal yeah used aws step functions aws setup step functions combination lambda place airflow ankush said also something wanted say really depends production self-hosted cluster kubernetes wherever want use pure airflow version company could kind setup use native airflow cluster could cloud-based setup convenient option use managed services cloud composer aws also version called managed airflow	0
general architecture shown last week could go bit details technologies learning – docker terraform first	alexey 'm sure question internals technologies sejal maybe answer understand correctly talking architecture github repository 'm sure represents docker terraform docker terraform tools used order create data engineering framework actual data engineering framework – architecture 'll see github repository – looking framework main technologies would starting week week 2 onwards airflow airflow orchestration also dbt transformations spark another kind batch processing transformation-related job kafka streaming core set technologies generally used data engineering also covering terms architecture ’ talking understood question correctly alexey 're saying foundational technology 's visible sejal agrees	1
anything allowed use example want use hyper-parameter tuning allowed use gridsearchcv randomizedsearchcv	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
anything allowed use example want use hyper-parameter tuning allowed use gridsearchcv randomizedsearchcv	n't think anything use want ask document something use part course somebody reviewing project new information give context explain “ used gridsearchcv easier do. ” describe give peers chance learn ’ way lost able grade homework use whatever want materials course like use logistic regression rich regression xgboost random forest n't need really go details documenting would expect people understand without much explanation maybe 's also good idea somebody let 's say wants use xgboost lightgbm catboost different implementations gradient boosting – 're free use project well use fastapi instead flask example use poetry instead pipenv use conda instead pipenv 're basically free explore use want play different tools sure document	1
anything allowed use example want use hyper-parameter tuning allowed use gridsearchcv randomizedsearchcv	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
anything allowed use example want use hyper-parameter tuning allowed use gridsearchcv randomizedsearchcv	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
anything allowed use example want use hyper-parameter tuning allowed use gridsearchcv randomizedsearchcv	alexey one thing maybe n't clear enough – please always include code solution maybe people n't watch first said n't submit code homework get zero points saw submissions people put dot empty put link towards data science article example please n't please put code n't 'll get zero points entire week 'll make explicit form week three well please n't forget put code	0
regarding topics covered course data mesh	victoria would say yes go-to dbt yes dbt enforces go read viewpoint 'll explain also models kind structure mentioned modern concepts concept comes 80s kimball defined kitchen analogy find useful normally go course complex gets may need steps 's okay always try separated – presentation source need typecasting like example maybe duplication middle may models go data pipeline let ’ say flow less would look like alexey kitchen analogy victoria kitchen analogy way model data similar restaurants model food warehouse raw food source data everyone allowed fact 's actually dangerous 's security measures actually go warehouse stored stored things like kitchen people cook raw food make food 're going serve allowed would data engineers data analysts maybe analytics engineers would data warehouse trying process raw data end part restaurant people eat – dining hall presentation layer data warehouse fact tables look like data marts 're building data marts business stakeholders everyone allowed get foot let ’ say restrictions 's already presented 're presenting raw alexey think tableau self-service tool go cubes presentation layer right victoria yeah technically bi tools self-serve could everything could expose everything right external tables create tables inaudible create section model create factories could everything could even ones development exposed self-service bi tool like tableau looker whatever analogy says use presentation layer parts present raw data people would n't know use 's going like “ oh n't find phones ones maybe 's transformation ” things like alexey nice analogy	0
regarding topics covered course data mesh	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
regarding topics covered course data mesh	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
regarding topics covered course data mesh	alexey 's early talk future iterations data engineering zoomcamp think chuckles ml camp yes – videos difference homework 's still discussion n't know 'll see ankush yeah think slight discussion met sure going repeat ml de zoomcamp let 's see response participation feel definitely success definitely feel free pass around course give us feedback maybe make better decision alexey re-recording videos… 's much effort n't think 'll want let 's say ml zoomcamp example much effort 'd rather reuse videos instead making new ones hope videos good decide rerun need re-record yeah 'll see	0
regarding topics covered course data mesh	victoria data mesh one topics going cover going smaller video 's practice around anything like like weeks 's around explaining concept familiarity deepen concept need start working let 's say data mesh specifically 'll recording end week 'm back berlin 'm sure plan upload soon enough – deadline alexey actually want actually record one videos spark something planned like month ago still n't done want week probably see problems reading google cloud running things dataproc want first take care mlops video least two videos – data mesh mlops n't know rest	1
select approach cross validation k-folds leave p stratified etc	building locally fine	0
select approach cross validation k-folds leave p stratified etc	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
select approach cross validation k-folds leave p stratified etc	far week two week three follow simple approach simple approach described take entire dataset split three parts train validation test 'll use week two week three week four talk cross validation – explain right want say approach validation sufficient many cases think 's pretty safe say many many many applications split enough cross validation course nicer learn week four right go detail try explain wait video “ select approach cross validation ” would say multiple options k-fold… 'm sure many people understand n't covered yet basically dataset big n't need k-fold dataset smaller use k-fold “ leave p ” – 've never used personally	1
select approach cross validation k-folds leave p stratified etc	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
select approach cross validation k-folds leave p stratified etc	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
decision idea “ top n public leaderboard	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
decision idea “ top n public leaderboard	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
decision idea “ top n public leaderboard	alexey let show use another course example something like well public leaderboard maybe 'll design better right 's boring – textural 's less 100 people people wanted share information 'll something like n't know question actually n't know question actually asking idea behind public leaderboard think look something like victoria also want say something points – homework something like alexey one points homework anonymized victoria also n't think could disclose alexey yes exactly stay like victoria mean least kind know see hash know 'll know 're first place alexey 's want go thing hash thing name contact whatever link want share sent email 100 people everyone replied everyone wants page totally understand sejal towards end going least reveal names top three people leaderboard going declare reasons victoria 's ca n't say names unless 's approval somehow otherwise goes gdpr alexey n't ask permission share names collecting data sejal okay fair enough	1
decision idea “ top n public leaderboard	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
decision idea “ top n public leaderboard	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
difference resource manager master	victoria 'm sure answer mean yes definitely adding part project least workshop reason want people learn also consider people entirely new concept thought would much overhead try cover things already quite complex day-to-day even though work daily dbt work slow-changing dimensions daily basis example 's also think 's something look moment use definitely dbt especially deletions implementing load step done hooks think 's section advanced knowledge link hooks like incremental models mentioned changing dimensions n't link could link something could use snapshots think added concept updates guess could also pre-hook post-hook also something would incremental model incremental model loads new chunk data use merge depending workers use use could insert update 's something would work unique key project also final project 's understand complex go 's limit hope answers	0
difference resource manager master	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
difference resource manager master	alexey maybe like every time heard new language new tool new something wanted try realized new tools usually pretty raw look shiny start using lot problems come sometimes 's better use time-proven tools bit conservative something happened experience trying tools seeing even though look shiny still bugs especially 's new tool mitigate mindset workplace n't know guess depends use case ask team “ much new tool actually bring pros cons much work add integrate new tool potential benefits ” benefits outweigh headaches go usually analysis see maybe n't actually need new tool old one fine victoria critical mindset go something 's hype always go read everything kind see “ something 'm already using add much value much headaches add ” also 's another tool maintain consider critical mindset definitely	0
difference resource manager master	alexey resource manager – let 's say yarn thing yarn yet another resource negotiator remember picture image driver master executors resource manager – also think kubernetes kubernetes also –what 's locating new sources submit spark job say “ spark job need executors amount memory amount cpu need 10 them. ” submit master communicate resource manager resource manager provisions executors yarn mostly used thing called mesos think works spark lately think kubernetes usually used ankush let 's assume say “ want four worker nodes. ” resource manager would responsible spinning four worker nodes alexey bunch machines look like “ okay machines let allocate executor node ’ take worker node there. ” yarn think spark running yarn master also runs yarn master kind things time master thing sent requests 's closely connected resource manager thing provisions resources right ankush agrees remember days would google “ yarn ” would find things hadoop ankush still find things hadoop alexey 's almost end guess ’ clicked couple times sejal yeah think search curated user 're techie would get technical definitions alexey yeah javascript package manager popular “ yarn software packaging system. ” n't get hadoop yarn still n't know difference npm yarn maybe topic different discussion think days kubernetes getting popular olx yarn well workloads run spark kubernetes ankush ankush yeah use flink also running kubernetes kubernetes resource manager pretty cool alexey remember attending talk like three four years ago berlin buzzwords running spark kubernetes difficult many details ran many problems managed run end thought “ okay 's worth it. ” go aws click aws emr 'll yarn cluster without worrying kubernetes becoming popular ankush yeah everybody 's using kubernetes wants spend hardware running spark jobs alexey yeah already kubernetes n't want another resource manager nearby yarn think also usually install whole hadoop thing – need hdfs stuff n't typically use days keep data cloud storage n't need hdfs kind stuff still use hdfs ankush sejal ankush one company used sejal yeah long time ago ankush exactly sejal ever since aws s3 come think hdfs less forgotten alexey think today talk indeed company job searches on-premise hadoop cluster run spark think becomes rare days think criteo company talked huge dataset point bragging largest hadoop cluster europe thought “ okay really good thing ” chuckles sejal must old enterprise company ankush legacy alexey yeah probably ankush think soundcloud also ’ know ’ heard	1
difference resource manager master	alexey well good job keep working almost finished first week finish start week two finish week three chuckles follow sequence videos playlist github sufficient think questions go slack ankush keep going alexey chuckles yes exactly ’ almost finished first week good sign first week year pretty tough many people lot problems docker whatnot 're almost finished good sign ’ good track keep working	0
used tf.keras.layers.globalaveragepooling2d model difference using globalaveragepooling1d 2d 3d pooling layers	really depends cloud showed elastic beanstalk n't think needed change much anything n't remember changing anything thing changed url service test.py script thing needed change think heroku 's similar actually couple tutorials using pythonanywhere heroku probably explain need change go see need change anything probably much	0
used tf.keras.layers.globalaveragepooling2d model difference using globalaveragepooling1d 2d 3d pooling layers	n't know 's easy 's job think 's difficult invest time know minimum requirements work minimum requirements minimum requirement end web service deployed docker try multiple models also description project basically 're good want spend time – means satisfy requirements get good enough score ’ worry also ask many hours actually invested quite interesting see much time people spent see students spend like 20 hours materials iterations course already know much time people need invest submit homework please tell much time spent	0
used tf.keras.layers.globalaveragepooling2d model difference using globalaveragepooling1d 2d 3d pooling layers	like said 's always problem- data-specific datasets get good performance	0
used tf.keras.layers.globalaveragepooling2d model difference using globalaveragepooling1d 2d 3d pooling layers	let 's say two dimensional thing 2d want turn 1d case need use pooling2d 'm sure right want quickly check reason remember showing usually go defining model build sort layer layer every time add one layer model predicts see output based see kind pooling layer example need let 's say want turn 2d 1d think need use 2d pooling 'm exactly sure whether 's 2d 1d think 1d pooling needed something one dimensional want turn one value use 1d pooling 3d pooling three dimensional thing want turn one-dimensional thing image 2 things always confuse honest 's follow step step try different poolings want make sure convert image vector presentation something one-dimensional usually size number images times something 's like 2d array image one-dimensional vector based try different poolings sometimes also flatten flatten takes whatever – let 's say kd – want turn 1d use flatten many different options think 's clear difference might remember exactly use 1d 2d difference kind input take kind output produce 's cube 's 2d 's hypercube three dimensions 's something else	1
used tf.keras.layers.globalaveragepooling2d model difference using globalaveragepooling1d 2d 3d pooling layers	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
choosing datasets project things consider either get project avoid unnecessary headaches	alexey 's want look project 're behind right n't worry take time decide whether give certificate based projects homework 's – catch week 3 lighter week 1 maybe fast-forward directly docker compose run week 2 depending much time maybe go solution published today give everything need week 3 use transfer service video already watching video probably learn lot already like homework solution	0
choosing datasets project things consider either get project avoid unnecessary headaches	alexey add comment least workplace airflow instance set us data scientist would go instance airflow managed data engineers devops engineers would use never needed actually run docker compose moment multiple dags orchestration tool quite useful would personally maybe go luigi favorite one need think people well since people know luigi many like maybe airflow would actually better choice consider sejal would also like add question docker compose used docker compose airflow style use official setup seeing first time well 's unnecessarily complicated especially xcom variables forth generally docker compose acts wrapper docker containers production workflows docker containers used integration ci/cd pipelines wherever 're going deploy docker compose docker compose part helpful development environments case let 's say mock version airflow environment production want test dags local development environment use docker compose make things easier running simple commands like ‘ docker compose ’ would make fine instead running ‘ docker run container name etc whatever want ’ terms convenience terms usage ’ generally used development environments production alexey think first part question “ close usual data engineer role workflows ” workflows see work complex steps probably also case need start something already – would n't call complex pipeline – three four steps already something eventually might grow bigger bigger add steps ankush would also like add 're cloud maybe use airflow deploy docker maybe actually use service provided cloud provider one similar airflow think airflow would used internally like cloud composite google cloud platform would using terraform sejal yeah used aws step functions aws setup step functions combination lambda place airflow ankush said also something wanted say really depends production self-hosted cluster kubernetes wherever want use pure airflow version company could kind setup use native airflow cluster could cloud-based setup convenient option use managed services cloud composer aws also version called managed airflow	0
choosing datasets project things consider either get project avoid unnecessary headaches	alexey 'm sure last part –how docker provided terraform ankush run stuff terraform think always ... set airflow cluster terraform n't think execute job alexey 's something like use terraform prepare environment prepare thing run need actually run something least two separate commands first “ terraform apply ” “ python run ” something like 50:02 1 upvotes allowed contact instructors directly discuss projects things unsure regarding project 'm using new tools alexey think 's best ask things slack channel others also interested learning answer scales better instead everyone writing us directly – answer everyone sees answer b others answer available right somebody knows tool maybe n't example pulsar one students might know able help 's best use channel contact us directly	0
choosing datasets project things consider either get project avoid unnecessary headaches	ankush yes yes think transfer service right yes question data using aws want run test around data source feel free also leave comment maybe even know goes azure blob storage	0
choosing datasets project things consider either get project avoid unnecessary headaches	alexey one thing use images unless really know 're probably n't super small dataset one megabyte ideally victoria probably one file taxis four least two think also 's important understand course focused concepts everything 're going use focused building pipeline processing data actual presentation maybe 's super cool analysis images saying reality need practice processing data 's 's easier use something else alexey maybe come across dataset 're sure 's good dataset ask slack 'll help think list gave quite good 's good start course talk kaggle tons different datasets contain images tweets example way also use tweets structured information extract tweets well maybe actually good let 's say want practice streaming get stream tweets coming twitter process fly think could also good project sometimes also parse something example websites like ebay selling buying things build parser getting data thing could go every day take data something data put csv example put data warehouse something like n't spend much time scraping think already scrapers available maybe google “ ebay scraper ” find code n't spend lot time trying build scraper victoria also something like twitter suggested instagram well – apis makes part super easy want something like find something already api 's probably easiest option also get practice also good	1
know literature testing data pipelines	victoria set warehouse end define connection difference would use adapter dbt several adapters would first want use dbt go check adapters make sure 's supported several official example redshift could use community-supported azure alexey yeah aws athena azure athena redshift presto well example presto cluster use aws – within aws presto cluster potentially similar setup connect dbt presto victoria general add little bit answer let 's say project right bigquery would change profile 's set connection may make changes 's something sql supports bigquery way around able run project minutes	0
know literature testing data pipelines	alexey well use judgment want try force register account azure run want n't maybe look code see errors 's really ankush maybe matching ask criteria – like used – maybe match corresponding people alexey 's nice idea let 's see 'm afraid complicate things bit already script matching n't use criteria 's random ankush need modify script one column alexey yeah perhaps let 's see ankush easier enough group different things alexey create form submitting ask cloud used right ankush exactly right alexey makes sense machine learning zoomcamp – wanted re-execute whole thing re-executed 's really want learn project – execute grading also learning – want learn really want learn things work execute learn n't time since executing different cloud might take lot time – n't much time really ask invest something like five hours figuring azure works use judgment guess 's answer try matching minimize	0
know literature testing data pipelines	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
know literature testing data pipelines	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	1
know literature testing data pipelines	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
flask app showing error many marks deduct things correct flask app showing error output	flask app produce result actually return prediction 're good n't need fix answer question regarding many points deducted think whole project kind assumes managed deploy flask n't able put docker able deploy cloud lose like five six points something along lines think 's good idea make sure flask returns something problems please share code think know asking question talked already slack maybe error somewhere predict.py script need make sure fix error work perhaps take code lectures – think xgboost model logistic regression need put x variable t-matrix xgboost instead using predict_proba need use predict think 's – n't work please write slack let 's try figure 's working	1
flask app showing error many marks deduct things correct flask app showing error output	really depends cloud showed elastic beanstalk n't think needed change much anything n't remember changing anything thing changed url service test.py script thing needed change think heroku 's similar actually couple tutorials using pythonanywhere heroku probably explain need change go see need change anything probably much	0
flask app showing error many marks deduct things correct flask app showing error output	's really case-dependent saw slack remember correctly carlos asked needs drop features – seemed okay drop highly correlated feature assistant name target variable churn basically people work one particular person tend turn less people work somebody else think case seems like relevant feature include n't know – 's really case-dependent usually need sort domain expertise decide feature introduces leakage carlos ’ case think 's safe leave drop see performance validation dataset changes gives idea important feature	0
flask app showing error many marks deduct things correct flask app showing error output	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
flask app showing error many marks deduct things correct flask app showing error output	honest n't used uvicorn used gunicorn n't know matters n't actually checked benchmarks use gunicorn also use others – n't remember ones reason basically	0
difficult expect course following weeks ’ wondering need allocate time course week 1 12 hours week 2 10 hours	alexey week 2 yes – upload video solution also update leaderboard probably take time depending clean data – need lot data cleaning take lot time please n't put things n't look like emails email field fill please careful takes time clean later	0
difficult expect course following weeks ’ wondering need allocate time course week 1 12 hours week 2 10 hours	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
difficult expect course following weeks ’ wondering need allocate time course week 1 12 hours week 2 10 hours	alexey 33 submissions think decision extend deadline good one hopefully end week see higher number understand project much difficult watching course following along 33 people – probably put lot effort thanks saw one person write put 100 hours already project outstanding sorry cruel 100 hours lot	0
difficult expect course following weeks ’ wondering need allocate time course week 1 12 hours week 2 10 hours	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
difficult expect course following weeks ’ wondering need allocate time course week 1 12 hours week 2 10 hours	alexey n't know week 3 seems lighter previously n't need lot local setup think local setup time spent case correct ankush think alexey week 4 n't know complex set things sejal think 's cloud-based right victoria victoria yeah 's called cloud-based 's option installing locally 's simple pip install brew install something like also provide links use docker image everyone docker expert n't super hard terms installation would expect little bit complexity trying understand whole concept project simple hopefully follow along videos sejal would like add week 3 well setup 've done already available week 1 week 2 thing – extend setup add bigquery tables would barely time invested setup side things definitely docker docker related issues chuckles hopefully alexey yeah week 5 spark spark need jvm short java virtual machine n't know need install locally need install specific version java virtual machine think 8th 11th already tutorial windows windows things usually get tricky think kafka also need jvm ankush confirms hopefully smoother ankush kafka planning docker guess run jvm issue might run memory issues – may run memory basically alexey said hope figure workaround maybe shutting chrome alexey chuckles maybe use browser phone somehow phone works better computer n't know ankush yeah maybe firefox chuckles alexey yeah also amount time need allocate also depends first experience docker case could tough nothing wrong know background everyone give good estimate exactly much time spend however end week submit homework ask much time spent watching videos homework way get better understanding exactly much time spent things us first time 're course 's hard good estimate everyone different backgrounds makes even harder think less time week four 's also cloud-based let 's see hopefully much time	1
n't automatically scale features using model	n't think takes lot time first need record videos prepare homework imagine adding collaboration startups well maybe sometime future n't know somebody startup listening right want hire people graduate zoomcamp please reach let 's arrange something	0
n't automatically scale features using model	start working n't inform imagine everyone started informing take lot time answer finish answering questions first november yeah n't inform please write slack 're sure think good dataset go ahead use	0
n't automatically scale features using model	alexey think answered question less dmitry think always need scale features dmitry really depends model use 're talking linear model sure need scale audio corrupted situation ’ already less scaled n't need anything overall yeah alexey particular dataset saw features different scales model could confusing – use iterative solver find best solution confusing kinds solvers iterative solvers	1
n't automatically scale features using model	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
n't automatically scale features using model	classification classification let 's say n't predicting price let 's say predicting 's average number hours image 1 case would need another layer – rather layer 's called activation n't activation 're still… ’ trying formulate way 's confusing think 's better go lecture rewatch ’ essentially retelling thing need from_logits price variable loss mean squared error logits loss categorical cross-entropy categorical cross entropy usually used multiple categories need use logits 's numerically stable n't use – stay activation softmax n't use legit hope 's clear regression n't need use homework need use from_logits either homework two classes 's binary classification problem multiple classes – let 's say three classes four classes five classes – need use logits loss categorical cross-entropy loss categorical cross-entropy 's recommended use from_logits=true 's numerically stable	0
want understand peer project review late joiners work	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
want understand peer project review late joiners work	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
want understand peer project review late joiners work	alexey homework use automatic grading need review ’ homework assignments project something ’ planning week 7 week 10 basically end course nobody 's late yet n't happened yet n't worry	1
want understand peer project review late joiners work	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
want understand peer project review late joiners work	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
could extend project deadline one day	likely think complicate logistics also want see many people actually manage submit deadline make decision bit hesitant extend deadline reason remember third project reason time still project part third project example able get certificate need two projects three reason third project exactly – maybe somebody behind still wants take part 's n't time part third project	1
could extend project deadline one day	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
could extend project deadline one day	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
could extend project deadline one day	two people want work dataset individually please feel free imagine way control go kaggle select competition even ’ accidentally happen long 's exactly code – long n't copy – ’ fine yeah please n't copy code	0
could extend project deadline one day	flask – let draw let 's say want create web service image 5 web service gets requests web service responds something could predictions could something else – n't related machine learning implement web service use flask use fastapi use framework creating web services use flask use something else – flask allows use post example let 's say “ get ” usually parse parameters query let 's say go google put “ test ” see q=test https //www.google.com/search q=test oq=test aqs=chrome.0.69i59l2j69i65l3j69i60j69i61l2.1264j0j1 sourceid=chrome ie=utf-8 image 6 parameter pass “ get ” request “ get ” request sometimes let 's say want score customer n't want put gender=female contract=one year string send json post post also add body request generally use post general use case flask able create web service	0
spark airflow different machine vm dag file must able spark submit executing spark job	alexey yes think n't think draw anywhere let 's say one machine spark another machine airflow machine airflow needs able send request spark spark submit need live network case sparks submit save url ip address spark computer 's enough one thing might need though probably need install spark airflow order able maybe sejal correct think need need java need spark submit script order able actually send jar network spark master need modify airflow container – used docker images course need modify need install java need install spark spark submit say “ -- master ” specify master virtual machine spark would actually instead running spark virtual machine somewhere would use dataproc relatively simple run need click buttons wait five minutes creates cluster cluster 's pretty convenient	1
spark airflow different machine vm dag file must able spark submit executing spark job	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	0
spark airflow different machine vm dag file must able spark submit executing spark job	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
spark airflow different machine vm dag file must able spark submit executing spark job	alexey well ’ aws account go transfer service 's probably easiest need click buttons want set gcp virtual machine need spend time setting everything 's worth weeks – spark probably kafka well – useful probably worth time investment want quickly copy files transfer service probably easiest way	0
spark airflow different machine vm dag file must able spark submit executing spark job	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
need features scale linear regression example engine horsepower 1 100 mileage 100 10 000 – need normalize	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
need features scale linear regression example engine horsepower 1 100 mileage 100 10 000 – need normalize	yes good references – course course called machine learning zoomcamp cover parts except conda git assume midterm project use git conda – use conda python interpreter install pipenv manage dependencies n't know good references honest maybe n't know answered question hope references course good enough ’ please let know think improved think missing	0
need features scale linear regression example engine horsepower 1 100 mileage 100 10 000 – need normalize	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
need features scale linear regression example engine horsepower 1 100 mileage 100 10 000 – need normalize	case everyone uses docker problem n't use docker n't get point using docker ’ run write “ sorry run this. ” hope n't happen 's actually whole idea behind docker – 's transferable different environments let 's say use gunicorn n't use docker able test case use scoring matrix evaluate accordingly gunicorn tick box would encourage still try learn submission even run reason think reason run something windows ubuntu gunicorn everything runnable	0
need features scale linear regression example engine horsepower 1 100 mileage 100 10 000 – need normalize	dmitry usually 're working linear models sure need scale ’ good question n't covered homework due us trying make bit simpler initial stages sure important know 're working linear model need scale alexey use normal equation normal equation maybe matters less course features vary significantly one case features billions another zero one probably need normalization think cases like mileage engine horsepower – yes different normal equation used 's important types finding solutions n't talk stochastic gradient descent gradient descent course general talk bit neural networks type finding solution need scale features normal equation though normal equation fine time think except huge discrepancy dmitry yeah sure affect said huge difference definitely alexey yeah saw question multiple times already way stochastic gradient descent course andrew ng – n't remember whether starts normal equation gradient descent way around today dmitry shared good article maybe share channels well think 's nice one 're interested ways finding solutions linear regression 's good read stochastic gradient descent need normalize features	1
victoria ’ experience much environment data engineering changed feels like many changes/tools short amount time	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
victoria ’ experience much environment data engineering changed feels like many changes/tools short amount time	alexey 'm sure last part –how docker provided terraform ankush run stuff terraform think always ... set airflow cluster terraform n't think execute job alexey 's something like use terraform prepare environment prepare thing run need actually run something least two separate commands first “ terraform apply ” “ python run ” something like 50:02 1 upvotes allowed contact instructors directly discuss projects things unsure regarding project 'm using new tools alexey think 's best ask things slack channel others also interested learning answer scales better instead everyone writing us directly – answer everyone sees answer b others answer available right somebody knows tool maybe n't example pulsar one students might know able help 's best use channel contact us directly	0
victoria ’ experience much environment data engineering changed feels like many changes/tools short amount time	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
victoria ’ experience much environment data engineering changed feels like many changes/tools short amount time	victoria actually tell funny anecdote built program parents java database mysql database yesterday trying fix something checked everything tried use workbench inaudible old version 2014. hard use like “ oh god well things improved years find difficult compared still studying. ” n't much experience anything would say definitely tools made things easy fact pipeline five minutes tool like leap airflow instead inaudible alexey hand mysql still mysql right 10 years ago things changed things still stay things changed indeed fundamental things like postgres n't think changed significantly started postgres – first used – usual desktop app pgadmin ’ web app 's different kind still look feel previously needed install postgres run docker helpful least know many problems running docker remember difficult first install postgres figure actually delete longer need runs service 's hard kill remember gave problems docker start provided works longer need forget ’ pretty cool victoria 'm sure used thing used use virtualbox lot well docker definitely game changer remember thesis distributed system like five year virtualboxes running machine fake five servers web cloud warehouses thing free trials things like alexey docker compose start five docker containers think mentioned etleap tools – 're like no-code tools almost connect things arrows say “ okay data source destination. ” connect works also cool victoria way options overwhelming years ago data warehouse probably one big ones company like oracle sql server right lot options 's lot competition technology improves fast also easier things still code also no-code still get parts need alexey 're worried learning something new tool appears – fundamentally things n't changed much still etl processes instead using tools like informatica use something else	1
victoria ’ experience much environment data engineering changed feels like many changes/tools short amount time	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
think read somewhere mlops zoomcamp one right took data engineering zoomcamp good condition go mlops zoomcamp	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
think read somewhere mlops zoomcamp one right took data engineering zoomcamp good condition go mlops zoomcamp	alexey 'm sure last part –how docker provided terraform ankush run stuff terraform think always ... set airflow cluster terraform n't think execute job alexey 's something like use terraform prepare environment prepare thing run need actually run something least two separate commands first “ terraform apply ” “ python run ” something like 50:02 1 upvotes allowed contact instructors directly discuss projects things unsure regarding project 'm using new tools alexey think 's best ask things slack channel others also interested learning answer scales better instead everyone writing us directly – answer everyone sees answer b others answer available right somebody knows tool maybe n't example pulsar one students might know able help 's best use channel contact us directly	0
think read somewhere mlops zoomcamp one right took data engineering zoomcamp good condition go mlops zoomcamp	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
think read somewhere mlops zoomcamp one right took data engineering zoomcamp good condition go mlops zoomcamp	alexey guess 're right may bit spoiler datatalks github already repo n't think contains anything lot content yeah course eventually sejal maybe take break data engineering alexey agrees 'm gon na go long vacation inaudible alexey courses dependent date engineering zoomcamp prerequisite mlops zoomcamp two separate courses yeah think took one understood everything n't difficult able finish think 're perfect condition mlops zoomcamp probably useful one actually machine learning zoomcamp ’ already talked things like deployment kubernetes things like one would useful	1
think read somewhere mlops zoomcamp one right took data engineering zoomcamp good condition go mlops zoomcamp	sejal think related orchestration since 've covered concepts orchestration alexey maybe take one putting two cents – airflow definitely meant orchestration multiple jobs sometimes even parallel asynchronously running jobs 's really meant sequential run small-scale usage definitely resource-intensive jobs alexey airflow quite heavy indeed benefits gives – mechanisms ui viewing history – useful moment three dags running parallel one airflow might overkill know 're going pipelines workflows 's time think kind workflow orchestration tool n't airflow airflow quite widespread let 's say team 're making decision workflow orchestration tool use airflow would quite good choice data engineers market know data scientists also know less usually like good tool course disadvantages – indeed resource-intensive needs run somewhere – typically n't set computer	0
someone wants apply data engineering jobs course else recommend sql python coding aws/gcp depth etc	alexey well 's really anonymous asked submit link github n't know need ask questions previous course think ask contact information project description n't know ankush think encourage might unfair different people first maybe 're available answer questions let 's put like grading maybe 's unfair want learn 's definitely helpful think grading done based upon submitted 's also really important document steps alexey yeah guess need contact author means really give four points reproducibility ankush case maybe instructions clear report via contact alexey yeah exactly n't remember problem previous course maybe also create issue github n't know	0
someone wants apply data engineering jobs course else recommend sql python coding aws/gcp depth etc	sejal official airflow setup like mentioned documentation quite overwhelming decided provide default level services required multi-node setup workshop using single machine multi-node setup compatible scalable clusters example suitable single-machine configuration especially working something like 8gb ram also made recent announcements created frills version removed unnecessary services redis queue enables multi-node setup also triggerer workers etc main things required actually scheduler executor threaded executor multi-node executor instead single-node executor local executor along web server want use gui-based platform airflow ram cpu usage conserved worked alexey comes smaller version airflow problems like maybe 's worth trying give try run cloud virtual machine n't problems see questions airflow memory-intensive resource best way solved personally using cloud virtual machine windows computer chrome open time 's ideal choose either chrome docker compose airflow chrome quite useful 's opted virtual machine maybe 'll work well ankush looks like 's time buy new machine alexey yeah think bought computer something like couple months ago 's already like blame chrome n't know matter good computer moment open couple tabs chrome says “ okay ram let take it. ” much left rest processes	0
someone wants apply data engineering jobs course else recommend sql python coding aws/gcp depth etc	ankush n't think 's focus course think focus course learn data engineering basics learn develop software kafka spark might cover test cases yes extensive writing good unit test cases course	0
someone wants apply data engineering jobs course else recommend sql python coding aws/gcp depth etc	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
someone wants apply data engineering jobs course else recommend sql python coding aws/gcp depth etc	alexey maybe would suggest start applying see ask ’ covered already sufficient start working opinion course already expected know sql becoming better sql always good idea python coding also helpful think best thing start applying either get job immediately get feedback companies see “ okay seems like need thing n't know let improve this. ” victoria feel way start applying know 're going get rejected 'll get good idea 're missing consideration every company expectations different things get sense stand try ask feedback well especially get home assignment challenge – get technical interview stage would super helpful get oriented people usually nice would say get technical interviews think people nice technical interviews actually discussion technology even orient little bit could good resource guess alexey also maybe projects instead focusing “ improve coding learn aws ” let 's say instead learning aws maybe come project use aws project opinion much productive rather preparing aws certification good thing course focus project focus things really need something end rather aws knowledge get something put cv people go github check would recommend projects instead learning things victoria 's also fun lot troubleshooting project plus motivation – analyze stream prepare data stuff like 's definitely entertaining well get know bit everything things 've got well 're working alexey thing called “ time learning ” problem need focus solving particular problem start learning exactly need projects cool	1
tell us average number hours homework today spent lot time usual want know please	eventually want analysis see much time people spent homework also want breakdown know people registered – sign course say 're data analyst data engineer student also want see breakdown per role – much time took let 's say software engineers think week n't difficult engineers probably difficult students perhaps maybe weeks way around want analytics point still finish editing videos week six first 'll need prepare guidelines regarding projects maybe 'll analytics	1
tell us average number hours homework today spent lot time usual want know please	building locally fine	0
tell us average number hours homework today spent lot time usual want know please	n't think takes lot time first need record videos prepare homework imagine adding collaboration startups well maybe sometime future n't know somebody startup listening right want hire people graduate zoomcamp please reach let 's arrange something	0
tell us average number hours homework today spent lot time usual want know please	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
tell us average number hours homework today spent lot time usual want know please	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
randomizing initial weights good strategy get better results reproducibility	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
randomizing initial weights good strategy get better results reproducibility	think one day half try actually want remind everyone third project optional 're behind midterm project keep working project pace without worrying deadlines time comes submit third project take project submit third project way able still submit something even skip week reason keep working project start today keep working submit third project fine	0
randomizing initial weights good strategy get better results reproducibility	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
randomizing initial weights good strategy get better results reproducibility	alexey think question actually make results reproducible neural networks every time actually create layer gets initialized random weights actually happen dmitry talk full reproducibility stochastic nature always fluctuations question severe alexey neural networks 's tricky make sure ’ reproducible 's homework module also bit tricky dmitry regarding answer 0.45. bit strange mentioned answer 0.45. alexey strange indeed think main reason shuffle dmitry agrees added shuffling maybe people submitted explicitly wrote instructions need shuffle could reason well let 's say 're evaluating get bad batch dogs maybe particular batch accuracy great happen	1
randomizing initial weights good strategy get better results reproducibility	alexey question lecture use exception output exception n't include top 5x5x2048 long long long tube – three-dimensional thing dmitry know 's 5x5 think convolutional layers 1x1 – quite end 2000-something filters result applying filters still lot feature maps 'm completely sure know 's going dmitry happens exception last layer dmitry think 's filters alexey yeah bunch of… dmitry convolutional nature alexey yeah something specified coming pretrained network	0
think apache hop 's shiny know	alexey know anything apache hop victoria first time hear according google 's data metadata orchestration platform interesting well since 's apache assume 's cool chuckles usually order become apache project – top-level project – project needs undergo process easy projects undergo think 're cool definition guess 'm still sure thing exactly victoria loading big datasets maybe focused orchestrating metadata alexey n't know 's shiny would n't classify documentation shiny honest shinier tools would say chuckles victoria yeah 's old school 's good chuckles	1
think apache hop 's shiny know	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	0
think apache hop 's shiny know	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
think apache hop 's shiny know	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
think apache hop 's shiny know	victoria 'm sure answer mean yes definitely adding part project least workshop reason want people learn also consider people entirely new concept thought would much overhead try cover things already quite complex day-to-day even though work daily dbt work slow-changing dimensions daily basis example 's also think 's something look moment use definitely dbt especially deletions implementing load step done hooks think 's section advanced knowledge link hooks like incremental models mentioned changing dimensions n't link could link something could use snapshots think added concept updates guess could also pre-hook post-hook also something would incremental model incremental model loads new chunk data use merge depending workers use use could insert update 's something would work unique key project also final project 's understand complex go 's limit hope answers	0
many models much parameter tuning expecting	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
many models much parameter tuning expecting	one course covered linear models covered tree-based models linear models saw c parameter 's rich regression think 's called alpha – tune try decision tree try random forest xgboost different parameters select best model basically going learned far trying different models different parameters models n't use xgboost example – use random forest suggest using xgboost gradient-boosting tree implementation try let 's say least three different models model try tune parameters sufficient	1
many models much parameter tuning expecting	building locally fine	0
many models much parameter tuning expecting	yes could soon post message slack need find time couple tweaks script	0
many models much parameter tuning expecting	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
articles website work final 20 pt article	yes submit anywhere want say 20 pt n't remember yeah work good article “ good ” meaning 's copy/pasting code nothing else actually need explain 's going also article – closer time working article guidelines well	1
articles website work final 20 pt article	multi-classification – yes clustering – 'm sure probably good idea n't cover hard – know evaluate peers know better clustering	0
articles website work final 20 pt article	building locally fine	0
articles website work final 20 pt article	's really case-dependent saw slack remember correctly carlos asked needs drop features – seemed okay drop highly correlated feature assistant name target variable churn basically people work one particular person tend turn less people work somebody else think case seems like relevant feature include n't know – 's really case-dependent usually need sort domain expertise decide feature introduces leakage carlos ’ case think 's safe leave drop see performance validation dataset changes gives idea important feature	0
articles website work final 20 pt article	think around 200 204 think something like	0
knn unsupervised learning	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
knn unsupervised learning	building locally fine	0
knn unsupervised learning	yes 're running windows first install wsl windows subsystem linux use second thing altboot install ubuntu computer way keep windows let 's say development work go use linux every time n't know something google least learned linux think able learn way well – command line try much possible without leaving command line example say ls want look inside file use “ less ” get familiar tools like simple unix tools like ls less cd vd – things like think sufficient work	0
knn unsupervised learning	still uses target information fit knn take entire dataset put inside model – contains labels supervised look price house see houses around look prices sense supervised learning look similar objects make decision price	1
knn unsupervised learning	n't think takes lot time first need record videos prepare homework imagine adding collaboration startups well maybe sometime future n't know somebody startup listening right want hire people graduate zoomcamp please reach let 's arrange something	0
'm storing data database data warehouse put pipeline data database may change future	alexey probably n't watched one hour twenty minute video spoiler – question covered remember exact time video ’ somewhere beginning maybe skip around see take quick look n't watch entire video know 's quite long probably daunting see video hour twenty minutes long chuckles sorry sejal would like add – specifically execution_date terms announcement best practices videos alexey discussed think rather creating separate video best practices would best merge things together prepared text-based version time codes – airflow/postgres video alexey prepared would provide reference links concepts alexey explained including unintelligible also using airflow configuration variables execution dates also backfilling forth 'll sharing today tomorrow stay tuned	0
'm storing data database data warehouse put pipeline data database may change future	victoria sure understand 're taking csv really database right alexey yeah think question general related homework understand correctly let 's say database ecommerce could orders something like data constantly changing example today price – tomorrow price changes question put data warehouse change know victoria n't know question targeting schema evolution example data changes right case would primary key would kind deduplication assume 'll original record modified record 're modeling data kind deduplication always take last record primary key let 's say “ order number ” case schema evolution 's question going little bit complex 'll somehow adapt pipeline 're using etl service tools like fivetran questions well tools also adapt schema evolution alexey another thing add – let 's say database records item id item name price let 's say price changes capture change new version know record database updated save change data warehouse therefore every time record changes save data warehouse bunch rows change basically capture every time something changes save data warehouse sort travel back time – see state record particular date 'm sure though 's specific data warehouse – least every time record changes capture change somehow sql query say “ want see date timestamp. ” see latest price example implement actually idea maybe fivetran databases – let 's say use mysql – something send data source changes something like postgres probably something similar victoria yeah 'll implement pipeline basically logging 's going even though original database maybe 's order like case update generate second entry like alexey mentioned 're logging fivetran could well course alexey fivetran low-code thing say “ okay source destination ” thing right moves data one location another works victoria yeah less chuckles set connection s3 bucket whatever want – several adapters set run rule scripts set destination	1
'm storing data database data warehouse put pipeline data database may change future	victoria 's buzzwords right alexey yeah go repo see like practical week like five 10 minute video explaining use 's planned ankush maybe spend couple minutes today well data mesh basically concept idea – let 's go back say “ data pipelines data engineering teams built different companies right ” people generating data people consuming data one big team middle called data engineering team dataops team whatever want call different companies different names teams responsible ingesting data transforming putting making way 's consumable right way right fashion data mesh wants basically decentralize particular role big team wants give power let 's say responsibility team generating data teams consuming data therefore ’ layer instead team taking whole role 's kind better idea build services give responsibility give services software teams generating data consuming basically going higher level respect case happens team ’ generating data responsible quality data answering different questions data things 's really useful multiple teams handling data idea data knowledge data lost best person best team answer questions best team knowledge data team 's generating also responsible putting data data lake data warehouse also responsible queries attaches whole thing together 's kind rough idea data mesh alexey ’ abstract concept actually go youtube channel longer explanation data mesh “ longer ” mean one hour long – hour six minutes chuckles 're kind stuff… way dataops 101 also quite nice one check well one modern data stack analytics engineering also good basically check three – 're good 're interested data mesh specifically first one goes lot detail second one – bit detail ankush 's also nice blog post alexey zhamak actually writing book right think 's early release 's written five six chapters n't remember 's progress 's going quite big book right ankush blog post like today 's date read couple days impossible read one go 's huge victoria think also download first two chapters something book give quick overview well ankush 's also another book o'reilly called data mesh practice max schultze alexey one free right ankush think free 's free	0
'm storing data database data warehouse put pipeline data database may change future	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
'm storing data database data warehouse put pipeline data database may change future	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
explain docker works internally impressed like evolution traditional virtual machines sure possible	yes mentioned mentioned homework use docker image prepared actually done purpose 's possible run model1.bin without docker get question wanted execute docker model2.bin available anywhere docker image instruction asking base solution image	0
explain docker works internally impressed like evolution traditional virtual machines sure possible	multi-classification – yes clustering – 'm sure probably good idea n't cover hard – know evaluate peers know better clustering	0
explain docker works internally impressed like evolution traditional virtual machines sure possible	n't think 'm best person 'm docker user n't know exactly works think uses something like cgroups 'm sure – n't quote perhaps look “ docker works. ” articles found google basically short answer – n't know works internally know use	1
explain docker works internally impressed like evolution traditional virtual machines sure possible	maybe n't know still need names think question referring part image 1 need know 1 means “ ok ” 2 means default need know 1 means “ rent ” 2 means “ owner ” “ private ” information somewhere json csv file whatever information somewhere load could stored json file use need know 1 stands “ rent ” 2 stands “ owner ” etc n't care machine learning models n't care – n't care 's “ rent ” “ owner ” 's 1 2 – case turn home variable string using something like image 2 stay encoded number string use dictionary vectorizer treat string number hope answers question	0
explain docker works internally impressed like evolution traditional virtual machines sure possible	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
post social networks n't get reaction affect score end care leaderboard	alexey n't affect score count links likes counting likes would difficult would need go linkedin then… 's difficult many different social networks count number posts make n't overdo – cap seven let 's say 're following course every day share progress – 's 's capped seven course share count seven victoria everything see says datatalks go react alexey also comment usually gets widened reach n't worry	1
post social networks n't get reaction affect score end care leaderboard	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
post social networks n't get reaction affect score end care leaderboard	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
post social networks n't get reaction affect score end care leaderboard	victoria sql definitely super important depend role company going personally n't really use much python outside maybe api scripts sql definitely one use every day lot could good 're least familiar python write come data modeling concepts keep mind focus lot transforming data lighter use 's important know transform data would need know bi – able expose data later bi tools n't mean need focus building dashboards things like least know data could used business perspective ’ also important know concepts around data warehousing – least know parts etl elt concepts know data gets data warehouse able understand kind cover parts would important	0
post social networks n't get reaction affect score end care leaderboard	sejal wanted address based feedback 've received course scoped around terraform particularly devops-based technology data engineering close overlap devops terms building infrastructure stuff also introduced terraform extra order use designed course scope way covering basics terraform intention like part week 1 prerequisites using terraform order build infrastructure focus core parts data engineering answer question would say junior de maybe important opinion maybe terraform associate certificate something ask whether current job company requiring – 're heavily focused infrastructure work otherwise would say learn basics terraform terraform really required job order build infrastructure stuff	0
necessary junior data scientists learn model deployment stages deep learning	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
necessary junior data scientists learn model deployment stages deep learning	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
necessary junior data scientists learn model deployment stages deep learning	44. two people submitted twice actually 's 42 unique submissions hope extension many people able work project know maybe many putting things last day something n't work happens time well focus putting things flask putting docker good ’ training model right stop go deployment part 'll good	0
necessary junior data scientists learn model deployment stages deep learning	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
necessary junior data scientists learn model deployment stages deep learning	model deployment stages yes deep learning mean 's good know deep learning kind problems solve interview junior data scientist person n't know convolutional neural networks work 's big deal actually ’ level interview senior data scientist n't know deep learning works – big deal especially position n't require deep learning position requires deep learning 's different question positions n't require olx general maybe companies specialize computer vision deep learning must would say majority companies require deep learning use models well deep learning	1
many submissions week	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
many submissions week	dmitry usually use numpy function alexey think also question “ need transform variable way apart logarithmic transformation transformations need know ” dmitry think box cox transformation certain types transformations target variable especially regression tasks make randomly distributed example sure leave like results especially linear model linear models really depend linearity normal distribution target variable results good sure throughout course zoomcamp talk nonlinear methods end symbols example end see example use without transformation – using trees alexey yeah thanks “ cox box transformation ” said dmitry agrees think one turns variable normal right dmitry agrees	0
many submissions week	yes think wrote code image 2 code used creating model2 take screenshot copy run – also send slack want tell need	0
many submissions week	think around 200 204 think something like	1
many submissions week	building locally fine	0
want feature engineering adding new categorical feature random undersampling balance data process first	building locally fine	0
want feature engineering adding new categorical feature random undersampling balance data process first	’ two weeks october 18 2021 meaning first november 2022	0
want feature engineering adding new categorical feature random undersampling balance data process first	yes addition ideally repo git folder git repo contains notebook showing eda data pre-processing notebook assume would play different models would play let 's say logistic regression decision tree random forest xgboost – maybe want play model want tune parameters also keep notebook – exploration trying different parameters notebook extract training script best model also need deploy model need create python file loads pickle file whatever way use save model loads serves web service need need way managing dependencies recommend using pipenv use something else like poetry virtualenv think ’ better stick pipenv also need package docker docker file also need readme explains actually run thing let 's say peer somebody reviewing wants run project – need clear instructions readme say “ first execute could dockerbuild -tname execute docker run -it –rm. ” something like – know exactly 're supposed run	0
want feature engineering adding new categorical feature random undersampling balance data process first	n't know 're referring image reference maybe watch video afterwards let know questions 'm sure exactly 're referring	0
want feature engineering adding new categorical feature random undersampling balance data process first	n't think matters actually think two things quite unrelated add categorical feature entire dataset take categorical feature small sample maybe first taking sample feature engineering better simply dataset smaller things faster n't think actually matters	1
plans write articles medium related zoomcamp articles teachers students	alexey yes 's homework first two questions yellow taxi data need write dag putting google cloud storage years 2019 2020 yellow taxi week video airflow get code use starter putting files google cloud storage need change little bit maybe little bit need change instructions “ re-running dags past dates ” tells exactly need order re-execute need set catch parameter true careful running many jobs parallel students reported problems computer froze tried execute many jobs parallel even virtual machine cloud also problems n't important thing need rename dag n't airflow see changed let 's say depending answer – answer different currently need rename order airflow pick also changed start date airflow also need rediscover 's want change name one extra thing might need files quite large helpful manually remove n't take lot space question 3 for-hire vehicles need thing yellow taxis dataset question 4 thing zones eventually form submitting solutions use forum submit thing week 1. think 's good high-level overview main point learn create dag learn move data web google cloud storage thing data week 3	0
plans write articles medium related zoomcamp articles teachers students	ankush use basically unit testing rigorously unit test small components end-to-end test order test whole pipeline basically 's literature chuckles alexey think thing called data kitchen book think dataops cookbook think goes bit data quality topics think talk bit n't remember detailed maybe something check seen one ankush 'm checking right 's point data kitchen alexey think 're consultants “ stop band-aiding data pipeline. ” think consult dataops properly course need provide email try get client chuckles multiple people told useful book	0
plans write articles medium related zoomcamp articles teachers students	ankush 're talking specifically interviews – employer sending software engineering interviews data engineer maybe 's right role maybe 's correct company maybe talking tech recruiter understanding role might give insights exact role honestly “ data engineer ” used lot days lot companies also use “ data engineers ” way describe data product developers software engineers working data-intensive applications might worth clearing knowledge algorithms data structures think basics software engineering – data engineering analytical data scientists learning algorithms data structures would always help career irrespective fact going towards data engineering data science domain alexey would add even data scientist less engineering-heavy specialization 's useful know data structures algorithms example using list instead set degrade performance data pipelines significantly sometimes one line change increase performance data pipelines like 10 times 100 times knowing things like useful need read kleppmann 's book cover cover maybe knowing lists sets hashmaps dictionaries – things could quite useful proficient leetcode ’ hard questions – n't know translates real life ankush yeah think would able clear lot interview tests lot leetcode might convert directly performing better job	0
plans write articles medium related zoomcamp articles teachers students	alexey n't curriculum project end victoria people write medium articles alexey course yeah maybe question comes fact course machine learning zoomcamp article one activities think towards end course people felt bit tired also three projects one course think time finish pretty exhausted want write articles want write please write write course talk lot want let us know think whether incorporate syllabus	1
plans write articles medium related zoomcamp articles teachers students	alexey add comment least workplace airflow instance set us data scientist would go instance airflow managed data engineers devops engineers would use never needed actually run docker compose moment multiple dags orchestration tool quite useful would personally maybe go luigi favorite one need think people well since people know luigi many like maybe airflow would actually better choice consider sejal would also like add question docker compose used docker compose airflow style use official setup seeing first time well 's unnecessarily complicated especially xcom variables forth generally docker compose acts wrapper docker containers production workflows docker containers used integration ci/cd pipelines wherever 're going deploy docker compose docker compose part helpful development environments case let 's say mock version airflow environment production want test dags local development environment use docker compose make things easier running simple commands like ‘ docker compose ’ would make fine instead running ‘ docker run container name etc whatever want ’ terms convenience terms usage ’ generally used development environments production alexey think first part question “ close usual data engineer role workflows ” workflows see work complex steps probably also case need start something already – would n't call complex pipeline – three four steps already something eventually might grow bigger bigger add steps ankush would also like add 're cloud maybe use airflow deploy docker maybe actually use service provided cloud provider one similar airflow think airflow would used internally like cloud composite google cloud platform would using terraform sejal yeah used aws step functions aws setup step functions combination lambda place airflow ankush said also something wanted say really depends production self-hosted cluster kubernetes wherever want use pure airflow version company could kind setup use native airflow cluster could cloud-based setup convenient option use managed services cloud composer aws also version called managed airflow	0
